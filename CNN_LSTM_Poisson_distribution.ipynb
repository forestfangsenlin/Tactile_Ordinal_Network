{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import keras\n",
    "from confusion_matrix import CM\n",
    "from numpy.lib.scimath import log\n",
    "from scipy.special import binom\n",
    "import operator as op\n",
    "from functools import reduce\n",
    "import numpy as np\n",
    "import math \n",
    "from scipy.special import factorial\n",
    "from scipy.stats import nbinom\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras import models, optimizers, layers\n",
    "from keras import backend as K\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Layer, Dense, Embedding,TimeDistributed,Conv2D,Input, Flatten,Reshape ,MaxPooling2D, Dropout,Activation, LSTM, Bidirectional, Lambda\n",
    "from keras.callbacks import ModelCheckpoint,LearningRateScheduler\n",
    "from keras import callbacks\n",
    "from keras import initializers, layers\n",
    "\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from functools import partial\n",
    "from bayes_opt import BayesianOptimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = np.load('/home/yi/forest/hardness/np/pressure_1.npy')\n",
    "liju = np.load('/home/yi/forest/hardness/np/torque_1.npy')\n",
    "finger1 = images[:,::6,:24]\n",
    "finger2 = images[:,::6,24:48]\n",
    "finger3 = images[:,::6,48:]\n",
    "time_steps = 12\n",
    "fg1 = finger1[:,:].reshape((780, time_steps, 8, 3))\n",
    "fg2 = finger2[:,:].reshape((780, time_steps, 8, 3))\n",
    "fg3 = finger3[:,:].reshape((780, time_steps, 8, 3))\n",
    "fgall = np.concatenate((fg1, fg3, fg2), axis=-1)\n",
    "\n",
    "liju_bet = liju[:,::6,:]\n",
    "\n",
    "liju_arr = np.concatenate((liju_bet, liju_bet, liju_bet), axis=-1)\n",
    "liju_exp = np.expand_dims(liju_arr, axis=-2)\n",
    "X = np.concatenate((fgall, liju_exp), axis=-2)\n",
    "X = np.expand_dims(X, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cats_nums = 13\n",
    "y_ohot = np.repeat(np.arange(cats_nums), 60)\n",
    "np.random.seed(42)\n",
    "shuffle_indices = np.random.permutation(np.arange(len(y_ohot)))\n",
    "\n",
    "X = X[shuffle_indices]\n",
    "y = y_ohot[shuffle_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "generalize one-hot encoding (ground truth) using Poisson distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    return (np.exp(x, dtype=float) / np.sum(np.exp(x, dtype=float), axis=0, dtype=float)).astype(float)\n",
    "\n",
    "def Poss_true(y,e):\n",
    "    \n",
    "    y_po = np.zeros((len(y),13))\n",
    "    \n",
    "    k = np.arange(1,14,1)\n",
    "\n",
    "    \n",
    "    for i in range(len(y)):\n",
    "        n = np.sqrt((y[i]+1)*(y[i]+2))\n",
    "        y_po[i] = k*log(n) - n - log(factorial(k))\n",
    "        \n",
    "        y_po[i] = y_po[i]\n",
    "        \n",
    "        y_po[i] = softmax(y_po[i]/e)\n",
    "    \n",
    "    return y_po"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(dr, es):\n",
    "    \n",
    "    K.set_learning_phase(1)\n",
    "    acc_per_fold = []\n",
    "    loss_per_fold = []\n",
    "    train_accur_per_fold = []\n",
    "    test_accur_per_fold = []\n",
    "    train_loss_per_fold = []\n",
    "    test_loss_per_fold = []\n",
    "    cm_data_per_fold = []\n",
    "    pred_per_fold = []\n",
    "    y_test_per_fold = []\n",
    "\n",
    "    fold_no = 1\n",
    "    es = int(es)\n",
    "    class MyLoss(Layer):\n",
    "        def __init__(self, var1):\n",
    "            super(MyLoss, self).__init__()\n",
    "            self.var1 = K.variable(var1) \n",
    "\n",
    "\n",
    "        def get_vars(self):\n",
    "            return self.var1\n",
    "\n",
    "        def custom_loss(self, y_true, y_pred):\n",
    "            return self.var1 * K.mean(K.square(y_true-y_pred))\n",
    "\n",
    "        def call(self, y_true, y_pred):\n",
    "            self.add_loss(self.custom_loss(y_true, y_pred))\n",
    "            return y_pred\n",
    "\n",
    "    for train, test in kfold.split(X, y):\n",
    "        model1 = Sequential()\n",
    "\n",
    "        model1.add(layers.Conv2D(filters=32, kernel_size=6,strides=1, padding='same', activation='relu', name='cl_conv1', \n",
    "                  input_shape=[9, 9, 1]))\n",
    "\n",
    "        model1.add(layers.BatchNormalization())\n",
    "        model1.add(layers.Activation('relu'))\n",
    "\n",
    "        model1.add(MaxPooling2D(pool_size=2,strides=2, padding='same', name='cl_maxpool1'))\n",
    "\n",
    "        model1.add(layers.Conv2D(filters=64, kernel_size=6,strides=1, padding='same', activation='relu', name='cl_conv2'))\n",
    "        model1.add(layers.BatchNormalization())\n",
    "        model1.add(layers.Activation('relu'))\n",
    "\n",
    "        model1.add(layers.MaxPooling2D(pool_size=2,strides=2, padding='same', name='cl_maxpool2'))\n",
    "        model1.add(Flatten())\n",
    "   \n",
    "\n",
    "        model2 = Sequential()\n",
    "        model2.add(TimeDistributed(model1,input_shape = (12,9,9,1)))\n",
    "\n",
    "        model2.add(LSTM(units=100, name=\"lstm_out\"))\n",
    "       \n",
    "        model2.add(Dense(units=13,  name=\"Dense\", activation='softmax'))\n",
    "        \n",
    "       \n",
    "        alpha = K.variable(0.2)\n",
    "        \n",
    "        class NewCallback(keras.callbacks.Callback):\n",
    "            def __init__(self, alpha):\n",
    "                self.alpha = alpha       \n",
    "            def on_epoch_end(self, epoch, logs={}):\n",
    "                K.set_value(self.alpha, K.get_value(self.alpha)**dr)#0,99\n",
    "        \n",
    "        def custom_loss(alpha):\n",
    "            \n",
    "            def loss(y_true, y_pred,**kwargs):\n",
    "\n",
    "            #weights = tf.constant(np.arange(0,13,1), dtype=tf.float32)\n",
    "            #y_class = K.argmax(y_true, axis=1)\n",
    "            #w = tf.gather(weights, y_class)\n",
    "                y_true = y_true / alpha\n",
    "                y_true = K.softmax(y_true)\n",
    "                return keras.losses.categorical_crossentropy(y_true, y_pred)\n",
    "                \n",
    "            return loss\n",
    "        \n",
    "        early_stop = callbacks.EarlyStopping(monitor='val_accuracy', mode='max',patience=es)\n",
    "        reduce_lr = ReduceLROnPlateau(monitor='val_acc', factor=0.70434, patience=10, mode='max')\n",
    "        model2.compile(loss=custom_loss(alpha),optimizer=optimizers.RMSprop(lr=0.001), metrics = [\"accuracy\"])#QWK loss\n",
    "        tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "        y_true = to_categorical(y[train], cats_nums)\n",
    "        \n",
    "               \n",
    "        y_tr = Poss_true(y[train],0.2)\n",
    "        y_ts = Poss_true(y[test],0.2)\n",
    "\n",
    "        history = model2.fit(X[train], y_tr, batch_size=30, nb_epoch=100,verbose=1,validation_data=(X[test], y_ts), callbacks=[reduce_lr, early_stop])\n",
    "\n",
    "        train_accur_per_fold.append(history.history[\"acc\"])\n",
    "        test_accur_per_fold.append(history.history[\"val_acc\"])\n",
    "        train_loss_per_fold.append(history.history[\"loss\"])\n",
    "        test_loss_per_fold.append(history.history[\"val_loss\"])\n",
    "\n",
    "        scores = model2.evaluate(X[test], y_ts, verbose=1)\n",
    "        print(f'Score for fold {fold_no}: {model2.metrics_names[0]} of {scores[0]}; {model2.metrics_names[1]} of {scores[1]*100}%')\n",
    "        acc_per_fold.append(scores[1] * 100)\n",
    "        loss_per_fold.append(scores[0])\n",
    "\n",
    "        pred = model2.predict(X[test])\n",
    "        pred_per_fold.append(pred)\n",
    "        y_test_per_fold.append(y[test])\n",
    "        cm_data = confusion_matrix(y_ts.argmax(axis=1), pred.argmax(axis=1))\n",
    "        cm_data_per_fold.append(cm_data)\n",
    "        fold_no = fold_no + 1\n",
    "\n",
    "    train_acc = np.mean(train_accur_per_fold, axis = 0)\n",
    "    test_acc = np.mean(test_accur_per_fold, axis = 0)\n",
    "    train_loss = np.mean(train_loss_per_fold, axis = 0)\n",
    "    test_loss = np.mean(test_loss_per_fold, axis = 0)\n",
    "    cm_data = np.sum(cm_data_per_fold, axis=0)\n",
    "\n",
    "    acc = np.mean(np.array(acc_per_fold))\n",
    "    \n",
    "\n",
    "    name = \"fa_0.70434_pa_10_dr_\" + str(dr)+ \"es_\" + str(es)\n",
    "    train_test_acc_loss = np.concatenate((np.expand_dims(train_acc, axis=-1), \n",
    "                    np.expand_dims(test_acc, axis=-1), \n",
    "                    np.expand_dims(train_loss, axis=-1),\n",
    "                    np.expand_dims(train_loss, axis=-1)), axis=-1)\n",
    "\n",
    "    path = \"/home/yi/forest/hardness/paper_code/Acc/result1/data_shape1/\"\n",
    "    np.save(path + 'pred/'+name + '.npy', np.array(pred_per_fold))\n",
    "    np.save(path + 'y_test/'+name + '.npy', np.array(y_test_per_fold))\n",
    "    np.save(path + 'train_test_acc_loss/'+name + '.npy', train_test_acc_loss) \n",
    "    np.save(path + 'cm_data/'+name + '.npy', cm_data)\n",
    "    np.save(path + 'final_acc/'+name + '.npy', np.array(acc_per_fold))\n",
    "    CM(cm_data, name)\n",
    "    \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2340"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "780*3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   |    dr     |    es     |\n",
      "-------------------------------------------------\n",
      "WARNING:tensorflow:From /home/yi/anaconda3/envs/tf/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:138: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/yi/anaconda3/envs/tf/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:492: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/yi/anaconda3/envs/tf/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3630: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/yi/anaconda3/envs/tf/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3458: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/yi/anaconda3/envs/tf/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1259: calling reduce_prod_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From /home/yi/anaconda3/envs/tf/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1242: calling reduce_sum_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From /home/yi/anaconda3/envs/tf/lib/python3.6/site-packages/keras/optimizers.py:711: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/yi/anaconda3/envs/tf/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:2884: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "Train on 702 samples, validate on 78 samples\n",
      "Epoch 1/100\n",
      "702/702 [==============================] - 2s 3ms/step - loss: 2.4779 - acc: 0.2735 - val_loss: 2.4714 - val_acc: 0.3590\n",
      "Epoch 2/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.3133 - acc: 0.5499 - val_loss: 2.2756 - val_acc: 0.6923\n",
      "Epoch 3/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2476 - acc: 0.7379 - val_loss: 2.2540 - val_acc: 0.7179\n",
      "Epoch 4/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2140 - acc: 0.8533 - val_loss: 2.2494 - val_acc: 0.7051\n",
      "Epoch 5/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2073 - acc: 0.8761 - val_loss: 2.2372 - val_acc: 0.7949\n",
      "Epoch 6/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1941 - acc: 0.9046 - val_loss: 2.2124 - val_acc: 0.7821\n",
      "Epoch 7/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1887 - acc: 0.9302 - val_loss: 2.1897 - val_acc: 0.9103\n",
      "Epoch 8/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1919 - acc: 0.9501 - val_loss: 2.1911 - val_acc: 0.8974\n",
      "Epoch 9/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1792 - acc: 0.9729 - val_loss: 2.1879 - val_acc: 0.9359\n",
      "Epoch 10/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1784 - acc: 0.9658 - val_loss: 2.1909 - val_acc: 0.9103\n",
      "Epoch 11/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1763 - acc: 0.9772 - val_loss: 2.1850 - val_acc: 0.9359\n",
      "Epoch 12/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1833 - acc: 0.9573 - val_loss: 2.1857 - val_acc: 0.9231\n",
      "Epoch 13/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1751 - acc: 0.9786 - val_loss: 2.1924 - val_acc: 0.8846\n",
      "Epoch 14/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1731 - acc: 0.9915 - val_loss: 2.1874 - val_acc: 0.8974\n",
      "Epoch 15/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1724 - acc: 0.9872 - val_loss: 2.1877 - val_acc: 0.9103\n",
      "Epoch 16/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1750 - acc: 0.9829 - val_loss: 2.1833 - val_acc: 0.9487\n",
      "Epoch 17/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1718 - acc: 0.9915 - val_loss: 2.1826 - val_acc: 0.9231\n",
      "Epoch 18/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1703 - acc: 0.9943 - val_loss: 2.1805 - val_acc: 0.9487\n",
      "Epoch 19/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1719 - acc: 0.9786 - val_loss: 2.1806 - val_acc: 0.9487\n",
      "Epoch 20/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1701 - acc: 0.9957 - val_loss: 2.1823 - val_acc: 0.9359\n",
      "Epoch 21/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1702 - acc: 0.9915 - val_loss: 2.1792 - val_acc: 0.9487\n",
      "Epoch 22/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1724 - acc: 0.9815 - val_loss: 2.1817 - val_acc: 0.9487\n",
      "Epoch 23/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1689 - acc: 0.9929 - val_loss: 2.1786 - val_acc: 0.9487\n",
      "Epoch 24/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1709 - acc: 0.9886 - val_loss: 2.1808 - val_acc: 0.9487\n",
      "Epoch 25/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1694 - acc: 0.9886 - val_loss: 2.1800 - val_acc: 0.9487\n",
      "Epoch 26/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1687 - acc: 0.9915 - val_loss: 2.1832 - val_acc: 0.9487\n",
      "Epoch 27/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1685 - acc: 0.9972 - val_loss: 2.1879 - val_acc: 0.9359\n",
      "Epoch 28/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1677 - acc: 0.9972 - val_loss: 2.1799 - val_acc: 0.9487\n",
      "Epoch 29/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1676 - acc: 0.9986 - val_loss: 2.1818 - val_acc: 0.9615\n",
      "Epoch 30/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1677 - acc: 0.9943 - val_loss: 2.1810 - val_acc: 0.9487\n",
      "Epoch 31/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1674 - acc: 0.9957 - val_loss: 2.1799 - val_acc: 0.9359\n",
      "Epoch 32/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1678 - acc: 0.9943 - val_loss: 2.1814 - val_acc: 0.9615\n",
      "Epoch 33/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1671 - acc: 0.9986 - val_loss: 2.1797 - val_acc: 0.9615\n",
      "Epoch 34/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1670 - acc: 0.9986 - val_loss: 2.1790 - val_acc: 0.9487\n",
      "Epoch 35/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1672 - acc: 1.0000 - val_loss: 2.1787 - val_acc: 0.9615\n",
      "Epoch 36/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1673 - acc: 0.9972 - val_loss: 2.1777 - val_acc: 0.9615\n",
      "Epoch 37/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1668 - acc: 1.0000 - val_loss: 2.1770 - val_acc: 0.9487\n",
      "Epoch 38/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1668 - acc: 0.9943 - val_loss: 2.1788 - val_acc: 0.9487\n",
      "Epoch 39/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1669 - acc: 0.9986 - val_loss: 2.1782 - val_acc: 0.9615\n",
      "Epoch 40/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1669 - acc: 1.0000 - val_loss: 2.1780 - val_acc: 0.9744\n",
      "Epoch 41/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1671 - acc: 0.9972 - val_loss: 2.1794 - val_acc: 0.9487\n",
      "Epoch 42/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1666 - acc: 1.0000 - val_loss: 2.1774 - val_acc: 0.9487\n",
      "Epoch 43/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1667 - acc: 1.0000 - val_loss: 2.1812 - val_acc: 0.9615\n",
      "Epoch 44/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1668 - acc: 1.0000 - val_loss: 2.1778 - val_acc: 0.9615\n",
      "Epoch 45/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1668 - acc: 0.9986 - val_loss: 2.1775 - val_acc: 0.9487\n",
      "Epoch 46/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1768 - val_acc: 0.9487\n",
      "Epoch 47/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1669 - acc: 0.9929 - val_loss: 2.1767 - val_acc: 0.9487\n",
      "Epoch 48/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1667 - acc: 1.0000 - val_loss: 2.1803 - val_acc: 0.9487\n",
      "Epoch 49/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1801 - val_acc: 0.9487\n",
      "Epoch 50/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1784 - val_acc: 0.9487\n",
      "Epoch 51/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1668 - acc: 0.9986 - val_loss: 2.1749 - val_acc: 0.9744\n",
      "Epoch 52/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1771 - val_acc: 0.9487\n",
      "Epoch 53/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1763 - val_acc: 0.9487\n",
      "Epoch 54/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1771 - val_acc: 0.9487\n",
      "Epoch 55/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1805 - val_acc: 0.9487\n",
      "Epoch 56/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1768 - val_acc: 0.9615\n",
      "Epoch 57/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1769 - val_acc: 0.9487\n",
      "Epoch 58/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1765 - val_acc: 0.9487\n",
      "Epoch 59/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1763 - val_acc: 0.9487\n",
      "Epoch 60/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1772 - val_acc: 0.9487\n",
      "Epoch 61/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1809 - val_acc: 0.9487\n",
      "Epoch 62/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1793 - val_acc: 0.9487\n",
      "Epoch 63/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1775 - val_acc: 0.9487\n",
      "Epoch 64/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1772 - val_acc: 0.9487\n",
      "Epoch 65/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1765 - val_acc: 0.9487\n",
      "Epoch 66/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1775 - val_acc: 0.9487\n",
      "Epoch 67/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1779 - val_acc: 0.9487\n",
      "Epoch 68/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1772 - val_acc: 0.9487\n",
      "Epoch 69/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1774 - val_acc: 0.9487\n",
      "Epoch 70/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 0.9986 - val_loss: 2.1769 - val_acc: 0.9487\n",
      "Epoch 71/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1769 - val_acc: 0.9487\n",
      "Epoch 72/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1775 - val_acc: 0.9487\n",
      "Epoch 73/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1772 - val_acc: 0.9487\n",
      "Epoch 74/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1774 - val_acc: 0.9487\n",
      "Epoch 75/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1770 - val_acc: 0.9487\n",
      "Epoch 76/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1770 - val_acc: 0.9487\n",
      "Epoch 77/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1766 - val_acc: 0.9487\n",
      "Epoch 78/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1772 - val_acc: 0.9487\n",
      "Epoch 79/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1765 - val_acc: 0.9487\n",
      "Epoch 80/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1768 - val_acc: 0.9487\n",
      "Epoch 81/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1761 - val_acc: 0.9487\n",
      "Epoch 82/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1770 - val_acc: 0.9487\n",
      "Epoch 83/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1762 - val_acc: 0.9487\n",
      "Epoch 84/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1771 - val_acc: 0.9487\n",
      "Epoch 85/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1769 - val_acc: 0.9487\n",
      "Epoch 86/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1772 - val_acc: 0.9487\n",
      "Epoch 87/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1766 - val_acc: 0.9487\n",
      "Epoch 88/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1772 - val_acc: 0.9487\n",
      "Epoch 89/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1764 - val_acc: 0.9487\n",
      "Epoch 90/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1764 - val_acc: 0.9487\n",
      "Epoch 91/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1774 - val_acc: 0.9487\n",
      "Epoch 92/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1767 - val_acc: 0.9487\n",
      "Epoch 93/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1774 - val_acc: 0.9487\n",
      "Epoch 94/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1765 - val_acc: 0.9487\n",
      "Epoch 95/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1766 - val_acc: 0.9487\n",
      "Epoch 96/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1769 - val_acc: 0.9487\n",
      "Epoch 97/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1768 - val_acc: 0.9487\n",
      "Epoch 98/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1767 - val_acc: 0.9487\n",
      "Epoch 99/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1763 - val_acc: 0.9487\n",
      "Epoch 100/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1768 - val_acc: 0.9487\n",
      "78/78 [==============================] - 0s 316us/step\n",
      "Score for fold 1: loss of 2.1769898121173563; acc of 94.87179487179486%\n",
      "Train on 702 samples, validate on 78 samples\n",
      "Epoch 1/100\n",
      "702/702 [==============================] - 2s 2ms/step - loss: 2.5071 - acc: 0.2521 - val_loss: 2.3963 - val_acc: 0.4615\n",
      "Epoch 2/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.3224 - acc: 0.5271 - val_loss: 2.3579 - val_acc: 0.5385\n",
      "Epoch 3/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2515 - acc: 0.7094 - val_loss: 2.3128 - val_acc: 0.6154\n",
      "Epoch 4/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2253 - acc: 0.7920 - val_loss: 2.2194 - val_acc: 0.8718\n",
      "Epoch 5/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2058 - acc: 0.8832 - val_loss: 2.2083 - val_acc: 0.8333\n",
      "Epoch 6/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1894 - acc: 0.9387 - val_loss: 2.2022 - val_acc: 0.7949\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1900 - acc: 0.9387 - val_loss: 2.1903 - val_acc: 0.9615\n",
      "Epoch 8/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1844 - acc: 0.9601 - val_loss: 2.1846 - val_acc: 0.9487\n",
      "Epoch 9/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1840 - acc: 0.9558 - val_loss: 2.1844 - val_acc: 0.8974\n",
      "Epoch 10/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1780 - acc: 0.9758 - val_loss: 2.1912 - val_acc: 0.8462\n",
      "Epoch 11/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1771 - acc: 0.9672 - val_loss: 2.1909 - val_acc: 0.8974\n",
      "Epoch 12/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1794 - acc: 0.9658 - val_loss: 2.1766 - val_acc: 0.9872\n",
      "Epoch 13/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1722 - acc: 0.9843 - val_loss: 2.1763 - val_acc: 0.9615\n",
      "Epoch 14/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1743 - acc: 0.9758 - val_loss: 2.1756 - val_acc: 0.9872\n",
      "Epoch 15/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1709 - acc: 0.9943 - val_loss: 2.1735 - val_acc: 0.9744\n",
      "Epoch 16/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1763 - acc: 0.9829 - val_loss: 2.1745 - val_acc: 0.9744\n",
      "Epoch 17/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1705 - acc: 0.9772 - val_loss: 2.1826 - val_acc: 0.9615\n",
      "Epoch 18/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1697 - acc: 0.9943 - val_loss: 2.1725 - val_acc: 0.9872\n",
      "Epoch 19/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1730 - acc: 0.9886 - val_loss: 2.1735 - val_acc: 0.9487\n",
      "Epoch 20/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1696 - acc: 0.9929 - val_loss: 2.1832 - val_acc: 0.9615\n",
      "Epoch 21/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1704 - acc: 0.9900 - val_loss: 2.1860 - val_acc: 0.9231\n",
      "Epoch 22/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1698 - acc: 0.9943 - val_loss: 2.1776 - val_acc: 0.9744\n",
      "Epoch 23/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1698 - acc: 0.9972 - val_loss: 2.1705 - val_acc: 1.0000\n",
      "Epoch 24/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1686 - acc: 0.9915 - val_loss: 2.1748 - val_acc: 0.9231\n",
      "Epoch 25/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1716 - acc: 0.9886 - val_loss: 2.1718 - val_acc: 1.0000\n",
      "Epoch 26/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1685 - acc: 0.9972 - val_loss: 2.2170 - val_acc: 0.8718\n",
      "Epoch 27/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1709 - acc: 0.9900 - val_loss: 2.1710 - val_acc: 0.9872\n",
      "Epoch 28/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1691 - acc: 0.9929 - val_loss: 2.1712 - val_acc: 1.0000\n",
      "Epoch 29/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1694 - acc: 0.9900 - val_loss: 2.1688 - val_acc: 1.0000\n",
      "Epoch 30/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1676 - acc: 0.9957 - val_loss: 2.1698 - val_acc: 1.0000\n",
      "Epoch 31/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1691 - acc: 0.9972 - val_loss: 2.1735 - val_acc: 0.9615\n",
      "Epoch 32/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1692 - acc: 0.9900 - val_loss: 2.1776 - val_acc: 0.9615\n",
      "Epoch 33/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1686 - acc: 0.9957 - val_loss: 2.1807 - val_acc: 0.9744\n",
      "Epoch 34/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1672 - acc: 0.9986 - val_loss: 2.1755 - val_acc: 0.9744\n",
      "Epoch 35/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1672 - acc: 0.9986 - val_loss: 2.1690 - val_acc: 0.9872\n",
      "Epoch 36/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1671 - acc: 0.9957 - val_loss: 2.1811 - val_acc: 0.9872\n",
      "Epoch 37/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1670 - acc: 1.0000 - val_loss: 2.1695 - val_acc: 0.9744\n",
      "Epoch 38/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1669 - acc: 1.0000 - val_loss: 2.1693 - val_acc: 0.9872\n",
      "Epoch 39/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1668 - acc: 0.9986 - val_loss: 2.1757 - val_acc: 0.9872\n",
      "Epoch 40/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1667 - acc: 1.0000 - val_loss: 2.1681 - val_acc: 1.0000\n",
      "Epoch 41/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1667 - acc: 1.0000 - val_loss: 2.1699 - val_acc: 0.9872\n",
      "Epoch 42/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1679 - acc: 0.9972 - val_loss: 2.1699 - val_acc: 0.9872\n",
      "Epoch 43/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1691 - val_acc: 1.0000\n",
      "Epoch 44/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1666 - acc: 1.0000 - val_loss: 2.1685 - val_acc: 1.0000\n",
      "Epoch 45/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1677 - val_acc: 1.0000\n",
      "Epoch 46/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1681 - val_acc: 1.0000\n",
      "Epoch 47/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1681 - val_acc: 1.0000\n",
      "Epoch 48/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1686 - val_acc: 1.0000\n",
      "Epoch 49/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 0.9986 - val_loss: 2.1708 - val_acc: 0.9872\n",
      "Epoch 50/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1677 - val_acc: 1.0000\n",
      "Epoch 51/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1681 - val_acc: 1.0000\n",
      "Epoch 52/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1680 - val_acc: 1.0000\n",
      "Epoch 53/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1696 - val_acc: 0.9744\n",
      "Epoch 54/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1677 - val_acc: 1.0000\n",
      "Epoch 55/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1681 - val_acc: 1.0000\n",
      "Epoch 56/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1681 - val_acc: 1.0000\n",
      "Epoch 57/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1677 - val_acc: 1.0000\n",
      "Epoch 58/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1691 - val_acc: 0.9615\n",
      "Epoch 59/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1679 - val_acc: 1.0000\n",
      "Epoch 60/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1680 - val_acc: 1.0000\n",
      "Epoch 61/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1679 - val_acc: 1.0000\n",
      "Epoch 62/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1681 - val_acc: 1.0000\n",
      "Epoch 63/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1674 - val_acc: 1.0000\n",
      "Epoch 64/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1679 - val_acc: 0.9872\n",
      "Epoch 65/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1677 - val_acc: 1.0000\n",
      "Epoch 66/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1674 - val_acc: 1.0000\n",
      "Epoch 67/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1675 - val_acc: 1.0000\n",
      "Epoch 68/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1675 - val_acc: 1.0000\n",
      "Epoch 69/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1677 - val_acc: 1.0000\n",
      "Epoch 70/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1675 - val_acc: 1.0000\n",
      "Epoch 71/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1676 - val_acc: 1.0000\n",
      "Epoch 72/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1676 - val_acc: 1.0000\n",
      "Epoch 73/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1678 - val_acc: 1.0000\n",
      "Epoch 74/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1677 - val_acc: 1.0000\n",
      "Epoch 75/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1675 - val_acc: 1.0000\n",
      "Epoch 76/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1675 - val_acc: 1.0000\n",
      "Epoch 77/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1676 - val_acc: 1.0000\n",
      "Epoch 78/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1676 - val_acc: 1.0000\n",
      "Epoch 79/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1675 - val_acc: 1.0000\n",
      "Epoch 80/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1675 - val_acc: 1.0000\n",
      "Epoch 81/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1674 - val_acc: 1.0000\n",
      "Epoch 82/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1676 - val_acc: 1.0000\n",
      "Epoch 83/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1675 - val_acc: 1.0000\n",
      "Epoch 84/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1676 - val_acc: 1.0000\n",
      "Epoch 85/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1674 - val_acc: 1.0000\n",
      "Epoch 86/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1675 - val_acc: 1.0000\n",
      "Epoch 87/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1675 - val_acc: 1.0000\n",
      "Epoch 88/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1675 - val_acc: 1.0000\n",
      "Epoch 89/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1674 - val_acc: 1.0000\n",
      "Epoch 90/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1673 - val_acc: 1.0000\n",
      "Epoch 91/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1675 - val_acc: 1.0000\n",
      "Epoch 92/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1675 - val_acc: 1.0000\n",
      "Epoch 93/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1674 - val_acc: 1.0000\n",
      "Epoch 94/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1675 - val_acc: 1.0000\n",
      "Epoch 95/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1673 - val_acc: 1.0000\n",
      "Epoch 96/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1675 - val_acc: 1.0000\n",
      "Epoch 97/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1675 - val_acc: 1.0000\n",
      "Epoch 98/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1674 - val_acc: 1.0000\n",
      "Epoch 99/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1675 - val_acc: 1.0000\n",
      "Epoch 100/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1674 - val_acc: 1.0000\n",
      "78/78 [==============================] - 0s 342us/step\n",
      "Score for fold 2: loss of 2.167462715735802; acc of 100.0%\n",
      "Train on 702 samples, validate on 78 samples\n",
      "Epoch 1/100\n",
      "702/702 [==============================] - 2s 3ms/step - loss: 2.5055 - acc: 0.2536 - val_loss: 2.3414 - val_acc: 0.4231\n",
      "Epoch 2/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.3210 - acc: 0.5057 - val_loss: 2.3047 - val_acc: 0.6154\n",
      "Epoch 3/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2504 - acc: 0.6909 - val_loss: 2.2603 - val_acc: 0.6923\n",
      "Epoch 4/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2184 - acc: 0.8333 - val_loss: 2.2171 - val_acc: 0.8333\n",
      "Epoch 5/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2172 - acc: 0.8148 - val_loss: 2.2031 - val_acc: 0.8718\n",
      "Epoch 6/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1982 - acc: 0.8946 - val_loss: 2.1979 - val_acc: 0.8846\n",
      "Epoch 7/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1959 - acc: 0.9060 - val_loss: 2.1923 - val_acc: 0.9359\n",
      "Epoch 8/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1880 - acc: 0.9459 - val_loss: 2.2169 - val_acc: 0.9231\n",
      "Epoch 9/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1798 - acc: 0.9672 - val_loss: 2.2208 - val_acc: 0.7949\n",
      "Epoch 10/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1814 - acc: 0.9544 - val_loss: 2.2218 - val_acc: 0.8718\n",
      "Epoch 11/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1763 - acc: 0.9715 - val_loss: 2.1931 - val_acc: 0.9231\n",
      "Epoch 12/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1764 - acc: 0.9758 - val_loss: 2.1852 - val_acc: 0.8846\n",
      "Epoch 13/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1748 - acc: 0.9715 - val_loss: 2.1876 - val_acc: 0.9103\n",
      "Epoch 14/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1751 - acc: 0.9772 - val_loss: 2.1863 - val_acc: 0.9487\n",
      "Epoch 15/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1727 - acc: 0.9843 - val_loss: 2.1896 - val_acc: 0.9359\n",
      "Epoch 16/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1743 - acc: 0.9829 - val_loss: 2.1845 - val_acc: 0.9487\n",
      "Epoch 17/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1712 - acc: 0.9943 - val_loss: 2.1828 - val_acc: 0.9872\n",
      "Epoch 18/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1722 - acc: 0.9943 - val_loss: 2.1784 - val_acc: 0.9744\n",
      "Epoch 19/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1750 - acc: 0.9786 - val_loss: 2.1766 - val_acc: 0.9872\n",
      "Epoch 20/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1696 - acc: 0.9957 - val_loss: 2.1825 - val_acc: 1.0000\n",
      "Epoch 21/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1737 - acc: 0.9815 - val_loss: 2.1811 - val_acc: 0.9744\n",
      "Epoch 22/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1713 - acc: 0.9915 - val_loss: 2.1796 - val_acc: 0.9231\n",
      "Epoch 23/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1709 - acc: 0.9858 - val_loss: 2.1817 - val_acc: 1.0000\n",
      "Epoch 24/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1693 - acc: 0.9915 - val_loss: 2.1746 - val_acc: 0.9872\n",
      "Epoch 25/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1688 - acc: 0.9957 - val_loss: 2.1852 - val_acc: 0.9744\n",
      "Epoch 26/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1701 - acc: 0.9872 - val_loss: 2.1726 - val_acc: 0.9615\n",
      "Epoch 27/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1682 - acc: 0.9957 - val_loss: 2.1818 - val_acc: 0.9872\n",
      "Epoch 28/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1713 - acc: 0.9929 - val_loss: 2.1844 - val_acc: 0.9744\n",
      "Epoch 29/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1684 - acc: 0.9972 - val_loss: 2.1731 - val_acc: 1.0000\n",
      "Epoch 30/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1697 - acc: 0.9886 - val_loss: 2.1826 - val_acc: 0.9744\n",
      "Epoch 31/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1679 - acc: 0.9986 - val_loss: 2.1786 - val_acc: 0.9872\n",
      "Epoch 32/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1674 - acc: 0.9972 - val_loss: 2.1807 - val_acc: 0.9872\n",
      "Epoch 33/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1673 - acc: 0.9986 - val_loss: 2.1745 - val_acc: 0.9744\n",
      "Epoch 34/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1679 - acc: 0.9972 - val_loss: 2.1796 - val_acc: 0.9872\n",
      "Epoch 35/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1672 - acc: 0.9986 - val_loss: 2.1767 - val_acc: 0.9744\n",
      "Epoch 36/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1671 - acc: 1.0000 - val_loss: 2.1857 - val_acc: 1.0000\n",
      "Epoch 37/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1670 - acc: 0.9986 - val_loss: 2.1839 - val_acc: 0.9744\n",
      "Epoch 38/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1683 - acc: 0.9957 - val_loss: 2.1781 - val_acc: 1.0000\n",
      "Epoch 39/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1668 - acc: 0.9986 - val_loss: 2.1797 - val_acc: 1.0000\n",
      "Epoch 40/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1667 - acc: 1.0000 - val_loss: 2.1823 - val_acc: 0.9744\n",
      "Epoch 41/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1671 - acc: 1.0000 - val_loss: 2.1768 - val_acc: 0.9615\n",
      "Epoch 42/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1668 - acc: 0.9986 - val_loss: 2.1890 - val_acc: 0.9744\n",
      "Epoch 43/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1896 - val_acc: 0.9744\n",
      "Epoch 44/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1750 - val_acc: 1.0000\n",
      "Epoch 45/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1666 - acc: 1.0000 - val_loss: 2.1791 - val_acc: 0.9615\n",
      "Epoch 46/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1821 - val_acc: 1.0000\n",
      "Epoch 47/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1902 - val_acc: 0.9872\n",
      "Epoch 48/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1666 - acc: 1.0000 - val_loss: 2.1829 - val_acc: 1.0000\n",
      "Epoch 49/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1917 - val_acc: 0.9872\n",
      "Epoch 50/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1804 - val_acc: 0.9744\n",
      "Epoch 51/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1909 - val_acc: 0.9615\n",
      "Epoch 52/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1866 - val_acc: 0.9872\n",
      "Epoch 53/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1867 - val_acc: 0.9744\n",
      "Epoch 54/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 0.9986 - val_loss: 2.1812 - val_acc: 1.0000\n",
      "Epoch 55/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1866 - val_acc: 0.9744\n",
      "Epoch 56/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1882 - val_acc: 0.9744\n",
      "Epoch 57/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1868 - val_acc: 0.9744\n",
      "Epoch 58/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1797 - val_acc: 0.9872\n",
      "Epoch 59/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1822 - val_acc: 0.9872\n",
      "Epoch 60/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1849 - val_acc: 1.0000\n",
      "Epoch 61/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1869 - val_acc: 0.9872\n",
      "Epoch 62/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1873 - val_acc: 0.9872\n",
      "Epoch 63/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1842 - val_acc: 0.9872\n",
      "Epoch 64/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1829 - val_acc: 1.0000\n",
      "Epoch 65/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1811 - val_acc: 0.9872\n",
      "Epoch 66/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1820 - val_acc: 1.0000\n",
      "Epoch 67/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1844 - val_acc: 0.9872\n",
      "Epoch 68/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1820 - val_acc: 1.0000\n",
      "Epoch 69/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1829 - val_acc: 0.9872\n",
      "Epoch 70/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1838 - val_acc: 1.0000\n",
      "Epoch 71/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1815 - val_acc: 1.0000\n",
      "Epoch 72/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1826 - val_acc: 0.9872\n",
      "Epoch 73/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1834 - val_acc: 1.0000\n",
      "Epoch 74/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1797 - val_acc: 0.9872\n",
      "Epoch 75/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1838 - val_acc: 0.9872\n",
      "Epoch 76/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1846 - val_acc: 0.9872\n",
      "Epoch 77/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1826 - val_acc: 1.0000\n",
      "Epoch 78/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1817 - val_acc: 0.9872\n",
      "Epoch 79/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1816 - val_acc: 0.9872\n",
      "Epoch 80/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1813 - val_acc: 1.0000\n",
      "Epoch 81/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1812 - val_acc: 0.9872\n",
      "Epoch 82/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1826 - val_acc: 0.9872\n",
      "Epoch 83/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1809 - val_acc: 0.9872\n",
      "Epoch 84/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1816 - val_acc: 0.9872\n",
      "Epoch 85/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1802 - val_acc: 0.9872\n",
      "Epoch 86/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1802 - val_acc: 0.9872\n",
      "Epoch 87/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1837 - val_acc: 0.9872\n",
      "Epoch 88/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1835 - val_acc: 0.9872\n",
      "Epoch 89/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1837 - val_acc: 0.9872\n",
      "Epoch 90/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1838 - val_acc: 1.0000\n",
      "Epoch 91/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1813 - val_acc: 0.9872\n",
      "Epoch 92/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1815 - val_acc: 0.9872\n",
      "Epoch 93/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1819 - val_acc: 0.9872\n",
      "Epoch 94/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1820 - val_acc: 0.9872\n",
      "Epoch 95/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1823 - val_acc: 0.9872\n",
      "Epoch 96/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1815 - val_acc: 0.9872\n",
      "Epoch 97/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1813 - val_acc: 0.9872\n",
      "Epoch 98/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1816 - val_acc: 0.9872\n",
      "Epoch 99/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1823 - val_acc: 0.9872\n",
      "Epoch 100/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1822 - val_acc: 0.9872\n",
      "78/78 [==============================] - 0s 357us/step\n",
      "Score for fold 3: loss of 2.1815074651669235; acc of 98.71794871794873%\n",
      "Train on 702 samples, validate on 78 samples\n",
      "Epoch 1/100\n",
      "702/702 [==============================] - 2s 3ms/step - loss: 2.5064 - acc: 0.2692 - val_loss: 2.4208 - val_acc: 0.3462\n",
      "Epoch 2/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.3144 - acc: 0.5598 - val_loss: 2.2716 - val_acc: 0.6154\n",
      "Epoch 3/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2506 - acc: 0.7450 - val_loss: 2.2357 - val_acc: 0.7564\n",
      "Epoch 4/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2181 - acc: 0.8476 - val_loss: 2.2121 - val_acc: 0.8333\n",
      "Epoch 5/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2030 - acc: 0.8960 - val_loss: 2.2125 - val_acc: 0.8333\n",
      "Epoch 6/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1955 - acc: 0.8875 - val_loss: 2.2863 - val_acc: 0.7564\n",
      "Epoch 7/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1857 - acc: 0.9487 - val_loss: 2.2323 - val_acc: 0.7949\n",
      "Epoch 8/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1926 - acc: 0.9316 - val_loss: 2.2138 - val_acc: 0.8590\n",
      "Epoch 9/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1849 - acc: 0.9402 - val_loss: 2.1777 - val_acc: 0.9487\n",
      "Epoch 10/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1755 - acc: 0.9658 - val_loss: 2.2010 - val_acc: 0.9103\n",
      "Epoch 11/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1789 - acc: 0.9601 - val_loss: 2.1808 - val_acc: 0.9487\n",
      "Epoch 12/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1822 - acc: 0.9501 - val_loss: 2.1956 - val_acc: 0.9359\n",
      "Epoch 13/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1736 - acc: 0.9858 - val_loss: 2.1770 - val_acc: 0.9487\n",
      "Epoch 14/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1755 - acc: 0.9815 - val_loss: 2.2241 - val_acc: 0.8718\n",
      "Epoch 15/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1719 - acc: 0.9943 - val_loss: 2.1736 - val_acc: 0.9744\n",
      "Epoch 16/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1723 - acc: 0.9843 - val_loss: 2.1774 - val_acc: 0.9872\n",
      "Epoch 17/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1704 - acc: 0.9900 - val_loss: 2.1720 - val_acc: 1.0000\n",
      "Epoch 18/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1725 - acc: 0.9872 - val_loss: 2.1707 - val_acc: 1.0000\n",
      "Epoch 19/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1726 - acc: 0.9872 - val_loss: 2.2292 - val_acc: 0.8077\n",
      "Epoch 20/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1700 - acc: 0.9886 - val_loss: 2.1722 - val_acc: 0.9744\n",
      "Epoch 21/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1689 - acc: 0.9943 - val_loss: 2.1709 - val_acc: 0.9872\n",
      "Epoch 22/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1692 - acc: 0.9972 - val_loss: 2.1794 - val_acc: 0.9872\n",
      "Epoch 23/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1704 - acc: 0.9929 - val_loss: 2.1711 - val_acc: 1.0000\n",
      "Epoch 24/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1709 - acc: 0.9915 - val_loss: 2.1717 - val_acc: 0.9872\n",
      "Epoch 25/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1684 - acc: 0.9929 - val_loss: 2.1720 - val_acc: 0.9872\n",
      "Epoch 26/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1686 - acc: 0.9972 - val_loss: 2.1711 - val_acc: 0.9872\n",
      "Epoch 27/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1688 - acc: 0.9972 - val_loss: 2.1711 - val_acc: 0.9872\n",
      "Epoch 28/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1682 - acc: 0.9957 - val_loss: 2.1765 - val_acc: 0.9487\n",
      "Epoch 29/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1676 - acc: 0.9972 - val_loss: 2.1700 - val_acc: 0.9744\n",
      "Epoch 30/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1679 - acc: 0.9858 - val_loss: 2.1696 - val_acc: 0.9872\n",
      "Epoch 31/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1669 - acc: 1.0000 - val_loss: 2.1688 - val_acc: 0.9872\n",
      "Epoch 32/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1679 - acc: 0.9972 - val_loss: 2.1687 - val_acc: 0.9872\n",
      "Epoch 33/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1669 - acc: 1.0000 - val_loss: 2.1694 - val_acc: 0.9872\n",
      "Epoch 34/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1670 - acc: 1.0000 - val_loss: 2.1688 - val_acc: 1.0000\n",
      "Epoch 35/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1668 - acc: 1.0000 - val_loss: 2.1683 - val_acc: 1.0000\n",
      "Epoch 36/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1673 - acc: 0.9986 - val_loss: 2.1692 - val_acc: 0.9872\n",
      "Epoch 37/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1667 - acc: 1.0000 - val_loss: 2.1687 - val_acc: 1.0000\n",
      "Epoch 38/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1675 - acc: 0.9972 - val_loss: 2.1680 - val_acc: 1.0000\n",
      "Epoch 39/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1682 - val_acc: 0.9872\n",
      "Epoch 40/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1695 - val_acc: 1.0000\n",
      "Epoch 41/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1685 - val_acc: 1.0000\n",
      "Epoch 42/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1682 - val_acc: 0.9872\n",
      "Epoch 43/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1693 - val_acc: 0.9872\n",
      "Epoch 44/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1682 - val_acc: 0.9872\n",
      "Epoch 45/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1688 - val_acc: 0.9872\n",
      "Epoch 46/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1683 - val_acc: 0.9872\n",
      "Epoch 47/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1667 - acc: 0.9943 - val_loss: 2.1687 - val_acc: 0.9744\n",
      "Epoch 48/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1681 - val_acc: 0.9872\n",
      "Epoch 49/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1683 - val_acc: 0.9744\n",
      "Epoch 50/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1680 - val_acc: 1.0000\n",
      "Epoch 51/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1681 - val_acc: 0.9872\n",
      "Epoch 52/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1683 - val_acc: 0.9872\n",
      "Epoch 53/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1686 - val_acc: 0.9872\n",
      "Epoch 54/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1705 - val_acc: 0.9872\n",
      "Epoch 55/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1683 - val_acc: 0.9872\n",
      "Epoch 56/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1686 - val_acc: 0.9744\n",
      "Epoch 57/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1691 - val_acc: 0.9744\n",
      "Epoch 58/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1683 - val_acc: 0.9872\n",
      "Epoch 59/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1681 - val_acc: 0.9872\n",
      "Epoch 60/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1682 - val_acc: 0.9872\n",
      "Epoch 61/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1681 - val_acc: 0.9872\n",
      "Epoch 62/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1682 - val_acc: 1.0000\n",
      "Epoch 63/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1680 - val_acc: 1.0000\n",
      "Epoch 64/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1678 - val_acc: 1.0000\n",
      "Epoch 65/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1676 - val_acc: 0.9872\n",
      "Epoch 66/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1681 - val_acc: 1.0000\n",
      "Epoch 67/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1677 - val_acc: 1.0000\n",
      "Epoch 68/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1679 - val_acc: 1.0000\n",
      "Epoch 69/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1683 - val_acc: 0.9872\n",
      "Epoch 70/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1680 - val_acc: 1.0000\n",
      "Epoch 71/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1683 - val_acc: 0.9872\n",
      "Epoch 72/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1679 - val_acc: 1.0000\n",
      "Epoch 73/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1682 - val_acc: 0.9872\n",
      "Epoch 74/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1681 - val_acc: 0.9872\n",
      "Epoch 75/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1683 - val_acc: 0.9872\n",
      "Epoch 76/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1681 - val_acc: 0.9872\n",
      "Epoch 77/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1680 - val_acc: 1.0000\n",
      "Epoch 78/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1682 - val_acc: 0.9872\n",
      "Epoch 79/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1678 - val_acc: 0.9872\n",
      "Epoch 80/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1680 - val_acc: 0.9872\n",
      "Epoch 81/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1681 - val_acc: 0.9872\n",
      "Epoch 82/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1680 - val_acc: 1.0000\n",
      "Epoch 83/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1681 - val_acc: 0.9872\n",
      "Epoch 84/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1678 - val_acc: 1.0000\n",
      "Epoch 85/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1680 - val_acc: 0.9872\n",
      "Epoch 86/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1679 - val_acc: 1.0000\n",
      "Epoch 87/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1679 - val_acc: 1.0000\n",
      "Epoch 88/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1680 - val_acc: 0.9872\n",
      "Epoch 89/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1680 - val_acc: 1.0000\n",
      "Epoch 90/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1680 - val_acc: 0.9872\n",
      "Epoch 91/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1678 - val_acc: 1.0000\n",
      "Epoch 92/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1680 - val_acc: 0.9872\n",
      "Epoch 93/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1679 - val_acc: 1.0000\n",
      "Epoch 94/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1679 - val_acc: 1.0000\n",
      "Epoch 95/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1679 - val_acc: 1.0000\n",
      "Epoch 96/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1679 - val_acc: 0.9872\n",
      "Epoch 97/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1679 - val_acc: 1.0000\n",
      "Epoch 98/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1681 - val_acc: 0.9872\n",
      "Epoch 99/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1680 - val_acc: 1.0000\n",
      "Epoch 100/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1681 - val_acc: 0.9872\n",
      "78/78 [==============================] - 0s 366us/step\n",
      "Score for fold 4: loss of 2.168748617172241; acc of 98.71794825945145%\n",
      "Train on 702 samples, validate on 78 samples\n",
      "Epoch 1/100\n",
      "702/702 [==============================] - 2s 3ms/step - loss: 2.5181 - acc: 0.2564 - val_loss: 2.4154 - val_acc: 0.2564\n",
      "Epoch 2/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.3132 - acc: 0.4915 - val_loss: 2.3004 - val_acc: 0.6667\n",
      "Epoch 3/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2664 - acc: 0.6895 - val_loss: 2.3625 - val_acc: 0.4615\n",
      "Epoch 4/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2338 - acc: 0.7821 - val_loss: 2.2226 - val_acc: 0.8333\n",
      "Epoch 5/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2117 - acc: 0.8519 - val_loss: 2.2276 - val_acc: 0.7692\n",
      "Epoch 6/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2027 - acc: 0.8917 - val_loss: 2.2343 - val_acc: 0.8205\n",
      "Epoch 7/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1939 - acc: 0.9088 - val_loss: 2.2072 - val_acc: 0.8590\n",
      "Epoch 8/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1895 - acc: 0.9402 - val_loss: 2.1886 - val_acc: 0.9359\n",
      "Epoch 9/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1891 - acc: 0.9302 - val_loss: 2.2019 - val_acc: 0.8974\n",
      "Epoch 10/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1781 - acc: 0.9687 - val_loss: 2.2213 - val_acc: 0.8333\n",
      "Epoch 11/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1831 - acc: 0.9558 - val_loss: 2.2023 - val_acc: 0.8590\n",
      "Epoch 12/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1765 - acc: 0.9772 - val_loss: 2.1837 - val_acc: 0.9359\n",
      "Epoch 13/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1854 - acc: 0.9459 - val_loss: 2.1920 - val_acc: 0.8974\n",
      "Epoch 14/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1747 - acc: 0.9786 - val_loss: 2.1787 - val_acc: 0.9744\n",
      "Epoch 15/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1723 - acc: 0.9815 - val_loss: 2.2230 - val_acc: 0.8077\n",
      "Epoch 16/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1785 - acc: 0.9729 - val_loss: 2.1751 - val_acc: 0.9744\n",
      "Epoch 17/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1710 - acc: 0.9872 - val_loss: 2.1815 - val_acc: 0.9231\n",
      "Epoch 18/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1726 - acc: 0.9829 - val_loss: 2.2101 - val_acc: 0.8846\n",
      "Epoch 19/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1741 - acc: 0.9829 - val_loss: 2.1748 - val_acc: 0.9872\n",
      "Epoch 20/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1716 - acc: 0.9900 - val_loss: 2.1794 - val_acc: 0.9872\n",
      "Epoch 21/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1704 - acc: 0.9886 - val_loss: 2.1919 - val_acc: 0.8718\n",
      "Epoch 22/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1730 - acc: 0.9729 - val_loss: 2.1748 - val_acc: 0.9872\n",
      "Epoch 23/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1723 - acc: 0.9858 - val_loss: 2.1746 - val_acc: 0.9872\n",
      "Epoch 24/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1692 - acc: 0.9972 - val_loss: 2.1936 - val_acc: 0.8846\n",
      "Epoch 25/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1691 - acc: 0.9929 - val_loss: 2.1741 - val_acc: 0.9872\n",
      "Epoch 26/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1728 - acc: 0.9829 - val_loss: 2.1757 - val_acc: 0.9872\n",
      "Epoch 27/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1688 - acc: 0.9900 - val_loss: 2.1754 - val_acc: 0.9872\n",
      "Epoch 28/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1692 - acc: 0.9957 - val_loss: 2.1730 - val_acc: 0.9615\n",
      "Epoch 29/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1688 - acc: 0.9957 - val_loss: 2.1738 - val_acc: 0.9744\n",
      "Epoch 30/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1678 - acc: 0.9972 - val_loss: 2.1721 - val_acc: 0.9744\n",
      "Epoch 31/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1683 - acc: 0.9929 - val_loss: 2.1842 - val_acc: 0.9487\n",
      "Epoch 32/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1673 - acc: 0.9986 - val_loss: 2.1706 - val_acc: 0.9872\n",
      "Epoch 33/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1672 - acc: 0.9986 - val_loss: 2.2058 - val_acc: 0.9103\n",
      "Epoch 34/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1699 - acc: 0.9915 - val_loss: 2.1709 - val_acc: 0.9872\n",
      "Epoch 35/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1667 - acc: 1.0000 - val_loss: 2.1746 - val_acc: 0.9744\n",
      "Epoch 36/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1678 - acc: 0.9957 - val_loss: 2.1774 - val_acc: 0.9872\n",
      "Epoch 37/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1671 - acc: 0.9986 - val_loss: 2.1739 - val_acc: 0.9872\n",
      "Epoch 38/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1667 - acc: 1.0000 - val_loss: 2.1744 - val_acc: 0.9744\n",
      "Epoch 39/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1678 - acc: 0.9943 - val_loss: 2.1736 - val_acc: 0.9872\n",
      "Epoch 40/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1668 - acc: 1.0000 - val_loss: 2.1714 - val_acc: 0.9872\n",
      "Epoch 41/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1730 - val_acc: 0.9615\n",
      "Epoch 42/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1706 - val_acc: 0.9872\n",
      "Epoch 43/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1667 - acc: 0.9986 - val_loss: 2.1705 - val_acc: 0.9872\n",
      "Epoch 44/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1666 - acc: 0.9972 - val_loss: 2.1713 - val_acc: 0.9872\n",
      "Epoch 45/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1666 - acc: 0.9986 - val_loss: 2.1726 - val_acc: 0.9872\n",
      "Epoch 46/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1714 - val_acc: 0.9872\n",
      "Epoch 47/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1746 - val_acc: 0.9872\n",
      "Epoch 48/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1666 - acc: 0.9986 - val_loss: 2.1710 - val_acc: 0.9872\n",
      "Epoch 49/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1716 - val_acc: 0.9744\n",
      "Epoch 50/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1718 - val_acc: 0.9872\n",
      "Epoch 51/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1720 - val_acc: 0.9872\n",
      "Epoch 52/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1717 - val_acc: 0.9872\n",
      "Epoch 53/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1709 - val_acc: 0.9872\n",
      "Epoch 54/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1735 - val_acc: 0.9872\n",
      "Epoch 55/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1733 - val_acc: 0.9872\n",
      "Epoch 56/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1708 - val_acc: 0.9872\n",
      "Epoch 57/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1710 - val_acc: 0.9872\n",
      "Epoch 58/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1715 - val_acc: 0.9872\n",
      "Epoch 59/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1711 - val_acc: 0.9872\n",
      "Epoch 60/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1730 - val_acc: 0.9744\n",
      "Epoch 61/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1712 - val_acc: 0.9744\n",
      "Epoch 62/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1712 - val_acc: 0.9872\n",
      "Epoch 63/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1716 - val_acc: 0.9872\n",
      "Epoch 64/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1717 - val_acc: 0.9872\n",
      "Epoch 65/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1718 - val_acc: 0.9872\n",
      "Epoch 66/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1712 - val_acc: 0.9872\n",
      "Epoch 67/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1717 - val_acc: 0.9872\n",
      "Epoch 68/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1715 - val_acc: 0.9872\n",
      "Epoch 69/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1708 - val_acc: 0.9872\n",
      "Epoch 70/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1697 - val_acc: 0.9872\n",
      "Epoch 71/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1713 - val_acc: 0.9744\n",
      "Epoch 72/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1708 - val_acc: 0.9872\n",
      "Epoch 73/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1715 - val_acc: 0.9872\n",
      "Epoch 74/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1713 - val_acc: 0.9872\n",
      "Epoch 75/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1716 - val_acc: 0.9872\n",
      "Epoch 76/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1710 - val_acc: 0.9872\n",
      "Epoch 77/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1719 - val_acc: 0.9872\n",
      "Epoch 78/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1750 - val_acc: 0.9744\n",
      "Epoch 79/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1715 - val_acc: 0.9872\n",
      "Epoch 80/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1726 - val_acc: 0.9872\n",
      "Epoch 81/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1718 - val_acc: 0.9872\n",
      "Epoch 82/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1714 - val_acc: 0.9872\n",
      "Epoch 83/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1726 - val_acc: 0.9872\n",
      "Epoch 84/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1710 - val_acc: 0.9872\n",
      "Epoch 85/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1721 - val_acc: 0.9872\n",
      "Epoch 86/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1719 - val_acc: 0.9872\n",
      "Epoch 87/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1717 - val_acc: 0.9872\n",
      "Epoch 88/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1716 - val_acc: 0.9872\n",
      "Epoch 89/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1719 - val_acc: 0.9872\n",
      "Epoch 90/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1718 - val_acc: 0.9872\n",
      "Epoch 91/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1715 - val_acc: 0.9872\n",
      "Epoch 92/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1720 - val_acc: 0.9872\n",
      "Epoch 93/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1711 - val_acc: 0.9872\n",
      "Epoch 94/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1715 - val_acc: 0.9872\n",
      "Epoch 95/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1725 - val_acc: 0.9872\n",
      "Epoch 96/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1710 - val_acc: 0.9872\n",
      "Epoch 97/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1716 - val_acc: 0.9872\n",
      "Epoch 98/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1723 - val_acc: 0.9872\n",
      "Epoch 99/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1728 - val_acc: 0.9872\n",
      "Epoch 100/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1718 - val_acc: 0.9872\n",
      "78/78 [==============================] - 0s 319us/step\n",
      "Score for fold 5: loss of 2.171527850322234; acc of 98.71794871794873%\n",
      "Train on 702 samples, validate on 78 samples\n",
      "Epoch 1/100\n",
      "702/702 [==============================] - 3s 4ms/step - loss: 2.4808 - acc: 0.3162 - val_loss: 2.3472 - val_acc: 0.4487\n",
      "Epoch 2/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.3030 - acc: 0.6054 - val_loss: 2.3071 - val_acc: 0.5256\n",
      "Epoch 3/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2443 - acc: 0.7650 - val_loss: 2.2444 - val_acc: 0.7949\n",
      "Epoch 4/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2118 - acc: 0.8647 - val_loss: 2.2451 - val_acc: 0.7949\n",
      "Epoch 5/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2082 - acc: 0.8561 - val_loss: 2.2119 - val_acc: 0.8462\n",
      "Epoch 6/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1928 - acc: 0.9316 - val_loss: 2.2172 - val_acc: 0.7692\n",
      "Epoch 7/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1907 - acc: 0.9345 - val_loss: 2.1926 - val_acc: 0.8590\n",
      "Epoch 8/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1845 - acc: 0.9387 - val_loss: 2.1812 - val_acc: 0.9487\n",
      "Epoch 9/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1838 - acc: 0.9444 - val_loss: 2.1801 - val_acc: 0.9487\n",
      "Epoch 10/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1808 - acc: 0.9772 - val_loss: 2.1807 - val_acc: 0.9359\n",
      "Epoch 11/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1749 - acc: 0.9658 - val_loss: 2.1819 - val_acc: 0.9487\n",
      "Epoch 12/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1810 - acc: 0.9658 - val_loss: 2.1896 - val_acc: 0.9872\n",
      "Epoch 13/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1727 - acc: 0.9915 - val_loss: 2.1801 - val_acc: 0.9615\n",
      "Epoch 14/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1740 - acc: 0.9872 - val_loss: 2.1785 - val_acc: 0.9872\n",
      "Epoch 15/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1728 - acc: 0.9786 - val_loss: 2.1792 - val_acc: 0.9615\n",
      "Epoch 16/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1719 - acc: 0.9957 - val_loss: 2.1991 - val_acc: 0.9231\n",
      "Epoch 17/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1721 - acc: 0.9843 - val_loss: 2.1795 - val_acc: 0.9872\n",
      "Epoch 18/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1750 - acc: 0.9744 - val_loss: 2.1818 - val_acc: 0.9744\n",
      "Epoch 19/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1693 - acc: 0.9986 - val_loss: 2.1804 - val_acc: 0.9744\n",
      "Epoch 20/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1730 - acc: 0.9886 - val_loss: 2.1751 - val_acc: 0.9872\n",
      "Epoch 21/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1692 - acc: 0.9957 - val_loss: 2.1796 - val_acc: 0.9487\n",
      "Epoch 22/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1702 - acc: 0.9843 - val_loss: 2.1783 - val_acc: 0.9872\n",
      "Epoch 23/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1686 - acc: 0.9972 - val_loss: 2.1766 - val_acc: 0.9744\n",
      "Epoch 24/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1684 - acc: 0.9986 - val_loss: 2.1752 - val_acc: 0.9744\n",
      "Epoch 25/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1693 - acc: 0.9943 - val_loss: 2.1724 - val_acc: 0.9872\n",
      "Epoch 26/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1677 - acc: 0.9972 - val_loss: 2.1835 - val_acc: 0.9744\n",
      "Epoch 27/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1678 - acc: 0.9943 - val_loss: 2.1729 - val_acc: 0.9744\n",
      "Epoch 28/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1675 - acc: 0.9986 - val_loss: 2.1740 - val_acc: 0.9744\n",
      "Epoch 29/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1677 - acc: 0.9986 - val_loss: 2.1748 - val_acc: 0.9872\n",
      "Epoch 30/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1678 - acc: 0.9972 - val_loss: 2.1723 - val_acc: 0.9872\n",
      "Epoch 31/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1676 - acc: 0.9972 - val_loss: 2.1916 - val_acc: 0.8974\n",
      "Epoch 32/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1680 - acc: 0.9943 - val_loss: 2.1710 - val_acc: 0.9872\n",
      "Epoch 33/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1673 - acc: 0.9986 - val_loss: 2.1714 - val_acc: 0.9872\n",
      "Epoch 34/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1669 - acc: 0.9986 - val_loss: 2.1716 - val_acc: 0.9872\n",
      "Epoch 35/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1668 - acc: 0.9986 - val_loss: 2.1714 - val_acc: 0.9872\n",
      "Epoch 36/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1668 - acc: 0.9986 - val_loss: 2.1701 - val_acc: 0.9872\n",
      "Epoch 37/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1670 - acc: 0.9986 - val_loss: 2.1705 - val_acc: 0.9872\n",
      "Epoch 38/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1667 - acc: 1.0000 - val_loss: 2.1709 - val_acc: 0.9872\n",
      "Epoch 39/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1667 - acc: 1.0000 - val_loss: 2.1721 - val_acc: 0.9744\n",
      "Epoch 40/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1667 - acc: 1.0000 - val_loss: 2.1707 - val_acc: 0.9744\n",
      "Epoch 41/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1669 - acc: 1.0000 - val_loss: 2.1747 - val_acc: 0.9744\n",
      "Epoch 42/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1667 - acc: 1.0000 - val_loss: 2.1732 - val_acc: 0.9872\n",
      "Epoch 43/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1666 - acc: 1.0000 - val_loss: 2.1715 - val_acc: 0.9872\n",
      "Epoch 44/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1720 - val_acc: 0.9744\n",
      "Epoch 45/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1706 - val_acc: 0.9872\n",
      "Epoch 46/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1709 - val_acc: 0.9872\n",
      "Epoch 47/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1734 - val_acc: 0.9872\n",
      "Epoch 48/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1706 - val_acc: 0.9744\n",
      "Epoch 49/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1711 - val_acc: 0.9744\n",
      "Epoch 50/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1714 - val_acc: 0.9872\n",
      "Epoch 51/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1710 - val_acc: 0.9872\n",
      "Epoch 52/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1738 - val_acc: 0.9872\n",
      "Epoch 53/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1702 - val_acc: 0.9872\n",
      "Epoch 54/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1712 - val_acc: 0.9872\n",
      "Epoch 55/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1706 - val_acc: 0.9872\n",
      "Epoch 56/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1710 - val_acc: 0.9744\n",
      "Epoch 57/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1698 - val_acc: 0.9872\n",
      "Epoch 58/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1701 - val_acc: 0.9744\n",
      "Epoch 59/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1705 - val_acc: 0.9872\n",
      "Epoch 60/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1714 - val_acc: 0.9744\n",
      "Epoch 61/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1703 - val_acc: 0.9744\n",
      "Epoch 62/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1700 - val_acc: 0.9872\n",
      "Epoch 63/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1703 - val_acc: 0.9872\n",
      "Epoch 64/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1699 - val_acc: 0.9872\n",
      "Epoch 65/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1716 - val_acc: 0.9872\n",
      "Epoch 66/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1708 - val_acc: 0.9872\n",
      "Epoch 67/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1705 - val_acc: 0.9872\n",
      "Epoch 68/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1702 - val_acc: 0.9872\n",
      "Epoch 69/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1699 - val_acc: 0.9872\n",
      "Epoch 70/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1705 - val_acc: 0.9872\n",
      "Epoch 71/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1705 - val_acc: 0.9872\n",
      "Epoch 72/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1699 - val_acc: 0.9744\n",
      "Epoch 73/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1700 - val_acc: 0.9872\n",
      "Epoch 74/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1702 - val_acc: 0.9744\n",
      "Epoch 75/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1704 - val_acc: 0.9744\n",
      "Epoch 76/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1705 - val_acc: 0.9744\n",
      "Epoch 77/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1698 - val_acc: 0.9872\n",
      "Epoch 78/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1703 - val_acc: 0.9744\n",
      "Epoch 79/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1699 - val_acc: 0.9744\n",
      "Epoch 80/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1701 - val_acc: 0.9872\n",
      "Epoch 81/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1701 - val_acc: 0.9744\n",
      "Epoch 82/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1704 - val_acc: 0.9872\n",
      "Epoch 83/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1701 - val_acc: 0.9872\n",
      "Epoch 84/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1701 - val_acc: 0.9872\n",
      "Epoch 85/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1701 - val_acc: 0.9872\n",
      "Epoch 86/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1702 - val_acc: 0.9872\n",
      "Epoch 87/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1701 - val_acc: 0.9744\n",
      "Epoch 88/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1702 - val_acc: 0.9872\n",
      "Epoch 89/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1699 - val_acc: 0.9872\n",
      "Epoch 90/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1702 - val_acc: 0.9872\n",
      "Epoch 91/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1702 - val_acc: 0.9872\n",
      "Epoch 92/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1700 - val_acc: 0.9872\n",
      "Epoch 93/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1704 - val_acc: 0.9872\n",
      "Epoch 94/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1700 - val_acc: 0.9872\n",
      "Epoch 95/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1702 - val_acc: 0.9872\n",
      "Epoch 96/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1700 - val_acc: 0.9872\n",
      "Epoch 97/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1699 - val_acc: 0.9872\n",
      "Epoch 98/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1700 - val_acc: 0.9872\n",
      "Epoch 99/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1700 - val_acc: 0.9872\n",
      "Epoch 100/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1701 - val_acc: 0.9872\n",
      "78/78 [==============================] - 0s 306us/step\n",
      "Score for fold 6: loss of 2.1698707984044003; acc of 98.71794825945145%\n",
      "Train on 702 samples, validate on 78 samples\n",
      "Epoch 1/100\n",
      "702/702 [==============================] - 3s 4ms/step - loss: 2.5090 - acc: 0.2863 - val_loss: 2.3698 - val_acc: 0.4487\n",
      "Epoch 2/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.3110 - acc: 0.5783 - val_loss: 2.2882 - val_acc: 0.5769\n",
      "Epoch 3/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2480 - acc: 0.7137 - val_loss: 2.2530 - val_acc: 0.7051\n",
      "Epoch 4/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2159 - acc: 0.8319 - val_loss: 2.2388 - val_acc: 0.8333\n",
      "Epoch 5/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2053 - acc: 0.8846 - val_loss: 2.2267 - val_acc: 0.8077\n",
      "Epoch 6/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1976 - acc: 0.8903 - val_loss: 2.1961 - val_acc: 0.9103\n",
      "Epoch 7/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1807 - acc: 0.9630 - val_loss: 2.2005 - val_acc: 0.8333\n",
      "Epoch 8/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1869 - acc: 0.9359 - val_loss: 2.2015 - val_acc: 0.8077\n",
      "Epoch 9/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1833 - acc: 0.9672 - val_loss: 2.1898 - val_acc: 0.8974\n",
      "Epoch 10/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1752 - acc: 0.9815 - val_loss: 2.1851 - val_acc: 0.9615\n",
      "Epoch 11/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1768 - acc: 0.9701 - val_loss: 2.1878 - val_acc: 0.9231\n",
      "Epoch 12/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1767 - acc: 0.9758 - val_loss: 2.1827 - val_acc: 0.9359\n",
      "Epoch 13/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1751 - acc: 0.9843 - val_loss: 2.2080 - val_acc: 0.9231\n",
      "Epoch 14/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1738 - acc: 0.9872 - val_loss: 2.1833 - val_acc: 0.9487\n",
      "Epoch 15/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1747 - acc: 0.9829 - val_loss: 2.1833 - val_acc: 0.9359\n",
      "Epoch 16/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1712 - acc: 0.9929 - val_loss: 2.1820 - val_acc: 0.9744\n",
      "Epoch 17/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1716 - acc: 0.9886 - val_loss: 2.1824 - val_acc: 0.9615\n",
      "Epoch 18/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1697 - acc: 0.9915 - val_loss: 2.1791 - val_acc: 0.9487\n",
      "Epoch 19/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1714 - acc: 0.9900 - val_loss: 2.1775 - val_acc: 0.9487\n",
      "Epoch 20/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1709 - acc: 0.9886 - val_loss: 2.1813 - val_acc: 0.9615\n",
      "Epoch 21/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1693 - acc: 0.9929 - val_loss: 2.1791 - val_acc: 0.9744\n",
      "Epoch 22/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1683 - acc: 0.9972 - val_loss: 2.1795 - val_acc: 0.9744\n",
      "Epoch 23/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1701 - acc: 0.9929 - val_loss: 2.1771 - val_acc: 0.9615\n",
      "Epoch 24/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1695 - acc: 0.9972 - val_loss: 2.1776 - val_acc: 0.9615\n",
      "Epoch 25/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1678 - acc: 0.9957 - val_loss: 2.1801 - val_acc: 0.9615\n",
      "Epoch 26/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1693 - acc: 0.9801 - val_loss: 2.1806 - val_acc: 0.9487\n",
      "Epoch 27/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1676 - acc: 0.9986 - val_loss: 2.1796 - val_acc: 0.9487\n",
      "Epoch 28/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1674 - acc: 0.9943 - val_loss: 2.1741 - val_acc: 0.9744\n",
      "Epoch 29/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1678 - acc: 0.9986 - val_loss: 2.1758 - val_acc: 0.9744\n",
      "Epoch 30/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1669 - acc: 1.0000 - val_loss: 2.1758 - val_acc: 0.9744\n",
      "Epoch 31/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1672 - acc: 0.9957 - val_loss: 2.1791 - val_acc: 0.9744\n",
      "Epoch 32/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1675 - acc: 0.9972 - val_loss: 2.1779 - val_acc: 0.9615\n",
      "Epoch 33/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1670 - acc: 0.9986 - val_loss: 2.1783 - val_acc: 0.9615\n",
      "Epoch 34/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1668 - acc: 1.0000 - val_loss: 2.1774 - val_acc: 0.9744\n",
      "Epoch 35/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1669 - acc: 1.0000 - val_loss: 2.1755 - val_acc: 0.9744\n",
      "Epoch 36/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1667 - acc: 1.0000 - val_loss: 2.1765 - val_acc: 0.9744\n",
      "Epoch 37/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1666 - acc: 1.0000 - val_loss: 2.1788 - val_acc: 0.9744\n",
      "Epoch 38/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1667 - acc: 1.0000 - val_loss: 2.1781 - val_acc: 0.9615\n",
      "Epoch 39/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1785 - val_acc: 0.9744\n",
      "Epoch 40/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1794 - val_acc: 0.9744\n",
      "Epoch 41/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1802 - val_acc: 0.9615\n",
      "Epoch 42/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1753 - val_acc: 0.9744\n",
      "Epoch 43/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1810 - val_acc: 0.9744\n",
      "Epoch 44/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1666 - acc: 1.0000 - val_loss: 2.1771 - val_acc: 0.9744\n",
      "Epoch 45/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1772 - val_acc: 0.9744\n",
      "Epoch 46/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1780 - val_acc: 0.9744\n",
      "Epoch 47/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1784 - val_acc: 0.9744\n",
      "Epoch 48/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1778 - val_acc: 0.9744\n",
      "Epoch 49/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1732 - val_acc: 0.9872\n",
      "Epoch 50/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1726 - val_acc: 0.9872\n",
      "Epoch 51/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1761 - val_acc: 0.9744\n",
      "Epoch 52/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1763 - val_acc: 0.9744\n",
      "Epoch 53/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1758 - val_acc: 0.9744\n",
      "Epoch 54/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1766 - val_acc: 0.9615\n",
      "Epoch 55/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1765 - val_acc: 0.9744\n",
      "Epoch 56/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1761 - val_acc: 0.9744\n",
      "Epoch 57/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1794 - val_acc: 0.9744\n",
      "Epoch 58/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1751 - val_acc: 0.9744\n",
      "Epoch 59/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1766 - val_acc: 0.9744\n",
      "Epoch 60/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1778 - val_acc: 0.9744\n",
      "Epoch 61/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1756 - val_acc: 0.9744\n",
      "Epoch 62/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1757 - val_acc: 0.9744\n",
      "Epoch 63/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1754 - val_acc: 0.9744\n",
      "Epoch 64/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1762 - val_acc: 0.9744\n",
      "Epoch 65/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1747 - val_acc: 0.9744\n",
      "Epoch 66/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1743 - val_acc: 0.9744\n",
      "Epoch 67/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1756 - val_acc: 0.9744\n",
      "Epoch 68/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1753 - val_acc: 0.9744\n",
      "Epoch 69/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1769 - val_acc: 0.9744\n",
      "Epoch 70/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1779 - val_acc: 0.9615\n",
      "Epoch 71/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1747 - val_acc: 0.9744\n",
      "Epoch 72/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1789 - val_acc: 0.9744\n",
      "Epoch 73/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1752 - val_acc: 0.9744\n",
      "Epoch 74/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1759 - val_acc: 0.9744\n",
      "Epoch 75/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1766 - val_acc: 0.9744\n",
      "Epoch 76/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1768 - val_acc: 0.9744\n",
      "Epoch 77/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1759 - val_acc: 0.9744\n",
      "Epoch 78/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1749 - val_acc: 0.9744\n",
      "Epoch 79/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1757 - val_acc: 0.9744\n",
      "Epoch 80/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1747 - val_acc: 0.9744\n",
      "Epoch 81/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1752 - val_acc: 0.9744\n",
      "Epoch 82/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1752 - val_acc: 0.9744\n",
      "Epoch 83/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1762 - val_acc: 0.9744\n",
      "Epoch 84/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1745 - val_acc: 0.9744\n",
      "Epoch 85/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1769 - val_acc: 0.9744\n",
      "Epoch 86/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1768 - val_acc: 0.9744\n",
      "Epoch 87/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1758 - val_acc: 0.9744\n",
      "Epoch 88/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1765 - val_acc: 0.9744\n",
      "Epoch 89/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1765 - val_acc: 0.9744\n",
      "Epoch 90/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1765 - val_acc: 0.9744\n",
      "Epoch 91/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1775 - val_acc: 0.9744\n",
      "Epoch 92/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1774 - val_acc: 0.9744\n",
      "Epoch 93/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1772 - val_acc: 0.9744\n",
      "Epoch 94/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1770 - val_acc: 0.9744\n",
      "Epoch 95/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1766 - val_acc: 0.9744\n",
      "Epoch 96/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1764 - val_acc: 0.9744\n",
      "Epoch 97/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1759 - val_acc: 0.9744\n",
      "Epoch 98/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1762 - val_acc: 0.9744\n",
      "Epoch 99/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1763 - val_acc: 0.9744\n",
      "Epoch 100/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1767 - val_acc: 0.9744\n",
      "78/78 [==============================] - 0s 321us/step\n",
      "Score for fold 7: loss of 2.1757248670626907; acc of 97.43589697740018%\n",
      "Train on 702 samples, validate on 78 samples\n",
      "Epoch 1/100\n",
      "702/702 [==============================] - 3s 4ms/step - loss: 2.4736 - acc: 0.2991 - val_loss: 2.3692 - val_acc: 0.4615\n",
      "Epoch 2/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.3073 - acc: 0.5541 - val_loss: 2.3119 - val_acc: 0.4872\n",
      "Epoch 3/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2533 - acc: 0.7123 - val_loss: 2.2440 - val_acc: 0.7692\n",
      "Epoch 4/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2200 - acc: 0.8077 - val_loss: 2.2244 - val_acc: 0.8077\n",
      "Epoch 5/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1990 - acc: 0.8832 - val_loss: 2.2816 - val_acc: 0.7436\n",
      "Epoch 6/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1986 - acc: 0.9003 - val_loss: 2.2214 - val_acc: 0.7949\n",
      "Epoch 7/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1915 - acc: 0.9117 - val_loss: 2.2001 - val_acc: 0.7692\n",
      "Epoch 8/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1878 - acc: 0.9274 - val_loss: 2.1986 - val_acc: 0.8846\n",
      "Epoch 9/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1783 - acc: 0.9345 - val_loss: 2.2046 - val_acc: 0.8462\n",
      "Epoch 10/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1813 - acc: 0.9473 - val_loss: 2.1940 - val_acc: 0.9103\n",
      "Epoch 11/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1793 - acc: 0.9573 - val_loss: 2.1912 - val_acc: 0.8846\n",
      "Epoch 12/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1745 - acc: 0.9801 - val_loss: 2.1891 - val_acc: 0.9231\n",
      "Epoch 13/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1804 - acc: 0.9558 - val_loss: 2.2007 - val_acc: 0.9231\n",
      "Epoch 14/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1733 - acc: 0.9858 - val_loss: 2.1956 - val_acc: 0.8974\n",
      "Epoch 15/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1774 - acc: 0.9786 - val_loss: 2.1819 - val_acc: 0.9359\n",
      "Epoch 16/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1749 - acc: 0.9786 - val_loss: 2.1833 - val_acc: 0.9615\n",
      "Epoch 17/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1708 - acc: 0.9886 - val_loss: 2.1887 - val_acc: 0.8846\n",
      "Epoch 18/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1721 - acc: 0.9858 - val_loss: 2.1824 - val_acc: 0.9103\n",
      "Epoch 19/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1722 - acc: 0.9829 - val_loss: 2.1823 - val_acc: 0.9744\n",
      "Epoch 20/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1689 - acc: 0.9957 - val_loss: 2.1839 - val_acc: 0.9615\n",
      "Epoch 21/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1725 - acc: 0.9929 - val_loss: 2.1820 - val_acc: 0.9615\n",
      "Epoch 22/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1696 - acc: 0.9972 - val_loss: 2.1794 - val_acc: 0.9744\n",
      "Epoch 23/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1694 - acc: 0.9929 - val_loss: 2.1861 - val_acc: 0.9615\n",
      "Epoch 24/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1711 - acc: 0.9886 - val_loss: 2.1871 - val_acc: 0.9487\n",
      "Epoch 25/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1703 - acc: 0.9929 - val_loss: 2.1848 - val_acc: 0.9615\n",
      "Epoch 26/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1699 - acc: 0.9943 - val_loss: 2.1898 - val_acc: 0.9615\n",
      "Epoch 27/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1690 - acc: 0.9943 - val_loss: 2.1780 - val_acc: 0.9744\n",
      "Epoch 28/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1693 - acc: 0.9972 - val_loss: 2.1843 - val_acc: 0.9615\n",
      "Epoch 29/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1688 - acc: 0.9972 - val_loss: 2.2074 - val_acc: 0.9231\n",
      "Epoch 30/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1694 - acc: 0.9929 - val_loss: 2.1817 - val_acc: 0.9615\n",
      "Epoch 31/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1676 - acc: 0.9957 - val_loss: 2.1807 - val_acc: 0.9744\n",
      "Epoch 32/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1676 - acc: 0.9986 - val_loss: 2.1778 - val_acc: 0.9744\n",
      "Epoch 33/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1673 - acc: 0.9986 - val_loss: 2.1758 - val_acc: 0.9872\n",
      "Epoch 34/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1674 - acc: 0.9972 - val_loss: 2.1797 - val_acc: 0.9744\n",
      "Epoch 35/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1679 - acc: 0.9957 - val_loss: 2.1798 - val_acc: 0.9744\n",
      "Epoch 36/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1670 - acc: 0.9986 - val_loss: 2.1823 - val_acc: 0.9487\n",
      "Epoch 37/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1669 - acc: 1.0000 - val_loss: 2.1794 - val_acc: 0.9744\n",
      "Epoch 38/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1670 - acc: 1.0000 - val_loss: 2.1802 - val_acc: 0.9744\n",
      "Epoch 39/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1668 - acc: 1.0000 - val_loss: 2.1819 - val_acc: 0.9615\n",
      "Epoch 40/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1670 - acc: 1.0000 - val_loss: 2.1782 - val_acc: 0.9744\n",
      "Epoch 41/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1667 - acc: 1.0000 - val_loss: 2.1777 - val_acc: 0.9744\n",
      "Epoch 42/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1669 - acc: 1.0000 - val_loss: 2.1796 - val_acc: 0.9744\n",
      "Epoch 43/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1668 - acc: 1.0000 - val_loss: 2.1810 - val_acc: 0.9744\n",
      "Epoch 44/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1667 - acc: 1.0000 - val_loss: 2.1792 - val_acc: 0.9744\n",
      "Epoch 45/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1668 - acc: 1.0000 - val_loss: 2.1779 - val_acc: 0.9744\n",
      "Epoch 46/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1770 - val_acc: 0.9744\n",
      "Epoch 47/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1789 - val_acc: 0.9744\n",
      "Epoch 48/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1781 - val_acc: 0.9744\n",
      "Epoch 49/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1783 - val_acc: 0.9744\n",
      "Epoch 50/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1785 - val_acc: 0.9744\n",
      "Epoch 51/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1808 - val_acc: 0.9487\n",
      "Epoch 52/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1852 - val_acc: 0.9615\n",
      "Epoch 53/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1790 - val_acc: 0.9744\n",
      "Epoch 54/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1821 - val_acc: 0.9615\n",
      "Epoch 55/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1781 - val_acc: 0.9744\n",
      "Epoch 56/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1776 - val_acc: 0.9744\n",
      "Epoch 57/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1779 - val_acc: 0.9744\n",
      "Epoch 58/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1780 - val_acc: 0.9744\n",
      "Epoch 59/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1766 - val_acc: 0.9744\n",
      "Epoch 60/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1771 - val_acc: 0.9744\n",
      "Epoch 61/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1780 - val_acc: 0.9744\n",
      "Epoch 62/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1784 - val_acc: 0.9744\n",
      "Epoch 63/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1768 - val_acc: 0.9744\n",
      "Epoch 64/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1778 - val_acc: 0.9744\n",
      "Epoch 65/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1787 - val_acc: 0.9744\n",
      "Epoch 66/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1797 - val_acc: 0.9744\n",
      "Epoch 67/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1778 - val_acc: 0.9744\n",
      "Epoch 68/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1778 - val_acc: 0.9744\n",
      "Epoch 69/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1783 - val_acc: 0.9744\n",
      "Epoch 70/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1773 - val_acc: 0.9744\n",
      "Epoch 71/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1785 - val_acc: 0.9744\n",
      "Epoch 72/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1783 - val_acc: 0.9744\n",
      "Epoch 73/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1772 - val_acc: 0.9744\n",
      "Epoch 74/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1773 - val_acc: 0.9744\n",
      "Epoch 75/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1776 - val_acc: 0.9744\n",
      "Epoch 76/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1776 - val_acc: 0.9744\n",
      "Epoch 77/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1778 - val_acc: 0.9744\n",
      "Epoch 78/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1780 - val_acc: 0.9744\n",
      "Epoch 79/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1777 - val_acc: 0.9744\n",
      "Epoch 80/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1775 - val_acc: 0.9744\n",
      "Epoch 81/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1765 - val_acc: 0.9744\n",
      "Epoch 82/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1783 - val_acc: 0.9744\n",
      "Epoch 83/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1779 - val_acc: 0.9744\n",
      "Epoch 84/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1778 - val_acc: 0.9744\n",
      "Epoch 85/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1777 - val_acc: 0.9744\n",
      "Epoch 86/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1772 - val_acc: 0.9744\n",
      "Epoch 87/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1775 - val_acc: 0.9744\n",
      "Epoch 88/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1779 - val_acc: 0.9744\n",
      "Epoch 89/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1774 - val_acc: 0.9744\n",
      "Epoch 90/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1769 - val_acc: 0.9744\n",
      "Epoch 91/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1773 - val_acc: 0.9744\n",
      "Epoch 92/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1787 - val_acc: 0.9744\n",
      "Epoch 93/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1775 - val_acc: 0.9744\n",
      "Epoch 94/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1768 - val_acc: 0.9744\n",
      "Epoch 95/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1775 - val_acc: 0.9744\n",
      "Epoch 96/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1773 - val_acc: 0.9744\n",
      "Epoch 97/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1772 - val_acc: 0.9744\n",
      "Epoch 98/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1778 - val_acc: 0.9744\n",
      "Epoch 99/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1776 - val_acc: 0.9744\n",
      "Epoch 100/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1775 - val_acc: 0.9744\n",
      "78/78 [==============================] - 0s 297us/step\n",
      "Score for fold 8: loss of 2.1775228732671494; acc of 97.43589743589743%\n",
      "Train on 702 samples, validate on 78 samples\n",
      "Epoch 1/100\n",
      "702/702 [==============================] - 3s 4ms/step - loss: 2.4974 - acc: 0.2863 - val_loss: 2.4456 - val_acc: 0.2949\n",
      "Epoch 2/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.3252 - acc: 0.5584 - val_loss: 2.2726 - val_acc: 0.7179\n",
      "Epoch 3/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2456 - acc: 0.7265 - val_loss: 2.2505 - val_acc: 0.7179\n",
      "Epoch 4/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2228 - acc: 0.8291 - val_loss: 2.2427 - val_acc: 0.7436\n",
      "Epoch 5/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2045 - acc: 0.8761 - val_loss: 2.2020 - val_acc: 0.8077\n",
      "Epoch 6/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2000 - acc: 0.8860 - val_loss: 2.2077 - val_acc: 0.9103\n",
      "Epoch 7/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1840 - acc: 0.9402 - val_loss: 2.1991 - val_acc: 0.9359\n",
      "Epoch 8/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1823 - acc: 0.9530 - val_loss: 2.1919 - val_acc: 0.9231\n",
      "Epoch 9/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1808 - acc: 0.9402 - val_loss: 2.1865 - val_acc: 0.9487\n",
      "Epoch 10/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1812 - acc: 0.9544 - val_loss: 2.1879 - val_acc: 0.9487\n",
      "Epoch 11/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1729 - acc: 0.9786 - val_loss: 2.1864 - val_acc: 0.8974\n",
      "Epoch 12/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1735 - acc: 0.9772 - val_loss: 2.1827 - val_acc: 0.9231\n",
      "Epoch 13/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1752 - acc: 0.9715 - val_loss: 2.1833 - val_acc: 0.9487\n",
      "Epoch 14/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1731 - acc: 0.9843 - val_loss: 2.1863 - val_acc: 0.8846\n",
      "Epoch 15/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1751 - acc: 0.9772 - val_loss: 2.1833 - val_acc: 0.9615\n",
      "Epoch 16/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1700 - acc: 0.9943 - val_loss: 2.1848 - val_acc: 0.8974\n",
      "Epoch 17/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1708 - acc: 0.9872 - val_loss: 2.1851 - val_acc: 0.9359\n",
      "Epoch 18/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1688 - acc: 0.9957 - val_loss: 2.1831 - val_acc: 0.9103\n",
      "Epoch 19/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1713 - acc: 0.9915 - val_loss: 2.1873 - val_acc: 0.8846\n",
      "Epoch 20/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1696 - acc: 0.9929 - val_loss: 2.1812 - val_acc: 0.9231\n",
      "Epoch 21/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1686 - acc: 0.9943 - val_loss: 2.1872 - val_acc: 0.9231\n",
      "Epoch 22/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1685 - acc: 0.9900 - val_loss: 2.1861 - val_acc: 0.8846\n",
      "Epoch 23/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1697 - acc: 0.9886 - val_loss: 2.1802 - val_acc: 0.9744\n",
      "Epoch 24/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1675 - acc: 1.0000 - val_loss: 2.1810 - val_acc: 0.9359\n",
      "Epoch 25/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1679 - acc: 0.9929 - val_loss: 2.1802 - val_acc: 0.9615\n",
      "Epoch 26/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1688 - acc: 0.9972 - val_loss: 2.1812 - val_acc: 0.9103\n",
      "Epoch 27/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1671 - acc: 0.9986 - val_loss: 2.1818 - val_acc: 0.9359\n",
      "Epoch 28/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1701 - acc: 0.9957 - val_loss: 2.1817 - val_acc: 0.9359\n",
      "Epoch 29/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1709 - acc: 0.9900 - val_loss: 2.1791 - val_acc: 0.9744\n",
      "Epoch 30/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1671 - acc: 1.0000 - val_loss: 2.1791 - val_acc: 0.9615\n",
      "Epoch 31/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1677 - acc: 0.9972 - val_loss: 2.1787 - val_acc: 0.9744\n",
      "Epoch 32/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1669 - acc: 0.9986 - val_loss: 2.1796 - val_acc: 0.9615\n",
      "Epoch 33/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1675 - acc: 0.9943 - val_loss: 2.1792 - val_acc: 0.9615\n",
      "Epoch 34/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1683 - acc: 0.9957 - val_loss: 2.1784 - val_acc: 0.9615\n",
      "Epoch 35/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1673 - acc: 0.9986 - val_loss: 2.1779 - val_acc: 0.9744\n",
      "Epoch 36/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1666 - acc: 1.0000 - val_loss: 2.1777 - val_acc: 0.9744\n",
      "Epoch 37/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1666 - acc: 1.0000 - val_loss: 2.1791 - val_acc: 0.9615\n",
      "Epoch 38/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1667 - acc: 0.9986 - val_loss: 2.1792 - val_acc: 0.9615\n",
      "Epoch 39/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1671 - acc: 0.9986 - val_loss: 2.1774 - val_acc: 0.9744\n",
      "Epoch 40/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1781 - val_acc: 0.9744\n",
      "Epoch 41/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1666 - acc: 1.0000 - val_loss: 2.1775 - val_acc: 0.9744\n",
      "Epoch 42/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1671 - acc: 0.9986 - val_loss: 2.1768 - val_acc: 0.9744\n",
      "Epoch 43/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1780 - val_acc: 0.9744\n",
      "Epoch 44/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1786 - val_acc: 0.9487\n",
      "Epoch 45/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1778 - val_acc: 0.9744\n",
      "Epoch 46/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1775 - val_acc: 0.9744\n",
      "Epoch 47/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1772 - val_acc: 0.9744\n",
      "Epoch 48/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1775 - val_acc: 0.9615\n",
      "Epoch 49/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1771 - val_acc: 0.9744\n",
      "Epoch 50/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1765 - val_acc: 0.9744\n",
      "Epoch 51/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1777 - val_acc: 0.9744\n",
      "Epoch 52/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1774 - val_acc: 0.9744\n",
      "Epoch 53/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1772 - val_acc: 0.9744\n",
      "Epoch 54/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1777 - val_acc: 0.9744\n",
      "Epoch 55/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1771 - val_acc: 0.9744\n",
      "Epoch 56/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1768 - val_acc: 0.9744\n",
      "Epoch 57/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1770 - val_acc: 0.9744\n",
      "Epoch 58/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1773 - val_acc: 0.9744\n",
      "Epoch 59/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1775 - val_acc: 0.9744\n",
      "Epoch 60/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1776 - val_acc: 0.9744\n",
      "Epoch 61/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1770 - val_acc: 0.9744\n",
      "Epoch 62/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1772 - val_acc: 0.9744\n",
      "Epoch 63/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1768 - val_acc: 0.9744\n",
      "Epoch 64/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1773 - val_acc: 0.9744\n",
      "Epoch 65/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1773 - val_acc: 0.9744\n",
      "Epoch 66/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1771 - val_acc: 0.9744\n",
      "Epoch 67/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1771 - val_acc: 0.9744\n",
      "Epoch 68/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1774 - val_acc: 0.9744\n",
      "Epoch 69/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1772 - val_acc: 0.9744\n",
      "Epoch 70/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1772 - val_acc: 0.9744\n",
      "Epoch 71/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1776 - val_acc: 0.9744\n",
      "Epoch 72/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1771 - val_acc: 0.9744\n",
      "Epoch 73/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1770 - val_acc: 0.9744\n",
      "Epoch 74/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1772 - val_acc: 0.9744\n",
      "Epoch 75/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1770 - val_acc: 0.9744\n",
      "Epoch 76/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1770 - val_acc: 0.9744\n",
      "Epoch 77/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1770 - val_acc: 0.9744\n",
      "Epoch 78/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1769 - val_acc: 0.9744\n",
      "Epoch 79/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1774 - val_acc: 0.9744\n",
      "Epoch 80/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1773 - val_acc: 0.9744\n",
      "Epoch 81/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1769 - val_acc: 0.9744\n",
      "Epoch 82/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1771 - val_acc: 0.9744\n",
      "Epoch 83/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1770 - val_acc: 0.9744\n",
      "Epoch 84/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1770 - val_acc: 0.9744\n",
      "Epoch 85/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1770 - val_acc: 0.9744\n",
      "Epoch 86/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1770 - val_acc: 0.9744\n",
      "Epoch 87/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1769 - val_acc: 0.9744\n",
      "Epoch 88/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1771 - val_acc: 0.9744\n",
      "Epoch 89/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1771 - val_acc: 0.9744\n",
      "Epoch 90/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1770 - val_acc: 0.9744\n",
      "Epoch 91/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1771 - val_acc: 0.9744\n",
      "Epoch 92/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1771 - val_acc: 0.9744\n",
      "Epoch 93/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1771 - val_acc: 0.9744\n",
      "Epoch 94/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1769 - val_acc: 0.9744\n",
      "Epoch 95/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1771 - val_acc: 0.9744\n",
      "Epoch 96/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1770 - val_acc: 0.9744\n",
      "Epoch 97/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1770 - val_acc: 0.9744\n",
      "Epoch 98/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1770 - val_acc: 0.9744\n",
      "Epoch 99/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1770 - val_acc: 0.9744\n",
      "Epoch 100/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1770 - val_acc: 0.9744\n",
      "78/78 [==============================] - 0s 307us/step\n",
      "Score for fold 9: loss of 2.1769713316208277; acc of 97.43589697740018%\n",
      "Train on 702 samples, validate on 78 samples\n",
      "Epoch 1/100\n",
      "702/702 [==============================] - 3s 5ms/step - loss: 2.4968 - acc: 0.2849 - val_loss: 2.3602 - val_acc: 0.3718\n",
      "Epoch 2/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.3016 - acc: 0.6068 - val_loss: 2.2711 - val_acc: 0.5641\n",
      "Epoch 3/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2454 - acc: 0.7564 - val_loss: 2.2543 - val_acc: 0.7051\n",
      "Epoch 4/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2174 - acc: 0.8348 - val_loss: 2.2071 - val_acc: 0.8590\n",
      "Epoch 5/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2020 - acc: 0.8860 - val_loss: 2.2850 - val_acc: 0.6410\n",
      "Epoch 6/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1968 - acc: 0.9174 - val_loss: 2.1973 - val_acc: 0.9103\n",
      "Epoch 7/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1886 - acc: 0.9345 - val_loss: 2.1997 - val_acc: 0.9103\n",
      "Epoch 8/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1815 - acc: 0.9601 - val_loss: 2.2034 - val_acc: 0.8205\n",
      "Epoch 9/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1855 - acc: 0.9402 - val_loss: 2.1799 - val_acc: 0.9103\n",
      "Epoch 10/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1765 - acc: 0.9715 - val_loss: 2.1847 - val_acc: 0.8846\n",
      "Epoch 11/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1783 - acc: 0.9601 - val_loss: 2.2170 - val_acc: 0.8462\n",
      "Epoch 12/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1767 - acc: 0.9729 - val_loss: 2.1829 - val_acc: 0.9487\n",
      "Epoch 13/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1783 - acc: 0.9615 - val_loss: 2.1853 - val_acc: 0.9359\n",
      "Epoch 14/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1724 - acc: 0.9772 - val_loss: 2.1803 - val_acc: 0.9487\n",
      "Epoch 15/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1735 - acc: 0.9843 - val_loss: 2.1729 - val_acc: 1.0000\n",
      "Epoch 16/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1703 - acc: 0.9972 - val_loss: 2.1756 - val_acc: 0.9487\n",
      "Epoch 17/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1725 - acc: 0.9815 - val_loss: 2.1723 - val_acc: 0.9872\n",
      "Epoch 18/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1693 - acc: 0.9957 - val_loss: 2.1820 - val_acc: 0.9487\n",
      "Epoch 19/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1742 - acc: 0.9786 - val_loss: 2.1723 - val_acc: 0.9744\n",
      "Epoch 20/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1687 - acc: 0.9957 - val_loss: 2.1805 - val_acc: 0.9359\n",
      "Epoch 21/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1742 - acc: 0.9772 - val_loss: 2.1756 - val_acc: 0.9744\n",
      "Epoch 22/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1683 - acc: 0.9972 - val_loss: 2.1764 - val_acc: 0.9615\n",
      "Epoch 23/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1685 - acc: 0.9943 - val_loss: 2.1730 - val_acc: 0.9872\n",
      "Epoch 24/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1684 - acc: 0.9943 - val_loss: 2.1768 - val_acc: 0.9872\n",
      "Epoch 25/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1690 - acc: 0.9972 - val_loss: 2.1709 - val_acc: 0.9744\n",
      "Epoch 26/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1677 - acc: 0.9972 - val_loss: 2.1770 - val_acc: 0.9615\n",
      "Epoch 27/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1683 - acc: 0.9972 - val_loss: 2.1713 - val_acc: 0.9744\n",
      "Epoch 28/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1679 - acc: 0.9986 - val_loss: 2.1704 - val_acc: 0.9872\n",
      "Epoch 29/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1668 - acc: 1.0000 - val_loss: 2.1700 - val_acc: 0.9872\n",
      "Epoch 30/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1670 - acc: 0.9986 - val_loss: 2.1723 - val_acc: 0.9744\n",
      "Epoch 31/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1670 - acc: 1.0000 - val_loss: 2.1713 - val_acc: 0.9872\n",
      "Epoch 32/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1669 - acc: 1.0000 - val_loss: 2.1704 - val_acc: 1.0000\n",
      "Epoch 33/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1679 - acc: 0.9986 - val_loss: 2.1709 - val_acc: 0.9744\n",
      "Epoch 34/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1668 - acc: 1.0000 - val_loss: 2.1703 - val_acc: 0.9744\n",
      "Epoch 35/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1669 - acc: 0.9943 - val_loss: 2.1702 - val_acc: 0.9744\n",
      "Epoch 36/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1669 - acc: 0.9986 - val_loss: 2.1722 - val_acc: 0.9615\n",
      "Epoch 37/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1666 - acc: 1.0000 - val_loss: 2.1704 - val_acc: 0.9872\n",
      "Epoch 38/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1697 - val_acc: 0.9872\n",
      "Epoch 39/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1669 - acc: 1.0000 - val_loss: 2.1708 - val_acc: 0.9744\n",
      "Epoch 40/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1699 - val_acc: 0.9872\n",
      "Epoch 41/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1698 - val_acc: 0.9744\n",
      "Epoch 42/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1695 - val_acc: 0.9872\n",
      "Epoch 43/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1711 - val_acc: 0.9872\n",
      "Epoch 44/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1720 - val_acc: 0.9744\n",
      "Epoch 45/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1667 - acc: 0.9986 - val_loss: 2.1706 - val_acc: 0.9744\n",
      "Epoch 46/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 0.9986 - val_loss: 2.1705 - val_acc: 0.9744\n",
      "Epoch 47/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 0.9972 - val_loss: 2.1693 - val_acc: 0.9744\n",
      "Epoch 48/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1698 - val_acc: 0.9744\n",
      "Epoch 49/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1700 - val_acc: 0.9744\n",
      "Epoch 50/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1724 - val_acc: 0.9744\n",
      "Epoch 51/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1694 - val_acc: 0.9872\n",
      "Epoch 52/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1692 - val_acc: 0.9744\n",
      "Epoch 53/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1708 - val_acc: 0.9744\n",
      "Epoch 54/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1666 - acc: 1.0000 - val_loss: 2.1699 - val_acc: 0.9744\n",
      "Epoch 55/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1698 - val_acc: 0.9744\n",
      "Epoch 56/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1698 - val_acc: 0.9744\n",
      "Epoch 57/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1697 - val_acc: 0.9744\n",
      "Epoch 58/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1697 - val_acc: 0.9744\n",
      "Epoch 59/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1702 - val_acc: 0.9744\n",
      "Epoch 60/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1694 - val_acc: 0.9744\n",
      "Epoch 61/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1695 - val_acc: 0.9872\n",
      "Epoch 62/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1701 - val_acc: 0.9744\n",
      "Epoch 63/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1696 - val_acc: 0.9872\n",
      "Epoch 64/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1701 - val_acc: 0.9744\n",
      "Epoch 65/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1697 - val_acc: 0.9744\n",
      "Epoch 66/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1697 - val_acc: 0.9744\n",
      "Epoch 67/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1696 - val_acc: 0.9744\n",
      "Epoch 68/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1698 - val_acc: 0.9744\n",
      "Epoch 69/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1692 - val_acc: 0.9744\n",
      "Epoch 70/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1695 - val_acc: 0.9872\n",
      "Epoch 71/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1698 - val_acc: 0.9744\n",
      "Epoch 72/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1696 - val_acc: 0.9744\n",
      "Epoch 73/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1698 - val_acc: 0.9872\n",
      "Epoch 74/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1696 - val_acc: 0.9744\n",
      "Epoch 75/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1699 - val_acc: 0.9744\n",
      "Epoch 76/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1693 - val_acc: 0.9872\n",
      "Epoch 77/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1694 - val_acc: 0.9744\n",
      "Epoch 78/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1695 - val_acc: 0.9872\n",
      "Epoch 79/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1694 - val_acc: 0.9744\n",
      "Epoch 80/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1694 - val_acc: 0.9872\n",
      "Epoch 81/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1695 - val_acc: 0.9872\n",
      "Epoch 82/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1700 - val_acc: 0.9744\n",
      "Epoch 83/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1696 - val_acc: 0.9872\n",
      "Epoch 84/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1698 - val_acc: 0.9744\n",
      "Epoch 85/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1696 - val_acc: 0.9744\n",
      "Epoch 86/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1697 - val_acc: 0.9872\n",
      "Epoch 87/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1694 - val_acc: 0.9744\n",
      "Epoch 88/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1696 - val_acc: 0.9744\n",
      "Epoch 89/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1697 - val_acc: 0.9744\n",
      "Epoch 90/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1694 - val_acc: 0.9872\n",
      "Epoch 91/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1694 - val_acc: 0.9744\n",
      "Epoch 92/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1697 - val_acc: 0.9744\n",
      "Epoch 93/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1696 - val_acc: 0.9744\n",
      "Epoch 94/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1694 - val_acc: 0.9872\n",
      "Epoch 95/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1694 - val_acc: 0.9744\n",
      "Epoch 96/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1694 - val_acc: 0.9872\n",
      "Epoch 97/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1695 - val_acc: 0.9872\n",
      "Epoch 98/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1696 - val_acc: 0.9744\n",
      "Epoch 99/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1696 - val_acc: 0.9744\n",
      "Epoch 100/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1694 - val_acc: 0.9872\n",
      "78/78 [==============================] - 0s 307us/step\n",
      "Score for fold 10: loss of 2.1693111688662796; acc of 98.71794871794873%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1296x1296 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m 1       \u001b[0m | \u001b[0m 98.08   \u001b[0m | \u001b[0m 0.9413  \u001b[0m | \u001b[0m 10.04   \u001b[0m |\n",
      "Train on 702 samples, validate on 78 samples\n",
      "Epoch 1/100\n",
      "702/702 [==============================] - 3s 5ms/step - loss: 2.4779 - acc: 0.3205 - val_loss: 2.3993 - val_acc: 0.3077\n",
      "Epoch 2/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2948 - acc: 0.6068 - val_loss: 2.2496 - val_acc: 0.7564\n",
      "Epoch 3/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2462 - acc: 0.7650 - val_loss: 2.2518 - val_acc: 0.7436\n",
      "Epoch 4/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2235 - acc: 0.8348 - val_loss: 2.2274 - val_acc: 0.8333\n",
      "Epoch 5/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2036 - acc: 0.8590 - val_loss: 2.2051 - val_acc: 0.8462\n",
      "Epoch 6/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1960 - acc: 0.9088 - val_loss: 2.2000 - val_acc: 0.8974\n",
      "Epoch 7/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1929 - acc: 0.9359 - val_loss: 2.2278 - val_acc: 0.7564\n",
      "Epoch 8/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1886 - acc: 0.9402 - val_loss: 2.2122 - val_acc: 0.8205\n",
      "Epoch 9/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1802 - acc: 0.9587 - val_loss: 2.1977 - val_acc: 0.8333\n",
      "Epoch 10/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1797 - acc: 0.9544 - val_loss: 2.2023 - val_acc: 0.8333\n",
      "Epoch 11/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1832 - acc: 0.9473 - val_loss: 2.1845 - val_acc: 0.9359\n",
      "Epoch 12/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1749 - acc: 0.9772 - val_loss: 2.1935 - val_acc: 0.8718\n",
      "Epoch 13/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1738 - acc: 0.9843 - val_loss: 2.1937 - val_acc: 0.9103\n",
      "Epoch 14/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1737 - acc: 0.9843 - val_loss: 2.1888 - val_acc: 0.8846\n",
      "Epoch 15/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1744 - acc: 0.9829 - val_loss: 2.1827 - val_acc: 0.9231\n",
      "Epoch 16/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1712 - acc: 0.9943 - val_loss: 2.1825 - val_acc: 0.9359\n",
      "Epoch 17/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1721 - acc: 0.9858 - val_loss: 2.1847 - val_acc: 0.9103\n",
      "Epoch 18/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1727 - acc: 0.9872 - val_loss: 2.1792 - val_acc: 0.9359\n",
      "Epoch 19/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1704 - acc: 0.9915 - val_loss: 2.1829 - val_acc: 0.9231\n",
      "Epoch 20/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1695 - acc: 0.9915 - val_loss: 2.1804 - val_acc: 0.9359\n",
      "Epoch 21/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1708 - acc: 0.9915 - val_loss: 2.1812 - val_acc: 0.9231\n",
      "Epoch 22/100\n",
      "702/702 [==============================] - 2s 2ms/step - loss: 2.1724 - acc: 0.9872 - val_loss: 2.1800 - val_acc: 0.9103\n",
      "Epoch 23/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1678 - acc: 0.9972 - val_loss: 2.1800 - val_acc: 0.9487\n",
      "Epoch 24/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1680 - acc: 0.9957 - val_loss: 2.1787 - val_acc: 0.9231\n",
      "Epoch 25/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1687 - acc: 0.9972 - val_loss: 2.1782 - val_acc: 0.9487\n",
      "Epoch 26/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1675 - acc: 0.9986 - val_loss: 2.1798 - val_acc: 0.9359\n",
      "Epoch 27/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1677 - acc: 0.9972 - val_loss: 2.1802 - val_acc: 0.9359\n",
      "Epoch 28/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1679 - acc: 0.9986 - val_loss: 2.1801 - val_acc: 0.9231\n",
      "Epoch 29/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1675 - acc: 0.9986 - val_loss: 2.1801 - val_acc: 0.9359\n",
      "Epoch 30/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1673 - acc: 0.9986 - val_loss: 2.1789 - val_acc: 0.9487\n",
      "Epoch 31/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1674 - acc: 0.9986 - val_loss: 2.1793 - val_acc: 0.9487\n",
      "Epoch 32/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1673 - acc: 0.9972 - val_loss: 2.1840 - val_acc: 0.9103\n",
      "Epoch 33/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1673 - acc: 0.9986 - val_loss: 2.1790 - val_acc: 0.9487\n",
      "Epoch 34/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1671 - acc: 0.9986 - val_loss: 2.1788 - val_acc: 0.9487\n",
      "Epoch 35/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1671 - acc: 1.0000 - val_loss: 2.1792 - val_acc: 0.9359\n",
      "Epoch 36/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1666 - acc: 1.0000 - val_loss: 2.1781 - val_acc: 0.9487\n",
      "Epoch 37/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1667 - acc: 1.0000 - val_loss: 2.1796 - val_acc: 0.9359\n",
      "Epoch 38/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1666 - acc: 1.0000 - val_loss: 2.1791 - val_acc: 0.9487\n",
      "Epoch 39/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1666 - acc: 1.0000 - val_loss: 2.1814 - val_acc: 0.9231\n",
      "Epoch 40/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1675 - acc: 0.9957 - val_loss: 2.1809 - val_acc: 0.9487\n",
      "Epoch 41/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1805 - val_acc: 0.9487\n",
      "Epoch 42/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1800 - val_acc: 0.9487\n",
      "Epoch 43/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1666 - acc: 1.0000 - val_loss: 2.1817 - val_acc: 0.9359\n",
      "Epoch 44/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1666 - acc: 1.0000 - val_loss: 2.1808 - val_acc: 0.9487\n",
      "Epoch 45/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1780 - val_acc: 0.9359\n",
      "Epoch 46/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1792 - val_acc: 0.9487\n",
      "Epoch 47/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1783 - val_acc: 0.9487\n",
      "Epoch 48/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1784 - val_acc: 0.9615\n",
      "Epoch 49/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1789 - val_acc: 0.9487\n",
      "Epoch 50/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1785 - val_acc: 0.9615\n",
      "Epoch 51/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1794 - val_acc: 0.9487\n",
      "Epoch 52/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1797 - val_acc: 0.9487\n",
      "Epoch 53/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1795 - val_acc: 0.9487\n",
      "Epoch 54/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1790 - val_acc: 0.9487\n",
      "Epoch 55/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1788 - val_acc: 0.9487\n",
      "Epoch 56/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1789 - val_acc: 0.9487\n",
      "Epoch 57/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1782 - val_acc: 0.9487\n",
      "Epoch 58/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1782 - val_acc: 0.9487\n",
      "Epoch 59/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1778 - val_acc: 0.9487\n",
      "Epoch 60/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1795 - val_acc: 0.9487\n",
      "Epoch 61/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1790 - val_acc: 0.9487\n",
      "Epoch 62/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1792 - val_acc: 0.9615\n",
      "Epoch 63/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1785 - val_acc: 0.9487\n",
      "Epoch 64/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1791 - val_acc: 0.9487\n",
      "Epoch 65/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1779 - val_acc: 0.9487\n",
      "Epoch 66/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1779 - val_acc: 0.9487\n",
      "Epoch 67/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1779 - val_acc: 0.9615\n",
      "Epoch 68/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1783 - val_acc: 0.9487\n",
      "Epoch 69/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1781 - val_acc: 0.9615\n",
      "Epoch 70/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1783 - val_acc: 0.9615\n",
      "Epoch 71/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1782 - val_acc: 0.9615\n",
      "Epoch 72/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1778 - val_acc: 0.9487\n",
      "Epoch 73/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1783 - val_acc: 0.9487\n",
      "Epoch 74/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1785 - val_acc: 0.9487\n",
      "Epoch 75/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1778 - val_acc: 0.9487\n",
      "Epoch 76/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1788 - val_acc: 0.9487\n",
      "Epoch 77/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1780 - val_acc: 0.9487\n",
      "Epoch 78/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1774 - val_acc: 0.9615\n",
      "Epoch 79/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1777 - val_acc: 0.9615\n",
      "Epoch 80/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1776 - val_acc: 0.9487\n",
      "Epoch 81/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1773 - val_acc: 0.9615\n",
      "Epoch 82/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1782 - val_acc: 0.9487\n",
      "Epoch 83/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1777 - val_acc: 0.9615\n",
      "Epoch 84/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1782 - val_acc: 0.9487\n",
      "Epoch 85/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1786 - val_acc: 0.9487\n",
      "Epoch 86/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1780 - val_acc: 0.9615\n",
      "Epoch 87/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1786 - val_acc: 0.9487\n",
      "Epoch 88/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1784 - val_acc: 0.9359\n",
      "Epoch 89/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1778 - val_acc: 0.9615\n",
      "Epoch 90/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1779 - val_acc: 0.9615\n",
      "Epoch 91/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1778 - val_acc: 0.9615\n",
      "Epoch 92/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1779 - val_acc: 0.9615\n",
      "Epoch 93/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1782 - val_acc: 0.9615\n",
      "Epoch 94/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1783 - val_acc: 0.9487\n",
      "Epoch 95/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1781 - val_acc: 0.9487\n",
      "Epoch 96/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1780 - val_acc: 0.9487\n",
      "Epoch 97/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1783 - val_acc: 0.9487\n",
      "Epoch 98/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1778 - val_acc: 0.9487\n",
      "Epoch 99/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1779 - val_acc: 0.9487\n",
      "Epoch 100/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1775 - val_acc: 0.9615\n",
      "78/78 [==============================] - 0s 308us/step\n",
      "Score for fold 1: loss of 2.1777605704772167; acc of 96.15384615384616%\n",
      "Train on 702 samples, validate on 78 samples\n",
      "Epoch 1/100\n",
      "702/702 [==============================] - 4s 5ms/step - loss: 2.5305 - acc: 0.2094 - val_loss: 2.4176 - val_acc: 0.3718\n",
      "Epoch 2/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.3335 - acc: 0.5556 - val_loss: 2.3178 - val_acc: 0.4872\n",
      "Epoch 3/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2607 - acc: 0.7450 - val_loss: 2.2408 - val_acc: 0.7821\n",
      "Epoch 4/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2235 - acc: 0.8362 - val_loss: 2.2636 - val_acc: 0.6538\n",
      "Epoch 5/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2110 - acc: 0.8632 - val_loss: 2.2173 - val_acc: 0.8462\n",
      "Epoch 6/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2067 - acc: 0.8618 - val_loss: 2.2141 - val_acc: 0.8205\n",
      "Epoch 7/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1903 - acc: 0.9217 - val_loss: 2.1945 - val_acc: 0.8974\n",
      "Epoch 8/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1816 - acc: 0.9587 - val_loss: 2.2345 - val_acc: 0.7436\n",
      "Epoch 9/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1840 - acc: 0.9416 - val_loss: 2.1858 - val_acc: 0.9103\n",
      "Epoch 10/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1819 - acc: 0.9544 - val_loss: 2.1927 - val_acc: 0.8846\n",
      "Epoch 11/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1811 - acc: 0.9715 - val_loss: 2.1920 - val_acc: 0.8846\n",
      "Epoch 12/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1761 - acc: 0.9815 - val_loss: 2.2033 - val_acc: 0.9487\n",
      "Epoch 13/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1781 - acc: 0.9786 - val_loss: 2.1896 - val_acc: 0.9231\n",
      "Epoch 14/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1751 - acc: 0.9858 - val_loss: 2.1963 - val_acc: 0.9103\n",
      "Epoch 15/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1786 - acc: 0.9701 - val_loss: 2.1775 - val_acc: 0.9487\n",
      "Epoch 16/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1702 - acc: 0.9957 - val_loss: 2.1833 - val_acc: 0.8718\n",
      "Epoch 17/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1746 - acc: 0.9815 - val_loss: 2.1781 - val_acc: 0.9487\n",
      "Epoch 18/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1698 - acc: 0.9943 - val_loss: 2.1737 - val_acc: 0.9744\n",
      "Epoch 19/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1744 - acc: 0.9858 - val_loss: 2.1742 - val_acc: 0.9744\n",
      "Epoch 20/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1702 - acc: 0.9943 - val_loss: 2.1745 - val_acc: 0.9872\n",
      "Epoch 21/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1697 - acc: 0.9957 - val_loss: 2.1727 - val_acc: 1.0000\n",
      "Epoch 22/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1705 - acc: 0.9929 - val_loss: 2.1784 - val_acc: 0.9744\n",
      "Epoch 23/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1705 - acc: 0.9929 - val_loss: 2.1737 - val_acc: 0.9872\n",
      "Epoch 24/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1709 - acc: 0.9915 - val_loss: 2.1759 - val_acc: 0.9872\n",
      "Epoch 25/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1683 - acc: 0.9972 - val_loss: 2.1753 - val_acc: 0.9872\n",
      "Epoch 26/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1694 - acc: 0.9972 - val_loss: 2.1735 - val_acc: 0.9744\n",
      "Epoch 27/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1699 - acc: 0.9900 - val_loss: 2.1758 - val_acc: 0.9487\n",
      "Epoch 28/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1675 - acc: 0.9986 - val_loss: 2.1753 - val_acc: 0.9744\n",
      "Epoch 29/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1695 - acc: 0.9900 - val_loss: 2.1976 - val_acc: 0.9359\n",
      "Epoch 30/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1693 - acc: 0.9957 - val_loss: 2.1860 - val_acc: 0.9487\n",
      "Epoch 31/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1680 - acc: 0.9986 - val_loss: 2.1717 - val_acc: 0.9744\n",
      "Epoch 32/100\n",
      "702/702 [==============================] - 2s 2ms/step - loss: 2.1691 - acc: 0.9972 - val_loss: 2.1899 - val_acc: 0.9359\n",
      "Epoch 33/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1673 - acc: 0.9986 - val_loss: 2.1711 - val_acc: 0.9615\n",
      "Epoch 34/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1668 - acc: 1.0000 - val_loss: 2.1719 - val_acc: 0.9744\n",
      "Epoch 35/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1670 - acc: 1.0000 - val_loss: 2.1739 - val_acc: 0.9744\n",
      "Epoch 36/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1669 - acc: 1.0000 - val_loss: 2.1723 - val_acc: 0.9744\n",
      "Epoch 37/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1668 - acc: 1.0000 - val_loss: 2.1752 - val_acc: 0.9744\n",
      "Epoch 38/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1667 - acc: 1.0000 - val_loss: 2.1725 - val_acc: 0.9744\n",
      "Epoch 39/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1667 - acc: 1.0000 - val_loss: 2.1720 - val_acc: 0.9872\n",
      "Epoch 40/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1669 - acc: 0.9972 - val_loss: 2.1707 - val_acc: 0.9615\n",
      "Epoch 41/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1704 - val_acc: 0.9744\n",
      "Epoch 42/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1675 - acc: 0.9986 - val_loss: 2.1698 - val_acc: 0.9872\n",
      "Epoch 43/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1706 - val_acc: 0.9744\n",
      "Epoch 44/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1707 - val_acc: 0.9615\n",
      "Epoch 45/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1705 - val_acc: 0.9872\n",
      "Epoch 46/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1666 - acc: 1.0000 - val_loss: 2.1700 - val_acc: 0.9872\n",
      "Epoch 47/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1702 - val_acc: 0.9744\n",
      "Epoch 48/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1706 - val_acc: 0.9744\n",
      "Epoch 49/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1701 - val_acc: 0.9744\n",
      "Epoch 50/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1686 - val_acc: 0.9872\n",
      "Epoch 51/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1692 - val_acc: 0.9872\n",
      "Epoch 52/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1687 - val_acc: 1.0000\n",
      "Epoch 53/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1693 - val_acc: 0.9872\n",
      "Epoch 54/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1690 - val_acc: 0.9872\n",
      "Epoch 55/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1704 - val_acc: 0.9872\n",
      "Epoch 56/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1690 - val_acc: 1.0000\n",
      "Epoch 57/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1699 - val_acc: 0.9872\n",
      "Epoch 58/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1688 - val_acc: 1.0000\n",
      "Epoch 59/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1697 - val_acc: 0.9872\n",
      "Epoch 60/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1692 - val_acc: 1.0000\n",
      "Epoch 61/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1692 - val_acc: 1.0000\n",
      "Epoch 62/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1687 - val_acc: 1.0000\n",
      "Epoch 63/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1682 - val_acc: 1.0000\n",
      "Epoch 64/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1686 - val_acc: 1.0000\n",
      "Epoch 65/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1697 - val_acc: 0.9744\n",
      "Epoch 66/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1689 - val_acc: 0.9872\n",
      "Epoch 67/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1690 - val_acc: 1.0000\n",
      "Epoch 68/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1691 - val_acc: 0.9872\n",
      "Epoch 69/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1687 - val_acc: 1.0000\n",
      "Epoch 70/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1688 - val_acc: 0.9872\n",
      "Epoch 71/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1693 - val_acc: 0.9872\n",
      "Epoch 72/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1685 - val_acc: 1.0000\n",
      "Epoch 73/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1689 - val_acc: 1.0000\n",
      "Epoch 74/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1691 - val_acc: 1.0000\n",
      "Epoch 75/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1691 - val_acc: 1.0000\n",
      "Epoch 76/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1686 - val_acc: 1.0000\n",
      "Epoch 77/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1687 - val_acc: 1.0000\n",
      "Epoch 78/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1687 - val_acc: 1.0000\n",
      "Epoch 79/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1705 - val_acc: 0.9872\n",
      "Epoch 80/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1698 - val_acc: 0.9872\n",
      "Epoch 81/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1690 - val_acc: 1.0000\n",
      "Epoch 82/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1695 - val_acc: 0.9872\n",
      "Epoch 83/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1699 - val_acc: 0.9872\n",
      "Epoch 84/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1704 - val_acc: 0.9872\n",
      "Epoch 85/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1704 - val_acc: 0.9872\n",
      "Epoch 86/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1690 - val_acc: 1.0000\n",
      "Epoch 87/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1694 - val_acc: 1.0000\n",
      "Epoch 88/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1695 - val_acc: 1.0000\n",
      "Epoch 89/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1694 - val_acc: 1.0000\n",
      "Epoch 90/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1692 - val_acc: 1.0000\n",
      "Epoch 91/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1692 - val_acc: 1.0000\n",
      "Epoch 92/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1691 - val_acc: 1.0000\n",
      "Epoch 93/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1691 - val_acc: 1.0000\n",
      "Epoch 94/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1691 - val_acc: 1.0000\n",
      "Epoch 95/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1694 - val_acc: 1.0000\n",
      "Epoch 96/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1689 - val_acc: 1.0000\n",
      "Epoch 97/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1690 - val_acc: 1.0000\n",
      "Epoch 98/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1694 - val_acc: 1.0000\n",
      "Epoch 99/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1695 - val_acc: 1.0000\n",
      "Epoch 100/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1697 - val_acc: 1.0000\n",
      "78/78 [==============================] - 0s 319us/step\n",
      "Score for fold 2: loss of 2.1698933870364456; acc of 100.0%\n",
      "Train on 702 samples, validate on 78 samples\n",
      "Epoch 1/100\n",
      "702/702 [==============================] - 4s 6ms/step - loss: 2.4536 - acc: 0.3276 - val_loss: 2.3728 - val_acc: 0.3974\n",
      "Epoch 2/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.3064 - acc: 0.5726 - val_loss: 2.2989 - val_acc: 0.5769\n",
      "Epoch 3/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2586 - acc: 0.7108 - val_loss: 2.2346 - val_acc: 0.8333\n",
      "Epoch 4/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2206 - acc: 0.8319 - val_loss: 2.2296 - val_acc: 0.7949\n",
      "Epoch 5/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2117 - acc: 0.8704 - val_loss: 2.2385 - val_acc: 0.7308\n",
      "Epoch 6/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2008 - acc: 0.8960 - val_loss: 2.1914 - val_acc: 0.9103\n",
      "Epoch 7/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1904 - acc: 0.9259 - val_loss: 2.1851 - val_acc: 0.9359\n",
      "Epoch 8/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1876 - acc: 0.9373 - val_loss: 2.1805 - val_acc: 0.9744\n",
      "Epoch 9/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1823 - acc: 0.9544 - val_loss: 2.1862 - val_acc: 0.9615\n",
      "Epoch 10/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1792 - acc: 0.9758 - val_loss: 2.1812 - val_acc: 0.9615\n",
      "Epoch 11/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1750 - acc: 0.9772 - val_loss: 2.2021 - val_acc: 0.9103\n",
      "Epoch 12/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1756 - acc: 0.9829 - val_loss: 2.1958 - val_acc: 0.8846\n",
      "Epoch 13/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1771 - acc: 0.9687 - val_loss: 2.1831 - val_acc: 0.9359\n",
      "Epoch 14/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1738 - acc: 0.9872 - val_loss: 2.1773 - val_acc: 0.9872\n",
      "Epoch 15/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1727 - acc: 0.9929 - val_loss: 2.1792 - val_acc: 0.9872\n",
      "Epoch 16/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1707 - acc: 0.9915 - val_loss: 2.1732 - val_acc: 0.9872\n",
      "Epoch 17/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1749 - acc: 0.9801 - val_loss: 2.1825 - val_acc: 0.9487\n",
      "Epoch 18/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1745 - acc: 0.9843 - val_loss: 2.1793 - val_acc: 0.9872\n",
      "Epoch 19/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1698 - acc: 0.9900 - val_loss: 2.1813 - val_acc: 0.9744\n",
      "Epoch 20/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1771 - acc: 0.9829 - val_loss: 2.1743 - val_acc: 0.9872\n",
      "Epoch 21/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1701 - acc: 0.9872 - val_loss: 2.1758 - val_acc: 1.0000\n",
      "Epoch 22/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1704 - acc: 0.9957 - val_loss: 2.1873 - val_acc: 0.9615\n",
      "Epoch 23/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1736 - acc: 0.9900 - val_loss: 2.1785 - val_acc: 0.9744\n",
      "Epoch 24/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1693 - acc: 0.9972 - val_loss: 2.1817 - val_acc: 0.9744\n",
      "Epoch 25/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1687 - acc: 0.9943 - val_loss: 2.1788 - val_acc: 0.9744\n",
      "Epoch 26/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1718 - acc: 0.9886 - val_loss: 2.1812 - val_acc: 1.0000\n",
      "Epoch 27/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1682 - acc: 0.9957 - val_loss: 2.1708 - val_acc: 1.0000\n",
      "Epoch 28/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1701 - acc: 0.9900 - val_loss: 2.1873 - val_acc: 0.9615\n",
      "Epoch 29/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1685 - acc: 0.9943 - val_loss: 2.1756 - val_acc: 0.9872\n",
      "Epoch 30/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1681 - acc: 0.9957 - val_loss: 2.1791 - val_acc: 1.0000\n",
      "Epoch 31/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1701 - acc: 0.9915 - val_loss: 2.1734 - val_acc: 0.9872\n",
      "Epoch 32/100\n",
      "702/702 [==============================] - 2s 2ms/step - loss: 2.1686 - acc: 0.9972 - val_loss: 2.1741 - val_acc: 1.0000\n",
      "Epoch 33/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1672 - acc: 0.9986 - val_loss: 2.1724 - val_acc: 0.9872\n",
      "Epoch 34/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1670 - acc: 0.9986 - val_loss: 2.1703 - val_acc: 0.9872\n",
      "Epoch 35/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1667 - acc: 1.0000 - val_loss: 2.1740 - val_acc: 0.9872\n",
      "Epoch 36/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1670 - acc: 1.0000 - val_loss: 2.1750 - val_acc: 0.9615\n",
      "Epoch 37/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1669 - acc: 0.9986 - val_loss: 2.1726 - val_acc: 1.0000\n",
      "Epoch 38/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1668 - acc: 1.0000 - val_loss: 2.1771 - val_acc: 1.0000\n",
      "Epoch 39/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1667 - acc: 1.0000 - val_loss: 2.1732 - val_acc: 1.0000\n",
      "Epoch 40/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1668 - acc: 0.9986 - val_loss: 2.1729 - val_acc: 0.9872\n",
      "Epoch 41/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1667 - acc: 1.0000 - val_loss: 2.1781 - val_acc: 1.0000\n",
      "Epoch 42/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1668 - acc: 0.9986 - val_loss: 2.1824 - val_acc: 0.9872\n",
      "Epoch 43/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1727 - val_acc: 0.9872\n",
      "Epoch 44/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1716 - val_acc: 1.0000\n",
      "Epoch 45/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1712 - val_acc: 1.0000\n",
      "Epoch 46/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1668 - acc: 0.9986 - val_loss: 2.1715 - val_acc: 1.0000\n",
      "Epoch 47/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1752 - val_acc: 1.0000\n",
      "Epoch 48/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1666 - acc: 0.9972 - val_loss: 2.1717 - val_acc: 1.0000\n",
      "Epoch 49/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1742 - val_acc: 1.0000\n",
      "Epoch 50/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1715 - val_acc: 1.0000\n",
      "Epoch 51/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1731 - val_acc: 1.0000\n",
      "Epoch 52/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1741 - val_acc: 1.0000\n",
      "Epoch 53/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1725 - val_acc: 1.0000\n",
      "Epoch 54/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1759 - val_acc: 1.0000\n",
      "Epoch 55/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1737 - val_acc: 1.0000\n",
      "Epoch 56/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1743 - val_acc: 1.0000\n",
      "Epoch 57/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1772 - val_acc: 0.9872\n",
      "Epoch 58/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 0.9986 - val_loss: 2.1711 - val_acc: 1.0000\n",
      "Epoch 59/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1715 - val_acc: 1.0000\n",
      "Epoch 60/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1728 - val_acc: 1.0000\n",
      "Epoch 61/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1742 - val_acc: 0.9872\n",
      "Epoch 62/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1755 - val_acc: 1.0000\n",
      "Epoch 63/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1763 - val_acc: 1.0000\n",
      "Epoch 64/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1765 - val_acc: 1.0000\n",
      "Epoch 65/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1743 - val_acc: 1.0000\n",
      "Epoch 66/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1749 - val_acc: 1.0000\n",
      "Epoch 67/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1772 - val_acc: 1.0000\n",
      "Epoch 68/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1776 - val_acc: 1.0000\n",
      "Epoch 69/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1754 - val_acc: 1.0000\n",
      "Epoch 70/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1761 - val_acc: 1.0000\n",
      "Epoch 71/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1760 - val_acc: 1.0000\n",
      "Epoch 72/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1778 - val_acc: 1.0000\n",
      "Epoch 73/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1779 - val_acc: 1.0000\n",
      "Epoch 74/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1669 - acc: 0.9986 - val_loss: 2.1726 - val_acc: 1.0000\n",
      "Epoch 75/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1730 - val_acc: 1.0000\n",
      "Epoch 76/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1742 - val_acc: 1.0000\n",
      "Epoch 77/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1758 - val_acc: 1.0000\n",
      "Epoch 78/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1763 - val_acc: 1.0000\n",
      "Epoch 79/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1764 - val_acc: 1.0000\n",
      "Epoch 80/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1765 - val_acc: 1.0000\n",
      "Epoch 81/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1760 - val_acc: 1.0000\n",
      "Epoch 82/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1763 - val_acc: 1.0000\n",
      "Epoch 83/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1769 - val_acc: 1.0000\n",
      "Epoch 84/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1768 - val_acc: 1.0000\n",
      "Epoch 85/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1774 - val_acc: 1.0000\n",
      "Epoch 86/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1767 - val_acc: 1.0000\n",
      "Epoch 87/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1771 - val_acc: 1.0000\n",
      "Epoch 88/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1774 - val_acc: 1.0000\n",
      "Epoch 89/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1776 - val_acc: 1.0000\n",
      "Epoch 90/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1766 - val_acc: 1.0000\n",
      "Epoch 91/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1778 - val_acc: 1.0000\n",
      "Epoch 92/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1769 - val_acc: 1.0000\n",
      "Epoch 93/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1771 - val_acc: 1.0000\n",
      "Epoch 94/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1766 - val_acc: 1.0000\n",
      "Epoch 95/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1764 - val_acc: 1.0000\n",
      "Epoch 96/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1760 - val_acc: 1.0000\n",
      "Epoch 97/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1763 - val_acc: 1.0000\n",
      "Epoch 98/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1764 - val_acc: 1.0000\n",
      "Epoch 99/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1760 - val_acc: 1.0000\n",
      "Epoch 100/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1765 - val_acc: 1.0000\n",
      "78/78 [==============================] - 0s 294us/step\n",
      "Score for fold 3: loss of 2.17662282479115; acc of 100.0%\n",
      "Train on 702 samples, validate on 78 samples\n",
      "Epoch 1/100\n",
      "702/702 [==============================] - 4s 6ms/step - loss: 2.4993 - acc: 0.2650 - val_loss: 2.4287 - val_acc: 0.3333\n",
      "Epoch 2/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "702/702 [==============================] - 1s 1ms/step - loss: 2.3028 - acc: 0.5954 - val_loss: 2.2744 - val_acc: 0.6154\n",
      "Epoch 3/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2412 - acc: 0.7422 - val_loss: 2.2337 - val_acc: 0.8205\n",
      "Epoch 4/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2217 - acc: 0.8305 - val_loss: 2.2478 - val_acc: 0.8077\n",
      "Epoch 5/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1997 - acc: 0.8875 - val_loss: 2.2252 - val_acc: 0.7308\n",
      "Epoch 6/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1992 - acc: 0.8889 - val_loss: 2.1869 - val_acc: 0.8974\n",
      "Epoch 7/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1857 - acc: 0.9402 - val_loss: 2.1822 - val_acc: 0.9487\n",
      "Epoch 8/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1869 - acc: 0.9387 - val_loss: 2.1779 - val_acc: 0.9359\n",
      "Epoch 9/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1786 - acc: 0.9516 - val_loss: 2.2307 - val_acc: 0.7564\n",
      "Epoch 10/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1786 - acc: 0.9587 - val_loss: 2.1876 - val_acc: 0.9615\n",
      "Epoch 11/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1782 - acc: 0.9672 - val_loss: 2.2001 - val_acc: 0.9103\n",
      "Epoch 12/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1764 - acc: 0.9815 - val_loss: 2.1742 - val_acc: 0.9231\n",
      "Epoch 13/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1735 - acc: 0.9872 - val_loss: 2.2019 - val_acc: 0.8846\n",
      "Epoch 14/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1719 - acc: 0.9843 - val_loss: 2.1759 - val_acc: 0.9744\n",
      "Epoch 15/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1763 - acc: 0.9715 - val_loss: 2.1739 - val_acc: 0.9615\n",
      "Epoch 16/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1703 - acc: 0.9915 - val_loss: 2.1731 - val_acc: 0.9615\n",
      "Epoch 17/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1715 - acc: 0.9858 - val_loss: 2.1788 - val_acc: 0.8846\n",
      "Epoch 18/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1707 - acc: 0.9929 - val_loss: 2.1752 - val_acc: 0.9487\n",
      "Epoch 19/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1699 - acc: 0.9915 - val_loss: 2.2074 - val_acc: 0.8974\n",
      "Epoch 20/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1713 - acc: 0.9929 - val_loss: 2.1731 - val_acc: 0.9231\n",
      "Epoch 21/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1685 - acc: 0.9957 - val_loss: 2.1759 - val_acc: 0.9615\n",
      "Epoch 22/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1715 - acc: 0.9886 - val_loss: 2.1705 - val_acc: 0.9744\n",
      "Epoch 23/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1685 - acc: 0.9943 - val_loss: 2.1736 - val_acc: 0.9744\n",
      "Epoch 24/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1685 - acc: 0.9957 - val_loss: 2.1694 - val_acc: 0.9872\n",
      "Epoch 25/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1698 - acc: 0.9957 - val_loss: 2.1702 - val_acc: 0.9872\n",
      "Epoch 26/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1681 - acc: 0.9972 - val_loss: 2.1702 - val_acc: 0.9744\n",
      "Epoch 27/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1697 - acc: 0.9886 - val_loss: 2.1850 - val_acc: 0.9615\n",
      "Epoch 28/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1682 - acc: 0.9986 - val_loss: 2.1695 - val_acc: 0.9872\n",
      "Epoch 29/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1683 - acc: 0.9972 - val_loss: 2.1723 - val_acc: 0.9744\n",
      "Epoch 30/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1692 - acc: 0.9957 - val_loss: 2.1695 - val_acc: 0.9872\n",
      "Epoch 31/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1690 - acc: 0.9915 - val_loss: 2.1711 - val_acc: 0.9744\n",
      "Epoch 32/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1674 - acc: 0.9986 - val_loss: 2.1686 - val_acc: 0.9872\n",
      "Epoch 33/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1671 - acc: 0.9986 - val_loss: 2.1691 - val_acc: 0.9872\n",
      "Epoch 34/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1685 - acc: 0.9957 - val_loss: 2.1709 - val_acc: 0.9744\n",
      "Epoch 35/100\n",
      "702/702 [==============================] - 2s 2ms/step - loss: 2.1681 - acc: 0.9972 - val_loss: 2.1700 - val_acc: 0.9872\n",
      "Epoch 36/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1671 - acc: 0.9986 - val_loss: 2.1707 - val_acc: 0.9872\n",
      "Epoch 37/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1670 - acc: 0.9986 - val_loss: 2.1690 - val_acc: 0.9872\n",
      "Epoch 38/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1668 - acc: 0.9972 - val_loss: 2.1704 - val_acc: 0.9487\n",
      "Epoch 39/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1668 - acc: 1.0000 - val_loss: 2.1688 - val_acc: 0.9872\n",
      "Epoch 40/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1667 - acc: 1.0000 - val_loss: 2.1684 - val_acc: 0.9872\n",
      "Epoch 41/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1668 - acc: 1.0000 - val_loss: 2.1739 - val_acc: 0.9103\n",
      "Epoch 42/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1669 - acc: 0.9972 - val_loss: 2.1719 - val_acc: 0.9744\n",
      "Epoch 43/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1666 - acc: 1.0000 - val_loss: 2.1691 - val_acc: 0.9744\n",
      "Epoch 44/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1669 - acc: 0.9986 - val_loss: 2.1695 - val_acc: 0.9872\n",
      "Epoch 45/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1666 - acc: 1.0000 - val_loss: 2.1687 - val_acc: 0.9872\n",
      "Epoch 46/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1690 - val_acc: 0.9872\n",
      "Epoch 47/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1694 - val_acc: 0.9744\n",
      "Epoch 48/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1683 - val_acc: 0.9872\n",
      "Epoch 49/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1675 - val_acc: 1.0000\n",
      "Epoch 50/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1679 - val_acc: 1.0000\n",
      "Epoch 51/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1684 - val_acc: 0.9872\n",
      "Epoch 52/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1681 - val_acc: 1.0000\n",
      "Epoch 53/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1681 - val_acc: 0.9872\n",
      "Epoch 54/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1696 - val_acc: 0.9872\n",
      "Epoch 55/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1681 - val_acc: 0.9872\n",
      "Epoch 56/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1687 - val_acc: 0.9744\n",
      "Epoch 57/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1692 - val_acc: 0.9615\n",
      "Epoch 58/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 0.9986 - val_loss: 2.1689 - val_acc: 0.9744\n",
      "Epoch 59/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1679 - val_acc: 0.9872\n",
      "Epoch 60/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1680 - val_acc: 0.9872\n",
      "Epoch 61/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1678 - val_acc: 0.9872\n",
      "Epoch 62/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1684 - val_acc: 0.9744\n",
      "Epoch 63/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1675 - val_acc: 0.9872\n",
      "Epoch 64/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1681 - val_acc: 0.9872\n",
      "Epoch 65/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1676 - val_acc: 0.9872\n",
      "Epoch 66/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1682 - val_acc: 0.9872\n",
      "Epoch 67/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1684 - val_acc: 0.9872\n",
      "Epoch 68/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1680 - val_acc: 0.9872\n",
      "Epoch 69/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1678 - val_acc: 0.9872\n",
      "Epoch 70/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1711 - val_acc: 0.9872\n",
      "Epoch 71/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1680 - val_acc: 0.9872\n",
      "Epoch 72/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1682 - val_acc: 0.9872\n",
      "Epoch 73/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1681 - val_acc: 0.9872\n",
      "Epoch 74/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1679 - val_acc: 0.9872\n",
      "Epoch 75/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1676 - val_acc: 0.9872\n",
      "Epoch 76/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1685 - val_acc: 0.9872\n",
      "Epoch 77/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1676 - val_acc: 0.9872\n",
      "Epoch 78/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1679 - val_acc: 0.9872\n",
      "Epoch 79/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1677 - val_acc: 0.9872\n",
      "Epoch 80/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1679 - val_acc: 0.9872\n",
      "Epoch 81/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1678 - val_acc: 0.9872\n",
      "Epoch 82/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1680 - val_acc: 0.9872\n",
      "Epoch 83/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1678 - val_acc: 0.9872\n",
      "Epoch 84/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1677 - val_acc: 0.9872\n",
      "Epoch 85/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1680 - val_acc: 0.9872\n",
      "Epoch 86/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1680 - val_acc: 0.9872\n",
      "Epoch 87/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1677 - val_acc: 0.9872\n",
      "Epoch 88/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1679 - val_acc: 0.9872\n",
      "Epoch 89/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1678 - val_acc: 0.9872\n",
      "Epoch 90/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1678 - val_acc: 0.9872\n",
      "Epoch 91/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1678 - val_acc: 0.9872\n",
      "Epoch 92/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1678 - val_acc: 0.9872\n",
      "Epoch 93/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1678 - val_acc: 0.9872\n",
      "Epoch 94/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1678 - val_acc: 0.9872\n",
      "Epoch 95/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1678 - val_acc: 0.9872\n",
      "Epoch 96/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1676 - val_acc: 0.9872\n",
      "Epoch 97/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1678 - val_acc: 0.9872\n",
      "Epoch 98/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1679 - val_acc: 0.9872\n",
      "Epoch 99/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1677 - val_acc: 0.9872\n",
      "Epoch 100/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1677 - val_acc: 0.9872\n",
      "78/78 [==============================] - 0s 315us/step\n",
      "Score for fold 4: loss of 2.167957678819314; acc of 98.71794825945145%\n",
      "Train on 702 samples, validate on 78 samples\n",
      "Epoch 1/100\n",
      "702/702 [==============================] - 4s 6ms/step - loss: 2.4879 - acc: 0.2821 - val_loss: 2.4253 - val_acc: 0.2564\n",
      "Epoch 2/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2939 - acc: 0.5726 - val_loss: 2.2850 - val_acc: 0.6667\n",
      "Epoch 3/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2492 - acc: 0.6923 - val_loss: 2.2497 - val_acc: 0.7308\n",
      "Epoch 4/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2121 - acc: 0.8519 - val_loss: 2.2409 - val_acc: 0.6667\n",
      "Epoch 5/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2024 - acc: 0.8803 - val_loss: 2.2107 - val_acc: 0.8333\n",
      "Epoch 6/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1879 - acc: 0.9302 - val_loss: 2.1945 - val_acc: 0.8590\n",
      "Epoch 7/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1953 - acc: 0.9074 - val_loss: 2.1946 - val_acc: 0.8718\n",
      "Epoch 8/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1825 - acc: 0.9530 - val_loss: 2.1902 - val_acc: 0.9359\n",
      "Epoch 9/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1802 - acc: 0.9601 - val_loss: 2.2380 - val_acc: 0.8590\n",
      "Epoch 10/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1782 - acc: 0.9729 - val_loss: 2.1950 - val_acc: 0.9103\n",
      "Epoch 11/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1771 - acc: 0.9772 - val_loss: 2.1889 - val_acc: 0.9359\n",
      "Epoch 12/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1770 - acc: 0.9701 - val_loss: 2.1814 - val_acc: 0.9744\n",
      "Epoch 13/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1724 - acc: 0.9886 - val_loss: 2.1821 - val_acc: 0.9615\n",
      "Epoch 14/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1730 - acc: 0.9843 - val_loss: 2.1869 - val_acc: 0.9359\n",
      "Epoch 15/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1713 - acc: 0.9900 - val_loss: 2.2067 - val_acc: 0.8718\n",
      "Epoch 16/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1755 - acc: 0.9701 - val_loss: 2.1739 - val_acc: 0.9744\n",
      "Epoch 17/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1726 - acc: 0.9900 - val_loss: 2.1753 - val_acc: 1.0000\n",
      "Epoch 18/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1706 - acc: 0.9886 - val_loss: 2.1822 - val_acc: 0.9872\n",
      "Epoch 19/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1715 - acc: 0.9900 - val_loss: 2.2082 - val_acc: 0.8974\n",
      "Epoch 20/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1708 - acc: 0.9900 - val_loss: 2.1843 - val_acc: 0.9487\n",
      "Epoch 21/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1691 - acc: 0.9957 - val_loss: 2.1726 - val_acc: 0.9744\n",
      "Epoch 22/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1704 - acc: 0.9843 - val_loss: 2.1749 - val_acc: 0.9872\n",
      "Epoch 23/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1684 - acc: 0.9986 - val_loss: 2.1744 - val_acc: 0.9872\n",
      "Epoch 24/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1696 - acc: 0.9929 - val_loss: 2.1738 - val_acc: 0.9872\n",
      "Epoch 25/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1692 - acc: 0.9900 - val_loss: 2.1726 - val_acc: 0.9872\n",
      "Epoch 26/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1709 - acc: 0.9886 - val_loss: 2.1715 - val_acc: 1.0000\n",
      "Epoch 27/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1679 - acc: 0.9972 - val_loss: 2.1733 - val_acc: 0.9872\n",
      "Epoch 28/100\n",
      "702/702 [==============================] - 2s 3ms/step - loss: 2.1705 - acc: 0.9929 - val_loss: 2.1732 - val_acc: 0.9872\n",
      "Epoch 29/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1673 - acc: 0.9986 - val_loss: 2.1717 - val_acc: 0.9872\n",
      "Epoch 30/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1678 - acc: 0.9957 - val_loss: 2.1765 - val_acc: 0.9744\n",
      "Epoch 31/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1675 - acc: 0.9943 - val_loss: 2.1715 - val_acc: 0.9872\n",
      "Epoch 32/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1678 - acc: 0.9972 - val_loss: 2.1736 - val_acc: 1.0000\n",
      "Epoch 33/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1674 - acc: 0.9972 - val_loss: 2.1725 - val_acc: 0.9744\n",
      "Epoch 34/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1672 - acc: 1.0000 - val_loss: 2.1731 - val_acc: 0.9872\n",
      "Epoch 35/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1673 - acc: 0.9972 - val_loss: 2.1723 - val_acc: 0.9872\n",
      "Epoch 36/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1670 - acc: 0.9957 - val_loss: 2.1714 - val_acc: 0.9872\n",
      "Epoch 37/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1669 - acc: 0.9986 - val_loss: 2.1705 - val_acc: 1.0000\n",
      "Epoch 38/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1669 - acc: 1.0000 - val_loss: 2.1725 - val_acc: 0.9872\n",
      "Epoch 39/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1666 - acc: 1.0000 - val_loss: 2.1723 - val_acc: 0.9744\n",
      "Epoch 40/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1672 - acc: 0.9986 - val_loss: 2.1710 - val_acc: 1.0000\n",
      "Epoch 41/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1698 - val_acc: 1.0000\n",
      "Epoch 42/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1696 - val_acc: 1.0000\n",
      "Epoch 43/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1691 - val_acc: 1.0000\n",
      "Epoch 44/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1711 - val_acc: 1.0000\n",
      "Epoch 45/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1692 - val_acc: 1.0000\n",
      "Epoch 46/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1689 - val_acc: 1.0000\n",
      "Epoch 47/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1666 - acc: 0.9972 - val_loss: 2.1693 - val_acc: 1.0000\n",
      "Epoch 48/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1690 - val_acc: 0.9872\n",
      "Epoch 49/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1689 - val_acc: 1.0000\n",
      "Epoch 50/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1687 - val_acc: 1.0000\n",
      "Epoch 51/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1688 - val_acc: 0.9872\n",
      "Epoch 52/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1692 - val_acc: 1.0000\n",
      "Epoch 53/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1695 - val_acc: 1.0000\n",
      "Epoch 54/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1693 - val_acc: 1.0000\n",
      "Epoch 55/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1695 - val_acc: 1.0000\n",
      "Epoch 56/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1703 - val_acc: 1.0000\n",
      "Epoch 57/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 0.9986 - val_loss: 2.1703 - val_acc: 0.9872\n",
      "Epoch 58/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1689 - val_acc: 0.9872\n",
      "Epoch 59/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1688 - val_acc: 1.0000\n",
      "Epoch 60/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1688 - val_acc: 0.9872\n",
      "Epoch 61/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1695 - val_acc: 1.0000\n",
      "Epoch 62/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1682 - val_acc: 0.9872\n",
      "Epoch 63/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1693 - val_acc: 1.0000\n",
      "Epoch 64/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1684 - val_acc: 1.0000\n",
      "Epoch 65/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1681 - val_acc: 1.0000\n",
      "Epoch 66/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1685 - val_acc: 1.0000\n",
      "Epoch 67/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1687 - val_acc: 1.0000\n",
      "Epoch 68/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1697 - val_acc: 0.9872\n",
      "Epoch 69/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1683 - val_acc: 1.0000\n",
      "Epoch 70/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1685 - val_acc: 1.0000\n",
      "Epoch 71/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1682 - val_acc: 1.0000\n",
      "Epoch 72/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1685 - val_acc: 0.9872\n",
      "Epoch 73/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1680 - val_acc: 1.0000\n",
      "Epoch 74/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1688 - val_acc: 0.9872\n",
      "Epoch 75/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1684 - val_acc: 1.0000\n",
      "Epoch 76/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1682 - val_acc: 1.0000\n",
      "Epoch 77/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1685 - val_acc: 1.0000\n",
      "Epoch 78/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1685 - val_acc: 1.0000\n",
      "Epoch 79/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1686 - val_acc: 0.9872\n",
      "Epoch 80/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1681 - val_acc: 1.0000\n",
      "Epoch 81/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1681 - val_acc: 1.0000\n",
      "Epoch 82/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1682 - val_acc: 1.0000\n",
      "Epoch 83/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1685 - val_acc: 0.9872\n",
      "Epoch 84/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1689 - val_acc: 1.0000\n",
      "Epoch 85/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1684 - val_acc: 1.0000\n",
      "Epoch 86/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1681 - val_acc: 1.0000\n",
      "Epoch 87/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1684 - val_acc: 0.9872\n",
      "Epoch 88/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1686 - val_acc: 1.0000\n",
      "Epoch 89/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1680 - val_acc: 1.0000\n",
      "Epoch 90/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1680 - val_acc: 1.0000\n",
      "Epoch 91/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1683 - val_acc: 0.9872\n",
      "Epoch 92/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1680 - val_acc: 1.0000\n",
      "Epoch 93/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1682 - val_acc: 1.0000\n",
      "Epoch 94/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1682 - val_acc: 1.0000\n",
      "Epoch 95/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1681 - val_acc: 1.0000\n",
      "Epoch 96/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1681 - val_acc: 1.0000\n",
      "Epoch 97/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1681 - val_acc: 1.0000\n",
      "Epoch 98/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1681 - val_acc: 1.0000\n",
      "Epoch 99/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1681 - val_acc: 1.0000\n",
      "Epoch 100/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1681 - val_acc: 1.0000\n",
      "78/78 [==============================] - 0s 293us/step\n",
      "Score for fold 5: loss of 2.1676995020646315; acc of 100.0%\n",
      "Train on 702 samples, validate on 78 samples\n",
      "Epoch 1/100\n",
      "702/702 [==============================] - 5s 7ms/step - loss: 2.5198 - acc: 0.2678 - val_loss: 2.4046 - val_acc: 0.4615\n",
      "Epoch 2/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.3155 - acc: 0.5698 - val_loss: 2.2672 - val_acc: 0.6667\n",
      "Epoch 3/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2366 - acc: 0.7707 - val_loss: 2.2489 - val_acc: 0.6538\n",
      "Epoch 4/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2208 - acc: 0.8333 - val_loss: 2.2775 - val_acc: 0.5513\n",
      "Epoch 5/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2097 - acc: 0.8761 - val_loss: 2.2046 - val_acc: 0.8846\n",
      "Epoch 6/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1961 - acc: 0.8989 - val_loss: 2.2367 - val_acc: 0.8462\n",
      "Epoch 7/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1893 - acc: 0.9274 - val_loss: 2.2400 - val_acc: 0.7564\n",
      "Epoch 8/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1901 - acc: 0.9373 - val_loss: 2.1926 - val_acc: 0.8590\n",
      "Epoch 9/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1776 - acc: 0.9587 - val_loss: 2.1804 - val_acc: 0.9744\n",
      "Epoch 10/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1809 - acc: 0.9544 - val_loss: 2.1954 - val_acc: 0.9103\n",
      "Epoch 11/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1771 - acc: 0.9744 - val_loss: 2.1912 - val_acc: 0.9487\n",
      "Epoch 12/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1775 - acc: 0.9758 - val_loss: 2.1807 - val_acc: 0.9744\n",
      "Epoch 13/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1743 - acc: 0.9786 - val_loss: 2.2184 - val_acc: 0.8974\n",
      "Epoch 14/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1727 - acc: 0.9858 - val_loss: 2.1873 - val_acc: 0.9359\n",
      "Epoch 15/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1801 - acc: 0.9715 - val_loss: 2.1754 - val_acc: 0.9872\n",
      "Epoch 16/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1712 - acc: 0.9886 - val_loss: 2.1750 - val_acc: 0.9872\n",
      "Epoch 17/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1711 - acc: 0.9915 - val_loss: 2.1950 - val_acc: 0.9615\n",
      "Epoch 18/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1759 - acc: 0.9715 - val_loss: 2.1901 - val_acc: 0.9359\n",
      "Epoch 19/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1723 - acc: 0.9915 - val_loss: 2.1814 - val_acc: 0.9744\n",
      "Epoch 20/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1695 - acc: 0.9858 - val_loss: 2.1711 - val_acc: 0.9744\n",
      "Epoch 21/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1731 - acc: 0.9872 - val_loss: 2.1779 - val_acc: 0.9487\n",
      "Epoch 22/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1710 - acc: 0.9858 - val_loss: 2.1748 - val_acc: 0.9872\n",
      "Epoch 23/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1707 - acc: 0.9929 - val_loss: 2.1717 - val_acc: 0.9872\n",
      "Epoch 24/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1689 - acc: 0.9943 - val_loss: 2.1720 - val_acc: 0.9744\n",
      "Epoch 25/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1695 - acc: 0.9943 - val_loss: 2.1789 - val_acc: 0.9615\n",
      "Epoch 26/100\n",
      "702/702 [==============================] - 2s 3ms/step - loss: 2.1682 - acc: 0.9986 - val_loss: 2.1736 - val_acc: 0.9872\n",
      "Epoch 27/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1676 - acc: 0.9957 - val_loss: 2.1711 - val_acc: 0.9872\n",
      "Epoch 28/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1672 - acc: 0.9986 - val_loss: 2.1733 - val_acc: 0.9872\n",
      "Epoch 29/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1679 - acc: 0.9957 - val_loss: 2.1707 - val_acc: 0.9744\n",
      "Epoch 30/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1685 - acc: 0.9972 - val_loss: 2.1706 - val_acc: 0.9872\n",
      "Epoch 31/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1669 - acc: 1.0000 - val_loss: 2.1701 - val_acc: 0.9872\n",
      "Epoch 32/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1669 - acc: 1.0000 - val_loss: 2.1764 - val_acc: 0.9615\n",
      "Epoch 33/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1670 - acc: 1.0000 - val_loss: 2.1705 - val_acc: 0.9872\n",
      "Epoch 34/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1674 - acc: 0.9986 - val_loss: 2.1707 - val_acc: 0.9872\n",
      "Epoch 35/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1670 - acc: 0.9986 - val_loss: 2.1705 - val_acc: 0.9872\n",
      "Epoch 36/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1676 - acc: 0.9986 - val_loss: 2.1721 - val_acc: 0.9744\n",
      "Epoch 37/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1671 - acc: 1.0000 - val_loss: 2.1695 - val_acc: 1.0000\n",
      "Epoch 38/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1667 - acc: 0.9986 - val_loss: 2.1696 - val_acc: 0.9872\n",
      "Epoch 39/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1668 - acc: 1.0000 - val_loss: 2.1698 - val_acc: 1.0000\n",
      "Epoch 40/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1666 - acc: 1.0000 - val_loss: 2.1701 - val_acc: 0.9872\n",
      "Epoch 41/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1699 - val_acc: 0.9872\n",
      "Epoch 42/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 0.9986 - val_loss: 2.1693 - val_acc: 0.9872\n",
      "Epoch 43/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1666 - acc: 1.0000 - val_loss: 2.1694 - val_acc: 1.0000\n",
      "Epoch 44/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1666 - acc: 1.0000 - val_loss: 2.1693 - val_acc: 1.0000\n",
      "Epoch 45/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1698 - val_acc: 0.9872\n",
      "Epoch 46/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1696 - val_acc: 0.9872\n",
      "Epoch 47/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1671 - acc: 0.9986 - val_loss: 2.1697 - val_acc: 0.9872\n",
      "Epoch 48/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1693 - val_acc: 0.9872\n",
      "Epoch 49/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1690 - val_acc: 1.0000\n",
      "Epoch 50/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1690 - val_acc: 1.0000\n",
      "Epoch 51/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1691 - val_acc: 0.9872\n",
      "Epoch 52/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1696 - val_acc: 0.9744\n",
      "Epoch 53/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1698 - val_acc: 0.9872\n",
      "Epoch 54/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1693 - val_acc: 1.0000\n",
      "Epoch 55/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1689 - val_acc: 1.0000\n",
      "Epoch 56/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1695 - val_acc: 0.9872\n",
      "Epoch 57/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1691 - val_acc: 0.9744\n",
      "Epoch 58/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1698 - val_acc: 0.9872\n",
      "Epoch 59/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1693 - val_acc: 0.9872\n",
      "Epoch 60/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1693 - val_acc: 0.9872\n",
      "Epoch 61/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1687 - val_acc: 1.0000\n",
      "Epoch 62/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1694 - val_acc: 0.9872\n",
      "Epoch 63/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1690 - val_acc: 0.9872\n",
      "Epoch 64/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1690 - val_acc: 0.9872\n",
      "Epoch 65/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1690 - val_acc: 0.9872\n",
      "Epoch 66/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1687 - val_acc: 0.9872\n",
      "Epoch 67/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1691 - val_acc: 0.9872\n",
      "Epoch 68/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1687 - val_acc: 0.9872\n",
      "Epoch 69/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1691 - val_acc: 0.9872\n",
      "Epoch 70/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1688 - val_acc: 0.9872\n",
      "Epoch 71/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1689 - val_acc: 1.0000\n",
      "Epoch 72/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1691 - val_acc: 0.9872\n",
      "Epoch 73/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1689 - val_acc: 0.9872\n",
      "Epoch 74/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1692 - val_acc: 0.9872\n",
      "Epoch 75/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1689 - val_acc: 0.9872\n",
      "Epoch 76/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1690 - val_acc: 0.9872\n",
      "Epoch 77/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1689 - val_acc: 0.9872\n",
      "Epoch 78/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1690 - val_acc: 0.9872\n",
      "Epoch 79/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1690 - val_acc: 0.9872\n",
      "Epoch 80/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1689 - val_acc: 0.9872\n",
      "Epoch 81/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1690 - val_acc: 0.9872\n",
      "Epoch 82/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1689 - val_acc: 0.9872\n",
      "Epoch 83/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1690 - val_acc: 0.9872\n",
      "Epoch 84/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1691 - val_acc: 0.9872\n",
      "Epoch 85/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1689 - val_acc: 0.9872\n",
      "Epoch 86/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1693 - val_acc: 0.9872\n",
      "Epoch 87/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1690 - val_acc: 0.9872\n",
      "Epoch 88/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1690 - val_acc: 0.9872\n",
      "Epoch 89/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1688 - val_acc: 0.9872\n",
      "Epoch 90/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1690 - val_acc: 0.9872\n",
      "Epoch 91/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1689 - val_acc: 0.9872\n",
      "Epoch 92/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1688 - val_acc: 0.9872\n",
      "Epoch 93/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1691 - val_acc: 0.9872\n",
      "Epoch 94/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1691 - val_acc: 0.9872\n",
      "Epoch 95/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1690 - val_acc: 0.9872\n",
      "Epoch 96/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1690 - val_acc: 0.9872\n",
      "Epoch 97/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1689 - val_acc: 0.9872\n",
      "Epoch 98/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1691 - val_acc: 0.9872\n",
      "Epoch 99/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1689 - val_acc: 0.9872\n",
      "Epoch 100/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1690 - val_acc: 0.9872\n",
      "78/78 [==============================] - 0s 308us/step\n",
      "Score for fold 6: loss of 2.1691841834630723; acc of 98.71794871794873%\n",
      "Train on 702 samples, validate on 78 samples\n",
      "Epoch 1/100\n",
      "702/702 [==============================] - 5s 7ms/step - loss: 2.4914 - acc: 0.2792 - val_loss: 2.3847 - val_acc: 0.4744\n",
      "Epoch 2/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.3205 - acc: 0.5641 - val_loss: 2.2587 - val_acc: 0.7436\n",
      "Epoch 3/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2535 - acc: 0.7564 - val_loss: 2.2403 - val_acc: 0.7179\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2260 - acc: 0.8162 - val_loss: 2.2184 - val_acc: 0.8718\n",
      "Epoch 5/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2083 - acc: 0.9046 - val_loss: 2.1991 - val_acc: 0.8590\n",
      "Epoch 6/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1957 - acc: 0.9046 - val_loss: 2.2172 - val_acc: 0.7949\n",
      "Epoch 7/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1893 - acc: 0.9217 - val_loss: 2.2172 - val_acc: 0.8205\n",
      "Epoch 8/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1809 - acc: 0.9601 - val_loss: 2.2032 - val_acc: 0.8462\n",
      "Epoch 9/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1808 - acc: 0.9615 - val_loss: 2.1818 - val_acc: 0.9744\n",
      "Epoch 10/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1762 - acc: 0.9786 - val_loss: 2.1820 - val_acc: 0.9359\n",
      "Epoch 11/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1758 - acc: 0.9758 - val_loss: 2.2054 - val_acc: 0.8974\n",
      "Epoch 12/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1771 - acc: 0.9843 - val_loss: 2.1845 - val_acc: 0.9487\n",
      "Epoch 13/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1752 - acc: 0.9829 - val_loss: 2.2360 - val_acc: 0.8205\n",
      "Epoch 14/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1762 - acc: 0.9715 - val_loss: 2.1805 - val_acc: 0.9615\n",
      "Epoch 15/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1708 - acc: 0.9957 - val_loss: 2.1953 - val_acc: 0.9231\n",
      "Epoch 16/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1744 - acc: 0.9858 - val_loss: 2.1826 - val_acc: 0.9487\n",
      "Epoch 17/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1695 - acc: 0.9972 - val_loss: 2.1842 - val_acc: 0.9231\n",
      "Epoch 18/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1725 - acc: 0.9886 - val_loss: 2.1891 - val_acc: 0.9231\n",
      "Epoch 19/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1716 - acc: 0.9943 - val_loss: 2.1803 - val_acc: 0.9744\n",
      "Epoch 20/100\n",
      "702/702 [==============================] - 2s 3ms/step - loss: 2.1692 - acc: 0.9972 - val_loss: 2.1798 - val_acc: 0.9487\n",
      "Epoch 21/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1682 - acc: 0.9986 - val_loss: 2.1781 - val_acc: 0.9744\n",
      "Epoch 22/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1676 - acc: 0.9986 - val_loss: 2.1788 - val_acc: 0.9744\n",
      "Epoch 23/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1690 - acc: 0.9972 - val_loss: 2.1840 - val_acc: 0.9615\n",
      "Epoch 24/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1680 - acc: 0.9972 - val_loss: 2.1885 - val_acc: 0.9615\n",
      "Epoch 25/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1684 - acc: 0.9957 - val_loss: 2.1810 - val_acc: 0.9615\n",
      "Epoch 26/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1675 - acc: 0.9972 - val_loss: 2.1767 - val_acc: 0.9872\n",
      "Epoch 27/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1674 - acc: 1.0000 - val_loss: 2.1813 - val_acc: 0.9615\n",
      "Epoch 28/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1673 - acc: 1.0000 - val_loss: 2.1847 - val_acc: 0.9744\n",
      "Epoch 29/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1672 - acc: 1.0000 - val_loss: 2.1813 - val_acc: 0.9615\n",
      "Epoch 30/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1676 - acc: 0.9957 - val_loss: 2.1791 - val_acc: 0.9744\n",
      "Epoch 31/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1681 - acc: 0.9986 - val_loss: 2.1813 - val_acc: 0.9744\n",
      "Epoch 32/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1668 - acc: 1.0000 - val_loss: 2.1779 - val_acc: 0.9744\n",
      "Epoch 33/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1669 - acc: 1.0000 - val_loss: 2.1766 - val_acc: 0.9744\n",
      "Epoch 34/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1673 - acc: 0.9986 - val_loss: 2.1742 - val_acc: 0.9872\n",
      "Epoch 35/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1667 - acc: 1.0000 - val_loss: 2.1795 - val_acc: 0.9744\n",
      "Epoch 36/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1668 - acc: 1.0000 - val_loss: 2.1773 - val_acc: 0.9872\n",
      "Epoch 37/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1674 - acc: 0.9986 - val_loss: 2.1823 - val_acc: 0.9744\n",
      "Epoch 38/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1666 - acc: 1.0000 - val_loss: 2.1745 - val_acc: 0.9872\n",
      "Epoch 39/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1743 - val_acc: 0.9872\n",
      "Epoch 40/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1666 - acc: 0.9957 - val_loss: 2.1786 - val_acc: 0.9744\n",
      "Epoch 41/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1763 - val_acc: 0.9744\n",
      "Epoch 42/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1766 - val_acc: 0.9744\n",
      "Epoch 43/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1736 - val_acc: 0.9744\n",
      "Epoch 44/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1667 - acc: 1.0000 - val_loss: 2.1788 - val_acc: 0.9744\n",
      "Epoch 45/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1666 - acc: 1.0000 - val_loss: 2.1750 - val_acc: 0.9872\n",
      "Epoch 46/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1758 - val_acc: 0.9872\n",
      "Epoch 47/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1774 - val_acc: 0.9744\n",
      "Epoch 48/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1785 - val_acc: 0.9744\n",
      "Epoch 49/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1733 - val_acc: 0.9872\n",
      "Epoch 50/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1758 - val_acc: 0.9744\n",
      "Epoch 51/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1757 - val_acc: 0.9744\n",
      "Epoch 52/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1763 - val_acc: 0.9744\n",
      "Epoch 53/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1737 - val_acc: 0.9872\n",
      "Epoch 54/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1765 - val_acc: 0.9744\n",
      "Epoch 55/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1767 - val_acc: 0.9744\n",
      "Epoch 56/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1764 - val_acc: 0.9872\n",
      "Epoch 57/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1783 - val_acc: 0.9744\n",
      "Epoch 58/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1741 - val_acc: 0.9872\n",
      "Epoch 59/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1750 - val_acc: 0.9744\n",
      "Epoch 60/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1746 - val_acc: 0.9872\n",
      "Epoch 61/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1747 - val_acc: 0.9744\n",
      "Epoch 62/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1755 - val_acc: 0.9872\n",
      "Epoch 63/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1789 - val_acc: 0.9744\n",
      "Epoch 64/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1750 - val_acc: 0.9744\n",
      "Epoch 65/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1761 - val_acc: 0.9744\n",
      "Epoch 66/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1768 - val_acc: 0.9744\n",
      "Epoch 67/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1742 - val_acc: 0.9872\n",
      "Epoch 68/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1739 - val_acc: 0.9872\n",
      "Epoch 69/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1742 - val_acc: 0.9872\n",
      "Epoch 70/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1748 - val_acc: 0.9744\n",
      "Epoch 71/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1766 - val_acc: 0.9744\n",
      "Epoch 72/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1748 - val_acc: 0.9744\n",
      "Epoch 73/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1754 - val_acc: 0.9744\n",
      "Epoch 74/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1744 - val_acc: 0.9744\n",
      "Epoch 75/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1747 - val_acc: 0.9744\n",
      "Epoch 76/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1745 - val_acc: 0.9872\n",
      "Epoch 77/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1743 - val_acc: 0.9872\n",
      "Epoch 78/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1761 - val_acc: 0.9744\n",
      "Epoch 79/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1755 - val_acc: 0.9744\n",
      "Epoch 80/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1749 - val_acc: 0.9744\n",
      "Epoch 81/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1746 - val_acc: 0.9744\n",
      "Epoch 82/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1763 - val_acc: 0.9744\n",
      "Epoch 83/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1761 - val_acc: 0.9744\n",
      "Epoch 84/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1755 - val_acc: 0.9744\n",
      "Epoch 85/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1746 - val_acc: 0.9744\n",
      "Epoch 86/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1749 - val_acc: 0.9744\n",
      "Epoch 87/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1761 - val_acc: 0.9744\n",
      "Epoch 88/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1747 - val_acc: 0.9744\n",
      "Epoch 89/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1742 - val_acc: 0.9872\n",
      "Epoch 90/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1772 - val_acc: 0.9744\n",
      "Epoch 91/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1748 - val_acc: 0.9744\n",
      "Epoch 92/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1747 - val_acc: 0.9744\n",
      "Epoch 93/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1745 - val_acc: 0.9744\n",
      "Epoch 94/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1745 - val_acc: 0.9744\n",
      "Epoch 95/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1748 - val_acc: 0.9744\n",
      "Epoch 96/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1737 - val_acc: 0.9872\n",
      "Epoch 97/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1746 - val_acc: 0.9744\n",
      "Epoch 98/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1746 - val_acc: 0.9744\n",
      "Epoch 99/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1744 - val_acc: 0.9744\n",
      "Epoch 100/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1746 - val_acc: 0.9744\n",
      "78/78 [==============================] - 0s 299us/step\n",
      "Score for fold 7: loss of 2.174389655773456; acc of 97.43589697740018%\n",
      "Train on 702 samples, validate on 78 samples\n",
      "Epoch 1/100\n",
      "702/702 [==============================] - 5s 7ms/step - loss: 2.4799 - acc: 0.3020 - val_loss: 2.5017 - val_acc: 0.2564\n",
      "Epoch 2/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.3259 - acc: 0.5299 - val_loss: 2.2787 - val_acc: 0.7051\n",
      "Epoch 3/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2467 - acc: 0.7521 - val_loss: 2.3004 - val_acc: 0.6026\n",
      "Epoch 4/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2400 - acc: 0.7849 - val_loss: 2.2249 - val_acc: 0.7821\n",
      "Epoch 5/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2054 - acc: 0.8604 - val_loss: 2.2106 - val_acc: 0.8077\n",
      "Epoch 6/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2030 - acc: 0.8746 - val_loss: 2.2212 - val_acc: 0.7308\n",
      "Epoch 7/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1914 - acc: 0.8860 - val_loss: 2.2022 - val_acc: 0.9231\n",
      "Epoch 8/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1851 - acc: 0.9345 - val_loss: 2.2090 - val_acc: 0.8333\n",
      "Epoch 9/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1878 - acc: 0.9259 - val_loss: 2.2011 - val_acc: 0.8846\n",
      "Epoch 10/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1792 - acc: 0.9672 - val_loss: 2.2021 - val_acc: 0.8333\n",
      "Epoch 11/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1816 - acc: 0.9516 - val_loss: 2.1994 - val_acc: 0.8590\n",
      "Epoch 12/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1779 - acc: 0.9786 - val_loss: 2.1915 - val_acc: 0.9103\n",
      "Epoch 13/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1739 - acc: 0.9858 - val_loss: 2.2545 - val_acc: 0.7179\n",
      "Epoch 14/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1802 - acc: 0.9530 - val_loss: 2.2907 - val_acc: 0.8718\n",
      "Epoch 15/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1739 - acc: 0.9872 - val_loss: 2.1939 - val_acc: 0.9359\n",
      "Epoch 16/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1714 - acc: 0.9900 - val_loss: 2.2055 - val_acc: 0.8846\n",
      "Epoch 17/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1717 - acc: 0.9900 - val_loss: 2.1863 - val_acc: 0.9359\n",
      "Epoch 18/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1721 - acc: 0.9829 - val_loss: 2.1821 - val_acc: 0.9615\n",
      "Epoch 19/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1718 - acc: 0.9843 - val_loss: 2.1894 - val_acc: 0.9359\n",
      "Epoch 20/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1717 - acc: 0.9915 - val_loss: 2.1837 - val_acc: 0.9103\n",
      "Epoch 21/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1697 - acc: 0.9900 - val_loss: 2.1876 - val_acc: 0.9231\n",
      "Epoch 22/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1724 - acc: 0.9843 - val_loss: 2.1856 - val_acc: 0.9487\n",
      "Epoch 23/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1697 - acc: 0.9929 - val_loss: 2.1864 - val_acc: 0.9359\n",
      "Epoch 24/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1739 - acc: 0.9772 - val_loss: 2.1861 - val_acc: 0.9487\n",
      "Epoch 25/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1685 - acc: 0.9972 - val_loss: 2.1977 - val_acc: 0.9487\n",
      "Epoch 26/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1691 - acc: 0.9943 - val_loss: 2.1902 - val_acc: 0.9231\n",
      "Epoch 27/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1692 - acc: 0.9915 - val_loss: 2.1859 - val_acc: 0.9231\n",
      "Epoch 28/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1691 - acc: 0.9957 - val_loss: 2.1845 - val_acc: 0.9359\n",
      "Epoch 29/100\n",
      "702/702 [==============================] - 2s 3ms/step - loss: 2.1683 - acc: 0.9943 - val_loss: 2.1900 - val_acc: 0.9231\n",
      "Epoch 30/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1689 - acc: 0.9900 - val_loss: 2.1799 - val_acc: 0.9487\n",
      "Epoch 31/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1673 - acc: 0.9986 - val_loss: 2.1843 - val_acc: 0.9487\n",
      "Epoch 32/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1672 - acc: 0.9986 - val_loss: 2.1858 - val_acc: 0.9487\n",
      "Epoch 33/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1673 - acc: 0.9986 - val_loss: 2.1835 - val_acc: 0.9359\n",
      "Epoch 34/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1674 - acc: 0.9972 - val_loss: 2.1890 - val_acc: 0.9487\n",
      "Epoch 35/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1672 - acc: 0.9972 - val_loss: 2.1847 - val_acc: 0.9615\n",
      "Epoch 36/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1671 - acc: 0.9986 - val_loss: 2.1812 - val_acc: 0.9615\n",
      "Epoch 37/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1669 - acc: 0.9986 - val_loss: 2.1820 - val_acc: 0.9615\n",
      "Epoch 38/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1670 - acc: 1.0000 - val_loss: 2.1764 - val_acc: 0.9615\n",
      "Epoch 39/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1681 - acc: 0.9915 - val_loss: 2.1824 - val_acc: 0.9487\n",
      "Epoch 40/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1666 - acc: 1.0000 - val_loss: 2.1818 - val_acc: 0.9615\n",
      "Epoch 41/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1666 - acc: 1.0000 - val_loss: 2.1816 - val_acc: 0.9487\n",
      "Epoch 42/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1666 - acc: 0.9986 - val_loss: 2.1809 - val_acc: 0.9744\n",
      "Epoch 43/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1825 - val_acc: 0.9615\n",
      "Epoch 44/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1803 - val_acc: 0.9487\n",
      "Epoch 45/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1780 - val_acc: 0.9615\n",
      "Epoch 46/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1807 - val_acc: 0.9615\n",
      "Epoch 47/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1872 - val_acc: 0.9231\n",
      "Epoch 48/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1669 - acc: 0.9986 - val_loss: 2.1794 - val_acc: 0.9744\n",
      "Epoch 49/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1783 - val_acc: 0.9744\n",
      "Epoch 50/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1782 - val_acc: 0.9744\n",
      "Epoch 51/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1793 - val_acc: 0.9615\n",
      "Epoch 52/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1783 - val_acc: 0.9615\n",
      "Epoch 53/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1802 - val_acc: 0.9744\n",
      "Epoch 54/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1794 - val_acc: 0.9744\n",
      "Epoch 55/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1808 - val_acc: 0.9744\n",
      "Epoch 56/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1798 - val_acc: 0.9744\n",
      "Epoch 57/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1803 - val_acc: 0.9744\n",
      "Epoch 58/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1803 - val_acc: 0.9615\n",
      "Epoch 59/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1778 - val_acc: 0.9615\n",
      "Epoch 60/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1789 - val_acc: 0.9744\n",
      "Epoch 61/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1795 - val_acc: 0.9744\n",
      "Epoch 62/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1786 - val_acc: 0.9615\n",
      "Epoch 63/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1796 - val_acc: 0.9615\n",
      "Epoch 64/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1790 - val_acc: 0.9615\n",
      "Epoch 65/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1785 - val_acc: 0.9615\n",
      "Epoch 66/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1779 - val_acc: 0.9615\n",
      "Epoch 67/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1777 - val_acc: 0.9615\n",
      "Epoch 68/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1791 - val_acc: 0.9744\n",
      "Epoch 69/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1756 - val_acc: 0.9744\n",
      "Epoch 70/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1781 - val_acc: 0.9744\n",
      "Epoch 71/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1777 - val_acc: 0.9615\n",
      "Epoch 72/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1785 - val_acc: 0.9744\n",
      "Epoch 73/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1826 - val_acc: 0.9615\n",
      "Epoch 74/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1792 - val_acc: 0.9615\n",
      "Epoch 75/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1791 - val_acc: 0.9744\n",
      "Epoch 76/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1773 - val_acc: 0.9744\n",
      "Epoch 77/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1777 - val_acc: 0.9615\n",
      "Epoch 78/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1784 - val_acc: 0.9615\n",
      "Epoch 79/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1773 - val_acc: 0.9615\n",
      "Epoch 80/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1782 - val_acc: 0.9615\n",
      "Epoch 81/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1762 - val_acc: 0.9615\n",
      "Epoch 82/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1774 - val_acc: 0.9615\n",
      "Epoch 83/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1773 - val_acc: 0.9744\n",
      "Epoch 84/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1779 - val_acc: 0.9615\n",
      "Epoch 85/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1774 - val_acc: 0.9744\n",
      "Epoch 86/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1774 - val_acc: 0.9744\n",
      "Epoch 87/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1773 - val_acc: 0.9615\n",
      "Epoch 88/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1780 - val_acc: 0.9744\n",
      "Epoch 89/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1782 - val_acc: 0.9615\n",
      "Epoch 90/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1780 - val_acc: 0.9615\n",
      "Epoch 91/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1781 - val_acc: 0.9615\n",
      "Epoch 92/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1779 - val_acc: 0.9615\n",
      "Epoch 93/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1778 - val_acc: 0.9615\n",
      "Epoch 94/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1780 - val_acc: 0.9615\n",
      "Epoch 95/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1778 - val_acc: 0.9615\n",
      "Epoch 96/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1781 - val_acc: 0.9615\n",
      "Epoch 97/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1779 - val_acc: 0.9615\n",
      "Epoch 98/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1776 - val_acc: 0.9615\n",
      "Epoch 99/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1798 - val_acc: 0.9744\n",
      "Epoch 100/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1790 - val_acc: 0.9615\n",
      "78/78 [==============================] - 0s 295us/step\n",
      "Score for fold 8: loss of 2.178801273688292; acc of 96.15384569534888%\n",
      "Train on 702 samples, validate on 78 samples\n",
      "Epoch 1/100\n",
      "702/702 [==============================] - 5s 7ms/step - loss: 2.4653 - acc: 0.3148 - val_loss: 2.3551 - val_acc: 0.5256\n",
      "Epoch 2/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2733 - acc: 0.6481 - val_loss: 2.2604 - val_acc: 0.6795\n",
      "Epoch 3/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2352 - acc: 0.7849 - val_loss: 2.2274 - val_acc: 0.8590\n",
      "Epoch 4/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2051 - acc: 0.8846 - val_loss: 2.2391 - val_acc: 0.7564\n",
      "Epoch 5/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2043 - acc: 0.9003 - val_loss: 2.2073 - val_acc: 0.8333\n",
      "Epoch 6/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1893 - acc: 0.9259 - val_loss: 2.1959 - val_acc: 0.8974\n",
      "Epoch 7/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1848 - acc: 0.9544 - val_loss: 2.1914 - val_acc: 0.9359\n",
      "Epoch 8/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1793 - acc: 0.9758 - val_loss: 2.1924 - val_acc: 0.8974\n",
      "Epoch 9/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1814 - acc: 0.9501 - val_loss: 2.1877 - val_acc: 0.9359\n",
      "Epoch 10/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1777 - acc: 0.9801 - val_loss: 2.1889 - val_acc: 0.9359\n",
      "Epoch 11/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1743 - acc: 0.9843 - val_loss: 2.2168 - val_acc: 0.8590\n",
      "Epoch 12/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1764 - acc: 0.9772 - val_loss: 2.1835 - val_acc: 0.9615\n",
      "Epoch 13/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1706 - acc: 0.9886 - val_loss: 2.1821 - val_acc: 0.9615\n",
      "Epoch 14/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1715 - acc: 0.9872 - val_loss: 2.1860 - val_acc: 0.9359\n",
      "Epoch 15/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1722 - acc: 0.9886 - val_loss: 2.1851 - val_acc: 0.9615\n",
      "Epoch 16/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1722 - acc: 0.9872 - val_loss: 2.1819 - val_acc: 0.9744\n",
      "Epoch 17/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1702 - acc: 0.9929 - val_loss: 2.1871 - val_acc: 0.9231\n",
      "Epoch 18/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1693 - acc: 0.9929 - val_loss: 2.1944 - val_acc: 0.8846\n",
      "Epoch 19/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1702 - acc: 0.9858 - val_loss: 2.1812 - val_acc: 0.9744\n",
      "Epoch 20/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1699 - acc: 0.9943 - val_loss: 2.1844 - val_acc: 0.9615\n",
      "Epoch 21/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1713 - acc: 0.9915 - val_loss: 2.1974 - val_acc: 0.9744\n",
      "Epoch 22/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1687 - acc: 0.9972 - val_loss: 2.1812 - val_acc: 0.9744\n",
      "Epoch 23/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1693 - acc: 0.9900 - val_loss: 2.1838 - val_acc: 0.9359\n",
      "Epoch 24/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1682 - acc: 0.9986 - val_loss: 2.1802 - val_acc: 0.9615\n",
      "Epoch 25/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1699 - acc: 0.9915 - val_loss: 2.1800 - val_acc: 0.9744\n",
      "Epoch 26/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1671 - acc: 1.0000 - val_loss: 2.1804 - val_acc: 0.9744\n",
      "Epoch 27/100\n",
      "702/702 [==============================] - 2s 3ms/step - loss: 2.1718 - acc: 0.9886 - val_loss: 2.1806 - val_acc: 0.9744\n",
      "Epoch 28/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1669 - acc: 1.0000 - val_loss: 2.1802 - val_acc: 0.9744\n",
      "Epoch 29/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1669 - acc: 0.9986 - val_loss: 2.1808 - val_acc: 0.9744\n",
      "Epoch 30/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1669 - acc: 0.9972 - val_loss: 2.1798 - val_acc: 0.9744\n",
      "Epoch 31/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1679 - acc: 0.9972 - val_loss: 2.1797 - val_acc: 0.9744\n",
      "Epoch 32/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1666 - acc: 1.0000 - val_loss: 2.1821 - val_acc: 0.9615\n",
      "Epoch 33/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1667 - acc: 1.0000 - val_loss: 2.1800 - val_acc: 0.9744\n",
      "Epoch 34/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1667 - acc: 1.0000 - val_loss: 2.1839 - val_acc: 0.9487\n",
      "Epoch 35/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1670 - acc: 0.9957 - val_loss: 2.1801 - val_acc: 0.9744\n",
      "Epoch 36/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1666 - acc: 1.0000 - val_loss: 2.1782 - val_acc: 0.9744\n",
      "Epoch 37/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1669 - acc: 1.0000 - val_loss: 2.1786 - val_acc: 0.9744\n",
      "Epoch 38/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1666 - acc: 1.0000 - val_loss: 2.1791 - val_acc: 0.9744\n",
      "Epoch 39/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1794 - val_acc: 0.9615\n",
      "Epoch 40/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1788 - val_acc: 0.9744\n",
      "Epoch 41/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1788 - val_acc: 0.9744\n",
      "Epoch 42/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1784 - val_acc: 0.9744\n",
      "Epoch 43/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1780 - val_acc: 0.9744\n",
      "Epoch 44/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1785 - val_acc: 0.9744\n",
      "Epoch 45/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1789 - val_acc: 0.9615\n",
      "Epoch 46/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1792 - val_acc: 0.9615\n",
      "Epoch 47/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1779 - val_acc: 0.9744\n",
      "Epoch 48/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1778 - val_acc: 0.9744\n",
      "Epoch 49/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1779 - val_acc: 0.9744\n",
      "Epoch 50/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1780 - val_acc: 0.9744\n",
      "Epoch 51/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1781 - val_acc: 0.9744\n",
      "Epoch 52/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1777 - val_acc: 0.9744\n",
      "Epoch 53/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1778 - val_acc: 0.9744\n",
      "Epoch 54/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1773 - val_acc: 0.9744\n",
      "Epoch 55/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1779 - val_acc: 0.9615\n",
      "Epoch 56/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1779 - val_acc: 0.9744\n",
      "Epoch 57/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1773 - val_acc: 0.9744\n",
      "Epoch 58/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1774 - val_acc: 0.9744\n",
      "Epoch 59/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1774 - val_acc: 0.9744\n",
      "Epoch 60/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1782 - val_acc: 0.9615\n",
      "Epoch 61/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1774 - val_acc: 0.9744\n",
      "Epoch 62/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1777 - val_acc: 0.9744\n",
      "Epoch 63/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1776 - val_acc: 0.9744\n",
      "Epoch 64/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1777 - val_acc: 0.9744\n",
      "Epoch 65/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1776 - val_acc: 0.9744\n",
      "Epoch 66/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1780 - val_acc: 0.9744\n",
      "Epoch 67/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1774 - val_acc: 0.9744\n",
      "Epoch 68/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1776 - val_acc: 0.9744\n",
      "Epoch 69/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1775 - val_acc: 0.9744\n",
      "Epoch 70/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1776 - val_acc: 0.9744\n",
      "Epoch 71/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1775 - val_acc: 0.9744\n",
      "Epoch 72/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1775 - val_acc: 0.9744\n",
      "Epoch 73/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1775 - val_acc: 0.9744\n",
      "Epoch 74/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1776 - val_acc: 0.9744\n",
      "Epoch 75/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1775 - val_acc: 0.9744\n",
      "Epoch 76/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1777 - val_acc: 0.9744\n",
      "Epoch 77/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1775 - val_acc: 0.9744\n",
      "Epoch 78/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1775 - val_acc: 0.9744\n",
      "Epoch 79/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1774 - val_acc: 0.9744\n",
      "Epoch 80/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1775 - val_acc: 0.9744\n",
      "Epoch 81/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1774 - val_acc: 0.9744\n",
      "Epoch 82/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1776 - val_acc: 0.9744\n",
      "Epoch 83/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1774 - val_acc: 0.9744\n",
      "Epoch 84/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1774 - val_acc: 0.9744\n",
      "Epoch 85/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1773 - val_acc: 0.9744\n",
      "Epoch 86/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1774 - val_acc: 0.9744\n",
      "Epoch 87/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1775 - val_acc: 0.9744\n",
      "Epoch 88/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1774 - val_acc: 0.9744\n",
      "Epoch 89/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1774 - val_acc: 0.9744\n",
      "Epoch 90/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1774 - val_acc: 0.9744\n",
      "Epoch 91/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1775 - val_acc: 0.9744\n",
      "Epoch 92/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1775 - val_acc: 0.9744\n",
      "Epoch 93/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1774 - val_acc: 0.9744\n",
      "Epoch 94/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1775 - val_acc: 0.9744\n",
      "Epoch 95/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1776 - val_acc: 0.9744\n",
      "Epoch 96/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1774 - val_acc: 0.9744\n",
      "Epoch 97/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1775 - val_acc: 0.9744\n",
      "Epoch 98/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1775 - val_acc: 0.9744\n",
      "Epoch 99/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1775 - val_acc: 0.9744\n",
      "Epoch 100/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1776 - val_acc: 0.9744\n",
      "78/78 [==============================] - 0s 304us/step\n",
      "Score for fold 9: loss of 2.1776209550026135; acc of 97.43589697740018%\n",
      "Train on 702 samples, validate on 78 samples\n",
      "Epoch 1/100\n",
      "702/702 [==============================] - 5s 8ms/step - loss: 2.4766 - acc: 0.3191 - val_loss: 2.3509 - val_acc: 0.4872\n",
      "Epoch 2/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2914 - acc: 0.6353 - val_loss: 2.2984 - val_acc: 0.4872\n",
      "Epoch 3/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2371 - acc: 0.7749 - val_loss: 2.2616 - val_acc: 0.7308\n",
      "Epoch 4/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2220 - acc: 0.8305 - val_loss: 2.2140 - val_acc: 0.8205\n",
      "Epoch 5/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2098 - acc: 0.8746 - val_loss: 2.2016 - val_acc: 0.8333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1898 - acc: 0.9131 - val_loss: 2.1995 - val_acc: 0.7821\n",
      "Epoch 7/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1913 - acc: 0.9174 - val_loss: 2.1879 - val_acc: 0.8333\n",
      "Epoch 8/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1822 - acc: 0.9473 - val_loss: 2.1881 - val_acc: 0.8462\n",
      "Epoch 9/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1833 - acc: 0.9459 - val_loss: 2.1893 - val_acc: 0.9103\n",
      "Epoch 10/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1807 - acc: 0.9615 - val_loss: 2.1791 - val_acc: 0.9744\n",
      "Epoch 11/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1767 - acc: 0.9615 - val_loss: 2.2130 - val_acc: 0.8590\n",
      "Epoch 12/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1776 - acc: 0.9772 - val_loss: 2.1811 - val_acc: 0.9487\n",
      "Epoch 13/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1739 - acc: 0.9815 - val_loss: 2.1778 - val_acc: 0.9872\n",
      "Epoch 14/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1750 - acc: 0.9772 - val_loss: 2.1791 - val_acc: 0.8974\n",
      "Epoch 15/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1726 - acc: 0.9872 - val_loss: 2.1804 - val_acc: 0.9615\n",
      "Epoch 16/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1738 - acc: 0.9872 - val_loss: 2.1738 - val_acc: 1.0000\n",
      "Epoch 17/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1730 - acc: 0.9900 - val_loss: 2.1929 - val_acc: 0.9231\n",
      "Epoch 18/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1713 - acc: 0.9886 - val_loss: 2.2423 - val_acc: 0.8333\n",
      "Epoch 19/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1701 - acc: 0.9915 - val_loss: 2.1781 - val_acc: 0.9744\n",
      "Epoch 20/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1719 - acc: 0.9858 - val_loss: 2.1726 - val_acc: 0.9872\n",
      "Epoch 21/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1700 - acc: 0.9900 - val_loss: 2.1948 - val_acc: 0.8974\n",
      "Epoch 22/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1698 - acc: 0.9957 - val_loss: 2.1754 - val_acc: 0.9487\n",
      "Epoch 23/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1689 - acc: 0.9929 - val_loss: 2.1729 - val_acc: 0.9744\n",
      "Epoch 24/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1695 - acc: 0.9972 - val_loss: 2.1715 - val_acc: 0.9872\n",
      "Epoch 25/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1692 - acc: 0.9972 - val_loss: 2.1804 - val_acc: 0.9615\n",
      "Epoch 26/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1684 - acc: 0.9929 - val_loss: 2.1873 - val_acc: 0.9615\n",
      "Epoch 27/100\n",
      "702/702 [==============================] - 2s 3ms/step - loss: 2.1681 - acc: 0.9986 - val_loss: 2.1717 - val_acc: 0.9744\n",
      "Epoch 28/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1679 - acc: 0.9986 - val_loss: 2.1712 - val_acc: 0.9615\n",
      "Epoch 29/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1673 - acc: 0.9972 - val_loss: 2.1720 - val_acc: 0.9615\n",
      "Epoch 30/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1685 - acc: 0.9972 - val_loss: 2.1717 - val_acc: 1.0000\n",
      "Epoch 31/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1679 - acc: 0.9972 - val_loss: 2.1715 - val_acc: 0.9615\n",
      "Epoch 32/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1671 - acc: 0.9986 - val_loss: 2.1723 - val_acc: 0.9487\n",
      "Epoch 33/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1674 - acc: 0.9986 - val_loss: 2.1719 - val_acc: 0.9872\n",
      "Epoch 34/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1670 - acc: 1.0000 - val_loss: 2.1709 - val_acc: 0.9872\n",
      "Epoch 35/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1668 - acc: 1.0000 - val_loss: 2.1757 - val_acc: 1.0000\n",
      "Epoch 36/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1685 - acc: 0.9972 - val_loss: 2.1723 - val_acc: 0.9744\n",
      "Epoch 37/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1667 - acc: 1.0000 - val_loss: 2.1711 - val_acc: 0.9872\n",
      "Epoch 38/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1694 - val_acc: 1.0000\n",
      "Epoch 39/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1666 - acc: 1.0000 - val_loss: 2.1739 - val_acc: 0.9487\n",
      "Epoch 40/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1666 - acc: 1.0000 - val_loss: 2.1720 - val_acc: 0.9744\n",
      "Epoch 41/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1667 - acc: 1.0000 - val_loss: 2.1740 - val_acc: 0.9615\n",
      "Epoch 42/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1666 - acc: 1.0000 - val_loss: 2.1701 - val_acc: 0.9872\n",
      "Epoch 43/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1699 - val_acc: 0.9744\n",
      "Epoch 44/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1699 - val_acc: 0.9872\n",
      "Epoch 45/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1691 - val_acc: 1.0000\n",
      "Epoch 46/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1722 - val_acc: 0.9872\n",
      "Epoch 47/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1666 - acc: 0.9986 - val_loss: 2.1705 - val_acc: 0.9615\n",
      "Epoch 48/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1720 - val_acc: 0.9615\n",
      "Epoch 49/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1691 - val_acc: 0.9872\n",
      "Epoch 50/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1691 - val_acc: 0.9872\n",
      "Epoch 51/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1700 - val_acc: 0.9744\n",
      "Epoch 52/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1691 - val_acc: 0.9872\n",
      "Epoch 53/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1693 - val_acc: 0.9872\n",
      "Epoch 54/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1699 - val_acc: 0.9744\n",
      "Epoch 55/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 0.9986 - val_loss: 2.1721 - val_acc: 0.9744\n",
      "Epoch 56/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1695 - val_acc: 0.9872\n",
      "Epoch 57/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1685 - val_acc: 1.0000\n",
      "Epoch 58/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1707 - val_acc: 0.9744\n",
      "Epoch 59/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1691 - val_acc: 0.9872\n",
      "Epoch 60/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1694 - val_acc: 0.9744\n",
      "Epoch 61/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1717 - val_acc: 0.9615\n",
      "Epoch 62/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1691 - val_acc: 0.9872\n",
      "Epoch 63/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1690 - val_acc: 0.9872\n",
      "Epoch 64/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1708 - val_acc: 0.9744\n",
      "Epoch 65/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1690 - val_acc: 0.9872\n",
      "Epoch 66/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1691 - val_acc: 0.9744\n",
      "Epoch 67/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1700 - val_acc: 0.9744\n",
      "Epoch 68/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1697 - val_acc: 0.9744\n",
      "Epoch 69/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1700 - val_acc: 0.9744\n",
      "Epoch 70/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1701 - val_acc: 0.9744\n",
      "Epoch 71/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1687 - val_acc: 0.9872\n",
      "Epoch 72/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1693 - val_acc: 0.9744\n",
      "Epoch 73/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1693 - val_acc: 0.9744\n",
      "Epoch 74/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1694 - val_acc: 0.9744\n",
      "Epoch 75/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1698 - val_acc: 0.9744\n",
      "Epoch 76/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1687 - val_acc: 0.9872\n",
      "Epoch 77/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1692 - val_acc: 0.9744\n",
      "Epoch 78/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1689 - val_acc: 0.9872\n",
      "Epoch 79/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1685 - val_acc: 0.9872\n",
      "Epoch 80/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1688 - val_acc: 0.9872\n",
      "Epoch 81/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1686 - val_acc: 0.9872\n",
      "Epoch 82/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1687 - val_acc: 0.9872\n",
      "Epoch 83/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1686 - val_acc: 1.0000\n",
      "Epoch 84/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1686 - val_acc: 0.9872\n",
      "Epoch 85/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1689 - val_acc: 0.9744\n",
      "Epoch 86/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1685 - val_acc: 0.9872\n",
      "Epoch 87/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1695 - val_acc: 0.9744\n",
      "Epoch 88/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1692 - val_acc: 0.9744\n",
      "Epoch 89/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1687 - val_acc: 0.9872\n",
      "Epoch 90/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1685 - val_acc: 1.0000\n",
      "Epoch 91/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1692 - val_acc: 0.9744\n",
      "Epoch 92/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1686 - val_acc: 0.9872\n",
      "Epoch 93/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1686 - val_acc: 0.9872\n",
      "Epoch 94/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1688 - val_acc: 0.9872\n",
      "Epoch 95/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1687 - val_acc: 0.9872\n",
      "Epoch 96/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1690 - val_acc: 0.9872\n",
      "Epoch 97/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1692 - val_acc: 0.9744\n",
      "Epoch 98/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1689 - val_acc: 0.9872\n",
      "Epoch 99/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1689 - val_acc: 0.9744\n",
      "Epoch 100/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1686 - val_acc: 0.9872\n",
      "78/78 [==============================] - 0s 304us/step\n",
      "Score for fold 10: loss of 2.1687117967850122; acc of 98.71794871794873%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1296x1296 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[95m 2       \u001b[0m | \u001b[95m 98.33   \u001b[0m | \u001b[95m 0.9     \u001b[0m | \u001b[95m 7.116   \u001b[0m |\n",
      "Train on 702 samples, validate on 78 samples\n",
      "Epoch 1/100\n",
      "702/702 [==============================] - 6s 8ms/step - loss: 2.4931 - acc: 0.2493 - val_loss: 2.3475 - val_acc: 0.4872\n",
      "Epoch 2/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.3168 - acc: 0.5499 - val_loss: 2.2871 - val_acc: 0.5641\n",
      "Epoch 3/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2473 - acc: 0.7365 - val_loss: 2.2257 - val_acc: 0.8077\n",
      "Epoch 4/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2265 - acc: 0.8219 - val_loss: 2.2328 - val_acc: 0.8077\n",
      "Epoch 5/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2007 - acc: 0.9060 - val_loss: 2.2007 - val_acc: 0.9103\n",
      "Epoch 6/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1990 - acc: 0.9060 - val_loss: 2.2111 - val_acc: 0.8846\n",
      "Epoch 7/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1871 - acc: 0.9373 - val_loss: 2.1976 - val_acc: 0.8718\n",
      "Epoch 8/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1817 - acc: 0.9573 - val_loss: 2.2182 - val_acc: 0.7179\n",
      "Epoch 9/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1805 - acc: 0.9416 - val_loss: 2.1899 - val_acc: 0.9103\n",
      "Epoch 10/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1815 - acc: 0.9672 - val_loss: 2.2020 - val_acc: 0.8846\n",
      "Epoch 11/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1756 - acc: 0.9772 - val_loss: 2.1999 - val_acc: 0.8974\n",
      "Epoch 12/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1778 - acc: 0.9744 - val_loss: 2.1876 - val_acc: 0.9103\n",
      "Epoch 13/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1738 - acc: 0.9786 - val_loss: 2.1914 - val_acc: 0.8974\n",
      "Epoch 14/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1766 - acc: 0.9729 - val_loss: 2.1817 - val_acc: 0.9231\n",
      "Epoch 15/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1716 - acc: 0.9886 - val_loss: 2.1883 - val_acc: 0.8590\n",
      "Epoch 16/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1720 - acc: 0.9786 - val_loss: 2.1841 - val_acc: 0.9231\n",
      "Epoch 17/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1725 - acc: 0.9929 - val_loss: 2.1862 - val_acc: 0.9103\n",
      "Epoch 18/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1695 - acc: 0.9957 - val_loss: 2.1913 - val_acc: 0.9487\n",
      "Epoch 19/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1717 - acc: 0.9915 - val_loss: 2.2270 - val_acc: 0.8718\n",
      "Epoch 20/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1750 - acc: 0.9843 - val_loss: 2.1858 - val_acc: 0.9487\n",
      "Epoch 21/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1706 - acc: 0.9915 - val_loss: 2.1802 - val_acc: 0.9487\n",
      "Epoch 22/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1704 - acc: 0.9915 - val_loss: 2.1826 - val_acc: 0.9487\n",
      "Epoch 23/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1686 - acc: 0.9986 - val_loss: 2.1818 - val_acc: 0.9103\n",
      "Epoch 24/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1690 - acc: 0.9986 - val_loss: 2.1801 - val_acc: 0.9487\n",
      "Epoch 25/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1734 - acc: 0.9858 - val_loss: 2.1803 - val_acc: 0.9359\n",
      "Epoch 26/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1681 - acc: 0.9957 - val_loss: 2.1840 - val_acc: 0.8974\n",
      "Epoch 27/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1682 - acc: 0.9943 - val_loss: 2.1790 - val_acc: 0.9231\n",
      "Epoch 28/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1697 - acc: 0.9872 - val_loss: 2.1802 - val_acc: 0.9487\n",
      "Epoch 29/100\n",
      "702/702 [==============================] - 2s 3ms/step - loss: 2.1709 - acc: 0.9900 - val_loss: 2.1809 - val_acc: 0.9359\n",
      "Epoch 30/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1676 - acc: 0.9986 - val_loss: 2.1792 - val_acc: 0.9487\n",
      "Epoch 31/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1674 - acc: 0.9972 - val_loss: 2.1796 - val_acc: 0.9487\n",
      "Epoch 32/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1674 - acc: 0.9972 - val_loss: 2.1785 - val_acc: 0.9487\n",
      "Epoch 33/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1673 - acc: 0.9986 - val_loss: 2.1788 - val_acc: 0.9487\n",
      "Epoch 34/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1672 - acc: 0.9972 - val_loss: 2.1785 - val_acc: 0.9359\n",
      "Epoch 35/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1669 - acc: 1.0000 - val_loss: 2.1801 - val_acc: 0.9487\n",
      "Epoch 36/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1670 - acc: 1.0000 - val_loss: 2.1799 - val_acc: 0.9487\n",
      "Epoch 37/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1678 - acc: 0.9943 - val_loss: 2.1809 - val_acc: 0.9487\n",
      "Epoch 38/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1677 - acc: 0.9986 - val_loss: 2.1769 - val_acc: 0.9487\n",
      "Epoch 39/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1667 - acc: 0.9986 - val_loss: 2.1775 - val_acc: 0.9359\n",
      "Epoch 40/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1667 - acc: 1.0000 - val_loss: 2.1784 - val_acc: 0.9487\n",
      "Epoch 41/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1775 - val_acc: 0.9359\n",
      "Epoch 42/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1776 - val_acc: 0.9359\n",
      "Epoch 43/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1779 - val_acc: 0.9487\n",
      "Epoch 44/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1783 - val_acc: 0.9487\n",
      "Epoch 45/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1788 - val_acc: 0.9359\n",
      "Epoch 46/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1773 - val_acc: 0.9359\n",
      "Epoch 47/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1790 - val_acc: 0.9487\n",
      "Epoch 48/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1673 - acc: 0.9986 - val_loss: 2.1798 - val_acc: 0.9487\n",
      "Epoch 49/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1802 - val_acc: 0.9359\n",
      "Epoch 50/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1669 - acc: 0.9986 - val_loss: 2.1771 - val_acc: 0.9359\n",
      "Epoch 51/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1782 - val_acc: 0.9359\n",
      "Epoch 52/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1779 - val_acc: 0.9487\n",
      "Epoch 53/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1781 - val_acc: 0.9359\n",
      "Epoch 54/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1779 - val_acc: 0.9359\n",
      "Epoch 55/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1791 - val_acc: 0.9359\n",
      "Epoch 56/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1798 - val_acc: 0.9359\n",
      "Epoch 57/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1792 - val_acc: 0.9359\n",
      "Epoch 58/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1774 - val_acc: 0.9487\n",
      "Epoch 59/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1775 - val_acc: 0.9231\n",
      "Epoch 60/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1776 - val_acc: 0.9359\n",
      "Epoch 61/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1803 - val_acc: 0.9487\n",
      "Epoch 62/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1776 - val_acc: 0.9359\n",
      "Epoch 63/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1777 - val_acc: 0.9359\n",
      "Epoch 64/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1797 - val_acc: 0.9359\n",
      "Epoch 65/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1777 - val_acc: 0.9359\n",
      "Epoch 66/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1795 - val_acc: 0.9359\n",
      "Epoch 67/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1786 - val_acc: 0.9487\n",
      "Epoch 68/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1775 - val_acc: 0.9359\n",
      "Epoch 69/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1786 - val_acc: 0.9359\n",
      "Epoch 70/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1783 - val_acc: 0.9359\n",
      "Epoch 71/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1776 - val_acc: 0.9487\n",
      "Epoch 72/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1776 - val_acc: 0.9359\n",
      "Epoch 73/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1786 - val_acc: 0.9359\n",
      "Epoch 74/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1778 - val_acc: 0.9359\n",
      "Epoch 75/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1783 - val_acc: 0.9359\n",
      "Epoch 76/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1780 - val_acc: 0.9359\n",
      "Epoch 77/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1786 - val_acc: 0.9487\n",
      "Epoch 78/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1783 - val_acc: 0.9359\n",
      "Epoch 79/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1783 - val_acc: 0.9359\n",
      "Epoch 80/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1779 - val_acc: 0.9359\n",
      "Epoch 81/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1783 - val_acc: 0.9359\n",
      "Epoch 82/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1781 - val_acc: 0.9359\n",
      "Epoch 83/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1785 - val_acc: 0.9359\n",
      "Epoch 84/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1781 - val_acc: 0.9359\n",
      "Epoch 85/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1783 - val_acc: 0.9359\n",
      "Epoch 86/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1786 - val_acc: 0.9487\n",
      "Epoch 87/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1782 - val_acc: 0.9359\n",
      "Epoch 88/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1784 - val_acc: 0.9359\n",
      "Epoch 89/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1775 - val_acc: 0.9359\n",
      "Epoch 90/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1774 - val_acc: 0.9359\n",
      "Epoch 91/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1781 - val_acc: 0.9359\n",
      "Epoch 92/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1781 - val_acc: 0.9359\n",
      "Epoch 93/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1785 - val_acc: 0.9359\n",
      "Epoch 94/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1783 - val_acc: 0.9359\n",
      "Epoch 95/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1784 - val_acc: 0.9359\n",
      "Epoch 96/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1784 - val_acc: 0.9359\n",
      "Epoch 97/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1786 - val_acc: 0.9359\n",
      "Epoch 98/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1778 - val_acc: 0.9359\n",
      "Epoch 99/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1782 - val_acc: 0.9359\n",
      "Epoch 100/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1784 - val_acc: 0.9359\n",
      "78/78 [==============================] - 0s 304us/step\n",
      "Score for fold 1: loss of 2.178477476804684; acc of 93.58974358974359%\n",
      "Train on 702 samples, validate on 78 samples\n",
      "Epoch 1/100\n",
      "702/702 [==============================] - 6s 8ms/step - loss: 2.4916 - acc: 0.2422 - val_loss: 2.3895 - val_acc: 0.4744\n",
      "Epoch 2/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.3094 - acc: 0.5655 - val_loss: 2.2856 - val_acc: 0.6667\n",
      "Epoch 3/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2419 - acc: 0.7707 - val_loss: 2.2557 - val_acc: 0.7179\n",
      "Epoch 4/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2229 - acc: 0.8276 - val_loss: 2.2648 - val_acc: 0.6795\n",
      "Epoch 5/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2052 - acc: 0.8818 - val_loss: 2.2411 - val_acc: 0.7821\n",
      "Epoch 6/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1982 - acc: 0.8989 - val_loss: 2.1972 - val_acc: 0.8974\n",
      "Epoch 7/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1836 - acc: 0.9587 - val_loss: 2.2653 - val_acc: 0.6923\n",
      "Epoch 8/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1899 - acc: 0.9274 - val_loss: 2.2075 - val_acc: 0.8590\n",
      "Epoch 9/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1871 - acc: 0.9402 - val_loss: 2.1841 - val_acc: 0.9359\n",
      "Epoch 10/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1781 - acc: 0.9701 - val_loss: 2.1902 - val_acc: 0.8718\n",
      "Epoch 11/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1801 - acc: 0.9615 - val_loss: 2.1814 - val_acc: 0.9487\n",
      "Epoch 12/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1791 - acc: 0.9658 - val_loss: 2.2082 - val_acc: 0.8590\n",
      "Epoch 13/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1730 - acc: 0.9815 - val_loss: 2.1808 - val_acc: 0.9359\n",
      "Epoch 14/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1744 - acc: 0.9815 - val_loss: 2.1955 - val_acc: 0.8974\n",
      "Epoch 15/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1757 - acc: 0.9729 - val_loss: 2.1786 - val_acc: 0.9359\n",
      "Epoch 16/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1716 - acc: 0.9929 - val_loss: 2.1785 - val_acc: 0.9231\n",
      "Epoch 17/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1704 - acc: 0.9915 - val_loss: 2.1814 - val_acc: 0.9231\n",
      "Epoch 18/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1713 - acc: 0.9858 - val_loss: 2.1810 - val_acc: 0.9487\n",
      "Epoch 19/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1754 - acc: 0.9858 - val_loss: 2.1968 - val_acc: 0.9103\n",
      "Epoch 20/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1697 - acc: 0.9943 - val_loss: 2.1763 - val_acc: 0.9487\n",
      "Epoch 21/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1703 - acc: 0.9929 - val_loss: 2.1755 - val_acc: 0.9615\n",
      "Epoch 22/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1698 - acc: 0.9943 - val_loss: 2.1811 - val_acc: 0.9359\n",
      "Epoch 23/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1703 - acc: 0.9943 - val_loss: 2.1744 - val_acc: 0.9359\n",
      "Epoch 24/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1690 - acc: 0.9986 - val_loss: 2.1768 - val_acc: 0.9615\n",
      "Epoch 25/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1684 - acc: 0.9957 - val_loss: 2.1749 - val_acc: 0.9359\n",
      "Epoch 26/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1687 - acc: 0.9943 - val_loss: 2.1773 - val_acc: 0.9615\n",
      "Epoch 27/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1701 - acc: 0.9886 - val_loss: 2.1832 - val_acc: 0.9487\n",
      "Epoch 28/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1681 - acc: 0.9972 - val_loss: 2.1750 - val_acc: 0.9615\n",
      "Epoch 29/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1677 - acc: 0.9986 - val_loss: 2.1760 - val_acc: 0.9615\n",
      "Epoch 30/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1699 - acc: 0.9900 - val_loss: 2.1750 - val_acc: 0.9615\n",
      "Epoch 31/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1677 - acc: 0.9972 - val_loss: 2.1804 - val_acc: 0.9615\n",
      "Epoch 32/100\n",
      "702/702 [==============================] - 2s 3ms/step - loss: 2.1679 - acc: 0.9986 - val_loss: 2.1724 - val_acc: 0.9615\n",
      "Epoch 33/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1672 - acc: 0.9972 - val_loss: 2.1727 - val_acc: 0.9487\n",
      "Epoch 34/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1671 - acc: 0.9986 - val_loss: 2.1826 - val_acc: 0.9615\n",
      "Epoch 35/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1676 - acc: 0.9957 - val_loss: 2.1747 - val_acc: 0.9744\n",
      "Epoch 36/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1670 - acc: 1.0000 - val_loss: 2.1704 - val_acc: 0.9872\n",
      "Epoch 37/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1668 - acc: 1.0000 - val_loss: 2.1756 - val_acc: 0.9744\n",
      "Epoch 38/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1669 - acc: 0.9986 - val_loss: 2.1722 - val_acc: 0.9615\n",
      "Epoch 39/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1667 - acc: 1.0000 - val_loss: 2.1846 - val_acc: 0.9359\n",
      "Epoch 40/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1678 - acc: 0.9972 - val_loss: 2.1724 - val_acc: 0.9872\n",
      "Epoch 41/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1726 - val_acc: 0.9615\n",
      "Epoch 42/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1667 - acc: 1.0000 - val_loss: 2.1738 - val_acc: 0.9872\n",
      "Epoch 43/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1666 - acc: 1.0000 - val_loss: 2.1773 - val_acc: 0.9744\n",
      "Epoch 44/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1727 - val_acc: 0.9744\n",
      "Epoch 45/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1672 - acc: 0.9986 - val_loss: 2.1721 - val_acc: 0.9744\n",
      "Epoch 46/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1666 - acc: 1.0000 - val_loss: 2.1725 - val_acc: 0.9744\n",
      "Epoch 47/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1720 - val_acc: 0.9615\n",
      "Epoch 48/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1710 - val_acc: 0.9744\n",
      "Epoch 49/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1727 - val_acc: 0.9744\n",
      "Epoch 50/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1762 - val_acc: 0.9615\n",
      "Epoch 51/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1709 - val_acc: 0.9872\n",
      "Epoch 52/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1703 - val_acc: 1.0000\n",
      "Epoch 53/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1716 - val_acc: 0.9744\n",
      "Epoch 54/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1699 - val_acc: 1.0000\n",
      "Epoch 55/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1667 - acc: 0.9986 - val_loss: 2.1705 - val_acc: 0.9744\n",
      "Epoch 56/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1702 - val_acc: 0.9872\n",
      "Epoch 57/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1706 - val_acc: 0.9744\n",
      "Epoch 58/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1721 - val_acc: 0.9744\n",
      "Epoch 59/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1733 - val_acc: 0.9744\n",
      "Epoch 60/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1707 - val_acc: 0.9744\n",
      "Epoch 61/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1711 - val_acc: 0.9615\n",
      "Epoch 62/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1707 - val_acc: 0.9744\n",
      "Epoch 63/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1696 - val_acc: 0.9872\n",
      "Epoch 64/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1688 - val_acc: 0.9872\n",
      "Epoch 65/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1703 - val_acc: 1.0000\n",
      "Epoch 66/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1702 - val_acc: 0.9872\n",
      "Epoch 67/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1705 - val_acc: 0.9744\n",
      "Epoch 68/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1694 - val_acc: 1.0000\n",
      "Epoch 69/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1700 - val_acc: 1.0000\n",
      "Epoch 70/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1706 - val_acc: 0.9872\n",
      "Epoch 71/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1706 - val_acc: 0.9872\n",
      "Epoch 72/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1699 - val_acc: 0.9872\n",
      "Epoch 73/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1761 - val_acc: 0.9872\n",
      "Epoch 74/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1707 - val_acc: 0.9744\n",
      "Epoch 75/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1703 - val_acc: 0.9872\n",
      "Epoch 76/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1705 - val_acc: 0.9872\n",
      "Epoch 77/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1704 - val_acc: 0.9872\n",
      "Epoch 78/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1700 - val_acc: 0.9872\n",
      "Epoch 79/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1699 - val_acc: 1.0000\n",
      "Epoch 80/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1705 - val_acc: 0.9872\n",
      "Epoch 81/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1697 - val_acc: 1.0000\n",
      "Epoch 82/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1702 - val_acc: 0.9872\n",
      "Epoch 83/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1699 - val_acc: 1.0000\n",
      "Epoch 84/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1699 - val_acc: 1.0000\n",
      "Epoch 85/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1697 - val_acc: 1.0000\n",
      "Epoch 86/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1703 - val_acc: 0.9744\n",
      "Epoch 87/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1701 - val_acc: 1.0000\n",
      "Epoch 88/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1696 - val_acc: 1.0000\n",
      "Epoch 89/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1704 - val_acc: 0.9872\n",
      "Epoch 90/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1698 - val_acc: 1.0000\n",
      "Epoch 91/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1701 - val_acc: 1.0000\n",
      "Epoch 92/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1702 - val_acc: 0.9872\n",
      "Epoch 93/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1697 - val_acc: 1.0000\n",
      "Epoch 94/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1701 - val_acc: 0.9872\n",
      "Epoch 95/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1699 - val_acc: 1.0000\n",
      "Epoch 96/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1698 - val_acc: 1.0000\n",
      "Epoch 97/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1704 - val_acc: 0.9872\n",
      "Epoch 98/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1697 - val_acc: 1.0000\n",
      "Epoch 99/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1700 - val_acc: 1.0000\n",
      "Epoch 100/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1699 - val_acc: 1.0000\n",
      "78/78 [==============================] - 0s 369us/step\n",
      "Score for fold 2: loss of 2.1700446911347218; acc of 98.71794871794873%\n",
      "Train on 702 samples, validate on 78 samples\n",
      "Epoch 1/100\n",
      "702/702 [==============================] - 6s 9ms/step - loss: 2.4822 - acc: 0.2721 - val_loss: 2.4171 - val_acc: 0.3846\n",
      "Epoch 2/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.3040 - acc: 0.6026 - val_loss: 2.3494 - val_acc: 0.6026\n",
      "Epoch 3/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2463 - acc: 0.7650 - val_loss: 2.2397 - val_acc: 0.7821\n",
      "Epoch 4/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2271 - acc: 0.8162 - val_loss: 2.2928 - val_acc: 0.6282\n",
      "Epoch 5/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2002 - acc: 0.8989 - val_loss: 2.2044 - val_acc: 0.8718\n",
      "Epoch 6/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1963 - acc: 0.9274 - val_loss: 2.2144 - val_acc: 0.8462\n",
      "Epoch 7/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1889 - acc: 0.9288 - val_loss: 2.1898 - val_acc: 0.9615\n",
      "Epoch 8/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1839 - acc: 0.9630 - val_loss: 2.1900 - val_acc: 0.9359\n",
      "Epoch 9/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1815 - acc: 0.9487 - val_loss: 2.1987 - val_acc: 0.9615\n",
      "Epoch 10/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1786 - acc: 0.9701 - val_loss: 2.1897 - val_acc: 0.9615\n",
      "Epoch 11/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1747 - acc: 0.9843 - val_loss: 2.1887 - val_acc: 0.9615\n",
      "Epoch 12/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1785 - acc: 0.9758 - val_loss: 2.1765 - val_acc: 0.9744\n",
      "Epoch 13/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1717 - acc: 0.9915 - val_loss: 2.1932 - val_acc: 0.9744\n",
      "Epoch 14/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1794 - acc: 0.9772 - val_loss: 2.1758 - val_acc: 0.9872\n",
      "Epoch 15/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1718 - acc: 0.9858 - val_loss: 2.2067 - val_acc: 0.9103\n",
      "Epoch 16/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1726 - acc: 0.9801 - val_loss: 2.1781 - val_acc: 0.9487\n",
      "Epoch 17/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1748 - acc: 0.9758 - val_loss: 2.1941 - val_acc: 0.9487\n",
      "Epoch 18/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1704 - acc: 0.9915 - val_loss: 2.1830 - val_acc: 0.9615\n",
      "Epoch 19/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1709 - acc: 0.9929 - val_loss: 2.1880 - val_acc: 0.9615\n",
      "Epoch 20/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1697 - acc: 0.9957 - val_loss: 2.1746 - val_acc: 0.9744\n",
      "Epoch 21/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1708 - acc: 0.9943 - val_loss: 2.1766 - val_acc: 0.9872\n",
      "Epoch 22/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1705 - acc: 0.9929 - val_loss: 2.1737 - val_acc: 0.9872\n",
      "Epoch 23/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1686 - acc: 0.9943 - val_loss: 2.1723 - val_acc: 0.9744\n",
      "Epoch 24/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1685 - acc: 0.9972 - val_loss: 2.1813 - val_acc: 0.9231\n",
      "Epoch 25/100\n",
      "702/702 [==============================] - 2s 3ms/step - loss: 2.1692 - acc: 0.9872 - val_loss: 2.2225 - val_acc: 0.8974\n",
      "Epoch 26/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1698 - acc: 0.9929 - val_loss: 2.1751 - val_acc: 0.9872\n",
      "Epoch 27/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1679 - acc: 1.0000 - val_loss: 2.1725 - val_acc: 0.9872\n",
      "Epoch 28/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1669 - acc: 1.0000 - val_loss: 2.1745 - val_acc: 1.0000\n",
      "Epoch 29/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1675 - acc: 1.0000 - val_loss: 2.1715 - val_acc: 1.0000\n",
      "Epoch 30/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1671 - acc: 0.9986 - val_loss: 2.1762 - val_acc: 0.9872\n",
      "Epoch 31/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1673 - acc: 1.0000 - val_loss: 2.1733 - val_acc: 0.9872\n",
      "Epoch 32/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1668 - acc: 1.0000 - val_loss: 2.1821 - val_acc: 0.9744\n",
      "Epoch 33/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1683 - acc: 0.9929 - val_loss: 2.1723 - val_acc: 0.9872\n",
      "Epoch 34/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1668 - acc: 1.0000 - val_loss: 2.1728 - val_acc: 0.9872\n",
      "Epoch 35/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1667 - acc: 1.0000 - val_loss: 2.1819 - val_acc: 0.9872\n",
      "Epoch 36/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1668 - acc: 1.0000 - val_loss: 2.1746 - val_acc: 0.9615\n",
      "Epoch 37/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1667 - acc: 1.0000 - val_loss: 2.1719 - val_acc: 0.9872\n",
      "Epoch 38/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1669 - acc: 0.9986 - val_loss: 2.1715 - val_acc: 0.9872\n",
      "Epoch 39/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1668 - acc: 0.9986 - val_loss: 2.1698 - val_acc: 1.0000\n",
      "Epoch 40/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1712 - val_acc: 1.0000\n",
      "Epoch 41/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1720 - val_acc: 1.0000\n",
      "Epoch 42/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1702 - val_acc: 0.9872\n",
      "Epoch 43/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1697 - val_acc: 1.0000\n",
      "Epoch 44/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1694 - val_acc: 1.0000\n",
      "Epoch 45/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1702 - val_acc: 1.0000\n",
      "Epoch 46/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1706 - val_acc: 1.0000\n",
      "Epoch 47/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 0.9986 - val_loss: 2.1699 - val_acc: 1.0000\n",
      "Epoch 48/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1695 - val_acc: 0.9872\n",
      "Epoch 49/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1712 - val_acc: 0.9872\n",
      "Epoch 50/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1703 - val_acc: 1.0000\n",
      "Epoch 51/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1707 - val_acc: 1.0000\n",
      "Epoch 52/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1713 - val_acc: 0.9872\n",
      "Epoch 53/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1702 - val_acc: 0.9872\n",
      "Epoch 54/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1701 - val_acc: 0.9872\n",
      "Epoch 55/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1710 - val_acc: 1.0000\n",
      "Epoch 56/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1724 - val_acc: 0.9872\n",
      "Epoch 57/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1705 - val_acc: 1.0000\n",
      "Epoch 58/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1698 - val_acc: 0.9872\n",
      "Epoch 59/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1705 - val_acc: 1.0000\n",
      "Epoch 60/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1723 - val_acc: 0.9872\n",
      "Epoch 61/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1704 - val_acc: 0.9872\n",
      "Epoch 62/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1714 - val_acc: 0.9872\n",
      "Epoch 63/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1702 - val_acc: 1.0000\n",
      "Epoch 64/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1712 - val_acc: 0.9872\n",
      "Epoch 65/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1700 - val_acc: 1.0000\n",
      "Epoch 66/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1726 - val_acc: 0.9872\n",
      "Epoch 67/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1709 - val_acc: 1.0000\n",
      "Epoch 68/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1704 - val_acc: 0.9872\n",
      "Epoch 69/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1711 - val_acc: 1.0000\n",
      "Epoch 70/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1700 - val_acc: 1.0000\n",
      "Epoch 71/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1689 - val_acc: 1.0000\n",
      "Epoch 72/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1714 - val_acc: 1.0000\n",
      "Epoch 73/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1705 - val_acc: 1.0000\n",
      "Epoch 74/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1703 - val_acc: 1.0000\n",
      "Epoch 75/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1693 - val_acc: 1.0000\n",
      "Epoch 76/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1697 - val_acc: 1.0000\n",
      "Epoch 77/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1695 - val_acc: 1.0000\n",
      "Epoch 78/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1693 - val_acc: 1.0000\n",
      "Epoch 79/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1715 - val_acc: 0.9872\n",
      "Epoch 80/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1706 - val_acc: 1.0000\n",
      "Epoch 81/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1708 - val_acc: 1.0000\n",
      "Epoch 82/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1703 - val_acc: 1.0000\n",
      "Epoch 83/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1713 - val_acc: 0.9872\n",
      "Epoch 84/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1702 - val_acc: 1.0000\n",
      "Epoch 85/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1708 - val_acc: 1.0000\n",
      "Epoch 86/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1710 - val_acc: 1.0000\n",
      "Epoch 87/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1715 - val_acc: 1.0000\n",
      "Epoch 88/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1715 - val_acc: 1.0000\n",
      "Epoch 89/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1704 - val_acc: 1.0000\n",
      "Epoch 90/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1706 - val_acc: 1.0000\n",
      "Epoch 91/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1712 - val_acc: 1.0000\n",
      "Epoch 92/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1717 - val_acc: 1.0000\n",
      "Epoch 93/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1716 - val_acc: 1.0000\n",
      "Epoch 94/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1712 - val_acc: 1.0000\n",
      "Epoch 95/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1708 - val_acc: 1.0000\n",
      "Epoch 96/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1706 - val_acc: 1.0000\n",
      "Epoch 97/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1710 - val_acc: 1.0000\n",
      "Epoch 98/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1711 - val_acc: 0.9872\n",
      "Epoch 99/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1705 - val_acc: 1.0000\n",
      "Epoch 100/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1708 - val_acc: 1.0000\n",
      "78/78 [==============================] - 0s 306us/step\n",
      "Score for fold 3: loss of 2.1706254054338503; acc of 98.71794825945145%\n",
      "Train on 702 samples, validate on 78 samples\n",
      "Epoch 1/100\n",
      "702/702 [==============================] - 6s 9ms/step - loss: 2.5005 - acc: 0.2749 - val_loss: 2.3662 - val_acc: 0.4487\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2937 - acc: 0.6040 - val_loss: 2.2823 - val_acc: 0.7051\n",
      "Epoch 3/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2482 - acc: 0.7293 - val_loss: 2.2415 - val_acc: 0.7821\n",
      "Epoch 4/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2148 - acc: 0.8291 - val_loss: 2.2097 - val_acc: 0.8846\n",
      "Epoch 5/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2074 - acc: 0.8519 - val_loss: 2.2139 - val_acc: 0.8590\n",
      "Epoch 6/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1972 - acc: 0.9174 - val_loss: 2.2342 - val_acc: 0.7692\n",
      "Epoch 7/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1888 - acc: 0.9288 - val_loss: 2.1815 - val_acc: 0.9231\n",
      "Epoch 8/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1863 - acc: 0.9416 - val_loss: 2.1851 - val_acc: 0.8718\n",
      "Epoch 9/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1835 - acc: 0.9430 - val_loss: 2.2095 - val_acc: 0.8718\n",
      "Epoch 10/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1813 - acc: 0.9573 - val_loss: 2.1883 - val_acc: 0.8590\n",
      "Epoch 11/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1783 - acc: 0.9658 - val_loss: 2.1757 - val_acc: 0.9615\n",
      "Epoch 12/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1774 - acc: 0.9772 - val_loss: 2.1923 - val_acc: 0.8077\n",
      "Epoch 13/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1733 - acc: 0.9715 - val_loss: 2.1776 - val_acc: 1.0000\n",
      "Epoch 14/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1758 - acc: 0.9829 - val_loss: 2.1734 - val_acc: 0.9487\n",
      "Epoch 15/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1728 - acc: 0.9886 - val_loss: 2.1748 - val_acc: 0.9744\n",
      "Epoch 16/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1724 - acc: 0.9872 - val_loss: 2.2018 - val_acc: 0.9103\n",
      "Epoch 17/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1762 - acc: 0.9815 - val_loss: 2.1723 - val_acc: 0.9872\n",
      "Epoch 18/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1715 - acc: 0.9915 - val_loss: 2.1717 - val_acc: 0.9744\n",
      "Epoch 19/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1733 - acc: 0.9872 - val_loss: 2.1721 - val_acc: 0.9744\n",
      "Epoch 20/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1692 - acc: 0.9957 - val_loss: 2.1718 - val_acc: 1.0000\n",
      "Epoch 21/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1733 - acc: 0.9815 - val_loss: 2.1702 - val_acc: 0.9872\n",
      "Epoch 22/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1689 - acc: 0.9957 - val_loss: 2.1753 - val_acc: 0.9487\n",
      "Epoch 23/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1706 - acc: 0.9915 - val_loss: 2.1717 - val_acc: 0.9615\n",
      "Epoch 24/100\n",
      "702/702 [==============================] - 3s 4ms/step - loss: 2.1709 - acc: 0.9915 - val_loss: 2.1767 - val_acc: 0.9744\n",
      "Epoch 25/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1681 - acc: 0.9972 - val_loss: 2.1718 - val_acc: 0.9872\n",
      "Epoch 26/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1683 - acc: 0.9986 - val_loss: 2.1701 - val_acc: 0.9872\n",
      "Epoch 27/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1684 - acc: 0.9943 - val_loss: 2.1705 - val_acc: 0.9872\n",
      "Epoch 28/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1676 - acc: 0.9986 - val_loss: 2.1707 - val_acc: 0.9872\n",
      "Epoch 29/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1676 - acc: 0.9986 - val_loss: 2.1701 - val_acc: 0.9872\n",
      "Epoch 30/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1680 - acc: 0.9986 - val_loss: 2.1696 - val_acc: 0.9872\n",
      "Epoch 31/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1672 - acc: 0.9986 - val_loss: 2.1693 - val_acc: 0.9872\n",
      "Epoch 32/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1679 - acc: 0.9972 - val_loss: 2.1686 - val_acc: 1.0000\n",
      "Epoch 33/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1673 - acc: 0.9986 - val_loss: 2.1724 - val_acc: 0.9744\n",
      "Epoch 34/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1685 - acc: 0.9957 - val_loss: 2.1695 - val_acc: 0.9615\n",
      "Epoch 35/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1668 - acc: 1.0000 - val_loss: 2.1691 - val_acc: 0.9872\n",
      "Epoch 36/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1668 - acc: 1.0000 - val_loss: 2.1705 - val_acc: 0.9744\n",
      "Epoch 37/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1668 - acc: 1.0000 - val_loss: 2.1696 - val_acc: 0.9744\n",
      "Epoch 38/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1667 - acc: 1.0000 - val_loss: 2.1693 - val_acc: 0.9872\n",
      "Epoch 39/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1666 - acc: 1.0000 - val_loss: 2.1697 - val_acc: 0.9872\n",
      "Epoch 40/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1667 - acc: 1.0000 - val_loss: 2.1703 - val_acc: 0.9872\n",
      "Epoch 41/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1667 - acc: 0.9986 - val_loss: 2.1688 - val_acc: 1.0000\n",
      "Epoch 42/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1666 - acc: 0.9986 - val_loss: 2.1689 - val_acc: 0.9872\n",
      "Epoch 43/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1666 - acc: 1.0000 - val_loss: 2.1690 - val_acc: 0.9872\n",
      "Epoch 44/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1669 - acc: 1.0000 - val_loss: 2.1688 - val_acc: 0.9872\n",
      "Epoch 45/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1692 - val_acc: 0.9872\n",
      "Epoch 46/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1687 - val_acc: 0.9872\n",
      "Epoch 47/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1684 - val_acc: 0.9872\n",
      "Epoch 48/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1689 - val_acc: 0.9872\n",
      "Epoch 49/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1691 - val_acc: 0.9872\n",
      "Epoch 50/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1689 - val_acc: 0.9872\n",
      "Epoch 51/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1688 - val_acc: 0.9872\n",
      "Epoch 52/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1685 - val_acc: 0.9872\n",
      "Epoch 53/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1690 - val_acc: 0.9872\n",
      "Epoch 54/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1687 - val_acc: 0.9872\n",
      "Epoch 55/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1685 - val_acc: 0.9872\n",
      "Epoch 56/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1688 - val_acc: 0.9872\n",
      "Epoch 57/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1690 - val_acc: 0.9744\n",
      "Epoch 58/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 0.9986 - val_loss: 2.1685 - val_acc: 0.9872\n",
      "Epoch 59/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1685 - val_acc: 0.9872\n",
      "Epoch 60/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1684 - val_acc: 0.9872\n",
      "Epoch 61/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1688 - val_acc: 0.9872\n",
      "Epoch 62/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1683 - val_acc: 0.9872\n",
      "Epoch 63/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1683 - val_acc: 0.9872\n",
      "Epoch 64/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1686 - val_acc: 0.9872\n",
      "Epoch 65/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1684 - val_acc: 0.9872\n",
      "Epoch 66/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1684 - val_acc: 0.9872\n",
      "Epoch 67/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1683 - val_acc: 0.9872\n",
      "Epoch 68/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1682 - val_acc: 0.9872\n",
      "Epoch 69/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1684 - val_acc: 0.9872\n",
      "Epoch 70/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1682 - val_acc: 0.9872\n",
      "Epoch 71/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1679 - val_acc: 0.9872\n",
      "Epoch 72/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1682 - val_acc: 0.9872\n",
      "Epoch 73/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1681 - val_acc: 0.9872\n",
      "Epoch 74/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1683 - val_acc: 0.9872\n",
      "Epoch 75/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1682 - val_acc: 0.9872\n",
      "Epoch 76/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1683 - val_acc: 0.9872\n",
      "Epoch 77/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1683 - val_acc: 0.9872\n",
      "Epoch 78/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1683 - val_acc: 0.9872\n",
      "Epoch 79/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1682 - val_acc: 0.9872\n",
      "Epoch 80/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1683 - val_acc: 0.9872\n",
      "Epoch 81/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1682 - val_acc: 0.9872\n",
      "Epoch 82/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1683 - val_acc: 0.9872\n",
      "Epoch 83/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1683 - val_acc: 0.9872\n",
      "Epoch 84/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1682 - val_acc: 0.9872\n",
      "Epoch 85/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1681 - val_acc: 0.9872\n",
      "Epoch 86/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1682 - val_acc: 0.9872\n",
      "Epoch 87/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1683 - val_acc: 0.9872\n",
      "Epoch 88/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1682 - val_acc: 0.9872\n",
      "Epoch 89/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1683 - val_acc: 0.9872\n",
      "Epoch 90/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1683 - val_acc: 0.9872\n",
      "Epoch 91/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1683 - val_acc: 0.9872\n",
      "Epoch 92/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1682 - val_acc: 0.9872\n",
      "Epoch 93/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1681 - val_acc: 0.9872\n",
      "Epoch 94/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1683 - val_acc: 0.9872\n",
      "Epoch 95/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1681 - val_acc: 0.9872\n",
      "Epoch 96/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1682 - val_acc: 0.9872\n",
      "Epoch 97/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1683 - val_acc: 0.9872\n",
      "Epoch 98/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1681 - val_acc: 0.9872\n",
      "Epoch 99/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1682 - val_acc: 0.9872\n",
      "Epoch 100/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1682 - val_acc: 0.9872\n",
      "78/78 [==============================] - 0s 331us/step\n",
      "Score for fold 4: loss of 2.1684941328488865; acc of 98.71794825945145%\n",
      "Train on 702 samples, validate on 78 samples\n",
      "Epoch 1/100\n",
      "702/702 [==============================] - 7s 9ms/step - loss: 2.4870 - acc: 0.2707 - val_loss: 2.3692 - val_acc: 0.5641\n",
      "Epoch 2/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2920 - acc: 0.6311 - val_loss: 2.2755 - val_acc: 0.7051\n",
      "Epoch 3/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2510 - acc: 0.7365 - val_loss: 2.2448 - val_acc: 0.8333\n",
      "Epoch 4/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2297 - acc: 0.8205 - val_loss: 2.2478 - val_acc: 0.7949\n",
      "Epoch 5/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2048 - acc: 0.8789 - val_loss: 2.2094 - val_acc: 0.8846\n",
      "Epoch 6/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1948 - acc: 0.9174 - val_loss: 2.1997 - val_acc: 0.8846\n",
      "Epoch 7/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1939 - acc: 0.9188 - val_loss: 2.2133 - val_acc: 0.8077\n",
      "Epoch 8/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1830 - acc: 0.9615 - val_loss: 2.1845 - val_acc: 0.8718\n",
      "Epoch 9/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1854 - acc: 0.9459 - val_loss: 2.1839 - val_acc: 0.9744\n",
      "Epoch 10/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1768 - acc: 0.9772 - val_loss: 2.2019 - val_acc: 0.8333\n",
      "Epoch 11/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1803 - acc: 0.9701 - val_loss: 2.1820 - val_acc: 0.9615\n",
      "Epoch 12/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1763 - acc: 0.9744 - val_loss: 2.1882 - val_acc: 0.9359\n",
      "Epoch 13/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1744 - acc: 0.9843 - val_loss: 2.1964 - val_acc: 0.8718\n",
      "Epoch 14/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1722 - acc: 0.9886 - val_loss: 2.1799 - val_acc: 0.9615\n",
      "Epoch 15/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1732 - acc: 0.9829 - val_loss: 2.1850 - val_acc: 0.9487\n",
      "Epoch 16/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1735 - acc: 0.9843 - val_loss: 2.1776 - val_acc: 0.9615\n",
      "Epoch 17/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1736 - acc: 0.9858 - val_loss: 2.1790 - val_acc: 0.9744\n",
      "Epoch 18/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1703 - acc: 0.9957 - val_loss: 2.1760 - val_acc: 0.9744\n",
      "Epoch 19/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1694 - acc: 0.9957 - val_loss: 2.1788 - val_acc: 0.9744\n",
      "Epoch 20/100\n",
      "702/702 [==============================] - 3s 4ms/step - loss: 2.1716 - acc: 0.9886 - val_loss: 2.1752 - val_acc: 0.9487\n",
      "Epoch 21/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1699 - acc: 0.9915 - val_loss: 2.1731 - val_acc: 0.9744\n",
      "Epoch 22/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1680 - acc: 0.9972 - val_loss: 2.1749 - val_acc: 0.9744\n",
      "Epoch 23/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1676 - acc: 1.0000 - val_loss: 2.1737 - val_acc: 0.9744\n",
      "Epoch 24/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1695 - acc: 0.9929 - val_loss: 2.1745 - val_acc: 0.9744\n",
      "Epoch 25/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1675 - acc: 0.9986 - val_loss: 2.1737 - val_acc: 0.9744\n",
      "Epoch 26/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1673 - acc: 0.9986 - val_loss: 2.1746 - val_acc: 0.9615\n",
      "Epoch 27/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1673 - acc: 1.0000 - val_loss: 2.1730 - val_acc: 0.9744\n",
      "Epoch 28/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1686 - acc: 0.9972 - val_loss: 2.1736 - val_acc: 0.9872\n",
      "Epoch 29/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1669 - acc: 1.0000 - val_loss: 2.1734 - val_acc: 0.9872\n",
      "Epoch 30/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1671 - acc: 1.0000 - val_loss: 2.1746 - val_acc: 0.9872\n",
      "Epoch 31/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1673 - acc: 1.0000 - val_loss: 2.1739 - val_acc: 0.9744\n",
      "Epoch 32/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1674 - acc: 1.0000 - val_loss: 2.1760 - val_acc: 0.9744\n",
      "Epoch 33/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1671 - acc: 1.0000 - val_loss: 2.1725 - val_acc: 0.9872\n",
      "Epoch 34/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1684 - acc: 0.9929 - val_loss: 2.1735 - val_acc: 0.9872\n",
      "Epoch 35/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1666 - acc: 1.0000 - val_loss: 2.1732 - val_acc: 1.0000\n",
      "Epoch 36/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1668 - acc: 1.0000 - val_loss: 2.1722 - val_acc: 0.9744\n",
      "Epoch 37/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1675 - acc: 1.0000 - val_loss: 2.1722 - val_acc: 0.9744\n",
      "Epoch 38/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1667 - acc: 1.0000 - val_loss: 2.1714 - val_acc: 0.9872\n",
      "Epoch 39/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1669 - acc: 0.9986 - val_loss: 2.1721 - val_acc: 0.9872\n",
      "Epoch 40/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1677 - acc: 0.9972 - val_loss: 2.1754 - val_acc: 0.9615\n",
      "Epoch 41/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1667 - acc: 1.0000 - val_loss: 2.1727 - val_acc: 0.9744\n",
      "Epoch 42/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1667 - acc: 0.9986 - val_loss: 2.1743 - val_acc: 0.9872\n",
      "Epoch 43/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1670 - acc: 0.9943 - val_loss: 2.1736 - val_acc: 0.9744\n",
      "Epoch 44/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1674 - acc: 0.9972 - val_loss: 2.1721 - val_acc: 0.9872\n",
      "Epoch 45/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1666 - acc: 1.0000 - val_loss: 2.1718 - val_acc: 0.9872\n",
      "Epoch 46/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1667 - acc: 1.0000 - val_loss: 2.1774 - val_acc: 0.9744\n",
      "Epoch 47/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1669 - acc: 1.0000 - val_loss: 2.1705 - val_acc: 0.9872\n",
      "Epoch 48/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1702 - val_acc: 0.9872\n",
      "Epoch 49/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1707 - val_acc: 0.9872\n",
      "Epoch 50/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1708 - val_acc: 0.9872\n",
      "Epoch 51/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1740 - val_acc: 0.9744\n",
      "Epoch 52/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1666 - acc: 0.9972 - val_loss: 2.1702 - val_acc: 0.9872\n",
      "Epoch 53/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1711 - val_acc: 0.9872\n",
      "Epoch 54/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1712 - val_acc: 0.9872\n",
      "Epoch 55/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1668 - acc: 0.9986 - val_loss: 2.1711 - val_acc: 0.9872\n",
      "Epoch 56/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1702 - val_acc: 0.9872\n",
      "Epoch 57/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1698 - val_acc: 0.9872\n",
      "Epoch 58/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1709 - val_acc: 0.9872\n",
      "Epoch 59/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1703 - val_acc: 0.9872\n",
      "Epoch 60/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1710 - val_acc: 1.0000\n",
      "Epoch 61/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1703 - val_acc: 0.9872\n",
      "Epoch 62/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1700 - val_acc: 0.9872\n",
      "Epoch 63/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1712 - val_acc: 0.9872\n",
      "Epoch 64/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1702 - val_acc: 0.9872\n",
      "Epoch 65/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1721 - val_acc: 0.9872\n",
      "Epoch 66/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1725 - val_acc: 0.9744\n",
      "Epoch 67/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1708 - val_acc: 0.9872\n",
      "Epoch 68/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1705 - val_acc: 0.9872\n",
      "Epoch 69/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1709 - val_acc: 0.9872\n",
      "Epoch 70/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1713 - val_acc: 0.9872\n",
      "Epoch 71/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1710 - val_acc: 0.9872\n",
      "Epoch 72/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1711 - val_acc: 0.9872\n",
      "Epoch 73/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1704 - val_acc: 1.0000\n",
      "Epoch 74/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1698 - val_acc: 0.9872\n",
      "Epoch 75/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1698 - val_acc: 0.9872\n",
      "Epoch 76/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1704 - val_acc: 0.9872\n",
      "Epoch 77/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1705 - val_acc: 0.9872\n",
      "Epoch 78/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1702 - val_acc: 0.9872\n",
      "Epoch 79/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1707 - val_acc: 0.9872\n",
      "Epoch 80/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1709 - val_acc: 0.9872\n",
      "Epoch 81/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1702 - val_acc: 0.9872\n",
      "Epoch 82/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1703 - val_acc: 0.9872\n",
      "Epoch 83/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1704 - val_acc: 0.9872\n",
      "Epoch 84/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1698 - val_acc: 0.9872\n",
      "Epoch 85/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1704 - val_acc: 0.9872\n",
      "Epoch 86/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1703 - val_acc: 0.9872\n",
      "Epoch 87/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1701 - val_acc: 0.9872\n",
      "Epoch 88/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1694 - val_acc: 0.9872\n",
      "Epoch 89/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1694 - val_acc: 0.9872\n",
      "Epoch 90/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1698 - val_acc: 0.9872\n",
      "Epoch 91/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1696 - val_acc: 0.9872\n",
      "Epoch 92/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1698 - val_acc: 0.9872\n",
      "Epoch 93/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1699 - val_acc: 0.9872\n",
      "Epoch 94/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1694 - val_acc: 0.9872\n",
      "Epoch 95/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1697 - val_acc: 0.9872\n",
      "Epoch 96/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1701 - val_acc: 0.9872\n",
      "Epoch 97/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1703 - val_acc: 0.9872\n",
      "Epoch 98/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1699 - val_acc: 0.9872\n",
      "Epoch 99/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1704 - val_acc: 0.9872\n",
      "Epoch 100/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1700 - val_acc: 0.9872\n",
      "78/78 [==============================] - 0s 298us/step\n",
      "Score for fold 5: loss of 2.16922141955449; acc of 98.71794871794873%\n",
      "Train on 702 samples, validate on 78 samples\n",
      "Epoch 1/100\n",
      "702/702 [==============================] - 7s 9ms/step - loss: 2.4712 - acc: 0.2892 - val_loss: 2.3428 - val_acc: 0.5769\n",
      "Epoch 2/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2950 - acc: 0.6339 - val_loss: 2.2526 - val_acc: 0.7051\n",
      "Epoch 3/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2486 - acc: 0.7336 - val_loss: 2.2134 - val_acc: 0.8846\n",
      "Epoch 4/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2160 - acc: 0.8291 - val_loss: 2.2234 - val_acc: 0.7949\n",
      "Epoch 5/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2044 - acc: 0.8932 - val_loss: 2.1954 - val_acc: 0.8846\n",
      "Epoch 6/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1948 - acc: 0.9202 - val_loss: 2.2054 - val_acc: 0.7949\n",
      "Epoch 7/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1978 - acc: 0.9245 - val_loss: 2.1983 - val_acc: 0.8590\n",
      "Epoch 8/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1810 - acc: 0.9587 - val_loss: 2.2305 - val_acc: 0.7949\n",
      "Epoch 9/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1856 - acc: 0.9359 - val_loss: 2.2104 - val_acc: 0.8077\n",
      "Epoch 10/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1797 - acc: 0.9687 - val_loss: 2.1842 - val_acc: 0.9615\n",
      "Epoch 11/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1783 - acc: 0.9715 - val_loss: 2.1888 - val_acc: 0.8974\n",
      "Epoch 12/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1735 - acc: 0.9900 - val_loss: 2.1834 - val_acc: 0.8718\n",
      "Epoch 13/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1774 - acc: 0.9701 - val_loss: 2.1895 - val_acc: 0.8846\n",
      "Epoch 14/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1759 - acc: 0.9815 - val_loss: 2.1775 - val_acc: 0.9744\n",
      "Epoch 15/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1750 - acc: 0.9915 - val_loss: 2.1802 - val_acc: 0.9487\n",
      "Epoch 16/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1754 - acc: 0.9829 - val_loss: 2.1772 - val_acc: 0.9487\n",
      "Epoch 17/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1712 - acc: 0.9929 - val_loss: 2.1757 - val_acc: 0.9744\n",
      "Epoch 18/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1728 - acc: 0.9829 - val_loss: 2.1727 - val_acc: 0.9744\n",
      "Epoch 19/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1709 - acc: 0.9843 - val_loss: 2.1724 - val_acc: 0.9872\n",
      "Epoch 20/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1715 - acc: 0.9915 - val_loss: 2.1772 - val_acc: 0.9615\n",
      "Epoch 21/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1719 - acc: 0.9900 - val_loss: 2.1727 - val_acc: 0.9872\n",
      "Epoch 22/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1687 - acc: 0.9972 - val_loss: 2.1738 - val_acc: 0.9103\n",
      "Epoch 23/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1706 - acc: 0.9900 - val_loss: 2.1787 - val_acc: 0.9744\n",
      "Epoch 24/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1707 - acc: 0.9929 - val_loss: 2.1744 - val_acc: 0.9872\n",
      "Epoch 25/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1680 - acc: 0.9957 - val_loss: 2.1746 - val_acc: 0.9615\n",
      "Epoch 26/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1680 - acc: 0.9986 - val_loss: 2.1724 - val_acc: 0.9872\n",
      "Epoch 27/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1694 - acc: 0.9972 - val_loss: 2.1830 - val_acc: 0.9487\n",
      "Epoch 28/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1680 - acc: 0.9972 - val_loss: 2.1733 - val_acc: 0.9744\n",
      "Epoch 29/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1680 - acc: 0.9957 - val_loss: 2.1747 - val_acc: 0.9615\n",
      "Epoch 30/100\n",
      "702/702 [==============================] - 3s 4ms/step - loss: 2.1713 - acc: 0.9929 - val_loss: 2.1726 - val_acc: 0.9615\n",
      "Epoch 31/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1673 - acc: 0.9972 - val_loss: 2.1707 - val_acc: 0.9872\n",
      "Epoch 32/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1671 - acc: 0.9972 - val_loss: 2.1708 - val_acc: 0.9872\n",
      "Epoch 33/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1670 - acc: 0.9986 - val_loss: 2.1720 - val_acc: 0.9872\n",
      "Epoch 34/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1674 - acc: 0.9986 - val_loss: 2.1724 - val_acc: 0.9872\n",
      "Epoch 35/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1669 - acc: 1.0000 - val_loss: 2.1718 - val_acc: 0.9872\n",
      "Epoch 36/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1667 - acc: 1.0000 - val_loss: 2.1726 - val_acc: 0.9744\n",
      "Epoch 37/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1669 - acc: 0.9986 - val_loss: 2.1709 - val_acc: 0.9872\n",
      "Epoch 38/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1672 - acc: 0.9986 - val_loss: 2.1716 - val_acc: 0.9872\n",
      "Epoch 39/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1667 - acc: 1.0000 - val_loss: 2.1703 - val_acc: 0.9872\n",
      "Epoch 40/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1666 - acc: 0.9986 - val_loss: 2.1704 - val_acc: 0.9872\n",
      "Epoch 41/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1701 - val_acc: 0.9872\n",
      "Epoch 42/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1707 - val_acc: 0.9872\n",
      "Epoch 43/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1703 - val_acc: 0.9872\n",
      "Epoch 44/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1714 - val_acc: 0.9872\n",
      "Epoch 45/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1705 - val_acc: 0.9872\n",
      "Epoch 46/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1713 - val_acc: 0.9615\n",
      "Epoch 47/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1708 - val_acc: 0.9872\n",
      "Epoch 48/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1705 - val_acc: 0.9872\n",
      "Epoch 49/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1707 - val_acc: 0.9872\n",
      "Epoch 50/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 0.9986 - val_loss: 2.1707 - val_acc: 0.9872\n",
      "Epoch 51/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1709 - val_acc: 0.9872\n",
      "Epoch 52/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1704 - val_acc: 0.9872\n",
      "Epoch 53/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1713 - val_acc: 0.9872\n",
      "Epoch 54/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1701 - val_acc: 0.9744\n",
      "Epoch 55/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1706 - val_acc: 0.9872\n",
      "Epoch 56/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1708 - val_acc: 0.9615\n",
      "Epoch 57/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1704 - val_acc: 0.9872\n",
      "Epoch 58/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1702 - val_acc: 0.9744\n",
      "Epoch 59/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1709 - val_acc: 0.9872\n",
      "Epoch 60/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1703 - val_acc: 0.9872\n",
      "Epoch 61/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1701 - val_acc: 0.9872\n",
      "Epoch 62/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1700 - val_acc: 0.9872\n",
      "Epoch 63/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1712 - val_acc: 0.9872\n",
      "Epoch 64/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1701 - val_acc: 0.9872\n",
      "Epoch 65/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1702 - val_acc: 0.9872\n",
      "Epoch 66/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1698 - val_acc: 0.9872\n",
      "Epoch 67/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1706 - val_acc: 0.9872\n",
      "Epoch 68/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1699 - val_acc: 0.9872\n",
      "Epoch 69/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1701 - val_acc: 0.9872\n",
      "Epoch 70/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1697 - val_acc: 0.9872\n",
      "Epoch 71/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1700 - val_acc: 0.9872\n",
      "Epoch 72/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1702 - val_acc: 0.9872\n",
      "Epoch 73/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1698 - val_acc: 0.9744\n",
      "Epoch 74/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1697 - val_acc: 0.9744\n",
      "Epoch 75/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1699 - val_acc: 0.9872\n",
      "Epoch 76/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1697 - val_acc: 0.9872\n",
      "Epoch 77/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1698 - val_acc: 0.9744\n",
      "Epoch 78/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1697 - val_acc: 0.9872\n",
      "Epoch 79/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1697 - val_acc: 0.9872\n",
      "Epoch 80/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1699 - val_acc: 0.9872\n",
      "Epoch 81/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1698 - val_acc: 0.9744\n",
      "Epoch 82/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1700 - val_acc: 0.9872\n",
      "Epoch 83/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1698 - val_acc: 0.9744\n",
      "Epoch 84/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1701 - val_acc: 0.9872\n",
      "Epoch 85/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1698 - val_acc: 0.9872\n",
      "Epoch 86/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1698 - val_acc: 0.9872\n",
      "Epoch 87/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1698 - val_acc: 0.9872\n",
      "Epoch 88/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1697 - val_acc: 0.9744\n",
      "Epoch 89/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1697 - val_acc: 0.9872\n",
      "Epoch 90/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1696 - val_acc: 0.9872\n",
      "Epoch 91/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1697 - val_acc: 0.9872\n",
      "Epoch 92/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1699 - val_acc: 0.9872\n",
      "Epoch 93/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1699 - val_acc: 0.9872\n",
      "Epoch 94/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1697 - val_acc: 0.9872\n",
      "Epoch 95/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1699 - val_acc: 0.9872\n",
      "Epoch 96/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1697 - val_acc: 0.9872\n",
      "Epoch 97/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1697 - val_acc: 0.9872\n",
      "Epoch 98/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1698 - val_acc: 0.9872\n",
      "Epoch 99/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1697 - val_acc: 0.9872\n",
      "Epoch 100/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1700 - val_acc: 0.9872\n",
      "78/78 [==============================] - 0s 299us/step\n",
      "Score for fold 6: loss of 2.1701784317310038; acc of 98.71794871794873%\n",
      "Train on 702 samples, validate on 78 samples\n",
      "Epoch 1/100\n",
      "702/702 [==============================] - 7s 10ms/step - loss: 2.5260 - acc: 0.2493 - val_loss: 2.4491 - val_acc: 0.3077\n",
      "Epoch 2/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.3269 - acc: 0.5071 - val_loss: 2.2702 - val_acc: 0.5385\n",
      "Epoch 3/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2594 - acc: 0.6966 - val_loss: 2.2248 - val_acc: 0.7308\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2200 - acc: 0.8291 - val_loss: 2.2263 - val_acc: 0.7564\n",
      "Epoch 5/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2029 - acc: 0.8903 - val_loss: 2.2661 - val_acc: 0.7821\n",
      "Epoch 6/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2141 - acc: 0.8632 - val_loss: 2.2175 - val_acc: 0.8718\n",
      "Epoch 7/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1866 - acc: 0.9259 - val_loss: 2.2060 - val_acc: 0.8846\n",
      "Epoch 8/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1820 - acc: 0.9601 - val_loss: 2.1880 - val_acc: 0.9487\n",
      "Epoch 9/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1833 - acc: 0.9516 - val_loss: 2.1915 - val_acc: 0.9103\n",
      "Epoch 10/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1764 - acc: 0.9715 - val_loss: 2.1992 - val_acc: 0.9744\n",
      "Epoch 11/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1759 - acc: 0.9872 - val_loss: 2.1903 - val_acc: 0.8974\n",
      "Epoch 12/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1753 - acc: 0.9801 - val_loss: 2.1867 - val_acc: 0.9359\n",
      "Epoch 13/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1813 - acc: 0.9801 - val_loss: 2.1826 - val_acc: 0.9487\n",
      "Epoch 14/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1726 - acc: 0.9957 - val_loss: 2.1798 - val_acc: 0.9615\n",
      "Epoch 15/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1750 - acc: 0.9786 - val_loss: 2.1816 - val_acc: 0.9744\n",
      "Epoch 16/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1726 - acc: 0.9843 - val_loss: 2.1921 - val_acc: 0.9103\n",
      "Epoch 17/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1742 - acc: 0.9815 - val_loss: 2.1786 - val_acc: 0.9615\n",
      "Epoch 18/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1693 - acc: 0.9972 - val_loss: 2.1763 - val_acc: 0.9615\n",
      "Epoch 19/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1754 - acc: 0.9858 - val_loss: 2.1979 - val_acc: 0.9103\n",
      "Epoch 20/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1694 - acc: 0.9957 - val_loss: 2.1774 - val_acc: 0.9744\n",
      "Epoch 21/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1691 - acc: 0.9929 - val_loss: 2.1760 - val_acc: 0.9872\n",
      "Epoch 22/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1688 - acc: 0.9972 - val_loss: 2.1911 - val_acc: 0.8846\n",
      "Epoch 23/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1687 - acc: 0.9957 - val_loss: 2.1819 - val_acc: 0.9615\n",
      "Epoch 24/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1705 - acc: 0.9915 - val_loss: 2.1793 - val_acc: 0.9615\n",
      "Epoch 25/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1685 - acc: 0.9986 - val_loss: 2.1825 - val_acc: 0.9615\n",
      "Epoch 26/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1693 - acc: 0.9815 - val_loss: 2.1799 - val_acc: 0.9487\n",
      "Epoch 27/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1674 - acc: 0.9986 - val_loss: 2.1947 - val_acc: 0.9487\n",
      "Epoch 28/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1697 - acc: 0.9900 - val_loss: 2.1750 - val_acc: 0.9744\n",
      "Epoch 29/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1672 - acc: 0.9986 - val_loss: 2.1772 - val_acc: 0.9744\n",
      "Epoch 30/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1679 - acc: 0.9986 - val_loss: 2.1802 - val_acc: 0.9744\n",
      "Epoch 31/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1676 - acc: 0.9957 - val_loss: 2.1803 - val_acc: 0.9744\n",
      "Epoch 32/100\n",
      "702/702 [==============================] - 3s 4ms/step - loss: 2.1685 - acc: 0.9957 - val_loss: 2.1764 - val_acc: 0.9744\n",
      "Epoch 33/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1667 - acc: 1.0000 - val_loss: 2.1780 - val_acc: 0.9744\n",
      "Epoch 34/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1667 - acc: 1.0000 - val_loss: 2.1746 - val_acc: 0.9872\n",
      "Epoch 35/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1668 - acc: 1.0000 - val_loss: 2.1763 - val_acc: 0.9744\n",
      "Epoch 36/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1666 - acc: 1.0000 - val_loss: 2.1816 - val_acc: 0.9487\n",
      "Epoch 37/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1666 - acc: 1.0000 - val_loss: 2.1804 - val_acc: 0.9615\n",
      "Epoch 38/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1668 - acc: 0.9986 - val_loss: 2.1777 - val_acc: 0.9615\n",
      "Epoch 39/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1787 - val_acc: 0.9744\n",
      "Epoch 40/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1666 - acc: 1.0000 - val_loss: 2.1767 - val_acc: 0.9744\n",
      "Epoch 41/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1768 - val_acc: 0.9744\n",
      "Epoch 42/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1666 - acc: 0.9986 - val_loss: 2.1759 - val_acc: 0.9744\n",
      "Epoch 43/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1747 - val_acc: 0.9744\n",
      "Epoch 44/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1777 - val_acc: 0.9744\n",
      "Epoch 45/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1814 - val_acc: 0.9615\n",
      "Epoch 46/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1767 - val_acc: 0.9744\n",
      "Epoch 47/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1800 - val_acc: 0.9744\n",
      "Epoch 48/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1786 - val_acc: 0.9744\n",
      "Epoch 49/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1792 - val_acc: 0.9744\n",
      "Epoch 50/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1792 - val_acc: 0.9744\n",
      "Epoch 51/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1810 - val_acc: 0.9744\n",
      "Epoch 52/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1795 - val_acc: 0.9744\n",
      "Epoch 53/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1793 - val_acc: 0.9744\n",
      "Epoch 54/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1799 - val_acc: 0.9744\n",
      "Epoch 55/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1783 - val_acc: 0.9744\n",
      "Epoch 56/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1797 - val_acc: 0.9744\n",
      "Epoch 57/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1783 - val_acc: 0.9744\n",
      "Epoch 58/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1791 - val_acc: 0.9744\n",
      "Epoch 59/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1789 - val_acc: 0.9744\n",
      "Epoch 60/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1787 - val_acc: 0.9744\n",
      "Epoch 61/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1776 - val_acc: 0.9744\n",
      "Epoch 62/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1769 - val_acc: 0.9744\n",
      "Epoch 63/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1786 - val_acc: 0.9744\n",
      "Epoch 64/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1787 - val_acc: 0.9744\n",
      "Epoch 65/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1777 - val_acc: 0.9744\n",
      "Epoch 66/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1789 - val_acc: 0.9744\n",
      "Epoch 67/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1781 - val_acc: 0.9744\n",
      "Epoch 68/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1784 - val_acc: 0.9744\n",
      "Epoch 69/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1779 - val_acc: 0.9744\n",
      "Epoch 70/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1751 - val_acc: 0.9744\n",
      "Epoch 71/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1759 - val_acc: 0.9744\n",
      "Epoch 72/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1777 - val_acc: 0.9744\n",
      "Epoch 73/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1777 - val_acc: 0.9744\n",
      "Epoch 74/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1773 - val_acc: 0.9744\n",
      "Epoch 75/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1771 - val_acc: 0.9744\n",
      "Epoch 76/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1767 - val_acc: 0.9744\n",
      "Epoch 77/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1772 - val_acc: 0.9744\n",
      "Epoch 78/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1776 - val_acc: 0.9744\n",
      "Epoch 79/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1752 - val_acc: 0.9744\n",
      "Epoch 80/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1772 - val_acc: 0.9744\n",
      "Epoch 81/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1770 - val_acc: 0.9744\n",
      "Epoch 82/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1781 - val_acc: 0.9744\n",
      "Epoch 83/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1771 - val_acc: 0.9744\n",
      "Epoch 84/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1769 - val_acc: 0.9744\n",
      "Epoch 85/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1777 - val_acc: 0.9744\n",
      "Epoch 86/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1780 - val_acc: 0.9744\n",
      "Epoch 87/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1769 - val_acc: 0.9744\n",
      "Epoch 88/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1776 - val_acc: 0.9744\n",
      "Epoch 89/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1778 - val_acc: 0.9744\n",
      "Epoch 90/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1781 - val_acc: 0.9744\n",
      "Epoch 91/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1779 - val_acc: 0.9744\n",
      "Epoch 92/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1768 - val_acc: 0.9744\n",
      "Epoch 93/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1779 - val_acc: 0.9744\n",
      "Epoch 94/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1775 - val_acc: 0.9744\n",
      "Epoch 95/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1780 - val_acc: 0.9744\n",
      "Epoch 96/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1769 - val_acc: 0.9744\n",
      "Epoch 97/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1775 - val_acc: 0.9744\n",
      "Epoch 98/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1771 - val_acc: 0.9744\n",
      "Epoch 99/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1765 - val_acc: 0.9744\n",
      "Epoch 100/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1772 - val_acc: 0.9744\n",
      "78/78 [==============================] - 0s 311us/step\n",
      "Score for fold 7: loss of 2.1772080262502036; acc of 97.43589697740018%\n",
      "Train on 702 samples, validate on 78 samples\n",
      "Epoch 1/100\n",
      "702/702 [==============================] - 7s 10ms/step - loss: 2.4879 - acc: 0.3091 - val_loss: 2.4064 - val_acc: 0.4103\n",
      "Epoch 2/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.3136 - acc: 0.5541 - val_loss: 2.2844 - val_acc: 0.7051\n",
      "Epoch 3/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2407 - acc: 0.7650 - val_loss: 2.2478 - val_acc: 0.7051\n",
      "Epoch 4/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2283 - acc: 0.8191 - val_loss: 2.2179 - val_acc: 0.8333\n",
      "Epoch 5/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2011 - acc: 0.8974 - val_loss: 2.2334 - val_acc: 0.6795\n",
      "Epoch 6/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1980 - acc: 0.9046 - val_loss: 2.2033 - val_acc: 0.8846\n",
      "Epoch 7/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1884 - acc: 0.9302 - val_loss: 2.2035 - val_acc: 0.8590\n",
      "Epoch 8/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1856 - acc: 0.9530 - val_loss: 2.1946 - val_acc: 0.8974\n",
      "Epoch 9/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1803 - acc: 0.9530 - val_loss: 2.2076 - val_acc: 0.8590\n",
      "Epoch 10/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1784 - acc: 0.9758 - val_loss: 2.2138 - val_acc: 0.8846\n",
      "Epoch 11/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1791 - acc: 0.9729 - val_loss: 2.1954 - val_acc: 0.9359\n",
      "Epoch 12/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1725 - acc: 0.9886 - val_loss: 2.1934 - val_acc: 0.9103\n",
      "Epoch 13/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1791 - acc: 0.9644 - val_loss: 2.1839 - val_acc: 0.9615\n",
      "Epoch 14/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1713 - acc: 0.9929 - val_loss: 2.1867 - val_acc: 0.9103\n",
      "Epoch 15/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1733 - acc: 0.9744 - val_loss: 2.2037 - val_acc: 0.8718\n",
      "Epoch 16/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1726 - acc: 0.9886 - val_loss: 2.1821 - val_acc: 0.9615\n",
      "Epoch 17/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1699 - acc: 0.9915 - val_loss: 2.2199 - val_acc: 0.8462\n",
      "Epoch 18/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1719 - acc: 0.9858 - val_loss: 2.1841 - val_acc: 0.9615\n",
      "Epoch 19/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1707 - acc: 0.9886 - val_loss: 2.2014 - val_acc: 0.8846\n",
      "Epoch 20/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1722 - acc: 0.9886 - val_loss: 2.1877 - val_acc: 0.8718\n",
      "Epoch 21/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1682 - acc: 0.9957 - val_loss: 2.1839 - val_acc: 0.9615\n",
      "Epoch 22/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1704 - acc: 0.9943 - val_loss: 2.1823 - val_acc: 0.9359\n",
      "Epoch 23/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1697 - acc: 0.9972 - val_loss: 2.1836 - val_acc: 0.9487\n",
      "Epoch 24/100\n",
      "702/702 [==============================] - 3s 4ms/step - loss: 2.1693 - acc: 0.9943 - val_loss: 2.1848 - val_acc: 0.9359\n",
      "Epoch 25/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1674 - acc: 0.9986 - val_loss: 2.1832 - val_acc: 0.9615\n",
      "Epoch 26/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1683 - acc: 0.9957 - val_loss: 2.1823 - val_acc: 0.9615\n",
      "Epoch 27/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1674 - acc: 0.9986 - val_loss: 2.1864 - val_acc: 0.9359\n",
      "Epoch 28/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1669 - acc: 1.0000 - val_loss: 2.1796 - val_acc: 0.9359\n",
      "Epoch 29/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1679 - acc: 0.9957 - val_loss: 2.1823 - val_acc: 0.9487\n",
      "Epoch 30/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1671 - acc: 0.9986 - val_loss: 2.1807 - val_acc: 0.9615\n",
      "Epoch 31/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1668 - acc: 1.0000 - val_loss: 2.1784 - val_acc: 0.9744\n",
      "Epoch 32/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1671 - acc: 0.9943 - val_loss: 2.1794 - val_acc: 0.9615\n",
      "Epoch 33/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1667 - acc: 1.0000 - val_loss: 2.1832 - val_acc: 0.9487\n",
      "Epoch 34/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1669 - acc: 0.9972 - val_loss: 2.1786 - val_acc: 0.9615\n",
      "Epoch 35/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1667 - acc: 1.0000 - val_loss: 2.1794 - val_acc: 0.9615\n",
      "Epoch 36/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1671 - acc: 0.9986 - val_loss: 2.1869 - val_acc: 0.9487\n",
      "Epoch 37/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1666 - acc: 1.0000 - val_loss: 2.1810 - val_acc: 0.9487\n",
      "Epoch 38/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1779 - val_acc: 0.9615\n",
      "Epoch 39/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1672 - acc: 1.0000 - val_loss: 2.1801 - val_acc: 0.9744\n",
      "Epoch 40/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1666 - acc: 1.0000 - val_loss: 2.1810 - val_acc: 0.9615\n",
      "Epoch 41/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1786 - val_acc: 0.9615\n",
      "Epoch 42/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1667 - acc: 1.0000 - val_loss: 2.1847 - val_acc: 0.9615\n",
      "Epoch 43/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1667 - acc: 0.9986 - val_loss: 2.1785 - val_acc: 0.9744\n",
      "Epoch 44/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1780 - val_acc: 0.9744\n",
      "Epoch 45/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1830 - val_acc: 0.9487\n",
      "Epoch 46/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 0.9972 - val_loss: 2.1773 - val_acc: 0.9615\n",
      "Epoch 47/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1666 - acc: 0.9986 - val_loss: 2.1791 - val_acc: 0.9615\n",
      "Epoch 48/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1800 - val_acc: 0.9615\n",
      "Epoch 49/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1795 - val_acc: 0.9615\n",
      "Epoch 50/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1859 - val_acc: 0.9615\n",
      "Epoch 51/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1789 - val_acc: 0.9744\n",
      "Epoch 52/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1811 - val_acc: 0.9615\n",
      "Epoch 53/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1751 - val_acc: 0.9744\n",
      "Epoch 54/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1779 - val_acc: 0.9744\n",
      "Epoch 55/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1774 - val_acc: 0.9744\n",
      "Epoch 56/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1767 - val_acc: 0.9744\n",
      "Epoch 57/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1770 - val_acc: 0.9744\n",
      "Epoch 58/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1762 - val_acc: 0.9615\n",
      "Epoch 59/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1778 - val_acc: 0.9744\n",
      "Epoch 60/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1777 - val_acc: 0.9744\n",
      "Epoch 61/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1752 - val_acc: 0.9744\n",
      "Epoch 62/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1781 - val_acc: 0.9744\n",
      "Epoch 63/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1787 - val_acc: 0.9744\n",
      "Epoch 64/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1763 - val_acc: 0.9744\n",
      "Epoch 65/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1784 - val_acc: 0.9744\n",
      "Epoch 66/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1758 - val_acc: 0.9744\n",
      "Epoch 67/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1774 - val_acc: 0.9744\n",
      "Epoch 68/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1791 - val_acc: 0.9615\n",
      "Epoch 69/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1789 - val_acc: 0.9615\n",
      "Epoch 70/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1773 - val_acc: 0.9744\n",
      "Epoch 71/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1785 - val_acc: 0.9615\n",
      "Epoch 72/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1765 - val_acc: 0.9744\n",
      "Epoch 73/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1784 - val_acc: 0.9744\n",
      "Epoch 74/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1778 - val_acc: 0.9744\n",
      "Epoch 75/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1786 - val_acc: 0.9744\n",
      "Epoch 76/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1773 - val_acc: 0.9744\n",
      "Epoch 77/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1776 - val_acc: 0.9744\n",
      "Epoch 78/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1783 - val_acc: 0.9744\n",
      "Epoch 79/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1769 - val_acc: 0.9744\n",
      "Epoch 80/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1766 - val_acc: 0.9744\n",
      "Epoch 81/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1762 - val_acc: 0.9744\n",
      "Epoch 82/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1768 - val_acc: 0.9744\n",
      "Epoch 83/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1775 - val_acc: 0.9744\n",
      "Epoch 84/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1760 - val_acc: 0.9744\n",
      "Epoch 85/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1777 - val_acc: 0.9744\n",
      "Epoch 86/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1785 - val_acc: 0.9744\n",
      "Epoch 87/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1768 - val_acc: 0.9744\n",
      "Epoch 88/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1776 - val_acc: 0.9744\n",
      "Epoch 89/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1773 - val_acc: 0.9744\n",
      "Epoch 90/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1774 - val_acc: 0.9744\n",
      "Epoch 91/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1784 - val_acc: 0.9744\n",
      "Epoch 92/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1775 - val_acc: 0.9744\n",
      "Epoch 93/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1771 - val_acc: 0.9744\n",
      "Epoch 94/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1763 - val_acc: 0.9744\n",
      "Epoch 95/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1771 - val_acc: 0.9744\n",
      "Epoch 96/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1772 - val_acc: 0.9744\n",
      "Epoch 97/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1770 - val_acc: 0.9744\n",
      "Epoch 98/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1771 - val_acc: 0.9744\n",
      "Epoch 99/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1765 - val_acc: 0.9744\n",
      "Epoch 100/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1778 - val_acc: 0.9744\n",
      "78/78 [==============================] - 0s 305us/step\n",
      "Score for fold 8: loss of 2.177809843650231; acc of 97.43589743589743%\n",
      "Train on 702 samples, validate on 78 samples\n",
      "Epoch 1/100\n",
      "702/702 [==============================] - 7s 10ms/step - loss: 2.5106 - acc: 0.2521 - val_loss: 2.3918 - val_acc: 0.3462\n",
      "Epoch 2/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.3126 - acc: 0.5769 - val_loss: 2.2831 - val_acc: 0.4615\n",
      "Epoch 3/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2575 - acc: 0.7322 - val_loss: 2.2700 - val_acc: 0.7436\n",
      "Epoch 4/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2279 - acc: 0.7849 - val_loss: 2.2529 - val_acc: 0.6923\n",
      "Epoch 5/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2143 - acc: 0.8262 - val_loss: 2.2324 - val_acc: 0.7692\n",
      "Epoch 6/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1964 - acc: 0.8889 - val_loss: 2.2256 - val_acc: 0.7949\n",
      "Epoch 7/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1865 - acc: 0.9231 - val_loss: 2.1903 - val_acc: 0.8846\n",
      "Epoch 8/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1864 - acc: 0.9302 - val_loss: 2.1991 - val_acc: 0.8462\n",
      "Epoch 9/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1794 - acc: 0.9630 - val_loss: 2.1915 - val_acc: 0.8718\n",
      "Epoch 10/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1854 - acc: 0.9416 - val_loss: 2.1847 - val_acc: 0.9487\n",
      "Epoch 11/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1735 - acc: 0.9801 - val_loss: 2.1847 - val_acc: 0.8846\n",
      "Epoch 12/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1744 - acc: 0.9786 - val_loss: 2.1876 - val_acc: 0.9487\n",
      "Epoch 13/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1736 - acc: 0.9744 - val_loss: 2.1986 - val_acc: 0.8846\n",
      "Epoch 14/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1762 - acc: 0.9672 - val_loss: 2.1819 - val_acc: 0.9487\n",
      "Epoch 15/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1715 - acc: 0.9900 - val_loss: 2.1953 - val_acc: 0.9231\n",
      "Epoch 16/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1700 - acc: 0.9915 - val_loss: 2.1947 - val_acc: 0.8462\n",
      "Epoch 17/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1713 - acc: 0.9872 - val_loss: 2.1803 - val_acc: 0.9487\n",
      "Epoch 18/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1740 - acc: 0.9886 - val_loss: 2.1827 - val_acc: 0.9615\n",
      "Epoch 19/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1684 - acc: 0.9972 - val_loss: 2.1860 - val_acc: 0.9359\n",
      "Epoch 20/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1689 - acc: 0.9872 - val_loss: 2.1833 - val_acc: 0.9487\n",
      "Epoch 21/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1698 - acc: 0.9957 - val_loss: 2.1845 - val_acc: 0.9359\n",
      "Epoch 22/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1697 - acc: 0.9900 - val_loss: 2.1793 - val_acc: 0.9615\n",
      "Epoch 23/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1703 - acc: 0.9915 - val_loss: 2.1909 - val_acc: 0.9231\n",
      "Epoch 24/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1683 - acc: 0.9986 - val_loss: 2.1789 - val_acc: 0.9487\n",
      "Epoch 25/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1708 - acc: 0.9943 - val_loss: 2.1786 - val_acc: 0.9615\n",
      "Epoch 26/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1680 - acc: 0.9972 - val_loss: 2.1786 - val_acc: 0.9487\n",
      "Epoch 27/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1678 - acc: 0.9972 - val_loss: 2.1899 - val_acc: 0.9231\n",
      "Epoch 28/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1716 - acc: 0.9915 - val_loss: 2.1796 - val_acc: 0.9615\n",
      "Epoch 29/100\n",
      "702/702 [==============================] - 3s 4ms/step - loss: 2.1674 - acc: 0.9986 - val_loss: 2.1810 - val_acc: 0.9231\n",
      "Epoch 30/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1676 - acc: 0.9943 - val_loss: 2.1789 - val_acc: 0.9615\n",
      "Epoch 31/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1668 - acc: 0.9986 - val_loss: 2.1776 - val_acc: 0.9744\n",
      "Epoch 32/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1671 - acc: 0.9957 - val_loss: 2.1790 - val_acc: 0.9615\n",
      "Epoch 33/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1668 - acc: 1.0000 - val_loss: 2.1779 - val_acc: 0.9744\n",
      "Epoch 34/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1669 - acc: 1.0000 - val_loss: 2.1794 - val_acc: 0.9615\n",
      "Epoch 35/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1668 - acc: 1.0000 - val_loss: 2.1813 - val_acc: 0.9487\n",
      "Epoch 36/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1673 - acc: 0.9986 - val_loss: 2.1779 - val_acc: 0.9615\n",
      "Epoch 37/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1673 - acc: 0.9972 - val_loss: 2.1779 - val_acc: 0.9615\n",
      "Epoch 38/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1668 - acc: 0.9986 - val_loss: 2.1770 - val_acc: 0.9615\n",
      "Epoch 39/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1671 - acc: 0.9986 - val_loss: 2.1774 - val_acc: 0.9615\n",
      "Epoch 40/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1666 - acc: 1.0000 - val_loss: 2.1771 - val_acc: 0.9615\n",
      "Epoch 41/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1666 - acc: 1.0000 - val_loss: 2.1757 - val_acc: 0.9744\n",
      "Epoch 42/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1668 - acc: 1.0000 - val_loss: 2.1777 - val_acc: 0.9615\n",
      "Epoch 43/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1767 - val_acc: 0.9615\n",
      "Epoch 44/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1666 - acc: 1.0000 - val_loss: 2.1780 - val_acc: 0.9615\n",
      "Epoch 45/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1763 - val_acc: 0.9615\n",
      "Epoch 46/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1773 - val_acc: 0.9615\n",
      "Epoch 47/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 0.9986 - val_loss: 2.1775 - val_acc: 0.9615\n",
      "Epoch 48/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1769 - val_acc: 0.9615\n",
      "Epoch 49/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1774 - val_acc: 0.9615\n",
      "Epoch 50/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1778 - val_acc: 0.9615\n",
      "Epoch 51/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1673 - acc: 0.9986 - val_loss: 2.1789 - val_acc: 0.9615\n",
      "Epoch 52/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1772 - val_acc: 0.9615\n",
      "Epoch 53/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1771 - val_acc: 0.9615\n",
      "Epoch 54/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1773 - val_acc: 0.9615\n",
      "Epoch 55/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1771 - val_acc: 0.9615\n",
      "Epoch 56/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1772 - val_acc: 0.9615\n",
      "Epoch 57/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1774 - val_acc: 0.9615\n",
      "Epoch 58/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1772 - val_acc: 0.9615\n",
      "Epoch 59/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1773 - val_acc: 0.9615\n",
      "Epoch 60/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1767 - val_acc: 0.9615\n",
      "Epoch 61/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1770 - val_acc: 0.9615\n",
      "Epoch 62/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1769 - val_acc: 0.9615\n",
      "Epoch 63/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1769 - val_acc: 0.9615\n",
      "Epoch 64/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1763 - val_acc: 0.9615\n",
      "Epoch 65/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1768 - val_acc: 0.9615\n",
      "Epoch 66/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1769 - val_acc: 0.9744\n",
      "Epoch 67/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1769 - val_acc: 0.9615\n",
      "Epoch 68/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1764 - val_acc: 0.9615\n",
      "Epoch 69/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1768 - val_acc: 0.9615\n",
      "Epoch 70/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1769 - val_acc: 0.9744\n",
      "Epoch 71/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1769 - val_acc: 0.9615\n",
      "Epoch 72/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1773 - val_acc: 0.9615\n",
      "Epoch 73/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1765 - val_acc: 0.9615\n",
      "Epoch 74/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1765 - val_acc: 0.9615\n",
      "Epoch 75/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1767 - val_acc: 0.9615\n",
      "Epoch 76/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1766 - val_acc: 0.9615\n",
      "Epoch 77/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1768 - val_acc: 0.9615\n",
      "Epoch 78/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1768 - val_acc: 0.9615\n",
      "Epoch 79/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1768 - val_acc: 0.9615\n",
      "Epoch 80/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1767 - val_acc: 0.9615\n",
      "Epoch 81/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1770 - val_acc: 0.9615\n",
      "Epoch 82/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1769 - val_acc: 0.9615\n",
      "Epoch 83/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1769 - val_acc: 0.9615\n",
      "Epoch 84/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1767 - val_acc: 0.9615\n",
      "Epoch 85/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1766 - val_acc: 0.9615\n",
      "Epoch 86/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1768 - val_acc: 0.9615\n",
      "Epoch 87/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1770 - val_acc: 0.9615\n",
      "Epoch 88/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1770 - val_acc: 0.9615\n",
      "Epoch 89/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1768 - val_acc: 0.9615\n",
      "Epoch 90/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1770 - val_acc: 0.9615\n",
      "Epoch 91/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1769 - val_acc: 0.9615\n",
      "Epoch 92/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1768 - val_acc: 0.9615\n",
      "Epoch 93/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1768 - val_acc: 0.9615\n",
      "Epoch 94/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1769 - val_acc: 0.9615\n",
      "Epoch 95/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1767 - val_acc: 0.9615\n",
      "Epoch 96/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1767 - val_acc: 0.9615\n",
      "Epoch 97/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1769 - val_acc: 0.9615\n",
      "Epoch 98/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1768 - val_acc: 0.9615\n",
      "Epoch 99/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1769 - val_acc: 0.9615\n",
      "Epoch 100/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1769 - val_acc: 0.9615\n",
      "78/78 [==============================] - 0s 312us/step\n",
      "Score for fold 9: loss of 2.1769125767243214; acc of 96.15384569534888%\n",
      "Train on 702 samples, validate on 78 samples\n",
      "Epoch 1/100\n",
      "702/702 [==============================] - 8s 11ms/step - loss: 2.4895 - acc: 0.2792 - val_loss: 2.3728 - val_acc: 0.3718\n",
      "Epoch 2/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.3194 - acc: 0.5100 - val_loss: 2.2943 - val_acc: 0.6154\n",
      "Epoch 3/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2415 - acc: 0.7365 - val_loss: 2.3018 - val_acc: 0.6282\n",
      "Epoch 4/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2263 - acc: 0.8105 - val_loss: 2.2059 - val_acc: 0.8718\n",
      "Epoch 5/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2001 - acc: 0.8846 - val_loss: 2.2264 - val_acc: 0.7692\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1978 - acc: 0.9188 - val_loss: 2.2109 - val_acc: 0.8462\n",
      "Epoch 7/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1898 - acc: 0.9160 - val_loss: 2.1803 - val_acc: 0.9359\n",
      "Epoch 8/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1829 - acc: 0.9501 - val_loss: 2.1802 - val_acc: 0.9615\n",
      "Epoch 9/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1770 - acc: 0.9687 - val_loss: 2.1885 - val_acc: 0.8974\n",
      "Epoch 10/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1872 - acc: 0.9330 - val_loss: 2.1827 - val_acc: 0.9872\n",
      "Epoch 11/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1746 - acc: 0.9858 - val_loss: 2.1770 - val_acc: 0.9615\n",
      "Epoch 12/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1744 - acc: 0.9772 - val_loss: 2.1873 - val_acc: 0.9359\n",
      "Epoch 13/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1756 - acc: 0.9858 - val_loss: 2.1774 - val_acc: 0.9744\n",
      "Epoch 14/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1732 - acc: 0.9858 - val_loss: 2.1911 - val_acc: 0.8846\n",
      "Epoch 15/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1759 - acc: 0.9715 - val_loss: 2.1726 - val_acc: 0.9744\n",
      "Epoch 16/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1715 - acc: 0.9886 - val_loss: 2.1727 - val_acc: 1.0000\n",
      "Epoch 17/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1760 - acc: 0.9744 - val_loss: 2.1776 - val_acc: 0.9744\n",
      "Epoch 18/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1692 - acc: 0.9986 - val_loss: 2.1735 - val_acc: 0.9615\n",
      "Epoch 19/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1713 - acc: 0.9900 - val_loss: 2.1734 - val_acc: 0.9615\n",
      "Epoch 20/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1733 - acc: 0.9843 - val_loss: 2.1744 - val_acc: 0.9615\n",
      "Epoch 21/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1701 - acc: 0.9957 - val_loss: 2.1829 - val_acc: 0.9231\n",
      "Epoch 22/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1709 - acc: 0.9872 - val_loss: 2.1742 - val_acc: 0.9744\n",
      "Epoch 23/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1711 - acc: 0.9915 - val_loss: 2.1787 - val_acc: 0.9615\n",
      "Epoch 24/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1699 - acc: 0.9929 - val_loss: 2.1771 - val_acc: 0.9487\n",
      "Epoch 25/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1689 - acc: 0.9915 - val_loss: 2.1733 - val_acc: 0.9744\n",
      "Epoch 26/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1699 - acc: 0.9943 - val_loss: 2.1736 - val_acc: 0.9615\n",
      "Epoch 27/100\n",
      "702/702 [==============================] - 3s 4ms/step - loss: 2.1682 - acc: 0.9915 - val_loss: 2.1716 - val_acc: 0.9744\n",
      "Epoch 28/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1674 - acc: 0.9972 - val_loss: 2.1726 - val_acc: 0.9615\n",
      "Epoch 29/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1676 - acc: 0.9986 - val_loss: 2.1709 - val_acc: 1.0000\n",
      "Epoch 30/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1674 - acc: 1.0000 - val_loss: 2.1703 - val_acc: 0.9872\n",
      "Epoch 31/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1669 - acc: 1.0000 - val_loss: 2.1711 - val_acc: 0.9744\n",
      "Epoch 32/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1670 - acc: 1.0000 - val_loss: 2.1723 - val_acc: 0.9744\n",
      "Epoch 33/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1675 - acc: 0.9972 - val_loss: 2.1724 - val_acc: 0.9872\n",
      "Epoch 34/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1671 - acc: 1.0000 - val_loss: 2.1708 - val_acc: 0.9872\n",
      "Epoch 35/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1670 - acc: 0.9986 - val_loss: 2.1718 - val_acc: 0.9744\n",
      "Epoch 36/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1668 - acc: 1.0000 - val_loss: 2.1724 - val_acc: 0.9615\n",
      "Epoch 37/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1669 - acc: 1.0000 - val_loss: 2.1727 - val_acc: 0.9744\n",
      "Epoch 38/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1698 - val_acc: 0.9872\n",
      "Epoch 39/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1667 - acc: 1.0000 - val_loss: 2.1714 - val_acc: 0.9744\n",
      "Epoch 40/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1713 - val_acc: 0.9744\n",
      "Epoch 41/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1708 - val_acc: 0.9872\n",
      "Epoch 42/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1666 - acc: 1.0000 - val_loss: 2.1718 - val_acc: 0.9744\n",
      "Epoch 43/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1711 - val_acc: 0.9872\n",
      "Epoch 44/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1667 - acc: 1.0000 - val_loss: 2.1707 - val_acc: 0.9872\n",
      "Epoch 45/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1705 - val_acc: 0.9744\n",
      "Epoch 46/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1708 - val_acc: 1.0000\n",
      "Epoch 47/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1736 - val_acc: 0.9615\n",
      "Epoch 48/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1708 - val_acc: 0.9872\n",
      "Epoch 49/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1705 - val_acc: 0.9872\n",
      "Epoch 50/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1699 - val_acc: 0.9872\n",
      "Epoch 51/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1703 - val_acc: 0.9744\n",
      "Epoch 52/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1701 - val_acc: 0.9872\n",
      "Epoch 53/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1695 - val_acc: 1.0000\n",
      "Epoch 54/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1705 - val_acc: 0.9744\n",
      "Epoch 55/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1697 - val_acc: 0.9744\n",
      "Epoch 56/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1704 - val_acc: 0.9872\n",
      "Epoch 57/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1668 - acc: 0.9986 - val_loss: 2.1695 - val_acc: 0.9872\n",
      "Epoch 58/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1696 - val_acc: 0.9872\n",
      "Epoch 59/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1693 - val_acc: 0.9872\n",
      "Epoch 60/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1692 - val_acc: 0.9872\n",
      "Epoch 61/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1697 - val_acc: 0.9872\n",
      "Epoch 62/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1690 - val_acc: 0.9872\n",
      "Epoch 63/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1691 - val_acc: 0.9872\n",
      "Epoch 64/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1689 - val_acc: 0.9872\n",
      "Epoch 65/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1695 - val_acc: 0.9872\n",
      "Epoch 66/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1693 - val_acc: 0.9872\n",
      "Epoch 67/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1692 - val_acc: 0.9872\n",
      "Epoch 68/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1692 - val_acc: 0.9872\n",
      "Epoch 69/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1691 - val_acc: 0.9872\n",
      "Epoch 70/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1688 - val_acc: 0.9872\n",
      "Epoch 71/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1690 - val_acc: 0.9872\n",
      "Epoch 72/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1692 - val_acc: 0.9872\n",
      "Epoch 73/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1698 - val_acc: 0.9872\n",
      "Epoch 74/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1690 - val_acc: 0.9872\n",
      "Epoch 75/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1695 - val_acc: 0.9872\n",
      "Epoch 76/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1694 - val_acc: 0.9872\n",
      "Epoch 77/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1691 - val_acc: 0.9872\n",
      "Epoch 78/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1694 - val_acc: 0.9872\n",
      "Epoch 79/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1691 - val_acc: 0.9872\n",
      "Epoch 80/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1691 - val_acc: 0.9872\n",
      "Epoch 81/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1693 - val_acc: 0.9872\n",
      "Epoch 82/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1692 - val_acc: 0.9872\n",
      "Epoch 83/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1688 - val_acc: 0.9872\n",
      "Epoch 84/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1690 - val_acc: 0.9872\n",
      "Epoch 85/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1692 - val_acc: 0.9872\n",
      "Epoch 86/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1692 - val_acc: 0.9872\n",
      "Epoch 87/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1689 - val_acc: 0.9872\n",
      "Epoch 88/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1689 - val_acc: 0.9872\n",
      "Epoch 89/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1691 - val_acc: 0.9872\n",
      "Epoch 90/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1690 - val_acc: 0.9872\n",
      "Epoch 91/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1687 - val_acc: 0.9872\n",
      "Epoch 92/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1689 - val_acc: 0.9872\n",
      "Epoch 93/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1688 - val_acc: 0.9872\n",
      "Epoch 94/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1693 - val_acc: 0.9872\n",
      "Epoch 95/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1688 - val_acc: 0.9872\n",
      "Epoch 96/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1687 - val_acc: 0.9872\n",
      "Epoch 97/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1689 - val_acc: 0.9872\n",
      "Epoch 98/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1691 - val_acc: 0.9872\n",
      "Epoch 99/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1690 - val_acc: 0.9872\n",
      "Epoch 100/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1688 - val_acc: 0.9872\n",
      "78/78 [==============================] - 0s 306us/step\n",
      "Score for fold 10: loss of 2.168758985323784; acc of 98.71794871794873%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABBwAAAP/CAYAAABj7N+nAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/d3fzzAAAACXBIWXMAAAsTAAALEwEAmpwYAADKnklEQVR4nOzdeZyN5f/H8fc1wzDZsozsS/aUdayJVFREhURUUolU0oJkyy4R7cu3HxWhUvZSlspSRJbIXsqaSVMZ2zBz/f44c44zxxnOcM85Z8zr+Xicx5z7uq77uj/31XE653Ou+7qNtVYAAAAAAABOigh1AAAAAAAA4NJDwgEAAAAAADiOhAMAAAAAAHAcCQcAAAAAAOA4Eg4AAAAAAMBxJBwAAAAAAIDjSDhkMGNMrDHGpvEoE+r4wgljFTjGKn0Yr8AxVoFjrALHWKUP4xU4xuoMxiJwjFXgGKvAMVb+kXDIeLskdZQ0zOmOjTGNjTEbfF7Mk50+ThA5NlbGmIbGmGeMMR8bY9YbY/YYY44ZY04aYw4aY741xgw1xpS7+LBDwsmxutoY85gx5n/GmFXGmF+NMfHGmNPGmCPGmB3GmM+MMQ8YY3JcfOghkWH/Dt2MMY/6+Z/LkIw6XgZydKzO8T9ef48OThwziDLy/f02Y8x7xpgtKf8eE40xfxpjNhtjPjHGPG+MudLp42YgJ9+zvknn68oaYyZc9BkEl+OvLWNMVWPMOGPMamPMYWPMKWPMCWPMAWPMUmNMf2PMFU4dL4gyYqxqGGNeS/n88E/KWP1ljPnRGDPGGFPaqWM5LGw/cxpjSqeM3U/GmL9TPo/tM8Z8YYzpZozJ7nTM5xG2Y5XSR0zK57Jk736cjjVAYTVWxpgSxpj7jTGvG2NWpnxO/Tvl3+k/xpiNxphJxphbnY43AOE2VpcbYzoaY140xnyd8pniz5TPFMdT3v+/M67vRGWcjtnDWssjCA9J10uyPo8yF9hXMUlT/fRnJU0O9bmGw1hJOui172xJj0l6WNJnPv0mShoqyYT6vEM4VtNT9kuW9KnXWL0iKcGn752SqoT6vEM5Xmn0W0zSv376HhLqcw71WKXxPpXWo0OozzvUrytJpST94NXPOkn9JN0n6VlJP3nVPRTqcw/FWEn6Jp2vKytpQqjPPZSvLUkvSEry6mOzpJ6S+kv6z6s8QVKbUJ93iMdqbMr/D919/JAyVsMkHUspOynpyVCfc0aPRUpfF/2ZU1IPScdT9jkqaYikLnJ9PnP3tU1SRcZKkXJ9Dov31w+vKytJo73a7pI0QNL9KeX/+PS1XFKxLDxWt3i13SrXe36XlL+/+PSVKKlnRoxHNiFTMcZ0kzROUrSk1+R6U0La+ltrR3ltv2uMGS7p+ZTt7JIGyvUPbXCwgwszT1prX/EuMMb8T9IqSTlTispJ+kTS1UGOLdy9JilvqINA5maMKSnXl5uiKUVTJN1vrU32ajNe0kxJtwc/wkztZKgDCBVjTHtJg3yK77DW7kip/1vSmynluSRNNcZcba3dFcQww4Ixpq+kZ7yK9km60Vp7NKV+p6TJkqIkvWyMOW2tfS3ogQaJE585jTEPSnrDq+gJa+17Kc8nG2NWSmogqaKkZcaYGtbaAxcXefA5NFaVJc2QVE3SakmnJTV0MMyw4OB3mfWSrrXWHvPq+wO5EvVRKUXXSlpijKlprT1+wUGHiINj9YOkJtbaRK++X5S0RNJ1KUXZJb1qjPnRWrv6wqM+G5dUZD73yPUmVMNa+3iogwlzeySN8VPuzoB662eMyZ/hEYWnJEmHlfoDgSTJWrtR0gqf4qrGmPLBCCwzMMbcIelOuX4xhH8vWGtNAI/poQ40xCbpTLLhuFwfzJO9G1hrkyT1kevXjp3BDS+s/H6+15OkziltraQPQhhrqD3ks/2PO9mQ4gef+pxyTQnOUowxOXXmxwi3r9zJhhSf+dSPNcaUyNjIQuqiPnMaY4pJetmn+PNzbBeW9Gp6jxMmnPh8Xl+uMXgg5fmOczfPtJz6LtPHO9kgSdbaXyR96NOukqSuF3GcULrYsUqW63P+S97JBkmy1p6W9I5PeyOp9YUEei7McMh8nrTWrg91EJnAXEmbfT+sS5K1NsEYs1FSY6/iKLky7AuCFF/YsNZ2Ok+TTJcRDhZjTF65Ms7HJT0haXFoI0JmZYxpKOlGr6LvrLXx/tpaa7frzJdp+GGMidCZL48zrbVZOSFYymf7v/NsS64pvFlNfUl5fMp+996w1h4xxhyWVDClKKekbjp7Bsml4mI/c3ZT6jH921r7t0+b7T7bbYwxZa21v13EcUPBic/n38l1WckRSTLGXHRQYepix+pnuWbbfptG/QpJD/qUNZH0+kUcM1QuaqystV/p3N/3g/IZnxkOIZayAMg8Y8yhlAU8dhtjxhtjfP+nJ0nKysmG9IyVtfZha+2Ec3S3z09ZPseCDbH0vq7O0U9hnT2db7219pL6ZfUixmu0pOJyXR/9a8ZHGnoX+9oyxmQzxuQzxkRmdKyhls6xus9ne4tXP9mNMXnNJfzpM51jNVnShPN02U5SFblmN2TYYrGhks7x+sNn23fx35w62yVzOUU6xqqon92PBVB2szORZrwQfOZs57Md56eNb5mR1OYij3vRQvH53Fr7qzvZkJkEe6ystVOtte19f7H3Eraf8cPwe98dPtvJOnsm18XLiIUheAS8eMhzck1zSfZTt0JSZAD9XtCiNOH8yKix8jnGXD/91A31uYfDWEnKL6mypA5yZZG9918iqXSozzscxkuuREyypA1yZY/L+Nl/SKjPOdRjlVL3kly/Nv+sMwvXJcuVpJksqWGozzfUYyVpo0+b0Sljttmrj5NyLYDVKdTnHOrX1XmOYVL+XVpJn4f6nEM9XnK9l3u3SZKUz6v+Dp/6OEmFQn3uwR4rP+Ng5VoHyvc4B33anJQUEerzd/p1k0a/AX/mlGs9kCSf9qv9tLvaT7/Ts9JYnaOPyb79ZPXXVYBxtvXT5xuMlZVca0GUkmvdhvd9+jooqV1GjAczHELrGUkt5Pp14Sa5XoRuDRUGGd4w4thYpfxKWMuneJukHy8yxnBxsWP1vVy/rk7TmcUhd0nqbK29wVr7e5p7Zk7pHi/jun3XO3K9QXezruvgsoILfW09LdflAi/JdW1gP0l/SSor18rSK4zrFpDBvi1aRgp4rFKm/1/ls38fSU9KmpjSdrFcl35dK2mKMeajlP0uBU7/v/B2uRZdky7B2Q1K53hZ19ooz8m1AJ3kmt36ijGmgjGmtlx3DHBbJ6mptfavjAk96NIzVuv97J9q1oMxJpvOXE7hFqXMsWhwsD9zltLZM6n9/SLtr6yMw7GkF5/PAxeOY1XbT9mUoEdxtnAYq15yXSr2nc7MrDwh12eNytbaTzPioJfKh5XMarS1dqG1NtFau1jSSp/65qEIKkw5OVbNlPr61ERJD9uU1N8l4GLH6gG5fukZIcl9rWU5ub7kfGOMqehotKF3IePVT1JVuTLmqzI8wvBxIWO1StKwlGTV+9ba+dbaMZIaKfW1g10l/S9jwg6J9IxVXrluhebNyLVo5DvW2llyfYmO96rvKFci51Lg9P8LB6T8nW+t/eniwws76R4va+1oud6zlqQU3SfXtfNrJFWX6xe3/5N0u7V2U4ZFHnwBj5W1drfOXofnWp/tBvJ/PXSuiw00CIL9mTOfn7IkP2X+EvaXOxtKuvH5PHBhNVYpP1zc41P8prXWN65QCIexmibpVkmPyvX5THIlQHpJ2mqM8b280xEkHEJrmc+27zVHJYMVSCbgyFgZY3Ip9YrJR+W657hv/5nZRY2VtfZ7a+1sa+0ASTUl7feqbiLXr9GX0mszXeNljKkk11T3fTp7RfNLXbpfW9ba+tbasxZUs66FD31Xkr7PGOP7AT+zSs9Y5U6jD88itta1Uv53PvV9LpG1MBz7f6ExpqXO/Lo19GKCCmPpfc+KMsaMlOuSphtSij+Q1F6u+7F/L9fnwa6SfjXGjLmEZs+k97X1sCTvWzLWNMaMM8ZUNMY0VtpJ0YSLiDFYgv2Z82LWnAn1D0B8Pg9cuI3VAEmlvbbfkxQud/UL+VhZa3+31n5prX1TrlkV3ndwukLS+8aYHk4f91L5H0pm5btQju99wv0t5JRVXfRYGdctrz7RmanLWyTVt9bOv/jwwopjrytr7R+SBvoUF9KltSJ3wOOVcjnO23ItuvaYtdbf6u6XMqffs5b7KfNdZCyzSs9Y+VuYLt5a+69P2W6f7UKSrkl/aGHHydeVe3bDQuvwfcTDSHrH62O5Lqlw35d+trX2fmvtJ9ba9+W63MndZza5LucZ4ly4IZWusbKuOyPUkuvaZvcMrKfkuuzya7kuvXzfp4/T8n+nj3AT7M+c//gp85cg9TdjxPe9L9j4fB64sBkrY8wDOvOZ9YRcn9Mesq7bSYeDsBkrSbKuO/k9rrMTpqNSfqB1DAmH0AqXfwCZwUWNlTHmCrk+LNya0tdYSbUusamjbk6/rr70U5ZpVuUOQHrG6yG5ZnkslrTcGFPI/ZBrsU1fl3m1SetX7MzE6dfWn37KKjh8jFBJz1j9K+mUT5m/X0z9rV5ePB3HCVeOvK6MMc3lurWhdOnObpDSMV7GmHpyXY7jLdVlA9ba4zo7+fe0MSb6wsILK+l+bVlrD1pru8g1rb+GXIu/1ZZ0ubW2s1Jf2iS5bsEd6l/kAxHsz5x75LpUx1uUn3b+ynY7Hk368Pk8cCEfK+PyvFyzGYykHyTVtNaG220wQz5WvlJ+OPvepzifpHpOHoeEAy55xpimktbKdc34ekn1rLV9rLUnUupzGGNKGGMuC2GYIWOMyXmeadmH/JQVyah4wpz7ukD3L4LeD3/Xij/rVf9aMALMZPxNuc0MH9wdlfLry0afYn9j46/MN1GRlblnNywOk+t1w4G/S5T8vaf7ll0m15oPWVbKddYbrLXfWmt/SknMSGdPe/b9sA5J1toESVt9iv0trumvbI3zEeFSlPKDzyxJw+W6TPpJSddaa7d6tSlijIkJSYAhlnJbbX9JPW8Z/jnf3zQm4JJgjMkh1xvQU3ItDNlf0lg/dxRoIGmpXIslTg5mjKFmjLlcrl9rRirt9Qh8V+SWziwmmdU8I/8zGSTXtW++qyB/qDPXx+1XFmOMeUPSZSm/FvpTzE/ZzoyLKKwtVOqVtf3dk9tf2a8ZE07mYoy5Xq7bfEmX9uyG9PKXTPb3Y5O/siyX/EtZcC5HypfltNT02fa9xAJnzFTqO/D4+9JXyGfbSvoswyLCJcMY00KuWQ1F5FrzqEfKpcC+fpBr1sz1QQsufHwiqY7OPRsywz/nk3DAJckYU0uuL3pVJX0j160Ld4Q0qPB2wznqbvJTtiijAgln1tq1adUZY8r4Kf7VWpslxyrFVZKqG2Mi07iG8no/ZZ9kbEhh6x25ElruXyLyGWMKWmsPe7W50mefLdbarJqg8eVeV+Zba63v4ppZme/MGcnnVo9plB2Ta92CrKanpJeNMY39LSad8tnC+9/h19baH4IWXebzjlw/+rivBy9gjMlvrfW+LMX3MrrZ1loSqUiTMSaPpPFyXeYaJ6mTtfaj0EYV1ooZYypZa896T09Zq6GBT/FxSSucDIBLKnDJSXkjWqUz00Gvl7TdGGP9PeSa3ZDV1TfGPOxbaIwpLtftMb0l6NJZUAwZ73L5WSE65YN7R5/i97PqVHhr7e86e5aR59r7lNlI13vvItfiflmeMaahpKYpm8xuSG2RXJcUemvhvZHy2rrOp80r5/mV/1I3OmWWpEfKB3Pva8IPyHVnD6TBWrtXZ9++906fbe81Rv6S9FiGBoVLwbtyJRsk16yZqWl9xk/5nF867a6yjNdTFs/3SFkI/WWdfQvbodZaf2tGXTBmOGSwlP9BtVLqKWVurYwxqyVtSmlT1qe+sDGmg6TfrLWrUvorq3Mv5FE2ZR+3uSm3Uwt7To2VXL/KXNKvbYfHyu2dlFvKfSvXVKqr5fowVcCrzU5JHTPbr6pO/zv06buVXL/e+JsqerXXv8dM8W8xg8bqZWNME7leW/FyLcT2sKTsKfVWrl/CMtUHTafHylr7kjEmm6Rhcr2HvZyy4O0hSY/ozO0z3atvz3P6nDJKRv4b1JnZDSustUucijmUnByvlPeoT+W6BZok3WiMmS9prlzvXQ/pzAfOZLnWmxmgTCKDXlsNJW00xkyW63K4UnJddunef5Wk9ilfqMNGOH7mtNa+nXKpykty3eHpFWNMKbmmuLfWmWTXTkmtrbW+twrMEOE4Vin9eLfxPa5v/aZgLH4ehmMVtncJCcOxcrtR0s/GmKlyff6PkevWyHW82pyQ9IK1dvQ5jndhrLU8MvAhqYxcH6bTekwOpI1Xf13O09b3USbUYxDssZLrF9X0jJH70SXUYxCCsTKSYuX6ojdFroUP/5BrFsMpub4cbpRrLYL2krKH+txDOV5p9L37Uvq36ORYSSohqZOkN+X6gP6rztyR4W+5bjE3QVL1UJ93qMfKp99ycn04/yllnE7LdYu5HyWNziyvpSCNVR2v+uahPs9wHi+57tL0P7kWT45P+Xd4Uq67xaxIeW1VDfW5h3KsJFWSK4H1uVy3zo7Tmf8XbpX0f5Jahvqcg/W6kYOfOVOO+6LX6y9RrlkiX0rqLimKsbJKZx9DsuJYybVIZHr2t5K+yaJjVVzS3XLNZFgmaYekw3J9rkiQ6zPsF3Itcl48o8bFpAQDAAAAAADgGNZwAAAAAAAAjiPhAAAAAAAAHEfCAQAAAAAAOI6EAwAAAAAAcBwJBwAAAAAA4DgSDgAAAAAAwHEkHAAAAAAAgONIOGQCxphuoY4hs2CsAsdYpQ/jFTjGKnCMVeAYq/RhvALHWAWOsUofxitwjFXgMttYkXDIHDLViyrEGKvAMVbpw3gFjrEKHGMVOMYqfRivwDFWgWOs0ofxChxjFbhMNVYkHAAAAAAAgOOMtTbUMYQNYwyDEaDatWuHOgS/4uLiFBMTE+owMgXGKn0Yr8AxVoFjrALHWKUP4xU4xipwjFX6MF6BY6wCF65jtXbt2r+stWcFRsLBCwmHwPG6AQAAAABIkjFmrbU21recSyoAAAAAAIDjSDgAAAAAAADHkXAAAAAAAACOI+EAAAAAAAAcR8IBAAAAAAA4joQDAAAAAABwHAkHAAAAAADgOBIOAAAAAADAcSQcAAAAAACA40g4AAAAAAAAx5FwAAAAAAAAjiPhAAAAAAAAHEfCAQAAAAAAOI6EAwAAAAAAcBwJBwAAAAAA4DgSDgAAAAAAwHEkHAAAAAAAgONIOAAAAAAAAMeRcAAAAAAAAI4j4QAAAAAAABxHwiEEChQooAkTJmjXrl3atm2bduzYoRUrVqhFixaSJGOM+vTpo+3bt+u3337T7t27NWrUKOXIkSPEkQMAAAAAEBgSDkGWO3durVixQp07d1arVq1UqVIlVa5cWTt37lSlSpUkSePHj9eYMWM0b948lS1bVsOGDVO/fv00bdq0EEcPAAAAAEBgjLU21DGEDWNMhg/GsGHDNGDAAL3yyivq1avXWfWlS5fWrl27FBkZqRtuuEFLly5V4cKF9eeff0qSrrvuOi1fvjyjwzwvXjcAAAAAAEkyxqy11sb6ljPDIcjuvvtuSVKhQoU0a9Ys7dixQz/88IM6dOggSWrZsqUiIyMlSYcOHZIkxcXFKTk5WZLUqlWrEEQNAAAAAED6ZAt1AFlJdHS0ypUrJ0lq0aKFrr76auXNm1cbNmzQtGnT9M8//6hixYqe9sePH5fkmk1w8uRJRUdHp6oHAAAAACBcMcMhiPLnz6+ICNeQf//999q3b5+2bNmijRs3SpL69++v3Llze9onJSV5nrtnOHjXAwAAAAAQrkg4BNHp06c9z//66y/P87i4OElS1apVlZCQ4Cl3X1ohyZOo8K4HAAAAACBcBSXhYIyJNcbYNB5lghFDOIiLi/MkDLwXXXQ/z5Ejh7Zv3+4pj46OluS6Tab7lpje9QAAAAAAhKtgzXDYJamjpGFOd2yMaWyM2eCTxJjs9HGcYK3VokWLJEkFChTwlBcsWFCStHHjRi1YsMBz+UThwoUluRaYdM9wmDdvXjBDBgAAAADgggQl4WCtjbfWTpe0xKk+jTHFjDFTJX0rqZpT/Wa0wYMH69ixY6pfv77y58+vkiVLqlo1V/ijR4/W7t279frrr0ty3bHC+++cOXO0bNmy0AQOAAAAAEA6GO+p/Rl+MGOul7TUp7istXZ3OvvpJmmcpGhJb0p6zKfJ+9baLhcQX1AGIzY2VsOHD9dVV12lyy67TLt379bIkSP12WefSXKt19CnTx899NBDioyMlDFGM2bM0ODBg3XixIlghHhewXzdAAAAAADClzFmrbU29qzyTJpw+EZSkqRe1tpNfhIFYZ1wuBSQcAAAAAAASGknHLKFIhgHPGmtXR/qIAAAAAAAgH9hcVvMlIUf5xljDhljEo0xu40x440xefy1J9kAAAAAAEB4C4eEQ0e5LrNoISlGUnZJpSX1lvSlMSYyhLEBAAAAAIALEA4Jh2fkSjbklHSTXGszuDWU1CYUQQEAAAAAgAsXDgmH0dbahdbaRGvtYkkrfeqbZ+TBjTHdjDFrjDFrMvI4AAAAAABkJeGwaOQyn+19PtslM/Lg1tp3JL0jcZcKAAAAAACcEg4zHOJ8tk/6bOcMViAAAAAAAMAZ4ZBwSDp/EwAAAAAAkJmEQ8IBAAAAAABcYkg4AAAAAAAAx5FwAAAAAAAAjiPhAAAAAAAAHBeUhIMxJpcxpoOkG/xUtzLG1PNqU9anvrAxpoMxpp5Xf2VTyjqk7OMrVb0xJpeDp+NRtGhRffzxx7LWytqz76j59NNPa8uWLVq9erW2bt2qZ5999oLa+Kpbt66WLl2qjRs3avv27Zo2bZqKFSuWrjZ9+vTRtm3btGnTJn3wwQeKiory1HXo0EELFixIz1AAAAAAAJCa+8tyRj4klZFkz/GYHEgbr/66nKet76NMgHEG3GfDhg3tL7/8YqdPn27dvOuff/55a621zz77rJVk+/bta621dtCgQelq4/uoUKGCTUhIsBs3brQRERG2ePHiNjEx0f7yyy82KioqoDY1atSw1lrbr18/W79+fWuttU888YSVZHPlymV37dply5cvf87zBwAAAADAWmslrbF+vmMHZYaDtXa3tdac49ElkDZe/U0+T1vfx26nz+ngwYOqW7euvvjii7PqoqOj1bdvX0nSypUrJUnfffedJNfMgly5cgXUxp++ffsqV65cWrVqlZKTk7Vv3z799ttvqlKliu65556A2lSoUEGSdOjQIR06dEiSVLFiRUnSoEGDNH36dO3cufPiBwkAAAAAkGWxhsMF+vXXX5WQkOC3LjY2Vnny5JEkxcfHS5L+/vtvSVKuXLlUp06dgNr407Rp01T7eO93/fXXB9Rm48aNSkpKUqlSpVS6dGlJ0rp161SpUiW1bdtWI0aMCHgcAAAAAADwJ1uoA7gUFS9e3PM8MTEx1V93fVJS0nnbnKtv77bu5+6687XZtm2bunTpou7du6t58+YaMWKEJk2apC+//FL9+vXTsWPH0nvKAAAAAACkQsIhSKzXopLGmAtuc679zrWPb5spU6ZoypQpnvp27dopIiJCM2fOVJ8+fVSvXj1FRERo0qRJmjNnTsCxAAAAAAAgcUlFhti3b5/nufvuDzly5EhVH0ibc/XtfVcJ937uukDaeIuOjtbo0aP1+OOP6/7779eYMWP08ssv66efftKnn36qcuXKnfecAQAAAADwRsIhA6xZs8azvkP+/PklSQUKFJAkHT16VKtXrw6ojeRKGhQsWNDT9zfffJNqH+/93HWBtPE2YMAAff7559qyZYtiY2MlSfv379e+ffuUPXt21axZ8wJGAQAAAACQlZFwyADHjx/Xiy++KElq2LChJKlRo0aSpHHjxuno0aMBtZFcyYv9+/d7FpF88cUXdezYMc8lD8WKFVPZsmW1bds2ffTRRwG3cStfvrw6duyoF154QZK0a9cuSVLhwoVVuHDhVGUAAAAAAATKeK8bkNUZYwIejDJlymjSpEkqUqSIKleuLMk1e+CXX35Rz549JUnPPvusHnzwQf3333/Kly+fJk2apNGjR6fq53xt5s2bp9jYWDVp0kTbtm2TJNWvX19jxoxR/vz5FR0drZ9++klPPfVUqsslAmkjSQsWLNDUqVM1depUSa7LK9577z1Vr15dUVFRmjRpkkaOHHnW+fO6AQAAAABIkjFmrbU29qxyvjiekZ6EQ1bH6wYAAAAAIKWdcOCSCgAAAAAA4DgSDgAAAAAAwHEkHAAAAAAAgONIOAAAAAAAAMeRcAAAAAAAAI4j4QAAAAAAABxHwgEAAAAAADiOhAMAAAAAAHAcCQcAAAAAAOA4Eg4AAAAAAMBxJBwAAAAAAIDjSDgAAAAAAADHkXAAAAAAAACOI+EAAAAAAAAcR8IBAAAAAAA4joQDAAAAAABwHAkHAAAAAADgOBIOAAAAAADAcdlCHUA4qV27ttasWRPqMDIFY0yoQ8g0rLWhDgEAAAAAgo4ZDgAAAAAAwHEkHAAAAAAAgONIOAAAAAAAAMeRcAAAAAAAAI4j4QAAAAAAABxHwgEAAAAAADiOhAMAAAAAAHAcCQcAAAAAAOA4Eg4AAAAAAMBxJBwAAAAAAIDjSDgAAAAAAADHkXAAAAAAAACOI+EAAAAAAAAcR8IBAAAAAAA4joQDAAAAAABwHAkHAAAAAADgOBIOAAAAAADAcSQcAAAAAACA40g4AAAAAAAAx5FwAAAAAAAAjiPhAAAAAAAAHEfCAWGtQIECmjBhgnbt2qVt27Zpx44dWrFihVq0aCFJMsaoT58+2r59u3777Tft3r1bo0aNUo4cOUIcOQAAAABkbSQcELZy586tFStWqHPnzmrVqpUqVaqkypUra+fOnapUqZIkafz48RozZozmzZunsmXLatiwYerXr5+mTZsW4ugBAAAAIGvLFuoAgLT07dtXlStX1iuvvKJffvlFkpSUlKT7779fklS6dGk9/vjjkqS5c+em+nvnnXeqUaNGWr58eQgiBwAAAAAwwwFh6+6775YkFSpUSLNmzdKOHTv0ww8/qEOHDpKkli1bKjIyUpJ06NAhSVJcXJySk5MlSa1atQpB1AAAAAAAiRkOCFPR0dEqV66cJKlFixa6+uqrlTdvXm3YsEHTpk3TP//8o4oVK3raHz9+XJJkrdXJkycVHR2dqh4AAAAAEFzMcEBYyp8/vyIiXC/P77//Xvv27dOWLVu0ceNGSVL//v2VO3duT/ukpCTPc/cMB+96AAAAAEBwkXBAWDp9+rTn+V9//eV5HhcXJ0mqWrWqEhISPOXuSyskeRIV3vUAAAAAgOAi4YCwFBcX50kYWGs95e7nOXLk0Pbt2z3l0dHRkly3yXTfEtO7HgAAAAAQXEFJOBhjYo0xNo1HmWDEgMzFWqtFixZJkgoUKOApL1iwoCRp48aNWrBggefyicKFC0tyLTDpnuEwb968YIYMAAAAAPASrBkOuyR1lDTsYjsyxjQ0xjxjjPnYGLPeGLPHGHPMGHPSGHPQGPOtMWaoMabcxYeNUBo8eLCOHTum+vXrK3/+/CpZsqSqVasmSRo9erR2796t119/XZLrjhXef+fMmaNly5aFJnAAAAAAgIz3dPUMP5gx10ta6lNc1lq7Ox19HJR0RcrmHElfSzop6VZJd3o1PSVptKTBNsCTjI2NtWvWrAk0lCzNGBOU48TGxmr48OG66qqrdNlll2n37t0aOXKkPvvsM0mu9Rr69Omjhx56SJGRkTLGaMaMGRo8eLBOnDgRlBjPJ5j/xgAAAAAg2Iwxa621sWeVZ+KEQ39r7SifuuGSnvfZZai1dnAgfZNwCFywEg6XAhIOAAAAAC5laSUcMuuikXskjfFTPlrSPz5l/Ywx+TM8IgAAAAAA4JEZEw5zJY231ib7VlhrEyRt9CmOktQgGIEBAAAAAACXsEg4GGMaG2PmGWMOGWMSjTG7jTHjjTF5fNtaax+21k44R3f7/JTlcyxYAAAAAABwXuGQcOgo17oOLSTFSMouqbSk3pK+NMZEprO/s5IUct0lAwAAAAAABEk4JByekSvZkFPSTZKSvOoaSmoTaEfGtZJhLZ/ibZJ+PMc+3Ywxa4wxa+Li4gIOGgAAAAAApC0cEg6jrbULrbWJ1trFklb61DdPR1/NJBXz2k6U9PC5botprX3HWhtrrY2NiYlJx6EAAAAAAEBawiHhsMxn23cNhpKBdGKMySXpZa+io5LaWGt9+wcAAAAAABksW6gDkOR7HcNJn+2c5+vAGJNT0ieSrkop2iKpvbV208WHBwAAAAAA0iscZjgknb9J2owxV0j6WtKtKX2NlVSLZAMAAAAAAKETDjMcLpgxpqmkDyUVl7Re0kPW2rVe9TnkuvPF39baYyEJEgAAAACALCgcZjikmzEmhzFmrKRFkgpK6i+pjneyIUUDSXsktQ9yiAAAAAAAZGmZboaDMaaWpA8kVZX0jaRu1todIQ0KAAAAAACkkqlmOBhj8khaJVeyQZKul7TdGGP9PSQtDVWsSK1o0aL6+OOPZa2Vv7uUPv3009qyZYtWr16trVu36tlnn72gNr7q1q2rpUuXauPGjdq+fbumTZumYsWKpatNnz59tG3bNm3atEkffPCBoqKiPHUdOnTQggUL0jMUAAAAAJAlBCXhYIzJZYzpIOkGP9WtjDH1vNqU9akvbIzpYIypJylSmXBWRlbXsGFDLV68WMnJyX7rn3/+eb300kv6v//7P9WtW1eTJk3Siy++qEGDBqWrja8KFSpoyZIlKliwoGrUqKGmTZuqbdu2WrRokSdpcL42NWrU0JgxYzRp0iQ99NBDuvfee9W9e3dJUq5cuTRixAg98cQTDo4WAAAAAFwagjXDIUbSNEkD/dS9IqmHV5vGPvVVUsp7ZGSAyDgHDx5U3bp19cUXX5xVFx0drb59+0qSVq5cKUn67rvvJLlmFuTKlSugNv707dtXuXLl0qpVq5ScnKx9+/bpt99+U5UqVXTPPfcE1KZChQqSpEOHDunQoUOSpIoVK0qSBg0apOnTp2vnzp0XP0gAAAAAcIkJymwBa+1uSSaApk61QRj59ddf06yLjY1Vnjx5JEnx8fGSpL///luSawZBnTp1lJSUdN4233zzzVl9N23aNNU+3vtdf/31mjx58nnbjBo1SklJSSpVqpRKly4tSVq3bp0qVaqktm3bqlq1aukZCgAAAADIMrg8ASFVvHhxz/PExMRUf931SUlJ521zrr6927qfu+vO12bbtm3q0qWLunfvrubNm2vEiBGaNGmSvvzyS/Xr10/HjnG3VQAAAADwh4QDwo73opLG+J/QEkibc+13rn1820yZMkVTpkzx1Ldr104RERGaOXOm+vTpo3r16ikiIkKTJk3SnDlzAo4FAAAAAC5lmeouFbj07Nu3z/PcvZBjjhw5UtUH0uZcfXvfVcK9n7sukDbeoqOjNXr0aD3++OO6//77NWbMGL388sv66aef9Omnn6pcuXLnPWcAAAAAyApIOCCk1qxZo4SEBElS/vz5JUkFChSQJB09elSrV68OqI3kShoULFjQ07d7XQf3Pt77uesCaeNtwIAB+vzzz7VlyxbFxsZKkvbv3699+/Ype/bsqlmz5gWMAgAAAABcekg4IKSOHz+uF198UZLr9pmS1KhRI0nSuHHjdPTo0YDaSK7kxf79+1WnTh1J0osvvqhjx455LnkoVqyYypYtq23btumjjz4KuI1b+fLl1bFjR73wwguSpF27dkmSChcurMKFC6cqAwAAAICsznhfC5/VxcbG2jVr1oQ6jEwhPesmlClTRpMmTVKRIkVUuXJlSa7ZA7/88ot69uwpSXr22Wf14IMP6r///lO+fPk0adIkjR49OlU/52szb948xcbGqkmTJtq2bZskqX79+hozZozy58+v6Oho/fTTT3rqqadSXS4RSBtJWrBggaZOnaqpU6dKcl1e8d5776l69eqKiorSpEmTNHLkyLPOn39jAAAAAC5lxpi11trYs8r5MnQGCYfApSfhkNXxbwwAAADApSythAOXVAAAAAAAAMeRcAAAAAAAAI4j4QAAAAAAABxHwgEAAAAAADiOhAMAAAAAAHAcCQcAAAAAAOA4Eg4AAAAAAMBxJBwAAAAAAIDjSDgAAAAAAADHkXAAAAAAAACOI+EAAAAAAAAcR8IBAAAAAAA4joQDAAAAAABwHAkHAAAAAADgOBIOAAAAAADAcSQcAAAAAACA40g4AAAAAAAAx5FwAAAAAAAAjssW6gCQOf3999+hDiHTyJ8/f6hDyFTi4+NDHQIAAAAABzDDAQAAAAAAOI6EAwAAAAAAcBwJBwAAAAAA4DgSDgAAAAAAwHEkHAAAAAAAgONIOAAAAAAAAMeRcAAAAAAAAI4j4QAAAAAAABxHwgEAAAAAADiOhAMAAAAAAHAcCQcAAAAAAOA4Eg4AAAAAAMBxJBwAAAAAAIDjSDgAAAAAAADHkXAAAAAAAACOI+EAAAAAAAAcR8IBAAAAAAA4joQDAAAAAABwHAkHAAAAAADgOBIOAAAAAADAcSQcAAAAAACA40g4IFOYNGmSChQooAIFCmj06NGhDies9O3bV/Hx8Wc91q5dm6pdlSpVNGnSJG3atEnff/+91q1bp08//VSlSpUKUeQAAAAALmXZQh0AcD7x8fEaMWJEqMMIa0eOHFFiYmKqsvj4eM/zatWqad68edqyZYuuvfZa/fvvv8qfP79mz56tggUL6o8//gh2yAAAAAAucSQcEPaGDRumRo0aac6cOaEOJWz17dtX06ZNS7N+zJgxypMnj1599VX9+++/klwJicaNGwcrRAAAAABZDJdUIKxt2LBBCxcuVJ8+fUIdSlirX7++pk+frrVr1+qbb77Rc889p+joaElS0aJFVb9+fUlS3bp1NWfOHG3YsEGffPKJqlevHsqwAQAAAFzCSDggbFlr1adPHw0cOFC5c+cOdThh6+TJk4qMjNSDDz6opk2b6tSpU+rTp49mzZqlyMhIVa1a1dO2Xr16atu2rcaNG6ebbrpJc+bMUUxMTAijBwAAAHCpIuGAsOW+RODuu+8OcSThbcKECXrsscd09OhR/ffff3rllVckuWYz3HnnncqfP7+n7YIFC3Tq1Cl99tlnkqS8efPq4YcfDkncAAAAAC5tJBwQlv777z8NHz5cY8aMkTEm1OFkKjt37vQ8r1Onjk6fPu3ZPnz4sCQpISFBJ06ckCRVrlw5uAECAAAAyBJIOCAsLV26VMYYPfHEE2rcuLHat2/vqZs8ebIaN26sdevWhTDC8FGsWLFU28nJyZ7nkZGRqe5AYa0963mOHDkyOEIAAAAAWVFQEg7GmFhjjE3jUSYYMSBzuf3227V582Z99913+u677/Txxx976rp06aLvvvtONWvWDGGE4eOLL75IddlE2bJlPc83bNigDRs26K+//pIkT7vo6GjPopKbN28OYrQAAAAAsopgzXDYJamjpGEX25Ex5mpjzGPGmP8ZY1YZY341xsQbY04bY44YY3YYYz4zxjxgjOGnW2QJ7nUYoqKi1KNHD0nS9u3b9emnn+r06dMaPny4JKlZs2aSpFtuuUWS9O+//+r//u//QhAxAAAAgEud8Z5ineEHM+Z6SUt9istaa3eno4/pku6WZCV9JukbSSclXSOpq6RcXs13SWplrd0SSN+xsbF2zZo1gYaSpcXHxwftWEOHDtW8efM8axMUKlRIhQoV0rJlyxQZGRm0OC7UlVdemaH99+rVS7feeqty5cql4sWL6+TJk1q4cKGGDRvmWbNBktq1a6eePXuqQIECyps3r9asWaMXXnhBmzZtytD40iuYry0AAAAAF88Ys9ZaG3tWeSZOOPSy1r7iU1dN0ipJOb2KN1trrw6kbxIOgeNLYeAyOuFwqeG1BQAAAGQuaSUcMuOikUmSDkt6w7fCWrtR0gqf4qrGmPLBCAwAAAAAALhkC3UA6WWt7XSeJseDEggAAAAAAEhTWMxwMMY0NsbMM8YcMsYkGmN2G2PGG2PypLOfwpIa+hSvt9budC5aAAAAAABwPuGQcOgo17oOLSTFSMouqbSk3pK+NMacc1VAY0x+Y0xlY0wHSYslFfCqXirpjowIGgAAAAAApC0cEg7PyJVsyCnpJrnWaHBrKKnNefb/XtIWSdMkuReH3CWps7X2Bmvt7+fa2RjTzRizxhizJi4u7kLiBwAAAAAAPsIh4TDaWrvQWptorV0saaVPffPz7P+AXLMYRkj6O6WsnKQpxphvjDEVz7WztfYda22stTY2JibmAsIHAAAAAAC+wiHhsMxne5/Pdslz7Wyt/d5aO9taO0BSTUn7vaqbSFphjDlnHwAAAAAAwFnhkHDwvY7hpM92zkA7stb+IWmgT3EhSYMuIC4AAAAAAHCBwiHhkHT+JunypZ+ymx0+BgAAAAAAOIdwSDikizEm53nuXHHIT1mRjIoHAAAAAACcLVMlHIwxl0s6LmnoOZoV9FP2t58yAAAAAACQQTJVwsHLDeeou8lP2aKMCgQAAAAAAJwtsyYc6htjHvYtNMYUl+v2mN4SJA0JRlAAAAAAAMAlKAkHY0wuY0wH+Z+Z0MoYU8+rTVmf+sLGmA7GmHo+5e8YY2YZY3obY+43xoyVtFFSaa82OyU1tdbudOxkcEG2bNmi++67T/Xq1VPLli1Vt25d9ezZM832x48f1/Dhw1W/fn01b95cjRo10i233KItW7ZIknr27KkCBQr4fcyfP1+SNHHiRNWpU0cNGjRQ9+7ddfLkmRugzJw5U3fddVfGnvQFyps3r8aOHauffvpJX3/9tVasWKEHHnjAUz9u3DgtXbpUn332mbZs2aK1a9dq4MCBypYtW5p9tm7dWl988YXmzJmjlStXauvWrfrwww9VqVKldLXp1auXfvzxR61cuVJvvfWWoqKiPHVt27bVJ5984vBoAAAAAMis0v6G4qwYSdPSqHtF0vtyzULw16ZKSvn7kh6QVEdS/ZTHVZJ6SyogKYdcsxl+lrRB0lxJn1trTzl1ErgwO3fu1C233KLq1avr22+/Vc6cObVr1y516dIlzX3uu+8+LVu2TIsXL1bVqlWVlJSkzp076++/zyzHUbx4cV122WWe7dOnT+u3335Tjhw5tHHjRr3wwgsaOHCgrr32Wt1yyy2qUaOGunfvroSEBA0fPlyffvppRp72BXv77bd1yy236NVXX9WgQYM0dOhQjR8/XlFRUXr77bd12223qU2bNtq8ebMKFiyoNWvW6KmnnpIkDRs2zG+fsbGx+vHHHzVo0CDPMdq3b6+aNWvq6quvDqjNNddcoyFDhmjo0KFavny5vvrqK61bt05vv/22cuXKpQEDBqht27ZBGCEAAAAAmUFQEg7W2t2STABNA2mzJuXx2sXEhOAZNWqUjhw5oq5duypnzpySpHLlymnZsmV+2y9atEiLFy9Ws2bNVLVqVUlSZGSkpk1LnY9688031ahRI8/2lClTNGrUKDVu3Ngzy6FQoUKKiYmRJO3atUuSNHbsWLVp00blypVz9kQdULhwYd1yyy2SpNWrV6f6+9RTT+mdd95Rjx49tHnzZknS4cOHtWvXLtWuXVvVqlVLs9+PP/5Yf/75p2d79erVat++vYoXL66YmBjFxcWdt417vOLi4hQXFydJKl++vCSpT58++uyzz/Trr786NRQAAAAAMrlgzXBAFmWt1aJFrjU7V61apRkzZmjv3r2qVauWBgwY4EkGePv6668lSYmJierZs6d++eUXFSxYUI8//riaNGkiSerbt6/y58+f6jivvvqqHn30UUVFRalq1aqKiIjQ3r17tWfPHknSNddco+3bt2vu3LlpJjtCrUSJEp7nx44dS/W3cOHCKleunJYsWeJpU7VqVVWpUkXJycn6/PPP0+x306ZNnufR0dFq0aKFJGn58uWe5MH52mzevFlJSUkqUaKESpYsKUnauHGjKlSooFatWqVK/gAAAAAACQdkqL///ltHjhyRJG3dulWfffaZxo0bp5EjR2rdunVaunSpIiMjU+3z+++/S3J90V27dq0kqXbt2vrmm2/01VdfqVatWipVqlSqfebPn6+4uDjdf//9kqSKFSvq9ddf16RJk7R06VI99dRT6tSpk9q1a6dBgwYpV65cGX3qF2Tfvn2e57lz55Yk5cmTx1NWsGBB7dzpWpJkzpw5atCggZKTkzV69Gh99NFH5+3/4YcfVv/+/XX55ZdrxYoV6tq1a8BtduzYoZ49e+qBBx5Q06ZNNW7cOE2dOlWffvqpXnjhBU9iBAAAAACkzHuXCmQS3gs1Nm3aVMYYNWvWTJLrF/Uff/wxzX3Kly+vUqVKqVSpUqpUqZKSk5M1efJkv8eZOHGiHnzwQc+XdEm6++679eWXX+qrr77SgAEDNHfuXFlr1bp1a02cOFH33XefOnfurAULFjh4xhfnzz//1JdffinJNV7efyXpxIkTnuetW7dWnTp1dOjQIfXv31+jR48+b//vvvuuKlasqKlTp+raa6/V4sWLlS9fvoDbzJgxQ7fccotuvvlmDR8+XK1atVJERITmzJmjXr166YMPPtCUKVN06623XvRYAAAAAMjcSDggQ11++eUyxrU0R968eSWl/sXe+xd9twIFCpzVzv3cX/sVK1bol19+0SOPPJJmHMeOHdPQoUM1evRoTZs2TS+88IJ69OihatWqqUuXLmG19sDDDz+s119/XbVq1dInn3yiv/76y1Pnnv3htnv3bk8S5qGHHlKOHDnO2/+pU6c0cuRISVLJkiV1xx13XFCb6OhoDR48WH379lXHjh01ZMgQvfnmm9qwYYPef/99lS3re8MZAAAAAFkJCQdkqMsuu8yzmKHvmgSS604TJ0+e1OHDhz1l9eq57oB6/PhxT5l7H+81DtwmTpyozp07q1ChQmnGMW7cOLVs2VKVK1fW+vXrJUlFihRR0aJFdfr0aW3cuPECz9B5CQkJGjBggJo0aaK77rpLCxculCT9+OOPioyMVO/evVO1d896iIyM9MzwiIqK8iRuJKlfv36pEjjeY+tOBAXSxtszzzyjefPmadu2bapZs6Yk6cCBAzpw4ICyZ89+zkUsAQAAAFz6SDggw7lv2ei+fGLVqlWSpKuvvlqxsbG64YYbdNVVV3nWa+jQoYOKFSumnTt36p9//lF8fLy2b9+uiIgI3Xvvvan63rx5s7799ls99thjaR5/165dmjlzpvr06SNJKlOmjCTX3RbcswfC6df4Tz75RNdee60kyRij7t27KzExUUOGDNFll12mXr16eRZtzJUrl+dWlMuXL/ckbpYuXaotW7aoVq1akqRrr71WnTp18hzDvdbFiRMn9MUXXwTcxu3KK69U27Zt9eKLL0qSfvvtN0lSTEyMZyFQdxkAAACArIlFI5HhWrVqpffee08TJkzQTTfdpMOHD6tdu3YaMmSIsmXLphIlSuivv/7y/LqeN29ezZs3T4MGDVKLFi2UlJSkq6++Wn369FFsbGyqvl955RXdeeedni/g/vTr10/9+/f39P/AAw9o3bp1euKJJ3Tq1Ck9//zzql69esYNQDr9/PPPmjBhguLi4lSgQAEdOHBAd9xxh77//nvlzZtXX3zxhaZMmaJ//vlHZcuW1bFjx/TSSy/p1Vdf9fSxd+9eFSpUyLNg59y5c9W2bVu1bNlSl19+ufLnz6/Zs2drwoQJnkUoA2njNmbMGI0cOVIJCQmSpEmTJqlWrVp65ZVXFBUVpeHDh4fVrBEAAAAAwWestaGOIWzExsbaNWvWhDqMTCE+Pj7UIWQaV155ZahDyFR4bQEAAACZizFmrbU21recSyoAAAAAAIDjSDgAAAAAAADHkXAAAAAAAACOI+EAAAAAAAAcR8IBAAAAAAA4joQDAAAAAABwHAkHAAAAAADgOBIOAAAAAADAcSQcAAAAAACA40g4AAAAAAAAx5FwAAAAAAAAjiPhAAAAAAAAHEfCAQAAAAAAOI6EAwAAAAAAcBwJBwAAAAAA4DgSDgAAAAAAwHEkHAAAAAAAgONIOAAAAAAAAMdlC3UAyJzy588f6hAyjfj4+FCHkKnkyZMn1CFkGgcOHAh1CJlG7ty5Qx0CAABAlsMMBwAAAAAA4DgSDgAAAAAAwHEkHAAAAAAAgONIOAAAAAAAAMeRcAAAAAAAAI4j4QAAAAAAABxHwgEAAAAAADiOhAMAAAAAAHAcCQcAAAAAAOA4Eg4AAAAAAMBxJBwAAAAAAIDjSDgAAAAAAADHkXAAAAAAAACOI+EAAAAAAAAcR8IBAAAAAAA4joQDAAAAAABwHAkHAAAAAADgOBIOAAAAAADAcSQcAAAAAACA40g4AAAAAAAAx5FwAAAAAAAAjiPhACBLee6553TkyJGzHuvXr5cklSpVym+9+9GpU6fQnkAQTZw4UTfddJOuu+46VaxYUWXKlFGLFi30xRdfhDo0AAAAZAIkHABkOUeOHNHhw4dTPeLj4wPaNzk5OYOjCx+zZs1Sq1attGzZMm3dulW33367li1bpg4dOuinn34KdXgAAAAIcyQcAGQ5zz77rMqUKZPq0bRpU0/9nj17zqp/5JFHdPLkSS1dujSEkQdX37599fDDD0uSIiIidOutt0pyJV3cM0IAAACAtGQLdQAAEGwNGjTQHXfcoQoVKujIkSP68ssvNX78eB0/flxHjhzRlClTdPjw4VT73Hffffrkk0908ODBEEUdfLfccovn+bFjxzR9+nRJUqFChdS8efNQhQUAAIBMghkOALKUkydPKjIyUl26dFGTJk106tQp9evXT3PnzlVkZKTi4+M1cuTIVPvExsaqQYMGmjhxYoiiDq1hw4apVKlSmjlzpqpUqaL58+erRIkSoQ4LAAAAYc5Ya0MdQ9iIjY21a9asCXUYQJaWJ0+eoB6vdevWmjp1qiSpa9eu+uSTT85q8+GHHypnzpy66667ghrb+Rw4cCBox0pISFDPnj312WefKX/+/Pryyy911VVXBe34Fyt37tyhDgEAAOCSZYxZa62N9S1nhgOALG3Hjh2e53Xr1j2r/sorr1SrVq00YcKEIEYVfnLnzq2XX35ZxhjFx8dr/PjxoQ4JAAAAYS4oCQdjTKwxxqbxKBOMGABAkooVK5Zq2/uuE5GRkWe1f+yxx7Ru3TqtWLEiw2MLN7t37061XaBAARUqVEhS6kQNAAAA4E+wZjjsktRR0rCMOoAx5lE/yYwhGXU8AJnTV199pQIFCni2r7zySs/zDRs2pGpbsGBBderUKcvObmjYsGGqhMyJEyc8tw91Jx4AAACAtAQl4WCtjbfWTpe0JCP6N8YUkzQqI/oGcOnp1q2bJCkqKko9e/aUJG3fvl0ff/zxWe0OHDigOXPmBD3GcHDkyBG9/PLLkiRrrYYOHarTp08rIiJCPXr0CHF0AAAACHeXyhoOr0nKG+ogAIS/9957TzfeeKNWrlypHTt2qFKlSpo8ebJuvvlmHT9+3NMuZ86c6tatm1577TVl1cV1e/bsqblz56pevXoqV66cpkyZoubNm2vu3Lm66aabQh0eAAAAwlxQ71JhjLle0lKf4rLW2t0X0ecdkj6XtFlSVZ/qF6y1QwLti7tUAKEX7LtUZGbBvEtFZsddKgAAADLOJXmXCmNMXrlmNxyX9ESIwwEAAAAAACnCIuFgjGlsjJlnjDlkjEk0xuw2xow3xpzvp87RkopLekHSrxkfKQAAAAAACEQ4JBw6ynWZRQtJMZKySyotqbekL40xZ9+nTpIxpqGk7pI2ShoXnFABAAAAAEAgwiHh8IxcyYackm6SlORV11BSG98djDHZJb0jyUrqZq09HYQ4AQAAAABAgMIh4TDaWrvQWptorV0saaVPfXM/+/STa4HIN6y1qy7m4MaYbsaYNcaYNXFxcRfTFQAAAAAASBEOCYdlPtv7fLZLem8YYypJej6l3fMXe3Br7TvW2lhrbWxMTMzFdgcAAAAAABQeCQffaQUnfbZzup8YY4yktyXlkPSYtfa/DI4NAAAAAABcgGyhDkCp12w4n4ckNZG0WNJyY0whr7r8ftpf5tXmhLU24QJjBAAAAAAA6RAOMxzS456UvzfKNTPC+/GTn/bPetW/FowAAQAAAABAeMxwSI9n5H8mgyRdIWmKT9mHkj5Ieb4/o4ICAAAAAACpZaqEg7V2bVp1xpgyfop/tdYuyriIAAAAAACAP5ntkgoAAAAAAJAJBCXhYIzJZYzpIOkGP9WtjDH1vNqU9akvbIzpYIypl0bfrVL2a+Wn+uqUfTsYY3Jd3FkACDf58uXTuHHjtGHDBi1ZskQ//PCDunbt6qnv1KmTjhw5ctajW7du5+w3NjZWCxYs0A8//KB169Zp0qRJKlq0aLra9O7dW+vWrdPq1av1zjvvKCoqylPXrl07zZw506FROL8pU6YoT548Zz3efvvtNPf58ccfdeutt6pevXqqUaOGunTpov3796erzfjx41WjRg3VqVNHDz/8sE6ePHMTok8++URt2rRx/mQBAAAQNoJ1SUWMpGlp1L0i6X1JQ9JoUyWl/H1Jq/zUvyqpdBp9t015SK5ExtHAwgWQGbz77ru69dZbNXHiRA0YMEAjRozQxIkTlSNHDr355puSpIMHD+q//1LfQfeff/5Js8/y5ctr3rx52r17txo2bKgiRYpo06ZNuuaaa9SwYUMlJiaet03lypU1dOhQDRkyRMuWLdPixYu1bt06vfnmm8qVK5cGDRqkO++8MyOH5ixXXHGF8ubNm6rs8ssv99t2x44duu2221SmTBmtXLlSBw8e1NVXX62ff/5ZK1euVI4cOc7bZuvWrRo8eLAGDx6s6667TjfddJNq1qypRx99VAkJCRo6dKg+//zzIJw5AAAAQiUoCQdr7W5JJoCmgbTx7btMevcBkPkVLlxYt956qyRp9erVkqRVq1w5yWeeeUZvvfWWJGnIkCGaOnVqwP327t1buXLl0po1a5ScnKz9+/fr999/V6VKldS+fXtNmTLlvG2OHnXlNuPi4hQXFyfJlciQpH79+mnmzJnatWuXMwMRoCFDhqhz584BtX355Zd17NgxxcbGKjIyUsWLF1fp0qW1fft2ffzxx7r33nvP2yZXLtekspiYGMXExEiSdu7cKUkaPXq02rZt6xkTAAAAXJpYwwFAplSyZEnPc/cXfPffwoULe77MNm7cWFOnTtXKlSv1ySefqEWLFufs97rrrpOUehZEfHx8qrrztdm8ebOSkpJUokQJT5wbN25UxYoV1bp1a40dO/aCzvlifPfdd7rnnnvUoEEDtWvXTvPnz0+z7bJlyySlngGRP3/+VHXna1O1alVFRERo79692rNnjySpWrVq2rZtm+bMmaNnn33WsXMDAABAeCLhACBT2rt3r+d57ty5JUl58uTxlBUsWFB//vmntm7dqnvvvVd33HGHqlSpohkzZuipp55Ks99ixYpJkhITEz1l7ufuNRrO12b79u3q3r27mjZtqsGDB2vs2LH68MMPNXbsWA0ePFjHjh27qHNPryuuuEKVK1fWhx9+qFmzZmnLli3q0KGDxo0b57e9ex0G73Un3M8PHDgQUJtKlSrprbfe0tKlS/XCCy/omWee0b333qtnn31WL7zwgmcGBAAAAC5dJBwAZEp//vmnvvjiC0nSjTfemOqvJJ04cUKLFi3Syy+/rOTkZB06dEjTp0+XJD399NOKjIwM+FjWWkmSMWlf9eXbZvr06WrWrJluvPFGDR06VK1bt1ZERIRmz56t3r17a+rUqZo2bZpatmyZjrO+MM2aNdNTTz2lyMhIXXHFFerQoYMkady4cTp9+nRAfbjPy32egbTp2LGjFi1apCVLlmjw4MGaM2eOkpOTdfvtt2v8+PG655571KFDB82bN+9iTg8AAABhioQDgEyra9eueu2111SrVi199tlnnvUSJOn3338/q/3BgwclSXnz5vWsK+DL3y/3OXLkSFUXSBtv0dHRnl/5O3XqpKFDh+r111/X+vXr9eGHH+rKK68M/KQdUKRIEUnSkSNHUo2Zm78ZHO47TLjrAmnj7dixYxo8eLBeeuklTZ06VYMHD1bPnj1Vo0YN3XvvvUFf0wIAAAAZj4QDgEwrISFBzz33nBo1aqQ2bdroyy+/lOS6XWN8fLxefPHFVO0LFiwoyTX74e+//5bkShq4yyVp+fLlkvyvTeCuC6SNtz59+mjevHnatm2batasKcl12cGBAweUPXt2VatW7cIGIEC+6yUcPnxYkitJUqBAAZ08eVJ//fWXp75Ro0aS/K9R4a4LpI23F198UbfddpsqV66sdevWSXJdflK0aFGdPn1aGzduvJhTBAAAQBgi4QAg05o5c6bny60xRj169FBiYqIGDhwoSbr11lsVGxsrybXOQ9u2rrvkTp482fPL/Hfffaft27erdu3akqQJEyZ47r4QERGhokWLqnTp0tqxY4c+/vjjgNu4lStXTu3atdOoUaMkSb/99puk1HdvcJdllC+++EI//vijJNeshpkzZ0qSunTpohw5cqhx48aqWLGi1qxZI0l68sknFR0drTVr1igpKclzF47y5curffv2Abdx27lzpz799FM999xzkqSyZctKSn0XD3cZAAAALh1BuS0mAGSEn3/+Wa+++qoOHTqkggULav/+/WrVqpVWrlwpybWOwosvvqiEhARdeeWVSkhIUJ8+ffTOO+94+ti7d69iYmJ05MgRSdL27dvVqlUrDR06VCtXrlTOnDk1d+5cPffcc55LBgJp4/biiy9q+PDhSkhIkCS99957qlWrll5//XVlz55dL7zwgjZs2JCh49ShQwf16dNHuXPn1q+//qrcuXNrzJgx6tatmySpRIkSiouL8yy6WalSJc2dO1eDBg1Sw4YNdeLECbVq1UqjRo1Szpw5A27j1qdPHw0YMMDT/4MPPqiffvpJPXv21KlTpzRo0CDVqFEjQ8cAAAAAwWfOtQBYVhMbG2vdv/ABCA3vO03g3Nx3jMD5ue9kAgAAAOcZY9Zaa2N9y7mkAgAAAAAAOI6EAwAAAAAAcBwJBwAAAAAA4DgSDgAAAAAAwHEkHAAAAAAAgONIOAAAAAAAAMeRcAAAAAAAAI4j4QAAAAAAABxHwgEAAAAAADiOhAMAAAAAAHAcCQcAAAAAAOA4Eg4AAAAAAMBxJBwAAAAAAIDjSDgAAAAAAADHkXAAAAAAAACOI+EAAAAAAAAcR8IBAAAAAAA4joQDAAAAAABwXLZQB4DM6fTp06EOIdPIlo1/Zunxxx9/hDqETKNChQqhDiHT2LNnT6hDyFR43wIAAE5ghgMAAAAAAHAcCQcAAAAAAOA4Eg4AAAAAAMBxJBwAAAAAAIDjSDgAAAAAAADHkXAAAAAAAACOI+EAAAAAAAAcR8IBAAAAAAA4joQDAAAAAABwHAkHAAAAAADgOBIOAAAAAADAcSQcAAAAAACA40g4AAAAAAAAx5FwAAAAAAAAjiPhAAAAAAAAHEfCAQAAAAAAOI6EAwAAAAAAcBwJBwAAAAAA4DgSDgAAAAAAwHEkHAAAAAAAgONIOAAAAAAAAMeRcEBYO3XqlEaPHq08efIoe/bsGjp0aKhDwiVk0qRJKlCggAoUKKDRo0eHOpyw8/TTT+vAgQNnPVauXOm3/Ztvvulp06BBgyBHGx54zwIAADgjW6gDANKyd+9e3X777SpevLhOnDgR6nBwiYmPj9eIESNCHUbYS0hIUGJiYqqyf/7556x2DRo00B133BGcoMIU71kAAACpkXBA2Dpy5IjGjRunMmXKqEKFCqEOB5eYYcOGqVGjRpozZ06oQwlrzz//vD7++ONztomMjNSIESM0d+5ctWrVKkiRhR/eswAAAFLjkgqErSpVquj6668PdRi4BG3YsEELFy5Unz59Qh1K2Ktbt64++OADrVy5Ul999ZWeffZZRUdHp2rTtWtX7d27V4sWLQpRlOGB9ywAAIDUSDgAyFKsterTp48GDhyo3LlzhzqcsHby5ElFRkaqe/fuuuWWW3Tq1Ck99dRTmjFjhiIjIyVJBQsW1OOPP65BgwaFOFoAAACEGxIOALKUadOmSZLuvvvuEEcS/l577TX17t1bx44d03///ac33nhDklSnTh21bt1akjRw4EBNnz5du3fvDmGkAAAACEes4QAgy/jvv/80fPhwffTRRzLGhDqcTGfnzp2e57Vr19bvv/+uRo0aqXHjxiGMCgAAAOEqKAkHY0yspB/TqC5rrd0djDgAZG1Lly6VMUZPPPGEJKW6+8LkyZO1YMECTZw4UTVr1gxViGGlaNGiOnDggGfbWut5HhkZqdatW+v06dOaPXu2JCl//vye+nHjxuno0aNq1qxZ8AIGAABAWAnWDIddkjpKukrSwIvtzBhjz9/Ko6O1dvrFHhNA5nf77bfr9ttv92z/8ccfqlGjhiSpS5cu6tevX4giC0+zZ8/WzTffrPj4eElS6dKlPXU///yzPvroIw0ZMsRT1r59e02cOFGS9PTTT+v7778ParwAAAAIL0FZw8FaG5/ypX9JMI4HAHDGAw88IEmKiopSt27dJLkurfj8889DGRYAAAAyARaNRNhKTExUjRo11LJlS0/ZW2+9pRo1amj6dCat4OIMHTpU7dq182z/3//9nxo2bKikpKQQRhVe3n//fTVp0kSLFi3S+vXrVaFCBU2dOlV33HGHjh8/nqrtjBkz1LdvX8/2hAkT9MorrwQ75JDiPQsAACA1431NboYfzJjrJS31KU73Gg4pl1S8YK0d4khgKWJjY+2aNWuc7PKSdfr06VCHkGlky8barOnhnr6P87vqqqtCHUKmsWfPnlCHkKnwvgUAANLDGLPWWhvrW84MBwAAAAAA4LiwSDgYYxobY+YZYw4ZYxKNMbuNMeONMXkC3D+bMSafMSYyo2MFAAAAAADnFw4Jh45yXWbRQlKMpOySSkvqLenLcyQRchtjnjfG/CzppKR/JJ0yxvxqjJlsjGmY8aEDAAAAAAB/wiHh8IxcyYackm6S5L1iW0NJbdLY72lJN0p6SVJrSf0k/SWprKT7Ja0wxrxnjMmeQXEDAAAAAIA0hEPCYbS1dqG1NtFau1jSSp/65n72WSVpmLX2Bmvt+9ba+dbaMZIaSfJeOr2rpP+d6+DGmG7GmDXGmDVxcXEXcx4AAAAAACBFOCQclvls7/PZLum7g7W2vrV2kJ/y7ZI+9Cm+zxhzbVoHt9a+Y62NtdbGxsTEBBozAAAAAAA4h3BIOPhOKzjps50znf0t91PWLp19AAAAAACAixAOCYek8zdJlz/9lFVw+BgAAAAAAOAcwiHh4DTjp8wGPQoAAAAAALKwTJdwMMa8YYyZfI4mxfyU7cygcAAAAAAAgB/ZQh3ABbhKUnVjTKS11t/lGNf7KfskY0MCAAAAAADeMt0MhxSXS3rct9AYU0tSR5/i9621vrfaBAAAAAAAGSgoCQdjTC5jTAdJN/ipbmWMqefVpqxPfWFjTAdjTD2f8peNMZ8bY540xtxvjHlZ0neSsqfUW0lvS3rIyXPBhdm/f786dOig7NmzK3v27Odtf/z4cQ0cOFDVqlVTo0aNVLNmTTVu3FibN2+WJHXt2tXTl+9j9uzZkqSxY8fqqquuUvXq1XX//ffr5MkzN0CZPn26brvttow5WQTVli1bdN9996levXpq2bKl6tatq549e6bZ/vjx4xo+fLjq16+v5s2bq1GjRrrlllu0ZcsWSVLPnj1VoEABv4/58+dLkiZOnKg6deqoQYMG6t69e6rX1syZM3XXXXdl7ElfgLx582rkyJH6/vvvNX/+fC1ZskT33Xdfqjbdu3fXsmXL9MUXX2jZsmV69NFHz9tvzZo1NXPmTC1ZskQrVqzQm2++qSJFiqSrTc+ePbV8+XJ98803evXVVxUVFeWpu+OOOzR16tSLPPv04z0LAADg4gXrkooYSdPSqHtF0vuShqTRpkpK+fuSVknqLKmJpEaSakl6QlJBSZdJOiJpi6QVkiZZazc4dga4YCtWrFD37t11zTXXBLzPXXfdpaVLl+r7779XtWrVlJSUpLZt2+rw4cOeNiVLltRll13m2T59+rR27dqlnDlzat26derfv7+GDx+uxo0bq3Hjxqpdu7aeeOIJJSQkaNCgQZ4vj8i8du7cqVtuuUXVq1fXt99+q5w5c2rXrl3q0qVLmvvcd999WrZsmRYvXqyqVasqKSlJnTt31t9//+1pU7x48bNeW7/99pty5MihjRs36oUXXtDAgQN17bXX6pZbblGNGjXUvXt3JSQkaPjw4fr0008z8rQvyKuvvqrmzZvrjTfe0LBhwzRo0CCNGTNGUVFR+t///qcnn3xSffv21bBhw/TGG2/oscce08CBA5UzZ06NHz/eb59XXnmlPv30U/3++++66aabdMUVV2jVqlWqWrWqbrrpJiUmJp63TcWKFTVgwACNHDlSK1eu1Lx587Rhwwb973//02WXXaZ+/fqpY0ffiWsZi/csAAAAZwRlhoO1dre11pzj0SWQNil97bXWTrXW9rDW1rPWXmmtzWetzW6tLWCtrWOtfZJkQ/goUqSIVq5cqZtvvjmg9gsXLtTChQt14403qlq1apKkyMhIzZo1S40bN/a0mzRpkjZt2uR59O3bV8WLF1fTpk21c6drndCYmBgVLlxYkrRjxw5J0vDhw9W+fXtVqMDdUjO7UaNG6ciRI+rataty5swpSSpXrpyWLVvmt/2iRYu0ePFiNWnSRFWrVpXkem1NmzZN1157rafdm2++qVWrVnkevXv3VtGiRdW4cWPt2rVLklSoUCHFxMRIkqds7NixatOmjcqVK5dh53whYmJi1Lx5c0nS2rVrJUlr1qyRJD3xxBOKjo72zApxl//www+SXLMPvL8ke3PX/fTTT0pOTtaBAwf0xx9/qEKFCrrzzjsDalO2rGtS219//aW//vpLkiuRIUlPPfWUZs+erd9++83xMTkX3rMAAACckVnXcEAmUq5cOeXJkyfg9gsWLJAknTx5Ul27dlXdunXVsmVLLVmyxNNm0KBBqlWrlmfbWqvx48erV69eioqK0jXXXKOIiAjt2bNHf/zxhySpRo0a2rp1qz7//HM999xzDp0dQsVaq0WLFkmSVq1apY4dO+q6665Tr169FBcX53efr7/+WpKUmJionj17qmnTpmrXrp2+/fZbT5u+ffuqevXqqY7z6quv6tFHH1VUVJSqVq2qiIgI7d27V3v27JEkXXPNNdq+fbvmzp2rp556KqNO+YIVL17c8/zYsWOp/sbExKhGjRrKnTu3JOmff/5J9feyyy5TjRo1/PbrTtL8+++/njL3fg0bNgyozZYtW5SUlKTixYurRIkSkqRNmzapfPnyatmypSZMmHABZ3xxeM8CAABwRma8SwUucbt375Ykffvtt9q6daskqXLlylq0aJGWL1+uOnXqqEyZMqn2mT17tv788089/PDDnvbvvfee3nnnHX399dfq16+funTpopYtW2rEiBHKlStXME8JGeDvv//WkSNHJElbt27VZ599pnHjxmnkyJFat26dli5dqsjIyFT7/P7775Kk5cuXe37pr127tr755ht99dVXqlWrlkqVKpVqn/nz5ysuLk7333+/JKlixYp6/fXXNWnSJC1dulRPPfWUOnXqpHbt2mnQoEFh+drav3+/57k7PneCQZLq1q3reX7q1ClJrqSMW9GiRf32616Hwb2P93N33fna7Ny5U08++aTuvfdeNWnSRBMnTtT06dP10UcfacSIETp+/Hh6TzfoeM8CAADwj4QDwo57obRKlSp5PqRXqVJFmzZt0rvvvqs6deqctc/YsWPVo0ePVF+iOnfurM6dO3u2P/30UyUnJ6tNmzYaO3asVq9ereTkZN1///1q3bp1xp4UHOe9oF7Tpk1ljFGzZs00cuRIbdq0ST/++KPq16/vd5/y5ct7EguVKlXSL7/8osmTJ6f6Bdpt4sSJevDBB1O9tu6++27dfffdnu1Zs2bJWqvWrVtr4sSJWrt2rZKTk3XPPfeoRYsWjp73hTh06JC++uorNW/eXNdff73mz5+v66+//pz7WGs9z40xAR/Lvd+59vFt8+mnn6Za9+K2225TRESE5s+fr549e6pWrVqKiIjQ9OnTtXDhwoBjCRbeswAAAPzjkgqEnYIFC0pSqinNefPmlSTt3bv3rPbfffedfv75Zz322GNp9nns2DE9//zzmjBhgj744AP1799fvXr1Us2aNXX33Xd7rp9G5nH55Zd7vrC6Xx/er5l9+/adtU+BAgXOaud+7q/9ihUr9Msvv+iRRx5JM45jx45p6NChGj16tKZNm6YXXnhBPXr0ULVq1dSlSxf9+uuvF3B2znv00Uf19ttvq3r16vroo4886yVIrktS3Nx3ZMiRI4en7MCBA377PHjwYKp9JHnuMOGuC6SNt+joaD3//PMaMGCA2rdvrwEDBuidd97Rxo0b9e677541UyAc8J4FAADgHwkHhNzJkydTfflp0KCBpDPXmEvS0aNHJblWefc1duxYPfDAA54F/PwZOXKkbr/9dl111VWeqfRFixZVsWLFdPr0aa1fv96JU0EQXXbZZZ4F+nzXJZBc6xacPHky1V0C6tVz3V3Xe5q+ex/3+gHeJk6cqM6dO6tQoUJpxjFu3Di1bNlSlStX9ryOihQpoqJFi+r06dPauHHjBZ6hs44ePaohQ4aoefPmuueeezzrWaxdu1YbNmzw/Bu7/PLLU/09duyY1q1bJ8mVKHAnbSRp5cqVkqR8+fJ5ytz7uesCaePtySef1BdffKHt27d71tI4ePCgDh48qOzZs+vqq6++4DFwCu9ZAAAAgSHhgJCrV6+eSpUqpdWrV0uS7r33XpUoUULbt29XfHy8/v77b23dulURERHq2rVrqn03btyoxYsXn3Ohvh07dmjGjBkaOHCgpDMr4B86dMizuKC7DJmL+7/7jz/+KOnML/VXX321YmNjdcMNN6T6wtahQwcVK1ZMO3fu1D///KP4+Hht375dERERuvfee1P1vXnzZn377bfn/BV6165dmjlzpvr06SNJnl/f4+LiPF9I3XdhCLWpU6d6vhgbY/TQQw8pMTHRs07C66+/LkmKjY2VdGZdh7feesvzRfrLL7/UunXrPItIvvHGGzp+/LjnkociRYqoVKlS2rlzpz7//POA27iVLVtWd9xxh8aNGyfpzJobhQoV8iR93GWhxHsWAABAYIz3dbpZXWxsrHXfEg7ndvr06YDb/vbbb3rooYf0559/atu2bZKkxo0bq0qVKnrttdfUunVrrV27VosXL1blypUlSb/++qv69eun7du36/Tp08qXL58GDhyoW265JVXf9913n4wxev/999M8/m233aaOHTuqU6dOkly/Qnbr1k0bN25UYmKi7r///gxdAT5bNpZKSY/4+Ph0tf/88881YcIEZc+eXYcPH1adOnU0ZMgQFStWTHfffbfWr1+vuXPnqmLFipJcC/wNGjRIO3fuVFJSkvLmzas+ffqoWbNmqfp95JFHZIzRW2+9leax77rrLt11111q3769JNdr64knntCmTZt06tQp3XPPPXr66afTOQKBu+qqqwJu279/f7Vo0UJ//fWXChQooAMHDmj8+PGpLqd49NFH1bFjRyUkJChPnjyaMWOGXn31VU/9hx9+qOrVq6tNmzaeKf21a9fWgAEDlC9fPuXMmVM///yzhgwZkuoyjEDaSK6kyGeffaaZM2dKcl1eMW7cOFWtWlXZs2fXjBkzNHHixAsaK/cdRQKR1d+zJN63AABA+hhj1lprY88qJ+FwBgmHwKUn4ZDV8cE9fdKbcMjK0pNwyOrSk3AA71sAACB90ko4cEkFAAAAAABwHAkHAAAAAADgOBIOAAAAAADAcSQcAAAAAACA40g4AAAAAAAAx5FwAAAAAAAAjiPhAAAAAAAAHEfCAQAAAAAAOI6EAwAAAAAAcBwJBwAAAAAA4DgSDgAAAAAAwHEkHAAAAAAAgONIOAAAAAAAAMeRcAAAAAAAAI4j4QAAAAAAABxHwgEAAAAAADiOhAMAAAAAAHAcCQcAAAAAAOC4bKEOAJlTtmy8dJAx8ufPH+oQMo09e/aEOoRMIzo6OtQhZCqnTp0KdQgAAOASwAwHAAAAAADgOBIOAAAAAADAcSQcAAAAAACA40g4AAAAAAAAx5FwAAAAAAAAjiPhAAAAAAAAHEfCAQAAAAAAOI6EAwAAAAAAcBwJBwAAAAAA4DgSDgAAAAAAwHEkHAAAAAAAgONIOAAAAAAAAMeRcAAAAAAAAI4j4QAAAAAAABxHwgEAAAAAADiOhAMAAAAAAHAcCQcAAAAAAOA4Eg4AAAAAAMBxJBwAAAAAAIDjSDgAAAAAAADHkXAAAAAAAACOI+EAADinU6dOafTo0cqTJ4+yZ8+uoUOHhjqksDNw4ECdOnXqrMeWLVs8bUqWLKlJkyZpx44d2rJli37++Wf17dtXERH8rxgAAFya+JQDAEjT3r17Vb9+fa1cuVInTpwIdThh7ciRI/rrr79SPeLj4yVJl112mb766it17txZn3zyiapUqaIPPvhAw4cP12uvvRbiyAEAADIGCQcAQJqOHDmicePG6ZVXXgl1KGHvySefVNGiRVM9GjZsKEm69dZbVb58eUnSokWLJElff/21JOnhhx/21AEAAFxKSDgAANJUpUoVXX/99aEOI1O49tprNWvWLG3ZskWrV6/W4MGDFR0dLUkqXbq0p11CQoIkVzLH7aabbgpusAAAAEFAwgEAgIt04sQJRUZGqlOnTqpfv75OnTqlAQMGaOHChYqMjNSePXs8bfPmzStJypcvn6esVKlSQY8ZAAAgo5FwAADgIo0dO1YPPfSQjh49qn///VcvvfSSJKlBgwa66667NG/ePO3evVuS1Lp1a0nSHXfc4dk/e/bswQ4ZAAAgw5FwAADAYdu3b/c8r1+/vo4fP64bbrhBU6ZMUdOmTbV8+XIlJiZ6Lqv4+++/QxUqAABAhskW6gAAAMjsihcvrn379nm2k5OTPc8jIyMlSXv27NEDDzzgKY+IiNDzzz8vSdq0aVOQIgUAAAieoMxwMMbEGmNsGo8ywYgBAICM8s0336hAgQKe7SuvvNLzfN26dZKkHj16pNqnevXqypYtm+Lj4z13rAAAALiUBOuSil2SOkoa5nTHxpjbjDHvGWO2GGPijTGJxpg/jTGbjTGfGGOeN8Zcef6eAAC4cI8++qgkKSoqSr169ZIkbd26VdOmTZMkvfjii2rbtq0kKTo6WqNGjVJycrKeeuopnThxIjRBAwAAZKCgJBystfHW2umSljjVpzGmlDHmB0lzJXWVdELSGEkPSXpJ0klJ7SQNl3SDU8cFgKwkMTFRNWrUUMuWLT1lb731lmrUqKHp06eHMLLw8vbbb6tZs2Zau3at9uzZo8qVK+u9995T06ZNdfz4cUnS3LlzNWbMGG3atEk7d+5UtmzZdPvtt2vKlCkhjh4AACBjGGtt8A5mzPWSlvoUl7XW7k5nPyUlrZJUNKVoiqT7rbXJXm0iJc2UdLukh621/ztfv7GxsXbNmjXpCQUAQub06dOhDiHTiI6ODnUImcqpU6dCHQIAAMhEjDFrrbWxvuWZddHISTqTbDgu6QnvZIMkWWuTjDF9JCVI2hnk+AAAAAAAyNIyXcLBGNNQ0o1eRd9Za+P9tbXWbpfUOSiBAQAAAAAAj2AtGnlOxpjGxph5xphDKYs+7jbGjDfG5PHT/D6f7S1e/WQ3xuQ1xpiMjRgAAAAAAJxLOCQcOsq1rkMLSTGSsksqLam3pC9T1mLw1tBn+2TKnSg2y7VQ5L+SThhjlhtjOmVs6AAAAAAAwJ9wSDg8I1eyIaekmyQledU1lNTGvWGMiZB0lc/+fSQ9KWliStvFkqIkXStpijHmo5T9/DLGdDPGrDHGrImLi7v4swEAAAAAAGGRcBhtrV1orU201i6WtNKnvrnX87ySfGc8GLkWjXzHWjtLrrtSeK/p0FHS02kdPGW/WGttbExMzAWfBAAAAAAAOCMcEg7LfLb3+WyX9HqeO40+FrifWGuPSvrOp76Pn0szAAAAAABABgmHhIPvdQwnfbZzej0/5mf/eGvtvz5lu322C0m6Jv2hAQAAAACACxEOCYek8zfx+FfSKZ+yBD/tjvgpK56O4wAAAAAAgIsQDgmHgFlrkyRt9Cn2dwtMf2W+iQoAAAAAAJBBMlXCIcVCn+08ftr4K/s1A2IBAAAAAAB+ZMaEwzuSEr228xljCvq0udJne4u1dmfGhgUAAAAAANwyXcLBWvu7pOd9im93PzHGXC7peu9dJPXJ8MAAIMzt379fHTp0UPbs2ZU9e/bztj9+/LgGDhyoatWqqVGjRqpZs6YaN26szZs3S5K6du3q6cv3MXv2bEnS2LFjddVVV6l69eq6//77dfLkmXWBp0+frttuuy1jTvYi5MuXT6+88oq2bt2qFStWaN26derWrZunvnLlypoxY4Z27Nihb7/9Vjt37tSbb76pQoUKpdlnmzZt9M033+jrr7/W+vXrtWfPHn3yySeqUqVKuto888wz2rx5s9avX6/JkycrKirKU3f33Xdr7ty5Do8GAADAhQtKwsEYk8sY00HSDX6qWxlj6nm1KetTX9gY08EYU89dYK19SdJzkk6nFL1sjHnOGPOgpK905vaZJyQ9bK2d5+gJAUAms2LFCt18882KiAj8bf+uu+7S+PHjNWXKFC1fvlxr1qxRgQIFdPjwYU+bkiVLqlKlSp5HuXLlJEk5c+bUunXr1L9/f91///1666239NFHH+ntt9+WJCUkJGjQoEF6+eWXnT1RB0yePFk9evTQrFmzdO211+qrr77S66+/rscff1ySNH/+fLVp00YffvihmjRpouXLl+uhhx7Shx9+mGaf9erV0w8//KBmzZqpRo0aWrJkie644w4tWLAg4DY1atTQqFGj9P7776t79+7q1KmTHnnkEUlSrly5NHToUPXu3TsDRwYAACB9gjXDIUbSNEkD/dS9IqmHV5vGPvVVUsp7eBdaa0dLqixpnKRdkp6V9LakipLWSBojqYq19j3HzgIAMqkiRYpo5cqVuvnmmwNqv3DhQi1cuFA33nijqlWrJkmKjIzUrFmz1LjxmbfpSZMmadOmTZ5H3759Vbx4cTVt2lQ7d7quZIuJiVHhwoUlSTt27JAkDR8+XO3bt1eFChWcPM2LdsUVV3hmXfzwww+SpO+//16S1LdvXxUuXFilSpWSJO3du1eS9Mcff0iSrr322jT7/eijjzR+/HjPtrvPEiVKeMbmfG3Kly8vSYqLi9OhQ4ckyTN+AwYM0Mcff+wZcwAAgHCQLRgHsdbulv87R/gKpI13v7skPXMhMQFAVuKeeRAo96/qJ0+eVNeuXbVp0ybFxMTo6aef1g03uCarDRo0SAULnllCx1qr8ePHq1evXoqKitI111yjiIgI7dmzx/OlvEaNGtq6das+//xz/fTTTw6dnXPcyQRJOnr0aKq/V1xxhS6//HJ9++23atKkiSpVqiRJqlixoqQzCQJ/NmzY4HkeHR2t1q1bS5K+/fZbT/LgfG1+/vlnJSUlqWTJkp44169fr0qVKunOO+9UrVq1Lu7kAQAAHBaUhAMAIHPZvXu3JNeX3a1bt0pyrV2waNEiLV++XHXq1FGZMmVS7TN79mz9+eefevjhhz3t33vvPb3zzjv6+uuv1a9fP3Xp0kUtW7bUiBEjlCtXrmCeUkD27NnjeZ4nj+uGR3nz5vWUFSpUSG3bttW0adP05JNPqkWLFqpcubJmzpzpOe9z6dmzpwYPHqz8+fPru+++0z333BNwm23btunBBx9Ut27d1KxZM40aNUqTJ0/W/Pnz9fzzz+vYsWMXe/oAAACOynSLRgIAMp57ccdKlSqpTJkyKlOmjKpUqaLk5GS9++67fvcZO3asevToody5c3vKOnfurO+++07Lly/XsGHD9Pnnnys5OVlt2rTR2LFjddddd6lt27aaM2dOUM7rfA4ePKh581zL/jRr1izVX0k6ffq0Fi5cqGbNmunJJ5/UNddco5deeklt27bViBEjztv/66+/ruLFi+v9999X48aNtXLlSl1++eUBt5k6daqaNGmi6667ToMGDdKdd96piIgIffbZZ3rmmWf08ccf69NPP1WrVq2cGRAAAICLQMIBAHAW96US7l/5pTO/9LvXLvD23Xff6eeff9Zjjz2WZp/Hjh3T888/rwkTJuiDDz5Q//791atXL9WsWVN333132Kw/cO+992rChAmKjY3VvHnzPJc8SK5LLmrXri3Jdc6SaxaIJPXo0UNXXul7V+aznTp1SoMHD5YklS5dWu3atbugNtHR0RoxYoSefPJJ3XfffRo1apQmTpyodevWacaMGem+jAYAAMBpJBwAADp58qT++usvz3aDBg0kKdU0ffdaBiVLljxr/7Fjx+qBBx5QTExMmscYOXKkbr/9dl111VVau3atJKlo0aIqVqyYTp8+rfXr1ztxKhctISFBzz77rOrUqaPbbrvNs57FqlWrUo2HtVaSlJyc7Clzz0SIiopKtb7FoEGDUiVvjh8/7nnuTuQE0sZb//79NXv2bG3ZssWTBDlw4ID279+v7Nmzq0aNGuk+dwAAACeRcAAAqF69eipVqpRWr14tyfUrf4kSJbR9+3bFx8fr77//1tatWxUREaGuXbum2nfjxo1avHixnnrqqTT737Fjh2bMmKGBA103K3LPBDh06JDi4uJSlYXa3LlzPXfiMMboscceU2Jiop577jl9//33OnjwoCSpevXqkuT5Yv/rr7/q559/luRKTvzxxx+qU6eOJKlx48Z64IEHPMd48MEHJUknTpzwXMIRSBu38uXL6+6779awYcM8x5akwoULe5I+7jIAAIBQYdFIAMgCfvvtNz300EP6888/PWU33nijqlSpotdee02lSpVSXFyc55f0fPnyafHixerXr5+aNm2q06dPq3r16ho4cKDq1auXqu+XXnpJd911l0qXLp3m8Xv37q0hQ4Z4fsF/5JFHtHbtWj3yyCNKTEzU0KFDw+YuCxs2bNCbb76pQ4cOqWDBgtq/f7+aN2+uFStWSJKaN2+ugQMHatCgQerevbuKFi2qqVOnaujQoTp16pQk160yY2Ji9N9//0mSZs2apbvvvlutW7dW/vz5lT9/fs2cOVNjx47V9u3bA27j9vLLL2vIkCFKSEiQJL399tuqXbu23n77bUVFRWngwIFat25dsIYMAADAL+OeEgopNjbWrlmzJtRhAEBATp8+HeoQMo3o6OhQh5CpuBMnAAAAgTDGrLXWxvqWc0kFAAAAAABwHAkHAAAAAADgOBIOAAAAAADAcSQcAAAAAACA40g4AAAAAAAAx5FwAAAAAAAAjiPhAAAAAAAAHEfCAQAAAAAAOI6EAwAAAAAAcBwJBwAAAAAA4DgSDgAAAAAAwHEkHAAAAAAAgONIOAAAAAAAAMeRcAAAAAAAAI4j4QAAAAAAABxHwgEAAAAAADiOhAMAAAAAAHAcCQcAAAAAAOA4Eg4AAAAAAMBx2UIdAAB4O3HiRKhDyDRy5swZ6hAyjVOnToU6hEwlT548oQ4h04iPjw91CJlGtmx87ASArIYZDgAAAAAAwHEkHAAAAAAAgONIOAAAAAAAAMeRcAAAAAAAAI4j4QAAAAAAABxHwgEAAAAAADiOhAMAAAAAAHAcCQcAAAAAAOA4Eg4AAAAAAMBxJBwAAAAAAIDjSDgAAAAAAADHkXAAAAAAAACOI+EAAAAAAAAcR8IBAAAAAAA4joQDAAAAAABwHAkHAAAAAADgOBIOAAAAAADAcSQcAAAAAACA40g4AAAAAAAAx5FwAAAAAAAAjiPhACDLSEpK0hNPPKG6deuqXr16KlasmKpXr67nn39ehw8fDnV4QJbx3HPP6ciRI2c91q9fL0kqVaqU33r3o1OnTqE9gSA7deqURo8erTx58ih79uwaOnRoqEMCACAgJBwAZBmnTp3Su+++q969e2vVqlVav369Tp06pfHjx6t58+ZKTEwMdYhAlnHkyBEdPnw41SM+Pj6gfZOTkzM4uvCxd+9e1a9fXytXrtSJEydCHQ4AAOlCwgFAlhEREaHrrrtOHTt2lCQVLlxY9957ryTpl19+0bfffhvK8IAs5dlnn1WZMmVSPZo2beqp37Nnz1n1jzzyiE6ePKmlS5eGMPLgOnLkiMaNG6dXXnkl1KEAAJBu2UIdAAAES1RUlL766qtUZYUKFfI8P3r0aLBDArKsBg0a6I477lCFChV05MgRffnllxo/fryOHz+uI0eOaMqUKWdd6nTffffpk08+0cGDB0MUdfBVqVJFVapU0e7du0MdCgAA6cYMBwBZ2q5duyRJOXPmVP369UMcDZA1nDx5UpGRkerSpYuaNGmiU6dOqV+/fpo7d64iIyMVHx+vkSNHptonNjZWDRo00MSJE0MUNQAASC8SDgCyrKNHj2r69OmSpFGjRqlIkSIhjgjIGsaPH68ePXro6NGj+vfffzVhwgRJUr169dSmTRu/+/Tq1UtfffWVtm7dGsRIAQDAxSDhACBLSkxMVJcuXXT06FFNmjRJ3bt3D3VIQJa1Y8cOz/O6deueVX/llVeqVatWnsQEAADIHFjDAUCWc+jQIXXu3FknT57UDz/8oHLlyungwYOKiopSgQIFQh0ecMkrVqyY9u/f79n2vutEZGTkWe0fe+wxrVu3TitWrAhKfAAAwBlBmeFgjIk1xtg0HmWCEQMASNI333yjRo0a6frrr9eSJUtUrlw5SdL//vc/zZ8/P8TRAVnDV199lSq5d+WVV3qeb9iwIVXbggULqlOnTsxuAAAgEwrWJRW7JHWUNOxiOzLGfHOO5EVajwkXfQYAMr39+/erZcuWOnjwoN544w2VLl1aJUqUUIkSJTR+/PhQhwdkKd26dZPkuntMz549JUnbt2/Xxx9/fFa7AwcOaM6cOUGPEQAAXJygJBystfHW2umSlgTjeADgz6lTp5ScnKxTp07p8OHDqR7Hjx8PdXhAlvHee+/pxhtv1MqVK7Vjxw5VqlRJkydP1s0335zq32LOnDnVrVs3vfbaa7LWhjDi0ElMTFSNGjXUsmVLT9lbb72lGjVqeBa9BQAgXGWVNRxOhjoAAKFXunRpEgtAGHj55Zf18ssvn7fdiRMnVLZs2SBEFL6ioqK0fv36UIcBAMAFyax3qfjdWmvO9ZDUOaWtlfRBCGMFAAAAACDLyawJh3MyxkRIej5lc6a1dnMo4wEAAAAAIKsJi4SDMaaxMWaeMeaQMSbRGLPbGDPeGJPHT/PJkiacp8t2kqrINbvhoheqBAAAAAAA6RMOazh0lDRckkl5SFJpSb0l1TPGNLbWJrkbW2snn6szY4zRmdkNs621Gx2PGAAAAAAAnFM4zHB4RlILSTkl3SQpyauuoaQ26ezvdknVUp4zuwEAAAAAgBAIh4TDaGvtQmttorV2saSVPvXN09nfgJS/8621P52vsTGmmzFmjTFmTVxcXDoPBQAAAAAA/AmHhMMyn+19PtslA+3IGNNSUu2UzaGB7GOtfcdaG2utjY2JiQn0UAAAAAAA4BzCIeHgO63gpM92znT05Z7dsNBau/rCQwIAAAAAABcjHBIOSedvcn7GmOaS6qdsBjS7AQAAAAAAZIxwSDg4xT27YbG11ncdCAAAAAAAEESXRMLBGHO9pOtSNpndAAAAAABAiF0SCQdJg1L+fmut/S6kkQAAAAAAgMyfcDDGNJTUNGWT2Q0AAAAAAISBoCQcjDG5jDEdJN3gp7qVMaaeV5uyPvWFjTEdjDH10ujePbthhbV2iVMxAwh/TzzxhBo2bKiWLVuqbNmyqlq1qgYNGqRTp075bf/ZZ5/phhtu0M0336xatWqpTJkyat++vbZs2ZKuNi+99JKuueYa1apVS127dtXJk2durjNjxgzdfvvtGXfSQJjJly+fxo0bpw0bNmjJkiX64Ycf1LVrV099p06ddOTIkbMe3bp1O2e/sbGxWrBggX744QetW7dOkyZNUtGiRdPVpnfv3lq3bp1Wr16td955R1FRUZ66du3aaebMmQ6NQuD279+vDh06KHv27MqePft52x8/flwDBw5UtWrV1KhRI9WsWVONGzfW5s2bJUldu3b19OX7mD17tiRp7Nixuuqqq1S9enXdf//9qd6zpk+frttuuy1jThYAkOVlC9JxYiRNS6PuFUnvSxqSRpsqKeXvS1rlXWGMqSPp5pRNZjcAWczs2bM1b948XXPNNYqLi1O1atU0duxYSdLQoWe/JaxevVr16tXTqFGjJEkPPPCApk+frrVr12rnzp0yxpy3zYYNGzRw4EANHTpU1113nZo2bapatWrpscceU0JCgoYMGaK5c+cGbxCAEHv33Xd16623auLEiRowYIBGjBihiRMnKkeOHHrzzTclSQcPHtR///2Xar9//vknzT7Lly+vefPmaffu3WrYsKGKFCmiTZs26ZprrlHDhg2VmJh43jaVK1fW0KFDNWTIEC1btkyLFy/WunXr9OabbypXrlwaNGiQ7rzzzowcmrOsWLFC3bt31zXXXBPwPnfddZeWLl2q77//XtWqVVNSUpLatm2rw4cPe9qULFlSl112mWf79OnT2rVrl3LmzKl169apf//+Gj58uBo3bqzGjRurdu3aeuKJJ5SQkKBBgwZp/vz5jp4nAABuQZnhYK3dba0153h0CaSNn35/9Kr/KhjnAiB8/O9///N8cI+JiVG5cuUkSRs2bPDbvmPHjnryySc92/Xru+6ku3//fh06dCigNjt37vQcr3DhwpLkKRs5cqTuuusulS9f3qEzBMJb4cKFdeutt0pyJfQkadUq128DzzzzjIwxkqQhQ4aodu3aqR4ff/xxmv327t1buXLl0po1a5ScnKz9+/fr999/V6VKldS+ffuA2rjfD+Li4hQXFydJnn+b/fr108yZM7Vr164MGJW0FSlSRCtXrtTNN998/saSFi5cqIULF+rGG29UtWrVJEmRkZGaNWuWGjdu7Gk3adIkbdq0yfPo27evihcvrqZNm/p9z9qxY4ckafjw4Wrfvr0qVKjg5GkCAOARrBkOAOC4Zs2aeZ7//PPP+uWXX2SMUdu2bf22r169uuf5sWPHPDMRrrvuOl1xxRUBtbnmmmsUERGhPXv26I8//vDss23bNs2aNUs//vijsycJhLGSJUt6nh89ejTV38KFC3u+4Ddu3FgtWrRQ2bJltW/fPk2aNEkLFixIs9/rrnPdeMp7FkR8fLynbsqUKedtM27cOCUlJalEiRKeODdu3KiKFSuqdevWatCgwcWc+gVxJ0EC5R6jkydPqmvXrtq0aZNiYmL09NNP64YbXFepDho0SAULFvTsY63V+PHj1atXL0VFRfl9z6pRo4a2bt2qzz//XD/99JNDZwcAwNlIOADI9G6++WatWLFCERERGjBggO67775ztn/jjTc0bNgw/fPPP2rUqJE+/PDDgNtUqlRJ7777rt59910tWrRIffr00X333afWrVtr2LBhypUrV4acIxCO9u7d63meO3duSVKePHk8ZQULFtSff/6prVu3auLEiSpUqJCWLFmiGTNmaPDgwRo/frzffosVKyZJSkxM9JS5n7vXaDhfm+3bt6t79+568MEHdeONN2rs2LH68MMP9fnnn2vw4ME6duzYRZ9/Rtu9e7ck6dtvv9XWrVslSZUrV9aiRYu0fPly1alTR2XKlEm1z+zZs/Xnn3/q4Ycf9rR/77339P/t3Xt8VdWd///XCjdjVAwIVaxc5K4dxRqFQeqlrX5/xaGtogNaO1ClVsfSeuNSL1BFFPD69VsfnXbGibUgaL0VGdSpVouAykTUaIFwkXRs0CaOOBouIsn6/RFyPDkGckI2CYHX8/E4j+y91mfvs/YZm+G8s/bav/71r/nDH/7A5MmTGTt2LGeffTbTp0/3d5YkaY9q9U+pkKRnn32W4uJivvSlLzFt2jSuvvrqXdb/8z//M3/5y1/4/ve/z+LFi/na176W+stoNjUXXnghL7zwAn/605+46aabePLJJ6muruacc87hjjvuYNSoUZx//vmu5aB93t/+9jeefvppAL7xjW/U+QmwdetWnnvuOe6++26qq6spLy9n3rx5AFxzzTW0adMm6/eKMQKkbtPIpmbevHmceeaZfOMb3+Dmm2/m29/+Njk5Ofz+97/nqquuYs6cOcydO5ezzz67EVfdfGoXd+zfvz89e/akZ8+eDBw4kOrqav71X/+13mNuv/12Lr/88lQABHDRRRexaNEiFi9ezLRp03jiiSeorq7m3HPP5fbbb+f8889n5MiRzJ8/v1muS5K0/zBwkLRPOProo7nkkksA+NWvfsXWrVt3Wd++fXumTKl5yM27777L448/vls1mzdv5sYbb+Suu+5i9uzZ3HjjjYwfP54TTjiBCy+8sNnvEZea28UXX8wvfvELvvrVr/L444+n1ksA+Mtf/vKF+vfffx+AQw45hC5dutR7zg0bNgDUeapEhw4d6vRlU5MuNzeXm266iWuvvZbvfe973Hzzzdx333288cYb/Pa3v+Xoo4/O/qKbSe2tEumzRg455BCg7uySWosWLeKtt97ixz/+8U7PuXnzZq6//nruueceHnzwQa677jp++tOfcsIJJzBq1KjUmg+SJCXBwEFSq1RRUZF6IkWtAw44AIDq6mo++eQTPv30Uz744INU/7Rp0+qslJ+bm5va/t///d+sa9LNmDGDb3/72wwcODB1L3S3bt3o1q0b27dv54033mjCVUp7v8rKSn72s58xbNgwzj33XJ555hkA/uu//ouNGzcya9asOvW1X6K3bt3Khx9+CNSEBunrECxevBiAQw89NNWWn59fpy+bmnQTJ05kwYIFlJSUcMIJJwDw3nvv8d5779GuXbvUoowtKfN3Vu06E+m3f9SukZG+fkat22+/nR/84Ac7DXKgZnHb73znOxxzzDG89tprQM0tKP7OkiTtCQYOklqlzZs3c+edd6b+glpZWcnvfvc7oGbBuC5dunDKKadw9NFHpxZyfOmll/jNb36TOse///u/AzV/Fa19Dn02NbXWrl3LI488wvXXXw9Ar169ACgvL0/9lXdv/KuplKTHHnuMYcOGATW3Mlx++eVs27aNG2+8EYBvfetbFBQUADXrPNQu6vrAAw+k1lxYtGgRq1ev5sQTTwTgnnvuYfPmzRQUFJCTk8MRRxxBjx49WLNmTerpFtnU1OrduzfnnXde6nG369evB2qe3FD75by2rSUNHjyY7t27p5748f3vf58vf/nLrF69mo0bN/Lhhx+yatUqcnJyuPjii+scW1xczPPPP7/LW8rWrFnDww8/nPq/Te3vJ39nSZL2FBeNlNQqdezYkbPPPptRo0Zx6KGH8s4773DggQcyadIkrrrqKqDmL4AVFRWpKcjf+c53eOSRR3jqqaf46KOP+PDDD/nud7/LtddeS79+/bKuqXXNNdcwderU1HTnH/7wh7z22mupL1w///nPU39JlfZVb731Fv/v//0/ysvL6dy5Mxs2bGDEiBEsXboUqFlHYdasWVRWVnL00UdTWVnJxIkT+fWvf506x1//+le6dOnCJ598AsDq1asZMWIEN998M0uXLuWAAw7gqaee4mc/+1lqXYNsamrNmjWLW265hcrKSgDuv/9+vvrVr3LffffRrl07brrppp0+TjdJ69evZ9y4cfztb39LtX3jG99g4MCB/OIXv6B79+51fmd17NiR559/nsmTJ3PGGWewfft2jj/+eG688UYGDx5c59x33HEH559/Pj169Njp+1911VX8/Oc/T/3O+tGPfsRrr73Gj370I7Zt28bNN9/MV7/61T1w5ZKk/VWoXWBJUFBQEIuKilp6GNJ+raG1F/S52ltIpKSlrxmgXctccFY717atf+eSpH1VCOG1GGNBZru3VEiSJEmSpMQZOEiSJEmSpMQZOEiSJEmSpMQZOEiSJEmSpMQZOEiSJEmSpMQZOEiSJEmSpMQZOEiSJEmSpMQZOEiSJEmSpMQZOEiSJEmSpMQZOEiSJEmSpMQZOEiSJEmSpMQZOEiSJEmSpMQZOEiSJEmSpMQZOEiSJEmSpMQZOEiSJEmSpMQZOEiSJEmSpMQZOEiSJEmSpMQZOEiSJEmSpMS1bekBSFK6Aw44oKWHIO33Pvnkk5YeQqsRQmjpIbQaMcaWHoIkqZk5w0GSJEmSJCXOwEGSJEmSJCXOwEGSJEmSJCXOwEGSJEmSJCXOwEGSJEmSJCXOwEGSJEmSJCXOwEGSJEmSJCXOwEGSJEmSJCXOwEGSJEmSJCXOwEGSJEmSJCXOwEGSJEmSJCXOwEGSJEmSJCXOwEGSJEmSJCXOwEGSJEmSJCXOwEGSJEmSJCXOwEGSJEmSJCXOwEGSJEmSJCXOwEGSJEmSJCXOwEGSJEmSJCXOwEGSJEmSJCXOwEGSJEmSJCXOwEGSJGkv16lTJ+655x7WrVtHSUkJa9asYcmSJQwfPhyAEAITJ05k9erVrF+/ntLSUm677TY6dOjQwiOXJO3PDBwkSZL2YgcddBBLlizhoosuYsSIEfTv358BAwawdu1a+vfvD8Bdd93FzJkzWbBgAb169WLatGlMnjyZuXPntvDoJUn7s7YtPQBJkiTt3KRJkxgwYAD33nsvK1asAKCqqooxY8YA0KNHD8aPHw/AU089VefnOeecw7Bhw1i8eHELjFyStL9zhoMkSdJebNSoUQAcdthhPPnkk6xZs4ZXXnmF0aNHA3D22WfTpk0bAMrLywGoqKiguroagBEjRrTAqCVJcoaDJEnSXis3N5fevXsDMHz4cL7yla9wyCGH8OabbzJ37lw++ugj+vXrl6rfsmULADFGPv30U3Jzc+v0S5LUnJzhIEmStJfKz88nJ6fmn2svv/wyZWVlrFy5kuLiYgCuu+46DjrooFR9VVVVart2hkN6vyRJzcnAQZIkaS+1ffv21PYHH3yQ2q6oqADg2GOPpbKyMtVee2sFkAoq0vslSWpOzRI4hBAKQghxJ6+ezTEGSZKk1qaioiIVGMQYU+212x06dGD16tWp9tzcXKDmMZm1j8RM75ckqTk11wyHdcAFwLSkThhCODaEcGcIYVkI4X9CCJ+FELaGEN4LIbwQQrguhPClpN5PkiSpucUYee655wDo1KlTqr1z584AFBcXs3DhwtTtE127dgVqFpisneGwYMGC5hyyJEkpzRI4xBg3xhjnAX9M4nwhhJuAYuBq4CTgfeBK4GYgDzgdmA6sCyGcm8R7SpIktYSpU6eyefNmhgwZQn5+PkcddRTHHXccADNmzKC0tJT77rsPqHliRfrP+fPn89JLL7XMwCVJ+72QPj1vj79ZCKcDL2Q094oxljbiHP8IPJzR3C/GuGZH/2XAL9P6tgJfiTGua+jcBQUFsaioKNuhSJKk/VwIoVnep6CggFtuuYVjjjmGAw88kNLSUm699VYef/xxoGa9hokTJzJu3DjatGlDCIGHH36YqVOnsnXr1mYZY0Oa89+ckqTmFUJ4LcZY8IX2Vhg4/CdwZlrTRzHG/LT+QcDrGYfdGGO8paFzGzhIkqTGaK7AYV9g4CBJ+66dBQ6t8SkV3TP2P25gH6DbHhqLJEmSJEmqx14ROIQQTg0hLAghlIcQtoUQSkMId4UQDq6n/L8z9jtk7B9QzzEN3k4hSZIkSZKSszcEDhdQc5vFcKAL0A7oAVwFPBNCaJNR/+8Z+11CCB3T9vtl9H8A/Ca54UqSJEmSpIbsDYHDtdSEDQcA3wSq0vqGAnWeMrHjaRc/A7bvaMoB7g0h9A0hnAj8PK38deCMGOMHe2bokiRJkiSpPntD4DAjxvhsjHFbjPF5YGlG/1mZB8QYZwDH8vljNv8JWA0UAccD1dTMhPhOjPHtXb15COHSEEJRCKGooqKiiZciSZIkSZJg7wgcMh8OXZaxf1T6TgihfQjhVuAt4Os7mh8E/hEYC7xMzXVdDLwTQpgZQtjpdcYYfx1jLIgxFnTp0mX3r0KSJEmSJKW0bekBAJnTCj7N2M9cBPIR4Dtp+7+PMY6p3QkhPAL8hZr1INoCE3ecc0oio5UkSZIkSQ3aG2Y4VDVcUiOEMJi6YQPA8+k7McYtwOKMmmtCCLm7NzxJkiRJktRYe0Pg0Bin1NNWnkXbgdSs+SBJkiRJkppBawscMh+RCfVfQ31tMeGxSJIkSZKknWhtgUNxPW1HZNG2GShJfjiSJEmSJKk+rS1weA54LaNtePpOCOFQ4GsZNffGGCv34LgkSZIkSVKaZgkcQgh5IYTRfP4Yy3QjQgiD02p6ZfR3DSGMDiEMjjFWASOApWn93wgh/EcI4bIQwjXUPBaz446+auBe4IZkr0iSJKnxjjjiCB555BFijMT4xbs9r7nmGlauXMmyZctYtWoVEyZM2K2aTCeffDIvvPACxcXFrF69mrlz59KtW7dG1UycOJGSkhLefvttHnzwQdq3b5/qGz16NAsXLmzMRyFJ2h/U/j+8PfkCelKzhsLOXg9kU5Nxzm8B/wa8AWwEPqPm8Zd/A5YAM4BjGzPOE088MUqSJGWrgX+71HkNHTo0rlixIs6bN6/e46+//voYY4wTJkyIQJw0aVKMMcYpU6Y0qibz1bdv31hZWRmLi4tjTk5OPPLII+O2bdviihUrYvv27bOqGTRoUIwxxsmTJ8chQ4bEGGP8yU9+EoGYl5cX161bF/v06bPL65ck7buAoljPd+xmmeEQYyyNMYZdvMZmU5NxzqdjjONijINijPkxxnYxxg4xxi/FGE+JMU6OMf65Oa5PkiSpIe+//z4nn3wyTz/99Bf6cnNzmTRpEgBLl9ZM5Fy0aBFQM7MgLy8vq5r6TJo0iby8PF599VWqq6spKytj/fr1DBw4kAsvvDCrmr59+wJQXl5OeXnNw8D69esHwJQpU5g3bx5r165t+ockSdqntLY1HCRJklqld955h8rK+peUKigo4OCDDwZg48aNAHz44YcA5OXlcdJJJ2VVU58zzjijzjHpx51++ulZ1RQXF1NVVUX37t3p0aMHAK+//jr9+/dn5MiRTJ8+PevPQZK0/2jb0gOQJEna3x155JGp7W3bttX5WdtfVVXVYM2uzp1eW7td29dQTUlJCWPHjuWyyy7jrLPOYvr06RQWFvLMM88wefJkNm/e3NhLliTtBwwcJEmS9kIxbVHJEMJu1+zquF0dk1kze/ZsZs+eneo/77zzyMnJ4bHHHmPixIkMHjyYnJwcCgsLmT9/ftZjkSTtu7ylQpIkqYWVlZWltmuf/tChQ4c6/dnU7Orc6U+VqD2uti+bmnS5ubnMmDGD8ePHM2bMGGbOnMndd9/N8uXLefTRR+ndu3eD1yxJ2vcZOEiSJLWwoqKi1PoO+fn5AHTq1AmATZs2sWzZsqxqoCY06Ny5c+rcL774Yp1j0o+r7cumJt0NN9zAE088wcqVKykoKABgw4YNlJWV0a5dO0444YTd+BQkSfsaAwdJkqQWtmXLFmbNmgXA0KFDARg2bBgAd955J5s2bcqqBmrCiw0bNqQWkZw1axabN29O3fLQrVs3evXqRUlJCQ899FDWNbX69OnDBRdcwE033QTAunXrAOjatStdu3at0yZJ2r+F9Hv/9ncFBQWxqKiopYchSZJaicasm9CzZ08KCws5/PDDGTBgAFAze2DFihVcccUVAEyYMIFLLrmEjz/+mI4dO1JYWMiMGTPqnKehmgULFlBQUMBpp51GSUkJAEOGDGHmzJnk5+eTm5vL8uXLufrqq+vcLpFNDcDChQuZM2cOc+bMAWpur7j//vs5/vjjad++PYWFhdx6661fuH7/zSlJ+64QwmsxxoIvtPvL/3MGDpIkqTEaEzjs7/w3pyTtu3YWOHhLhSRJkiRJSpyBgyRJkiRJSpyBgyRJkiRJSpyBgyRJkiRJSpyBgyRJkiRJSpyBgyRJkiRJSpyBgyRJkiRJSpyBgyRJkiRJSpyBgyRJkiRJSpyBgyRJkiRJSpyBgyRJkiRJSpyBgyRJkiRJSpyBgyRJkiRJSpyBgyRJkiRJSpyBgyRJkiRJSpyBgyRJkiRJSpyBgyRJkiRJSpyBgyRJkiRJSlzblh6AJElSa/XZZ5+19BBajXbt2rX0EFoV/9uStC9whoMkSZIkSUqcgYMkSZIkSUqcgYMkSZIkSUqcgYMkSZIkSUqcgYMkSZIkSUqcgYMkSZIkSUqcgYMkSZIkSUqcgYMkSZIkSUqcgYMkSZIkSUqcgYMkSZIkSUqcgYMkSZIkSUqcgYMkSZIkSUqcgYMkSZIkSUqcgYMkSZIkSUqcgYMkSZIkSUqcgYMkSZIkSUqcgYMkSZIkSUqcgYMkSZIkSUqcgYMkSZIkSUqcgYMkSZIkSUqcgYMkSZIkSUqcgYMkSdJe7LPPPmPGjBkcfPDBtGvXjptvvrmlh7RXuvHGG/nss8++8Fq5cmWq5qijjqKwsJA1a9awcuVK3nrrLSZNmkROjv8klqQ9wd+ukiRJe6m//vWvDBkyhKVLl7J169aWHs5e75NPPuGDDz6o89q4cSMABx54IP/5n//JRRddxO9+9zsGDhzIgw8+yC233MIvfvGLFh65JO2bDBwkSZL2Up988gl33nkn9957b0sPpVW48sorOeKII+q8hg4dCsC3vvUt+vTpA8Bzzz0HwB/+8AcAfvjDH6b6JEnJMXCQJEnaSw0cOJDTTz+9pYfRapxyyik8+eSTrFy5kmXLljF16lRyc3MB6NGjR6qusrISqAl0an3zm99s3sFK0n7AwEGSJEmt3tatW2nTpg3f+973GDJkCJ999hk33HADzz77LG3atOHdd99N1R5yyCEAdOzYMdXWvXv3Zh+zJO3rDBwkSZLU6t1+++2MGzeOTZs28b//+7/ccccdAPz93/89559/PgsWLKC0tBSAb3/72wB897vfTR3frl275h6yJO3zDBwkSZK0z1m9enVqe8iQIWzZsoWvf/3rzJ49mzPOOIPFixezbdu21G0VH374YUsNVZL2WW2b401CCAXAf+2ku1eMsbQ5xiFJkqR905FHHklZWVlqv7q6OrXdpk0bAN59911+8IMfpNpzcnK4/vrrAXj77bebaaSStP9orhkO64ALgGlJnTCEMCiE8IsQwhshhI9CCJ+FED4IIfxXCGFmCKFHw2eRJEnSvuDFF1+kU6dOqf2jjz46tf36668DcPnll9c55vjjj6dt27Zs3Lgx9cQKSVJymiVwiDFujDHOA/6YxPlCCLcDy4ErgOOBVcCVwC+BY4GJwOoQwpVJvJ8kSZL2fv/8z/8MQPv27fnpT38KwKpVq5g7dy4As2bNYuTIkQDk5uZy2223UV1dzdVXX83WrVtbZtCStA9rdWs4hBAmAdcCYUdTGfCNGON9McYbgdrouj1wdwjhxy0wTEmSpCbbtm0bgwYN4uyzz061/cu//AuDBg1i3rx5LTiyvc+vfvUrzjzzTF577TXeffddBgwYwP33388ZZ5zBli1bAHjqqaeYOXMmb7/9NmvXrqVt27Z85zvfYfbs2S08eknaN4UYY/O9WQinAy9kNGe9hkMI4QCgHDg4rbkwxnhxWs3BwMdp/VuBvjHGvzZ0/oKCglhUVJTNUCRJkti+fXtLD6HVyM3NbekhtCqfffZZSw9BkrIWQngtxliQ2d7aZjgMoW7YAPCX9J0Y4yfA/6Q1HQBcuofHJUmSJEmS0uwVgUMI4dQQwoIQQnkIYVsIoTSEcNeO2Qrpjqjn8M1ZtP2fZEYqSZIkSZKysTcEDhdQc5vFcKAL0A7oAVwFPBNCaJNWu6We49vV09Y+Y39QCGFvuFZJkiRJkvYLe8OX8GupCRsOAL4JVKX1DQXOTdt/o57j68x6CCG0BTpn1LQHDmnqQCVJkiRJUnb2hsBhRozx2Rjjthjj88DSjP6zajd2LC75fEb/KRn7fw+0red98up78xDCpSGEohBCUUVFReNGLkmSJEmS6rU3BA4vZeyXZewflbH/Q+C9tP0TQgh3hhD6hRBOBf5tJ+9TWV9jjPHXMcaCGGNBly5dsh60JEmSJEnaub0hcMicVvBpxv4B6TsxxvXAV4Hf8PmaDlcDJcAfgP/a0ZduO3UflSlJkiRJkvagvSFwqGq4pK4Y4/sxxrHAocAg4HTgRODQGONFwMaMQ/4cY4xNGqUkSZIkScpafWsdtBoxxm3Am/V0Zd6G8XIzDEeSJEmSJO2wN8xwaJQQQrsQwkENlJ2QsZ95i4UkSZIkSdqDWl3gAFwBfBJC+Fp9nSGErwJHpzX9Icb4SrOMTJIkSZIkAa0zcKg1I4TQIb0hhJAH3JfW9B5wcbOOSpIkSZIkNU/gEELICyGMBr5eT/eIEMLgtJpeGf1dQwijQwiDM9qHAsUhhJ+FEMaEEG4E3gKG7Oh/FRgSY/xrktciSZK0uzZs2MDo0aNp164d7dq1a7B+y5Yt3HjjjRx33HEMGzaME044gVNPPZU///nPAFx88cWpc2W+fv/73wNw++23c8wxx3D88cczZswYPv308weCzZs3j3/4h3/YMxfbBB07duTee+9l1apVLFmyhNdff51LL7001T9gwAAefvhh1qxZw5/+9CfWrl3LL3/5Sw477LCdnvPcc8/lxRdf5A9/+ANvvPEG7777Lr/73e8YOHBgo2quvfZa/vznP/PGG2/wwAMP0L59+1TfqFGjeOqppxL+NCSp9WquGQ5dgLnAjfX03QtcnlZzakb/wB3tl+/YfxqYCjwJVFPzSMx/2/FzG1AI/EOMcUiM8b8TvQpJkqTdtGTJEv7P//k/5ORk/8+v888/n7vuuovZs2ezePFiioqK6NSpE//zP/+TqjnqqKPo379/6tW7d28ADjjgAF5//XWuu+46xowZw7/8y7/w0EMP8atf/QqAyspKpkyZwt13353shSbggQce4PLLL+fJJ5/klFNO4T//8z+57777GD9+PAD/8R//wbnnnstvf/tbTjvtNBYvXsy4ceP47W9/u9NzDh48mFdeeYUzzzyTQYMG8cc//pHvfve7LFy4MOuaQYMGcdttt/Gb3/yGyy67jO9973v86Ec/AiAvL4+bb76Zq666ag9+MpLUujTLUypijKVAyKK0wZoYYwlwc1PHJEmS1JwOP/xwli5dyuOPP87vfve7BuufffZZnn32Wb71rW9x3HHHAdCmTRuefPLJOnWFhYWcdtppdfZvuukmzjjjjNQshy5dutC1a1cA1qxZA8Att9zCP/7jP9K3b98kLi8xX/rSl1KzLl55pWYZrpdfrnng2KRJk3j44Yfp3r07AH/9a81E1v/+75q/MZ1yyik7Pe9DDz3Ee++9l9p/+eWXufDCC/nyl79M165dKS8vb7CmT58+AFRUVFBeXg6Q+vxuuOEGHnnkEdauXdv0D0GS9hGt+rGYkiRJrUXtzINs1f5V/dNPP+Xiiy/m7bffpkuXLlxzzTV8/es1d6lOmTKFzp07p46JMXLXXXfx05/+lPbt2/N3f/d35OTk8O6776a+lA8aNIhVq1bxxBNPsHz58oSuLjm1YQLApk2b6vz80pe+xKGHHsqf/vQnTjvtNPr37w9Av379gM+Difq8+ebnT1LPzc3l29/+NgB/+tOfUuFBQzVvvfUWVVVVHHXUUalxvvHGG/Tv359zzjmHr371q027eEnaxxg4SJIk7YVKS0uBmi+7q1atAmrWLnjuuedYvHgxJ510Ej179qxzzO9//3v+9re/8cMf/jBVf//99/PrX/+aP/zhD0yePJmxY8dy9tlnM336dPLy8przkrLy7rvvprYPPvhgAA455JBU22GHHcbIkSOZO3cuV155JcOHD2fAgAE89thjqevelSuuuIKpU6eSn5/PokWLuPDCC7OuKSkp4ZJLLuHSSy/lzDPP5LbbbuOBBx7gP/7jP7j++uvZvHlzUy9fkvYprfkpFZIkSfus2sUd+/fvT8+ePenZsycDBw6kurqaf/3Xf633mNtvv53LL7+cgw46KNV20UUXsWjRIhYvXsy0adN44oknqK6u5txzz+X222/n/PPPZ+TIkcyfP79Zrqsh77//PgsWLADgzDPPrPMTYPv27Tz77LOceeaZXHnllfzd3/0dd9xxByNHjmT69OkNnv++++7jyCOP5De/+Q2nnnoqS5cu5dBDD826Zs6cOZx22ml87WtfY8qUKZxzzjnk5OTw+OOPc+211/LII4/w6KOPMmLEiGQ+EElqxQwcJEmS9kK1t0rU/pUfPv9Lf+3aBekWLVrEW2+9xY9//OOdnnPz5s1cf/313HPPPTz44INcd911/PSnP+WEE05g1KhRe836A9///ve55557KCgoYMGCBalbHqDmlosTTzwRqLlmqJkFAnD55Zdz9NFHN3j+zz77jKlTpwLQo0cPzjvvvN2qyc3NZfr06Vx55ZX80z/9E7fddhv/9//+X15//XUefvjhRt9GI0n7GgMHSZKkvcCnn37KBx98kNr/+7//e4A60/Rr1zI46qijvnD87bffzg9+8AO6dOmy0/e49dZb+c53vsMxxxzDa6+9BsARRxxBt27d2L59O2+88UYSl9JklZWVTJgwgZNOOol/+Id/SK1n8eqrr9b5PGKMAFRXV6faamcitG/fvs76FlOmTKkT3mzZsiW1XRvkZFOT7rrrruP3v/89K1euTIUg7733Hhs2bKBdu3YMGjSo0dcuSfsSAwdJkqS9wODBg+nevTvLli0Dav7K/+Uvf5nVq1ezceNGPvzwQ1atWkVOTg4XX3xxnWOLi4t5/vnnufrqq3d6/jVr1vDwww9z4401TymvnQlQXl5ORUVFnbaW9tRTT3HqqTVPSg8h8OMf/5ht27bxs5/9jJdffpn3338fgOOPPx4g9cX+nXfe4a233gJqwon//u//5qSTTgLg1FNP5Qc/+EHqPS655BIAtm7dmrqFI5uaWn369GHUqFFMmzYt9d4AXbt2TYU+tW2StL9y0UhJkqRmsH79esaNG8ff/va3VNs3vvENBg4cyC9+8Qu6d+9ORUVF6i/pHTt25Pnnn2fy5MmcccYZbN++neOPP54bb7yRwYMH1zn3HXfcwfnnn0+PHj12+v5XXXUVP//5z1N/wf/Rj37Ea6+9xo9+9CO2bdvGzTffvNc8ZeHNN9/kl7/8JeXl5XTu3JkNGzZw1llnsWTJEgDOOussbrzxRqZMmcJll13GEUccwZw5c7j55pv57LPPgJpHZXbp0oWPP/4YgCeffJJRo0bx7W9/m/z8fPLz83nssce4/fbbWb16ddY1te6++25+/vOfU1lZCcCvfvUrTjzxRH71q1/Rvn17brzxRl5//fXm+sgkaa8UaqeiCQoKCmJRUVFLD0OSJLUS27dvb+khtBq5ubktPYRWpTY4kaTWIITwWoyxILPdWyokSZIkSVLiDBwkSZIkSVLiDBwkSZIkSVLiDBwkSZIkSVLiDBwkSZIkSVLiDBwkSZIkSVLiDBwkSZIkSVLiDBwkSZIkSVLiDBwkSZIkSVLiDBwkSZIkSVLiDBwkSZIkSVLiDBwkSZIkSVLiDBwkSZIkSVLiDBwkSZIkSVLiDBwkSZIkSVLiDBwkSZIkSVLiDBwkSZIkSVLiDBwkSZIkSVLiDBwkSZIkSVLi2rb0ACRJklqrtm39p1S2Pvvss5YeQqsSQmjpIbQaMcaWHoKknXCGgyRJkiRJSpyBgyRJkiRJSpyBgyRJkiRJSpyBgyRJkiRJSpyBgyRJkiRJSpyBgyRJkiRJSpyBgyRJkiRJSpyBgyRJkiRJSpyBgyRJkiRJSpyBgyRJkiRJSpyBgyRJkiRJSpyBgyRJkiRJSpyBgyRJkiRJSpyBgyRJkiRJSpyBgyRJkiRJSpyBgyRJkiRJSpyBgyRJkiRJSpyBgyRJkiRJSpyBgyRJkiRJSpyBgyRJkiRJSpyBgyRJkrQf6tSpE/fccw/r1q2jpKSENWvWsGTJEoYPHw5ACIGJEyeyevVq1q9fT2lpKbfddhsdOnRo4ZFLai0MHCRJkqT9zEEHHcSSJUu46KKLGDFiBP3792fAgAGsXbuW/v37A3DXXXcxc+ZMFixYQK9evZg2bRqTJ09m7ty5LTx6Sa1F25YegCRJkqTmNWnSJAYMGMC9997LihUrAKiqqmLMmDEA9OjRg/HjxwPw1FNP1fl5zjnnMGzYMBYvXtwCI5fUmjjDQZIkSdrPjBo1CoDDDjuMJ598kjVr1vDKK68wevRoAM4++2zatGkDQHl5OQAVFRVUV1cDMGLEiBYYtaTWxhkOkiRJ0n4kNzeX3r17AzB8+HC+8pWvcMghh/Dmm28yd+5cPvroI/r165eq37JlCwAxRj799FNyc3Pr9EvSzjjDQZIkSdqP5Ofnk5NT8zXg5ZdfpqysjJUrV1JcXAzAddddx0EHHZSqr6qqSm3XznBI75eknTFwkCRJkvYj27dvT21/8MEHqe2KigoAjj32WCorK1PttbdWAKmgIr1fknbGwEGSJEnaj1RUVKQCgxhjqr12u0OHDqxevTrVnpubC9Q8JrP2kZjp/ZK0Mw0GDiGEghBC3MmrZzOMUZIkSVJCYow899xzAHTq1CnV3rlzZwCKi4tZuHBh6vaJrl27AjULTNbOcFiwYEFzDllSK5XNDId1wAXAtKTfPIRwagjhzYwQ44FGHN8jhDAzhLA8hPBhCOHTEEJZCOHpEMKlIYR2SY9ZkiRJau2mTp3K5s2bGTJkCPn5+Rx11FEcd9xxAMyYMYPS0lLuu+8+oOaJFek/58+fz0svvdQyA5fUqoT0aVS7LAzhdOCFjOZeMcbSRr9pCN2A24EL6+n+TYxxbBbnuBy4CzgA2LzjfKXAOcC3d5StBkbEGLOa81VQUBCLioqyKZUkSZL2mBDCHn+PgoICbrnlFo455hgOPPBASktLufXWW3n88ceBmvUaJk6cyLhx42jTpg0hBB5++GGmTp3K1q1b9/j4spXt9xlJe04I4bUYY8EX2ps7cAghXArcCeQCvwR+nFHSYOAQQrgE+Le0pnExxvvT+pcCf79jtxwYFGN8r6GxGThIkiRpb9AcgcO+wsBBank7CxxaYtHIC4Fl1IQA4xt78I7ZEXdnND+xi/2uwP9r7PtIkiRJkqTd17YF3vPKGOMbTTj+UuDgtP0PY4wfZtRk3kJxbgihV4xxfRPeV5IkSZIkZanJMxx2LPy4IIRQHkLYFkIoDSHcFUI4uL76JoYNAOdl7FfUU5PZFoBzm/i+kiRJkiQpS00NHC6gZl2H4UAXoB3QA7gKeCaE0KaJ568jhJAHDMxo/rie0vraTkpyLJIkSZIkaeeaGjhcS03YcADwTaAqrW8oyc8q6M4Xx7ytnrr62nrWd8Idj88sCiEUVVTUN1lCkiRJkiQ1VlMDhxkxxmdjjNtijM8DSzP6z2ri+TN1rKetqp627fW0HVrfCWOMv44xFsQYC7p06dKUsUmSJEmSpB2aGji8lLFflrF/VBPPn6kpzwfyeTmSJEmSJDWTpgYOmfcgfJqxf0ATz5/po3ra6lsnor6nb/xvskORJEmSJEk709TAob7bGfakd4HqjLb29dTV11aa+GgkSZIkSVK9mvxYzOYUY6wEVmU0H1JPaX1tRcmPSJIkSZIk1adVBQ47PJaxX99Kj4dl7Efg8T0zHEmSJEmSlKk1Bg6/Bjal7XcKIeRn1PTN2P99jPGdPTssSZIkSZJUq9UFDjHGvwLXZDSfk7H/nbTtD4Af79FBSZIkSZKkOhoMHEIIeSGE0cDX6+keEUIYnFbTK6O/awhhdAhhcNr5eu1oG73jmEx1+kMIeZkFMcZfAeP5/KkY94YQfh5CGBtCeBz42o72tcCpMcbMx3VKkiRJ+4QjjjiCRx55hBgjMX7xSfDXXHMNK1euZNmyZaxatYoJEybsVk2mk08+mRdeeIHi4mJWr17N3Llz6datW6NqJk6cSElJCW+//TYPPvgg7dt/vvb76NGjWbhwYWM+Ckl7m9pfTDt7AT2pWQNhZ68HsqlJO9/YBmozXz0bGNss4A1gI7ANeA94BrgMaN/Q9aW/TjzxxChJkiS1tGz/rTx06NC4YsWKOG/evHqPvf7662OMMU6YMCECcdKkSTHGGKdMmdKomsxX3759Y2VlZSwuLo45OTnxyCOPjNu2bYsrVqyI7du3z6pm0KBBMcYYJ0+eHIcMGRJjjPEnP/lJBGJeXl5ct25d7NOnT4OfgaSWBxTFer5jNzjDIcZYGmMMu3iNzaYm7XwPNFCb+SptYGwTY4yDYoz5Mcb2McYjYoz/X4zxX2KM2xq6PkmSJKm1ev/99zn55JN5+umnv9CXm5vLpEmTAFi6dCkAixYtAmpmFuTl5WVVU59JkyaRl5fHq6++SnV1NWVlZaxfv56BAwdy4YUXZlXTt2/Nsmvl5eWUl5cD0K9fPwCmTJnCvHnzWLt2bdM/JEktptWt4SBJkiSpxjvvvENlZWW9fQUFBRx88MEAbNy4EYAPP/wQgLy8PE466aSsaupzxhln1Dkm/bjTTz89q5ri4mKqqqro3r07PXr0AOD111+nf//+jBw5kunTp2f9OUjaO7Vt6QFIkiRJSt6RRx6Z2t62bVudn7X9VVVVDdbs6tzptbXbtX0N1ZSUlDB27Fguu+wyzjrrLKZPn05hYSHPPPMMkydPZvPmzY29ZEl7GQMHSZIkaT8R0xaVDCHsds2ujtvVMZk1s2fPZvbs2an+8847j5ycHB577DEmTpzI4MGDycnJobCwkPnz52c9Fkl7B2+pkCRJkvZBZWWfP6it9ukPHTp0qNOfTc2uzp3+VIna42r7sqlJl5uby4wZMxg/fjxjxoxh5syZ3H333SxfvpxHH32U3r17N3jNkvYuBg6SJEnSPqioqCi1vkN+fj4AnTp1AmDTpk0sW7YsqxqoCQ06d+6cOveLL75Y55j042r7sqlJd8MNN/DEE0+wcuVKCgoKANiwYQNlZWW0a9eOE044YTc+BUktycBBkiRJ2gdt2bKFWbNmATB06FAAhg0bBsCdd97Jpk2bsqqBmvBiw4YNqUUkZ82axebNm1O3PHTr1o1evXpRUlLCQw89lHVNrT59+nDBBRdw0003AbBu3ToAunbtSteuXeu0SWo9Qvo9Wvu7goKCWFRU1NLDkCRJ0n4u27UTevbsSWFhIYcffjgDBgwAamYPrFixgiuuuAKACRMmcMkll/Dxxx/TsWNHCgsLmTFjRp3zNFSzYMECCgoKOO200ygpKQFgyJAhzJw5k/z8fHJzc1m+fDlXX311ndslsqkBWLhwIXPmzGHOnDlAze0V999/P8cffzzt27ensLCQW2+9td7PwO8zUssLIbwWYyz4Qrv/A/2cgYMkSZL2Bo1ZrHF/5/cZqeXtLHDwlgpJkiRJkpQ4AwdJkiRJkpQ4AwdJkiRJkpQ4AwdJkiRJkpQ4AwdJkiRJkpQ4AwdJkiRJkpQ4AwdJkiRJkpQ4AwdJkiRJkpQ4AwdJkiRJkpQ4AwdJkiRJkpQ4AwdJkiRJkpQ4AwdJkiRJkpQ4AwdJkiRJkpQ4AwdJkiRJkpQ4AwdJkiRJkpQ4AwdJkiRJkpQ4AwdJkiRJkpQ4AwdJkiRJkpS4ti09AEmSJEl1xRhbegitRgihpYfQavjflZqbMxwkSZIkSVLiDBwkSZIkSVLiDBwkSZIkSVLiDBwkSZIkSVLiDBwkSZIkSVLiDBwkSZIkSVLiDBwkSZIkSVLiDBwkSZIkSVLiDBwkSZIkSVLiDBwkSZIkSVLiDBwkSZIkSVLiDBwkSZIkSVLiDBwkSZIkSVLiDBwkSZIkSVLiDBwkSZIkSVLiDBwkSZIkSVLiDBwkSZIkSVLiDBwkSZIkSVLiDBwkSZIkSVLiDBwkSZIkSVLiDBwkSZIkaRc6derEPffcw7p16ygpKWHNmjUsWbKE4cOHAxBCYOLEiaxevZr169dTWlrKbbfdRocOHVp45FLLMnCQJEmSpJ046KCDWLJkCRdddBEjRoygf//+DBgwgLVr19K/f38A7rrrLmbOnMmCBQvo1asX06ZNY/LkycydO7eFRy+1rLYtPQBJkiRJ2ltNmjSJAQMGcO+997JixQoAqqqqGDNmDAA9evRg/PjxADz11FN1fp5zzjkMGzaMxYsXt8DIpZbnDAdJkiRJ2olRo0YBcNhhh/Hkk0+yZs0aXnnlFUaPHg3A2WefTZs2bQAoLy8HoKKigurqagBGjBjRAqOW9g7OcJAkSZKkeuTm5tK7d28Ahg8fzle+8hUOOeQQ3nzzTebOnctHH31Ev379UvVbtmwBIMbIp59+Sm5ubp1+aX/jDAdJkiRJqkd+fj45OTVfmV5++WXKyspYuXIlxcXFAFx33XUcdNBBqfqqqqrUdu0Mh/R+aX9j4CBJkiRJ9di+fXtq+4MPPkhtV1RUAHDsscdSWVmZaq+9tQJIBRXp/dL+xsBBkiRJkupRUVGRCgxijKn22u0OHTqwevXqVHtubi5Q85jM2kdipvdL+5sGA4cQQkEIIe7k1bMZxihJkiRJzS7GyHPPPQdAp06dUu2dO3cGoLi4mIULF6Zun+jatStQs8Bk7QyHBQsWNOeQpb1KNjMc1gEXANOSfvMQwqkhhDczQowHGnmOLiGEfwshVKefJ+mxSpIkSdr/TJ06lc2bNzNkyBDy8/M56qijOO644wCYMWMGpaWl3HfffUDNEyvSf86fP5+XXnqpZQYu7QVC+tSgXRaGcDrwQkZzrxhjaaPfNIRuwO3AhfV0/ybGODaLc7QBLqcmCDk0sz/GGBo7roKCglhUVNTYwyRJkiS1kBAa/c/+RisoKOCWW27hmGOO4cADD6S0tJRbb72Vxx9/HKhZr2HixImMGzeONm3aEELg4YcfZurUqWzdunWPjy9b2X73kxorhPBajLHgC+3NHTiEEC4F7gRygV8CP84oaTBwCCEMAB4GjgOWAduBoek1Bg6SJEnSvq85Aod9hYGD9pSdBQ4tsWjkhdSEBINijON38xxDgK7AD3Zsr0lobJIkSZIkKQFtW+A9r4wxvtHEcywC+sUYPwFTTUmSJEmS9jZNnuGwY+HHBSGE8hDCthBCaQjhrhDCwfXVJxA2EGN8pzZskCRJkiRJe5+mBg4XULOuw3CgC9AO6AFcBTyzY2FHSZIkSZK0n2lq4HAtNWHDAcA3gaq0vqHAuU08vyRJkiRJaoWaGjjMiDE+G2PcFmN8Hlia0X9WE8+/x4UQLg0hFIUQiioqKlp6OJIkSZIk7ROaGji8lLFflrF/VBPPv8fFGH8dYyyIMRZ06dKlpYcjSZIkSdI+oamBQ+aUgE8z9g9o4vklSZIkSVIr1NTAoarhEkmSJEmStL9p8mMxJUmSJEmSMhk4SJIkSZKkxBk4SJIkSZKkxBk4SJIkSZKkxDUYOIQQ8kIIo4Gv19M9IoQwOK2mV0Z/1xDC6BDC4LTz9drRNnrHMZnq9IcQ8nYyrvRzZL4vGef4SkPXKUmSJGnfd8QRR/DII48QYyTG+IX+a665hpUrV7Js2TJWrVrFhAkTdqsm08knn8wLL7xAcXExq1evZu7cuXTr1q1RNRMnTqSkpIS3336bBx98kPbt26f6Ro8ezcKFCxvzUUh7Xu3/0Hb2AnoCcRevB7KpSTvf2AZqM189dzKuxpzj5w1dZ4yRE088MUqSJElqPRrzvWDo0KFxxYoVcd68efUef/3118cYY5wwYUIE4qRJk2KMMU6ZMqVRNZmvvn37xsrKylhcXBxzcnLikUceGbdt2xZXrFgR27dvn1XNoEGDYowxTp48OQ4ZMiTGGONPfvKTCMS8vLy4bt262KdPn11ev7SnAEWxnu/YDc5wiDGWxhjDLl5js6lJO98DDdRmvkp3Mq7GnOPnDV2nJEmSpH3b+++/z8knn8zTTz/9hb7c3FwmTZoEwNKlSwFYtGgRUDOzIC8vL6ua+kyaNIm8vDxeffVVqqurKSsrY/369QwcOJALL7wwq5q+ffsCUF5eTnl5OQD9+vUDYMqUKcybN4+1a9c2/UOSEuQaDpIkSZL2C++88w6VlZX19hUUFHDwwQcDsHHjRgA+/PBDAPLy8jjppJOyqqnPGWecUeeY9ONOP/30rGqKi4upqqqie/fu9OjRA4DXX3+d/v37M3LkSKZPn5715yA1l7YtPQBJkiRJamlHHnlkanvbtm11ftb2V1VVNVizq3On19Zu1/Y1VFNSUsLYsWO57LLLOOuss5g+fTqFhYU888wzTJ48mc2bNzf2kqU9zsBBkiRJkuoR0xaVDCHsds2ujtvVMZk1s2fPZvbs2an+8847j5ycHB577DEmTpzI4MGDycnJobCwkPnz52c9FmlP8ZYKSZIkSfu9srKy1Hbt0x86dOhQpz+bml2dO/2pErXH1fZlU5MuNzeXGTNmMH78eMaMGcPMmTO5++67Wb58OY8++ii9e/du8JqlPc3AQZIkSdJ+r6ioKLW+Q35+PgCdOnUCYNOmTSxbtiyrGqgJDTp37pw694svvljnmPTjavuyqUl3ww038MQTT7By5UoKCgoA2LBhA2VlZbRr144TTjhhNz4FKVkGDpIkSZL2e1u2bGHWrFkADB06FIBhw4YBcOedd7Jp06asaqAmvNiwYUNqEclZs2axefPm1C0P3bp1o1evXpSUlPDQQw9lXVOrT58+XHDBBdx0000ArFu3DoCuXbvStWvXOm1SSwrp9xzt7woKCmJRUVFLD0OSJElSlhqzbkLPnj0pLCzk8MMPZ8CAAUDN7IEVK1ZwxRVXADBhwgQuueQSPv74Yzp27EhhYSEzZsyoc56GahYsWEBBQQGnnXYaJSUlAAwZMoSZM2eSn59Pbm4uy5cv5+qrr65zu0Q2NQALFy5kzpw5zJkzB6i5veL+++/n+OOPp3379hQWFnLrrbd+4fr97qc9JYTwWoyx4Avt/kf3OQMHSZIkqXVpTOCwv/O7n/aUnQUO3lIhSZIkSZISZ+AgSZIkSZISZ+AgSZIkSZISZ+AgSZIkSZISZ+AgSZIkSZISZ+AgSZIkSZISZ+AgSZIkSZISZ+AgSZIkSZISZ+AgSZIkSZISZ+AgSZIkSZISZ+AgSZIkSZISZ+AgSZIkSZISZ+AgSZIkSZISZ+AgSZIkSZISZ+AgSZIkSZISZ+AgSZIkSZISZ+AgSZIkSZISZ+AgSZIkSZIS17alByBJkiRJuyvG2NJDaDVCCC09hFbD/66S4QwHSZIkSZKUOAMHSZIkSZKUOAMHSZIkSZKUOAMHSZIkSZKUOAMHSZIkSZKUOAMHSZIkSZKUOAMHSZIkSZKUOAMHSZIkSZKUOAMHSZIkSZKUOAMHSZIkSZKUOAMHSZIkSZKUOAMHSZIkSZKUOAMHSZIkSZKUOAMHSZIkSZKUOAMHSZIkSZKUOAMHSZIkSZKUOAMHSZIkSZKUOAMHSZIkSZKUOAMHSZIkSZKUOAMHSZIkSZKUOAMHSZIkSZKUOAMHSZIkSVJiOnXqxD333MO6desoKSlhzZo1LFmyhOHDhwMQQmDixImsXr2a9evXU1paym233UaHDh1aeORKmoGDJEmSJCkRBx10EEuWLOGiiy5ixIgR9O/fnwEDBrB27Vr69+8PwF133cXMmTNZsGABvXr1Ytq0aUyePJm5c+e28OiVtLYtPQBJkiRJ0r5h0qRJDBgwgHvvvZcVK1YAUFVVxZgxYwDo0aMH48ePB+Cpp56q8/Occ85h2LBhLF68uAVGrj3BGQ6SJEmSpESMGjUKgMMOO4wnn3ySNWvW8MorrzB69GgAzj77bNq0aQNAeXk5ABUVFVRXVwMwYsSIFhi19hRnOEiSJEmSmiw3N5fevXsDMHz4cL7yla9wyCGH8OabbzJ37lw++ugj+vXrl6rfsmULADFGPv30U3Jzc+v0q/VzhoMkSZIkqcny8/PJyan5ivnyyy9TVlbGypUrKS4uBuC6667joIMOStVXVVWltmtnOKT3q/UzcJAkSZIkNdn27dtT2x988EFqu6KiAoBjjz2WysrKVHvtrRVAKqhI71fr12DgEEIoCCHEnbx6NsMYJUmSJEl7uYqKilRgEGNMtddud+jQgdWrV6fac3NzgZrHZNY+EjO9X61fNjMc1gEXANOSfvMQwqkhhDczQowHGjjmyyGEMSGE+0IIS0MIa0IIH4YQPgshfBRCKA4hFIYQvpX0eCVJkiRJ9Ysx8txzzwHQqVOnVHvnzp0BKC4uZuHChanbJ7p27QrULDBZO8NhwYIFzTlk7WENBg4xxo0xxnnAH5N60xBCtxDCHOBPwHGNPPzHwAPAPwNfAn4DXAXcuaP/74CxwMIQwuIQQrckxixJkiRJ2rWpU6eyefNmhgwZQn5+PkcddRTHHVfzlW/GjBmUlpZy3333ATVPrEj/OX/+fF566aWWGbj2iGZ/SkUI4VJqwoFc4BfUBAi74w3glBjj5rRzPwi8DrTf0XQK8McQwgkxxi27PWhJkiRJUoOKi4s57bTTuOWWW3jzzTc58MAD+fOf/8ytt97K/PnzAbjyyivZsGED48aNY+TIkYQQmDVrFlOnTm3h0StpIf3eml0WhnA68EJGc68YY2mj3jCEF4Eq4KcxxrdDCJkD+E2Mcewujp8BTALOijH+oZ7+fwMuyWj+cYzxvobGVlBQEIuKihoqkyRJkqRWJ4TQ0kNoNbL9nqwaIYTXYowFme0t8ZSKK2OM34gxvr2bx78F/I6a2zHqs6SettN2870kSZIkSdJuaHLgsGPhxwUhhPIQwrYQQmkI4a4QwsH11ccY32jK+8UY58QY/zHGuG0nJWX1tHVsyntKkiRJkqTGaWrgcAE1t1kMB7oA7YAe1Czi+EwIoc0ujt1T6gs61jX7KCRJkiRJ2o81NXC4lpqw4QDgm9SszVBrKHBuE8+/O06sp212s49CkiRJkqT9WFMDhxkxxmdjjNtijM8DSzP6z2ri+RslhNAOuDCj+ZcxxsxxpR9zaQihKIRQVFFRsWcHKEmSJEnSfqKpgUPmQ1Iz1084qonnb6wbqLmlo9b9wPhdHRBj/HWMsSDGWNClS5c9OjhJkiRJkvYXTQ0cMqcEfJqxf0ATz5+1EMIPgBt37G6l5lGY42KMVbs4TJIkSZIk7QFNDRxa/Mt8qHE9NbMZAvAKcEKM8b6WHZkkSZIkSfuvJj8WsyWFEA4DngRuATYBVwKnxBhXpdUcHkLwXglJkiRJkppR25YewO4KIQynZlbD4cBC4PIY43/XU/oKUAqc3myDkyRJkiRpP9fqZjiEEA4OIfwr8B9AG+B7McazdxI2SJIkSZKkFtDqAgfgX4FxO7a7AHNCCHFnL+o+tUKSJEmSJDWDBgOHEEJeCGE08PV6ukeEEAan1fTK6O8aQhgdQhicdr5eO9pG7zgmU53+EEJeRn+zPflCkiRJkvZXRxxxBI888ggxRmKMX+i/5pprWLlyJcuWLWPVqlVMmDBht2oynXzyybzwwgsUFxezevVq5s6dS7du3RpVM3HiREpKSnj77bd58MEHad++fapv9OjRLFy4sDEfhXZX7X88O3sBPYG4i9cD2dSknW9sA7WZr54Z43mykcdH4MWGrjPGyIknnhglSZIkaV/UmO9QQ4cOjStWrIjz5s2r9/jrr78+xhjjhAkTIhAnTZoUY4xxypQpjarJfPXt2zdWVlbG4uLimJOTE4888si4bdu2uGLFiti+ffusagYNGhRjjHHy5MlxyJAhMcYYf/KTn0Qg5uXlxXXr1sU+ffrs8vrVOEBRrOc7doMzHGKMpTHGsIvX2Gxq0s73QAO1ma/SjPF8t5HHhxjj6Q1dpyRJkiSpxvvvv8/JJ5/M008//YW+3NxcJk2aBMDSpUsBWLRoEVAzsyAvLy+rmvpMmjSJvLw8Xn31VaqrqykrK2P9+vUMHDiQCy+8MKuavn37AlBeXk55eTkA/fr1A2DKlCnMmzePtWvXNv1DUoNa4xoOkiRJkqQ96J133qGysrLevoKCAg4++GAANm7cCMCHH34IQF5eHieddFJWNfU544wz6hyTftzpp5+eVU1xcTFVVVV0796dHj1qlvR7/fXX6d+/PyNHjmT69OlZfw5qmlb7WExJkiRJUvM78sgjU9vbtm2r87O2v6qqqsGaXZ07vbZ2u7avoZqSkhLGjh3LZZddxllnncX06dMpLCzkmWeeYfLkyWzevLmxl6zdZOAgSZIkSWqSmLaoZAhht2t2ddyujsmsmT17NrNnz071n3feeeTk5PDYY48xceJEBg8eTE5ODoWFhcyfPz/rsahxvKVCkiRJkpS1srKy1Hbt0x86dOhQpz+bml2dO/2pErXH1fZlU5MuNzeXGTNmMH78eMaMGcPMmTO5++67Wb58OY8++ii9e/du8Jq1ewwcJEmSJElZKyoqSq3vkJ+fD0CnTp0A2LRpE8uWLcuqBmpCg86dO6fO/eKLL9Y5Jv242r5satLdcMMNPPHEE6xcuZKCggIANmzYQFlZGe3ateOEE07YjU9B2TBwkCRJkiRlbcuWLcyaNQuAoUOHAjBs2DAA7rzzTjZt2pRVDdSEFxs2bEgtIjlr1iw2b96cuuWhW7du9OrVi5KSEh566KGsa2r16dOHCy64gJtuugmAdevWAdC1a1e6du1ap03JC+n30ezvCgoKYlFRUUsPQ5IkSZIS15h1E3r27ElhYSGHH344AwYMAGpmD6xYsYIrrrgCgAkTJnDJJZfw8ccf07FjRwoLC5kxY0ad8zRUs2DBAgoKCjjttNMoKSkBYMiQIcycOZP8/Hxyc3NZvnw5V199dZ3bJbKpAVi4cCFz5sxhzpw5QM3tFffffz/HH3887du3p7CwkFtvvfUL1+/35MYJIbwWYyz4Qrsf5OcMHCRJkiTtqxoTOOzv/J7cODsLHLylQpIkSZIkJc7AQZIkSZIkJc7AQZIkSZIkJc7AQZIkSZIkJc7AQZIkSZIkJc7AQZIkSZIkJc7AQZIkSZIkJc7AQZIkSZIkJc7AQZIkSZIkJc7AQZIkSZIkJc7AQZIkSZIkJc7AQZIkSZIkJc7AQZIkSZIkJc7AQZIkSZIkJc7AQZIkSZIkJc7AQZIkSZIkJc7AQZIkSZIkJc7AQZIkSZIkJc7AQZIkSZIkJa5tSw9AkiRJkrTnxRhbegitRgihpYewT3CGgyRJkiRJSpyBgyRJkiRJSpyBgyRJkiRJSpyBgyRJkiRJSpyBgyRJkiRJSpyBgyRJkiRJSpyBgyRJkiRJSpyBgyRJkiRJSpyBgyRJkiRJSpyBgyRJkiRJSpyBgyRJkiRJSpyBgyRJkiRJSpyBgyRJkiRJSpyBgyRJkiRJSpyBgyRJkiRJSpyBgyRJkiRJSpyBgyRJkiRJSpyBgyRJkiRJSpyBgyRJkiRJSpyBgyRJkiRJSpyBgyRJkiRJLaBTp07cc889rFu3jpKSEtasWcOSJUsYPnw4ACEEJk6cyOrVq1m/fj2lpaXcdtttdOjQoYVHnh0DB0mSJEmSmtlBBx3EkiVLuOiiixgxYgT9+/dnwIABrF27lv79+wNw1113MXPmTBYsWECvXr2YNm0akydPZu7cuS08+uy0bekBSJIkSZK0v5k0aRIDBgzg3nvvZcWKFQBUVVUxZswYAHr06MH48eMBeOqpp+r8POeccxg2bBiLFy9ugZFnzxkOkiRJkiQ1s1GjRgFw2GGH8eSTT7JmzRpeeeUVRo8eDcDZZ59NmzZtACgvLwegoqKC6upqAEaMGNECo24cZzhIkiRJktSMcnNz6d27NwDDhw/nK1/5Cocccghvvvkmc+fO5aOPPqJfv36p+i1btgAQY+TTTz8lNze3Tv/eyhkOkiRJkiQ1o/z8fHJyar6Ov/zyy5SVlbFy5UqKi4sBuO666zjooINS9VVVVant2hkO6f17KwMHSZIkSZKa0fbt21PbH3zwQWq7oqICgGOPPZbKyspUe+2tFUAqqEjv31sZOEiSJEmS1IwqKipSgUGMMdVeu92hQwdWr16das/NzQVqHpNZ+0jM9P69VYOBQwihIIQQd/Lq2QxjlCRJkiRpnxFj5LnnngOgU6dOqfbOnTsDUFxczMKFC1O3T3Tt2hWoWWCydobDggULmnPIuyWbGQ7rgAuAaUm/eQjh1BDCmxkhxgMNHHNoCOGCEMKsEMIfQggrQwh/CyFsCyFsCSG8F0JYFEK42UBEkiRJkrQ3mjp1Kps3b2bIkCHk5+dz1FFHcdxxxwEwY8YMSktLue+++4CaJ1ak/5w/fz4vvfRSywy8EUL69I1dFoZwOvBCRnOvGGNpo980hG7A7cCF9XT/JsY4dhfH/n/A0zt2S4AHgQ1AN+AiYGBa+WfAVTHG+7IZV0FBQSwqKsqmVJIkSZK0jwohNMv7FBQUcMstt3DMMcdw4IEHUlpayq233srjjz8O1KzXMHHiRMaNG0ebNm0IIfDwww8zdepUtm7d2ixjzNJrMcaCzMZmDxxCCJcCdwK5wC+BH2eUZBs4vAKcFmPcltbXFvgj8LW0QyIwJMa4rKGxGThIkiRJkporcNiH1Bs4tMSikRcCy4BBMcbxu3F8NVAF3JEeNgDEGLcDv86oD8C3d2egkiRJkiRp97Rtgfe8Msb4xu4eHGP8T3Y97i27e25JkiRJkpSMJs9w2LHw44IQQvmOhRtLQwh3hRAOrq++KWFDlr6bsV8NPL6H31OSJEmSJKVp6gyHC4BbqLltofYmlx7AVcDgEMKpMcaqJr7HLoUQcoEuO953HDULR9b6G/DjGOPyPTkGSZIkSZJUV1NnOFwLDAcOAL5JzdoKtYYC5zbx/Nn4KfAXYBHwTzvatgL/FxgQY3x0VweHEC4NIRSFEIoqKir27EglSZIkSdpPNDVwmBFjfDbGuC3G+DywNKP/rCaePxtzgW8B/wy8uqPtAGqCiFUhhH/a2YEAMcZfxxgLYowFXbp02bMjlSRJkiRpP9HUwOGljP2yjP2jmnj+BsUY/xJjfCbG+EtqZlU8mNb9JeA3IYTL9/Q4JEmSJEnS55oaOGTeg/Bpxv4BTTx/o8QYq4HxQGVG120hhLzmHIskSZIkSfuzpgYOe3RByN0RY/wYeDmjuSMwuAWGI0mSJEnSfqnJj8VsbiGEdiGE9g2UldfTdvieGI8kSZIkSfqiVhc4AL8D1jdQ07metg/3wFgkSZIkSVI9WmPgANAthNC/vo4dazX8fUbzFmDJHh+VJEmSJEkCWm/gAHBfCKHOopQhhADcTc2aDelujjF+0mwjkyRJkiTtN4444ggeeeQRYozEGL/Qf80117By5UqWLVvGqlWrmDBhwm7VZDr55JN54YUXKC4uZvXq1cydO5du3bo1qmbixImUlJTw9ttv8+CDD9K+/ecrGIwePZqFCxc25qOoo8HAIYSQF0IYDXy9nu4RIYTBaTW9Mvq7hhBGhxBSCzaGEHrtaBu945hMdfp38XSJbwBvhRB+HkIYE0K4FngV+GFazVbgZzHGGQ1dpyRJkiRJjTV06FCef/55qqur6+2//vrrueOOO/j3f/93Tj75ZAoLC5k1axZTpkxpVE2mvn378sc//pHOnTszaNAgzjjjDEaOHMlzzz2XCg0aqhk0aBAzZ86ksLCQcePG8f3vf5/LLrsMgLy8PKZPn85PfvKT3f5sspnh0AWYC9xYT9+9wOVpNadm9A/c0X55WttpO9pqX5lOzejvktF/BTAauAd4H/gecBcwAzgG+AvwDDAR6GPYIEmSJEnaU95//31OPvlknn766S/05ebmMmnSJACWLl0KwKJFi4CamQV5eXlZ1dRn0qRJ5OXl8eqrr1JdXU1ZWRnr169n4MCBXHjhhVnV9O3bF4Dy8nLKy2uevdCvXz8ApkyZwrx581i7du1ufzZtGyqIMZYCIYtzZVNDjPEB4IFsandyfBnw8I6XJEmSJEkt5p133tlpX0FBAQcffDAAGzduBODDD2ueZ5CXl8dJJ51EVVVVgzUvvvjiF859xhln1Dkm/bjTTz+dBx54oMGa2267jaqqKrp3706PHj0AeP311+nfvz8jR47kuOOOa8xH8QUNBg6SJEmSJKnxjjzyyNT2tm3b6vys7a+qqmqwZlfnTq+t3a7ta6impKSEsWPHctlll3HWWWcxffp0CgsLeeaZZ5g8eTKbN29u7CXXYeAgSZIkSVIzSV9Usua5B7tXs6vjdnVMZs3s2bOZPXt2qv+8884jJyeHxx57jIkTJzJ48GBycnIoLCxk/vz5WY8FWvdTKiRJkiRJ2muVlZWltmsXcuzQoUOd/mxqdnXu9KdK1B5X25dNTbrc3FxmzJjB+PHjGTNmDDNnzuTuu+9m+fLlPProo/Tu3bvBa05n4CBJkiRJ0h5QVFREZWUlAPn5+QB06tQJgE2bNrFs2bKsaqAmNOjcuXPq3LXrOtQek35cbV82NeluuOEGnnjiCVauXElBQQEAGzZsoKysjHbt2nHCCSc06voNHCRJkiRJ2gO2bNnCrFmzgJrHZwIMGzYMgDvvvJNNmzZlVQM14cWGDRs46aSTAJg1axabN29O3fLQrVs3evXqRUlJCQ899FDWNbX69OnDBRdcwE033QTAunXrAOjatStdu3at05atkH5vyP6uoKAgFhUVtfQwJEmSJEktqDHrJvTs2ZPCwkIOP/xwBgwYANTMHlixYgVXXHEFABMmTOCSSy7h448/pmPHjhQWFjJjxow652moZsGCBRQUFHDaaadRUlICwJAhQ5g5cyb5+fnk5uayfPlyrr766jq3S2RTA7Bw4ULmzJnDnDlzgJrbK+6//36OP/542rdvT2FhIbfeeuvOPobXYowFX/gcDRw+Z+AgSZIkSWpM4CBgJ4GDt1RIkiRJkqTEGThIkiRJkqTEGThIkiRJkqTEGThIkiRJkqTEGThIkiRJkqTEGThIkiRJkqTEGThIkiRJkqTEGThIkiRJkqTEGThIkiRJkqTEGThIkiRJkqTEGThIkiRJkqTEGThIkiRJkqTEGThIkiRJkqTEGThIkiRJkqTEGThIkiRJkqTEGThIkiRJkqTEGThIkiRJkqTEGThIkiRJkqTEGThIkiRJkqTEhRhjS49hrxFCqAD+0tLjkCRJkiSpFekRY+yS2WjgIEmSJEmSEuctFZIkSZIkKXEGDpIkSZIkKXEGDpIkSZIkKXEGDpIkSZIkKXEGDpIkSZIkKXH/PyEjVi7YbA2nAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 1296x1296 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m 3       \u001b[0m | \u001b[0m 97.69   \u001b[0m | \u001b[0m 0.9145  \u001b[0m | \u001b[0m 5.646   \u001b[0m |\n",
      "Train on 702 samples, validate on 78 samples\n",
      "Epoch 1/100\n",
      "702/702 [==============================] - 8s 11ms/step - loss: 2.4854 - acc: 0.2892 - val_loss: 2.3731 - val_acc: 0.5256\n",
      "Epoch 2/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2979 - acc: 0.5840 - val_loss: 2.2600 - val_acc: 0.5897\n",
      "Epoch 3/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2461 - acc: 0.7194 - val_loss: 2.2558 - val_acc: 0.7564\n",
      "Epoch 4/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2199 - acc: 0.8305 - val_loss: 2.2069 - val_acc: 0.8333\n",
      "Epoch 5/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2053 - acc: 0.8789 - val_loss: 2.2667 - val_acc: 0.5897\n",
      "Epoch 6/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1976 - acc: 0.8946 - val_loss: 2.2358 - val_acc: 0.7692\n",
      "Epoch 7/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1896 - acc: 0.9274 - val_loss: 2.2229 - val_acc: 0.8462\n",
      "Epoch 8/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1904 - acc: 0.9274 - val_loss: 2.1881 - val_acc: 0.9103\n",
      "Epoch 9/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1802 - acc: 0.9801 - val_loss: 2.2086 - val_acc: 0.8718\n",
      "Epoch 10/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1851 - acc: 0.9430 - val_loss: 2.1862 - val_acc: 0.9359\n",
      "Epoch 11/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1754 - acc: 0.9758 - val_loss: 2.1924 - val_acc: 0.9103\n",
      "Epoch 12/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1763 - acc: 0.9758 - val_loss: 2.1820 - val_acc: 0.9231\n",
      "Epoch 13/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1726 - acc: 0.9815 - val_loss: 2.1959 - val_acc: 0.9103\n",
      "Epoch 14/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1748 - acc: 0.9858 - val_loss: 2.2005 - val_acc: 0.9103\n",
      "Epoch 15/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1783 - acc: 0.9843 - val_loss: 2.1821 - val_acc: 0.9487\n",
      "Epoch 16/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1727 - acc: 0.9801 - val_loss: 2.1794 - val_acc: 0.9359\n",
      "Epoch 17/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1729 - acc: 0.9858 - val_loss: 2.1817 - val_acc: 0.9487\n",
      "Epoch 18/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1736 - acc: 0.9815 - val_loss: 2.1833 - val_acc: 0.9487\n",
      "Epoch 19/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1710 - acc: 0.9929 - val_loss: 2.1808 - val_acc: 0.9487\n",
      "Epoch 20/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1696 - acc: 0.9915 - val_loss: 2.1874 - val_acc: 0.8846\n",
      "Epoch 21/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1699 - acc: 0.9915 - val_loss: 2.1801 - val_acc: 0.9615\n",
      "Epoch 22/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1716 - acc: 0.9886 - val_loss: 2.1812 - val_acc: 0.9487\n",
      "Epoch 23/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1686 - acc: 0.9972 - val_loss: 2.1828 - val_acc: 0.9103\n",
      "Epoch 24/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1727 - acc: 0.9858 - val_loss: 2.1818 - val_acc: 0.9487\n",
      "Epoch 25/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1681 - acc: 0.9943 - val_loss: 2.1980 - val_acc: 0.9103\n",
      "Epoch 26/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1713 - acc: 0.9915 - val_loss: 2.1784 - val_acc: 0.9487\n",
      "Epoch 27/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1677 - acc: 0.9972 - val_loss: 2.1862 - val_acc: 0.9231\n",
      "Epoch 28/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1697 - acc: 0.9957 - val_loss: 2.1835 - val_acc: 0.9487\n",
      "Epoch 29/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1678 - acc: 0.9986 - val_loss: 2.1792 - val_acc: 0.9487\n",
      "Epoch 30/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1694 - acc: 0.9943 - val_loss: 2.1774 - val_acc: 0.9487\n",
      "Epoch 31/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1673 - acc: 0.9972 - val_loss: 2.1800 - val_acc: 0.9359\n",
      "Epoch 32/100\n",
      "702/702 [==============================] - 3s 4ms/step - loss: 2.1701 - acc: 0.9929 - val_loss: 2.1782 - val_acc: 0.9615\n",
      "Epoch 33/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1668 - acc: 0.9986 - val_loss: 2.1762 - val_acc: 0.9615\n",
      "Epoch 34/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1672 - acc: 0.9972 - val_loss: 2.1740 - val_acc: 0.9487\n",
      "Epoch 35/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1672 - acc: 0.9986 - val_loss: 2.1759 - val_acc: 0.9487\n",
      "Epoch 36/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1669 - acc: 1.0000 - val_loss: 2.1778 - val_acc: 0.9615\n",
      "Epoch 37/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1668 - acc: 1.0000 - val_loss: 2.1769 - val_acc: 0.9615\n",
      "Epoch 38/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1671 - acc: 1.0000 - val_loss: 2.1755 - val_acc: 0.9487\n",
      "Epoch 39/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1666 - acc: 1.0000 - val_loss: 2.1799 - val_acc: 0.9615\n",
      "Epoch 40/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1669 - acc: 0.9986 - val_loss: 2.1799 - val_acc: 0.9487\n",
      "Epoch 41/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1666 - acc: 1.0000 - val_loss: 2.1782 - val_acc: 0.9487\n",
      "Epoch 42/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1666 - acc: 1.0000 - val_loss: 2.1781 - val_acc: 0.9487\n",
      "Epoch 43/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1782 - val_acc: 0.9615\n",
      "Epoch 44/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1793 - val_acc: 0.9487\n",
      "Epoch 45/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1815 - val_acc: 0.9487\n",
      "Epoch 46/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1801 - val_acc: 0.9487\n",
      "Epoch 47/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1795 - val_acc: 0.9487\n",
      "Epoch 48/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1805 - val_acc: 0.9359\n",
      "Epoch 49/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 0.9986 - val_loss: 2.1804 - val_acc: 0.9615\n",
      "Epoch 50/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1799 - val_acc: 0.9487\n",
      "Epoch 51/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1666 - acc: 0.9986 - val_loss: 2.1787 - val_acc: 0.9487\n",
      "Epoch 52/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1801 - val_acc: 0.9487\n",
      "Epoch 53/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1799 - val_acc: 0.9487\n",
      "Epoch 54/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1753 - val_acc: 0.9487\n",
      "Epoch 55/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1777 - val_acc: 0.9487\n",
      "Epoch 56/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1781 - val_acc: 0.9487\n",
      "Epoch 57/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1774 - val_acc: 0.9487\n",
      "Epoch 58/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1799 - val_acc: 0.9487\n",
      "Epoch 59/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1787 - val_acc: 0.9615\n",
      "Epoch 60/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1789 - val_acc: 0.9487\n",
      "Epoch 61/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1773 - val_acc: 0.9615\n",
      "Epoch 62/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1789 - val_acc: 0.9615\n",
      "Epoch 63/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1783 - val_acc: 0.9615\n",
      "Epoch 64/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1772 - val_acc: 0.9615\n",
      "Epoch 65/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1791 - val_acc: 0.9487\n",
      "Epoch 66/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1792 - val_acc: 0.9487\n",
      "Epoch 67/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1794 - val_acc: 0.9615\n",
      "Epoch 68/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1786 - val_acc: 0.9487\n",
      "Epoch 69/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1784 - val_acc: 0.9487\n",
      "Epoch 70/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1781 - val_acc: 0.9487\n",
      "Epoch 71/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1793 - val_acc: 0.9615\n",
      "Epoch 72/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1781 - val_acc: 0.9487\n",
      "Epoch 73/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1778 - val_acc: 0.9487\n",
      "Epoch 74/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1779 - val_acc: 0.9487\n",
      "Epoch 75/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1781 - val_acc: 0.9487\n",
      "Epoch 76/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1772 - val_acc: 0.9487\n",
      "Epoch 77/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1781 - val_acc: 0.9487\n",
      "Epoch 78/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1773 - val_acc: 0.9487\n",
      "Epoch 79/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1781 - val_acc: 0.9615\n",
      "Epoch 80/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1776 - val_acc: 0.9487\n",
      "Epoch 81/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1783 - val_acc: 0.9615\n",
      "Epoch 82/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1783 - val_acc: 0.9487\n",
      "Epoch 83/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1794 - val_acc: 0.9487\n",
      "Epoch 84/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1789 - val_acc: 0.9487\n",
      "Epoch 85/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1787 - val_acc: 0.9487\n",
      "Epoch 86/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1788 - val_acc: 0.9487\n",
      "Epoch 87/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1785 - val_acc: 0.9487\n",
      "Epoch 88/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1782 - val_acc: 0.9487\n",
      "Epoch 89/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1789 - val_acc: 0.9487\n",
      "Epoch 90/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1785 - val_acc: 0.9487\n",
      "Epoch 91/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1787 - val_acc: 0.9487\n",
      "Epoch 92/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1784 - val_acc: 0.9487\n",
      "Epoch 93/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1784 - val_acc: 0.9487\n",
      "Epoch 94/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1784 - val_acc: 0.9487\n",
      "Epoch 95/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1784 - val_acc: 0.9487\n",
      "Epoch 96/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1773 - val_acc: 0.9487\n",
      "Epoch 97/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1778 - val_acc: 0.9487\n",
      "Epoch 98/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1778 - val_acc: 0.9615\n",
      "Epoch 99/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1786 - val_acc: 0.9615\n",
      "Epoch 100/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1786 - val_acc: 0.9487\n",
      "78/78 [==============================] - 0s 321us/step\n",
      "Score for fold 1: loss of 2.17880409803146; acc of 94.87179487179486%\n",
      "Train on 702 samples, validate on 78 samples\n",
      "Epoch 1/100\n",
      "702/702 [==============================] - 8s 11ms/step - loss: 2.5082 - acc: 0.2650 - val_loss: 2.3735 - val_acc: 0.4231\n",
      "Epoch 2/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.3169 - acc: 0.5598 - val_loss: 2.3267 - val_acc: 0.5513\n",
      "Epoch 3/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2608 - acc: 0.7308 - val_loss: 2.2284 - val_acc: 0.8462\n",
      "Epoch 4/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2221 - acc: 0.8476 - val_loss: 2.2494 - val_acc: 0.7564\n",
      "Epoch 5/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2083 - acc: 0.8661 - val_loss: 2.2291 - val_acc: 0.7308\n",
      "Epoch 6/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2055 - acc: 0.8818 - val_loss: 2.2055 - val_acc: 0.7949\n",
      "Epoch 7/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1854 - acc: 0.9473 - val_loss: 2.2008 - val_acc: 0.8846\n",
      "Epoch 8/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1927 - acc: 0.9145 - val_loss: 2.2021 - val_acc: 0.8846\n",
      "Epoch 9/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1849 - acc: 0.9459 - val_loss: 2.2056 - val_acc: 0.8846\n",
      "Epoch 10/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1779 - acc: 0.9544 - val_loss: 2.1868 - val_acc: 0.9359\n",
      "Epoch 11/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1771 - acc: 0.9658 - val_loss: 2.2108 - val_acc: 0.8205\n",
      "Epoch 12/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1762 - acc: 0.9786 - val_loss: 2.1817 - val_acc: 0.9615\n",
      "Epoch 13/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1754 - acc: 0.9815 - val_loss: 2.1797 - val_acc: 0.9615\n",
      "Epoch 14/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1756 - acc: 0.9815 - val_loss: 2.1785 - val_acc: 0.9615\n",
      "Epoch 15/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1718 - acc: 0.9843 - val_loss: 2.1872 - val_acc: 0.9615\n",
      "Epoch 16/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1833 - acc: 0.9801 - val_loss: 2.1914 - val_acc: 0.9359\n",
      "Epoch 17/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1717 - acc: 0.9915 - val_loss: 2.1790 - val_acc: 0.9744\n",
      "Epoch 18/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1718 - acc: 0.9886 - val_loss: 2.1739 - val_acc: 0.9744\n",
      "Epoch 19/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1699 - acc: 0.9957 - val_loss: 2.1857 - val_acc: 0.9103\n",
      "Epoch 20/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1718 - acc: 0.9858 - val_loss: 2.1855 - val_acc: 0.9487\n",
      "Epoch 21/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1711 - acc: 0.9900 - val_loss: 2.1952 - val_acc: 0.9231\n",
      "Epoch 22/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1698 - acc: 0.9915 - val_loss: 2.1771 - val_acc: 0.9744\n",
      "Epoch 23/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1699 - acc: 0.9943 - val_loss: 2.1737 - val_acc: 0.9872\n",
      "Epoch 24/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1691 - acc: 0.9957 - val_loss: 2.1720 - val_acc: 0.9872\n",
      "Epoch 25/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1704 - acc: 0.9943 - val_loss: 2.1775 - val_acc: 0.9744\n",
      "Epoch 26/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1687 - acc: 0.9943 - val_loss: 2.1784 - val_acc: 0.9744\n",
      "Epoch 27/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1692 - acc: 0.9972 - val_loss: 2.1846 - val_acc: 0.9487\n",
      "Epoch 28/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1682 - acc: 0.9972 - val_loss: 2.1854 - val_acc: 0.9615\n",
      "Epoch 29/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1700 - acc: 0.9943 - val_loss: 2.1715 - val_acc: 0.9872\n",
      "Epoch 30/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1684 - acc: 0.9929 - val_loss: 2.1717 - val_acc: 0.9744\n",
      "Epoch 31/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1684 - acc: 0.9972 - val_loss: 2.1731 - val_acc: 0.9615\n",
      "Epoch 32/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1678 - acc: 0.9957 - val_loss: 2.1779 - val_acc: 0.9615\n",
      "Epoch 33/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1681 - acc: 0.9972 - val_loss: 2.1735 - val_acc: 0.9744\n",
      "Epoch 34/100\n",
      "702/702 [==============================] - 3s 5ms/step - loss: 2.1679 - acc: 0.9943 - val_loss: 2.1737 - val_acc: 0.9872\n",
      "Epoch 35/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1670 - acc: 0.9986 - val_loss: 2.1717 - val_acc: 0.9744\n",
      "Epoch 36/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1669 - acc: 1.0000 - val_loss: 2.1707 - val_acc: 0.9872\n",
      "Epoch 37/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1684 - acc: 0.9972 - val_loss: 2.1717 - val_acc: 0.9744\n",
      "Epoch 38/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1669 - acc: 1.0000 - val_loss: 2.1720 - val_acc: 0.9615\n",
      "Epoch 39/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1666 - acc: 1.0000 - val_loss: 2.1720 - val_acc: 0.9872\n",
      "Epoch 40/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1670 - acc: 1.0000 - val_loss: 2.1702 - val_acc: 0.9872\n",
      "Epoch 41/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1668 - acc: 1.0000 - val_loss: 2.1699 - val_acc: 0.9872\n",
      "Epoch 42/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1668 - acc: 1.0000 - val_loss: 2.1697 - val_acc: 1.0000\n",
      "Epoch 43/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1666 - acc: 1.0000 - val_loss: 2.1712 - val_acc: 0.9615\n",
      "Epoch 44/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1667 - acc: 1.0000 - val_loss: 2.1703 - val_acc: 0.9744\n",
      "Epoch 45/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1666 - acc: 1.0000 - val_loss: 2.1731 - val_acc: 1.0000\n",
      "Epoch 46/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1671 - acc: 1.0000 - val_loss: 2.1708 - val_acc: 0.9872\n",
      "Epoch 47/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1702 - val_acc: 0.9872\n",
      "Epoch 48/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1668 - acc: 1.0000 - val_loss: 2.1704 - val_acc: 0.9872\n",
      "Epoch 49/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1717 - val_acc: 0.9744\n",
      "Epoch 50/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1666 - acc: 1.0000 - val_loss: 2.1791 - val_acc: 0.9615\n",
      "Epoch 51/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1666 - acc: 1.0000 - val_loss: 2.1709 - val_acc: 0.9744\n",
      "Epoch 52/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1700 - val_acc: 0.9872\n",
      "Epoch 53/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1666 - acc: 1.0000 - val_loss: 2.1734 - val_acc: 0.9744\n",
      "Epoch 54/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 0.9986 - val_loss: 2.1699 - val_acc: 0.9872\n",
      "Epoch 55/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1682 - val_acc: 1.0000\n",
      "Epoch 56/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1725 - val_acc: 0.9744\n",
      "Epoch 57/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1732 - val_acc: 0.9744\n",
      "Epoch 58/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1693 - val_acc: 0.9872\n",
      "Epoch 59/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1707 - val_acc: 0.9744\n",
      "Epoch 60/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1695 - val_acc: 0.9744\n",
      "Epoch 61/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1707 - val_acc: 0.9744\n",
      "Epoch 62/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1697 - val_acc: 0.9744\n",
      "Epoch 63/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1689 - val_acc: 0.9872\n",
      "Epoch 64/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1708 - val_acc: 0.9744\n",
      "Epoch 65/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1700 - val_acc: 0.9872\n",
      "Epoch 66/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1691 - val_acc: 1.0000\n",
      "Epoch 67/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1711 - val_acc: 0.9744\n",
      "Epoch 68/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1702 - val_acc: 0.9872\n",
      "Epoch 69/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1711 - val_acc: 0.9872\n",
      "Epoch 70/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1699 - val_acc: 0.9872\n",
      "Epoch 71/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1701 - val_acc: 0.9872\n",
      "Epoch 72/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1725 - val_acc: 0.9872\n",
      "Epoch 73/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1687 - val_acc: 0.9872\n",
      "Epoch 74/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1693 - val_acc: 0.9872\n",
      "Epoch 75/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1697 - val_acc: 0.9744\n",
      "Epoch 76/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1690 - val_acc: 0.9872\n",
      "Epoch 77/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1709 - val_acc: 0.9872\n",
      "Epoch 78/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1716 - val_acc: 0.9744\n",
      "Epoch 79/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1690 - val_acc: 0.9872\n",
      "Epoch 80/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1690 - val_acc: 1.0000\n",
      "Epoch 81/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1694 - val_acc: 0.9872\n",
      "Epoch 82/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1688 - val_acc: 0.9872\n",
      "Epoch 83/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1694 - val_acc: 0.9872\n",
      "Epoch 84/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1697 - val_acc: 0.9744\n",
      "Epoch 85/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1694 - val_acc: 0.9744\n",
      "Epoch 86/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1694 - val_acc: 0.9872\n",
      "Epoch 87/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1705 - val_acc: 0.9744\n",
      "Epoch 88/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1701 - val_acc: 0.9744\n",
      "Epoch 89/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1695 - val_acc: 0.9744\n",
      "Epoch 90/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1689 - val_acc: 0.9872\n",
      "Epoch 91/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1696 - val_acc: 0.9744\n",
      "Epoch 92/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1696 - val_acc: 0.9744\n",
      "Epoch 93/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1692 - val_acc: 0.9744\n",
      "Epoch 94/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1692 - val_acc: 0.9872\n",
      "Epoch 95/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1697 - val_acc: 0.9744\n",
      "Epoch 96/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1692 - val_acc: 0.9872\n",
      "Epoch 97/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1700 - val_acc: 0.9744\n",
      "Epoch 98/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1692 - val_acc: 0.9872\n",
      "Epoch 99/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1699 - val_acc: 0.9744\n",
      "Epoch 100/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1692 - val_acc: 0.9872\n",
      "78/78 [==============================] - 0s 350us/step\n",
      "Score for fold 2: loss of 2.1698164511949587; acc of 97.43589743589743%\n",
      "Train on 702 samples, validate on 78 samples\n",
      "Epoch 1/100\n",
      "702/702 [==============================] - 8s 12ms/step - loss: 2.4992 - acc: 0.2806 - val_loss: 2.3265 - val_acc: 0.5128\n",
      "Epoch 2/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.3008 - acc: 0.5969 - val_loss: 2.2309 - val_acc: 0.8590\n",
      "Epoch 3/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2439 - acc: 0.7650 - val_loss: 2.2280 - val_acc: 0.7821\n",
      "Epoch 4/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2152 - acc: 0.8604 - val_loss: 2.2895 - val_acc: 0.6282\n",
      "Epoch 5/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2094 - acc: 0.8533 - val_loss: 2.2121 - val_acc: 0.8974\n",
      "Epoch 6/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1962 - acc: 0.8989 - val_loss: 2.2131 - val_acc: 0.8333\n",
      "Epoch 7/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1857 - acc: 0.9345 - val_loss: 2.1971 - val_acc: 0.8462\n",
      "Epoch 8/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1847 - acc: 0.9387 - val_loss: 2.1791 - val_acc: 0.9615\n",
      "Epoch 9/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1820 - acc: 0.9615 - val_loss: 2.1831 - val_acc: 0.9487\n",
      "Epoch 10/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1760 - acc: 0.9786 - val_loss: 2.1897 - val_acc: 0.8846\n",
      "Epoch 11/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1770 - acc: 0.9672 - val_loss: 2.1767 - val_acc: 0.9744\n",
      "Epoch 12/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1774 - acc: 0.9729 - val_loss: 2.1821 - val_acc: 0.9615\n",
      "Epoch 13/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1726 - acc: 0.9915 - val_loss: 2.1740 - val_acc: 1.0000\n",
      "Epoch 14/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1727 - acc: 0.9843 - val_loss: 2.1969 - val_acc: 0.9231\n",
      "Epoch 15/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1746 - acc: 0.9801 - val_loss: 2.1789 - val_acc: 1.0000\n",
      "Epoch 16/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1730 - acc: 0.9843 - val_loss: 2.1969 - val_acc: 0.9744\n",
      "Epoch 17/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1711 - acc: 0.9900 - val_loss: 2.1807 - val_acc: 0.9359\n",
      "Epoch 18/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1707 - acc: 0.9915 - val_loss: 2.1775 - val_acc: 0.9872\n",
      "Epoch 19/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1699 - acc: 0.9929 - val_loss: 2.1804 - val_acc: 0.9872\n",
      "Epoch 20/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1721 - acc: 0.9744 - val_loss: 2.1713 - val_acc: 0.9872\n",
      "Epoch 21/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1686 - acc: 0.9986 - val_loss: 2.1761 - val_acc: 0.9872\n",
      "Epoch 22/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1716 - acc: 0.9858 - val_loss: 2.1900 - val_acc: 0.9744\n",
      "Epoch 23/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1732 - acc: 0.9900 - val_loss: 2.1765 - val_acc: 0.9872\n",
      "Epoch 24/100\n",
      "702/702 [==============================] - 3s 4ms/step - loss: 2.1681 - acc: 0.9972 - val_loss: 2.1746 - val_acc: 1.0000\n",
      "Epoch 25/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1682 - acc: 0.9972 - val_loss: 2.1710 - val_acc: 1.0000\n",
      "Epoch 26/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1682 - acc: 0.9972 - val_loss: 2.1689 - val_acc: 1.0000\n",
      "Epoch 27/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1675 - acc: 0.9986 - val_loss: 2.1707 - val_acc: 1.0000\n",
      "Epoch 28/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1683 - acc: 0.9972 - val_loss: 2.1712 - val_acc: 0.9872\n",
      "Epoch 29/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1674 - acc: 0.9972 - val_loss: 2.1733 - val_acc: 0.9872\n",
      "Epoch 30/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1675 - acc: 0.9972 - val_loss: 2.1749 - val_acc: 0.9872\n",
      "Epoch 31/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1674 - acc: 0.9986 - val_loss: 2.1700 - val_acc: 1.0000\n",
      "Epoch 32/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1672 - acc: 0.9972 - val_loss: 2.1707 - val_acc: 1.0000\n",
      "Epoch 33/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1671 - acc: 1.0000 - val_loss: 2.1752 - val_acc: 1.0000\n",
      "Epoch 34/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1672 - acc: 0.9986 - val_loss: 2.1727 - val_acc: 0.9872\n",
      "Epoch 35/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1675 - acc: 0.9986 - val_loss: 2.1685 - val_acc: 1.0000\n",
      "Epoch 36/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1701 - val_acc: 1.0000\n",
      "Epoch 37/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1667 - acc: 0.9972 - val_loss: 2.1733 - val_acc: 0.9872\n",
      "Epoch 38/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1671 - acc: 0.9986 - val_loss: 2.1696 - val_acc: 1.0000\n",
      "Epoch 39/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1666 - acc: 1.0000 - val_loss: 2.1707 - val_acc: 1.0000\n",
      "Epoch 40/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1690 - val_acc: 1.0000\n",
      "Epoch 41/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1666 - acc: 0.9986 - val_loss: 2.1686 - val_acc: 1.0000\n",
      "Epoch 42/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1688 - val_acc: 1.0000\n",
      "Epoch 43/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1666 - acc: 1.0000 - val_loss: 2.1720 - val_acc: 0.9872\n",
      "Epoch 44/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1693 - val_acc: 1.0000\n",
      "Epoch 45/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1714 - val_acc: 1.0000\n",
      "Epoch 46/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1701 - val_acc: 1.0000\n",
      "Epoch 47/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1691 - val_acc: 1.0000\n",
      "Epoch 48/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1732 - val_acc: 1.0000\n",
      "Epoch 49/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1685 - val_acc: 0.9872\n",
      "Epoch 50/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1704 - val_acc: 1.0000\n",
      "Epoch 51/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1704 - val_acc: 0.9872\n",
      "Epoch 52/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1684 - val_acc: 1.0000\n",
      "Epoch 53/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1687 - val_acc: 1.0000\n",
      "Epoch 54/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1705 - val_acc: 1.0000\n",
      "Epoch 55/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1697 - val_acc: 1.0000\n",
      "Epoch 56/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1699 - val_acc: 1.0000\n",
      "Epoch 57/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1695 - val_acc: 1.0000\n",
      "Epoch 58/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1692 - val_acc: 1.0000\n",
      "Epoch 59/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1692 - val_acc: 1.0000\n",
      "Epoch 60/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1689 - val_acc: 1.0000\n",
      "Epoch 61/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1706 - val_acc: 1.0000\n",
      "Epoch 62/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1713 - val_acc: 1.0000\n",
      "Epoch 63/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1693 - val_acc: 1.0000\n",
      "Epoch 64/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1695 - val_acc: 1.0000\n",
      "Epoch 65/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1683 - val_acc: 1.0000\n",
      "Epoch 66/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1694 - val_acc: 1.0000\n",
      "Epoch 67/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1688 - val_acc: 1.0000\n",
      "Epoch 68/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1695 - val_acc: 1.0000\n",
      "Epoch 69/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1690 - val_acc: 1.0000\n",
      "Epoch 70/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1703 - val_acc: 1.0000\n",
      "Epoch 71/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1687 - val_acc: 1.0000\n",
      "Epoch 72/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1691 - val_acc: 1.0000\n",
      "Epoch 73/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1695 - val_acc: 1.0000\n",
      "Epoch 74/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1691 - val_acc: 1.0000\n",
      "Epoch 75/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1686 - val_acc: 1.0000\n",
      "Epoch 76/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1691 - val_acc: 1.0000\n",
      "Epoch 77/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1691 - val_acc: 1.0000\n",
      "Epoch 78/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1699 - val_acc: 1.0000\n",
      "Epoch 79/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1692 - val_acc: 1.0000\n",
      "Epoch 80/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1695 - val_acc: 1.0000\n",
      "Epoch 81/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1692 - val_acc: 1.0000\n",
      "Epoch 82/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1687 - val_acc: 1.0000\n",
      "Epoch 83/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1689 - val_acc: 1.0000\n",
      "Epoch 84/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1693 - val_acc: 1.0000\n",
      "Epoch 85/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1689 - val_acc: 1.0000\n",
      "Epoch 86/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1693 - val_acc: 1.0000\n",
      "Epoch 87/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1693 - val_acc: 1.0000\n",
      "Epoch 88/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1698 - val_acc: 1.0000\n",
      "Epoch 89/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1692 - val_acc: 1.0000\n",
      "Epoch 90/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1692 - val_acc: 1.0000\n",
      "Epoch 91/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1693 - val_acc: 1.0000\n",
      "Epoch 92/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1692 - val_acc: 1.0000\n",
      "Epoch 93/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1691 - val_acc: 1.0000\n",
      "Epoch 94/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1696 - val_acc: 1.0000\n",
      "Epoch 95/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1689 - val_acc: 1.0000\n",
      "Epoch 96/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1691 - val_acc: 1.0000\n",
      "Epoch 97/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1693 - val_acc: 1.0000\n",
      "Epoch 98/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1692 - val_acc: 1.0000\n",
      "Epoch 99/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1695 - val_acc: 1.0000\n",
      "Epoch 100/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1690 - val_acc: 1.0000\n",
      "78/78 [==============================] - 0s 351us/step\n",
      "Score for fold 3: loss of 2.1688255224472437; acc of 100.0%\n",
      "Train on 702 samples, validate on 78 samples\n",
      "Epoch 1/100\n",
      "702/702 [==============================] - 9s 12ms/step - loss: 2.4935 - acc: 0.2650 - val_loss: 2.4052 - val_acc: 0.3205\n",
      "Epoch 2/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "702/702 [==============================] - 1s 1ms/step - loss: 2.3149 - acc: 0.6111 - val_loss: 2.3180 - val_acc: 0.5769\n",
      "Epoch 3/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2511 - acc: 0.7521 - val_loss: 2.2753 - val_acc: 0.6795\n",
      "Epoch 4/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2248 - acc: 0.8105 - val_loss: 2.2257 - val_acc: 0.7436\n",
      "Epoch 5/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2027 - acc: 0.8989 - val_loss: 2.2328 - val_acc: 0.7179\n",
      "Epoch 6/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1963 - acc: 0.9088 - val_loss: 2.2013 - val_acc: 0.8077\n",
      "Epoch 7/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1910 - acc: 0.9501 - val_loss: 2.2102 - val_acc: 0.7821\n",
      "Epoch 8/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1805 - acc: 0.9587 - val_loss: 2.1805 - val_acc: 0.9103\n",
      "Epoch 9/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1850 - acc: 0.9601 - val_loss: 2.1822 - val_acc: 0.9359\n",
      "Epoch 10/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1755 - acc: 0.9758 - val_loss: 2.1840 - val_acc: 0.9359\n",
      "Epoch 11/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1789 - acc: 0.9744 - val_loss: 2.1846 - val_acc: 0.8718\n",
      "Epoch 12/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1747 - acc: 0.9772 - val_loss: 2.1770 - val_acc: 0.9872\n",
      "Epoch 13/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1732 - acc: 0.9943 - val_loss: 2.1723 - val_acc: 0.9744\n",
      "Epoch 14/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1723 - acc: 0.9886 - val_loss: 2.1788 - val_acc: 0.9615\n",
      "Epoch 15/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1742 - acc: 0.9829 - val_loss: 2.1745 - val_acc: 0.9744\n",
      "Epoch 16/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1742 - acc: 0.9886 - val_loss: 2.1719 - val_acc: 0.9615\n",
      "Epoch 17/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1724 - acc: 0.9929 - val_loss: 2.1708 - val_acc: 0.9872\n",
      "Epoch 18/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1702 - acc: 0.9929 - val_loss: 2.1763 - val_acc: 0.9103\n",
      "Epoch 19/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1715 - acc: 0.9886 - val_loss: 2.1707 - val_acc: 0.9744\n",
      "Epoch 20/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1687 - acc: 0.9943 - val_loss: 2.1799 - val_acc: 0.8846\n",
      "Epoch 21/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1702 - acc: 0.9815 - val_loss: 2.1705 - val_acc: 0.9872\n",
      "Epoch 22/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1688 - acc: 0.9986 - val_loss: 2.1830 - val_acc: 0.9359\n",
      "Epoch 23/100\n",
      "702/702 [==============================] - 3s 5ms/step - loss: 2.1725 - acc: 0.9872 - val_loss: 2.1707 - val_acc: 0.9872\n",
      "Epoch 24/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1679 - acc: 0.9957 - val_loss: 2.1694 - val_acc: 0.9872\n",
      "Epoch 25/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1677 - acc: 0.9986 - val_loss: 2.1695 - val_acc: 0.9872\n",
      "Epoch 26/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1681 - acc: 0.9957 - val_loss: 2.1697 - val_acc: 0.9872\n",
      "Epoch 27/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1676 - acc: 0.9957 - val_loss: 2.1693 - val_acc: 0.9872\n",
      "Epoch 28/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1676 - acc: 0.9986 - val_loss: 2.1716 - val_acc: 0.9872\n",
      "Epoch 29/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1675 - acc: 0.9972 - val_loss: 2.1684 - val_acc: 1.0000\n",
      "Epoch 30/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1675 - acc: 0.9986 - val_loss: 2.1695 - val_acc: 0.9744\n",
      "Epoch 31/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1673 - acc: 0.9943 - val_loss: 2.1691 - val_acc: 0.9872\n",
      "Epoch 32/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1670 - acc: 0.9986 - val_loss: 2.1696 - val_acc: 0.9872\n",
      "Epoch 33/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1671 - acc: 0.9986 - val_loss: 2.1691 - val_acc: 0.9872\n",
      "Epoch 34/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1673 - acc: 0.9957 - val_loss: 2.1734 - val_acc: 0.9615\n",
      "Epoch 35/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1671 - acc: 0.9972 - val_loss: 2.1751 - val_acc: 0.9615\n",
      "Epoch 36/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1673 - acc: 0.9986 - val_loss: 2.1700 - val_acc: 0.9872\n",
      "Epoch 37/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1668 - acc: 0.9986 - val_loss: 2.1693 - val_acc: 0.9872\n",
      "Epoch 38/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1668 - acc: 1.0000 - val_loss: 2.1689 - val_acc: 0.9872\n",
      "Epoch 39/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1668 - acc: 1.0000 - val_loss: 2.1691 - val_acc: 0.9744\n",
      "Epoch 40/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1668 - acc: 1.0000 - val_loss: 2.1699 - val_acc: 0.9744\n",
      "Epoch 41/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1670 - acc: 0.9972 - val_loss: 2.1681 - val_acc: 1.0000\n",
      "Epoch 42/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1668 - acc: 0.9986 - val_loss: 2.1694 - val_acc: 0.9872\n",
      "Epoch 43/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1683 - val_acc: 0.9872\n",
      "Epoch 44/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1692 - val_acc: 0.9872\n",
      "Epoch 45/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1697 - val_acc: 0.9744\n",
      "Epoch 46/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1667 - acc: 0.9957 - val_loss: 2.1690 - val_acc: 0.9872\n",
      "Epoch 47/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1690 - val_acc: 0.9872\n",
      "Epoch 48/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1666 - acc: 1.0000 - val_loss: 2.1689 - val_acc: 0.9872\n",
      "Epoch 49/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1694 - val_acc: 0.9872\n",
      "Epoch 50/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1686 - val_acc: 0.9872\n",
      "Epoch 51/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1678 - val_acc: 1.0000\n",
      "Epoch 52/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1683 - val_acc: 0.9872\n",
      "Epoch 53/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1691 - val_acc: 0.9872\n",
      "Epoch 54/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1683 - val_acc: 0.9872\n",
      "Epoch 55/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1691 - val_acc: 0.9872\n",
      "Epoch 56/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1689 - val_acc: 0.9872\n",
      "Epoch 57/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1688 - val_acc: 0.9872\n",
      "Epoch 58/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1685 - val_acc: 0.9872\n",
      "Epoch 59/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1680 - val_acc: 0.9872\n",
      "Epoch 60/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1689 - val_acc: 0.9872\n",
      "Epoch 61/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1688 - val_acc: 0.9872\n",
      "Epoch 62/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1688 - val_acc: 0.9872\n",
      "Epoch 63/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1682 - val_acc: 0.9872\n",
      "Epoch 64/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1688 - val_acc: 0.9872\n",
      "Epoch 65/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1687 - val_acc: 0.9872\n",
      "Epoch 66/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1688 - val_acc: 0.9872\n",
      "Epoch 67/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1687 - val_acc: 0.9872\n",
      "Epoch 68/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1689 - val_acc: 0.9872\n",
      "Epoch 69/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1686 - val_acc: 0.9872\n",
      "Epoch 70/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1689 - val_acc: 0.9872\n",
      "Epoch 71/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1689 - val_acc: 0.9872\n",
      "Epoch 72/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1686 - val_acc: 0.9872\n",
      "Epoch 73/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1684 - val_acc: 0.9872\n",
      "Epoch 74/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1685 - val_acc: 0.9872\n",
      "Epoch 75/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1687 - val_acc: 0.9872\n",
      "Epoch 76/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1686 - val_acc: 0.9872\n",
      "Epoch 77/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1683 - val_acc: 0.9872\n",
      "Epoch 78/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1683 - val_acc: 0.9872\n",
      "Epoch 79/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1686 - val_acc: 0.9872\n",
      "Epoch 80/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1686 - val_acc: 0.9872\n",
      "Epoch 81/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1684 - val_acc: 0.9872\n",
      "Epoch 82/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1684 - val_acc: 0.9872\n",
      "Epoch 83/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1685 - val_acc: 0.9872\n",
      "Epoch 84/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1681 - val_acc: 0.9872\n",
      "Epoch 85/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1682 - val_acc: 0.9872\n",
      "Epoch 86/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1685 - val_acc: 0.9872\n",
      "Epoch 87/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1681 - val_acc: 0.9872\n",
      "Epoch 88/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1685 - val_acc: 0.9872\n",
      "Epoch 89/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1688 - val_acc: 0.9872\n",
      "Epoch 90/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1687 - val_acc: 0.9872\n",
      "Epoch 91/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1686 - val_acc: 0.9872\n",
      "Epoch 92/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1685 - val_acc: 0.9872\n",
      "Epoch 93/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1684 - val_acc: 0.9872\n",
      "Epoch 94/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1687 - val_acc: 0.9872\n",
      "Epoch 95/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1685 - val_acc: 0.9872\n",
      "Epoch 96/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1683 - val_acc: 0.9872\n",
      "Epoch 97/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1684 - val_acc: 0.9872\n",
      "Epoch 98/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1684 - val_acc: 0.9872\n",
      "Epoch 99/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1687 - val_acc: 0.9872\n",
      "Epoch 100/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1685 - val_acc: 0.9872\n",
      "78/78 [==============================] - 0s 352us/step\n",
      "Score for fold 4: loss of 2.168790896733602; acc of 98.71794825945145%\n",
      "Train on 702 samples, validate on 78 samples\n",
      "Epoch 1/100\n",
      "702/702 [==============================] - 9s 12ms/step - loss: 2.5057 - acc: 0.2749 - val_loss: 2.3998 - val_acc: 0.4103\n",
      "Epoch 2/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.3196 - acc: 0.5556 - val_loss: 2.3040 - val_acc: 0.5128\n",
      "Epoch 3/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2565 - acc: 0.7023 - val_loss: 2.2932 - val_acc: 0.6410\n",
      "Epoch 4/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2193 - acc: 0.8291 - val_loss: 2.2145 - val_acc: 0.8077\n",
      "Epoch 5/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2011 - acc: 0.8889 - val_loss: 2.2004 - val_acc: 0.8590\n",
      "Epoch 6/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2000 - acc: 0.8889 - val_loss: 2.2095 - val_acc: 0.9103\n",
      "Epoch 7/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1865 - acc: 0.9516 - val_loss: 2.1991 - val_acc: 0.8718\n",
      "Epoch 8/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1848 - acc: 0.9544 - val_loss: 2.2006 - val_acc: 0.8974\n",
      "Epoch 9/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1856 - acc: 0.9501 - val_loss: 2.1881 - val_acc: 0.9231\n",
      "Epoch 10/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1775 - acc: 0.9701 - val_loss: 2.1848 - val_acc: 0.9487\n",
      "Epoch 11/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1749 - acc: 0.9843 - val_loss: 2.1859 - val_acc: 0.9359\n",
      "Epoch 12/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1829 - acc: 0.9573 - val_loss: 2.1812 - val_acc: 1.0000\n",
      "Epoch 13/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1712 - acc: 0.9872 - val_loss: 2.1863 - val_acc: 0.9359\n",
      "Epoch 14/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1748 - acc: 0.9786 - val_loss: 2.1790 - val_acc: 0.9615\n",
      "Epoch 15/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1713 - acc: 0.9929 - val_loss: 2.1817 - val_acc: 0.9615\n",
      "Epoch 16/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1738 - acc: 0.9886 - val_loss: 2.1822 - val_acc: 0.9359\n",
      "Epoch 17/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1708 - acc: 0.9915 - val_loss: 2.1794 - val_acc: 0.9872\n",
      "Epoch 18/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1721 - acc: 0.9900 - val_loss: 2.1791 - val_acc: 0.9744\n",
      "Epoch 19/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1714 - acc: 0.9872 - val_loss: 2.1775 - val_acc: 0.9872\n",
      "Epoch 20/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1683 - acc: 0.9972 - val_loss: 2.1743 - val_acc: 0.9744\n",
      "Epoch 21/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1730 - acc: 0.9843 - val_loss: 2.1768 - val_acc: 0.9744\n",
      "Epoch 22/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1695 - acc: 0.9900 - val_loss: 2.1840 - val_acc: 0.9744\n",
      "Epoch 23/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "702/702 [==============================] - 3s 5ms/step - loss: 2.1688 - acc: 0.9972 - val_loss: 2.1871 - val_acc: 0.9487\n",
      "Epoch 24/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1691 - acc: 0.9929 - val_loss: 2.1770 - val_acc: 0.9872\n",
      "Epoch 25/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1678 - acc: 0.9972 - val_loss: 2.1740 - val_acc: 0.9744\n",
      "Epoch 26/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1676 - acc: 0.9986 - val_loss: 2.1748 - val_acc: 1.0000\n",
      "Epoch 27/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1677 - acc: 0.9986 - val_loss: 2.1768 - val_acc: 0.9615\n",
      "Epoch 28/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1678 - acc: 0.9972 - val_loss: 2.1720 - val_acc: 0.9872\n",
      "Epoch 29/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1673 - acc: 0.9986 - val_loss: 2.1732 - val_acc: 1.0000\n",
      "Epoch 30/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1675 - acc: 0.9972 - val_loss: 2.1748 - val_acc: 1.0000\n",
      "Epoch 31/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1674 - acc: 0.9972 - val_loss: 2.1726 - val_acc: 0.9744\n",
      "Epoch 32/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1676 - acc: 0.9986 - val_loss: 2.1736 - val_acc: 0.9872\n",
      "Epoch 33/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1678 - acc: 0.9972 - val_loss: 2.1722 - val_acc: 0.9872\n",
      "Epoch 34/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1669 - acc: 0.9986 - val_loss: 2.1726 - val_acc: 0.9744\n",
      "Epoch 35/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1668 - acc: 0.9986 - val_loss: 2.1718 - val_acc: 1.0000\n",
      "Epoch 36/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1668 - acc: 0.9986 - val_loss: 2.1725 - val_acc: 0.9744\n",
      "Epoch 37/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1667 - acc: 1.0000 - val_loss: 2.1730 - val_acc: 0.9872\n",
      "Epoch 38/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1667 - acc: 0.9986 - val_loss: 2.1718 - val_acc: 0.9872\n",
      "Epoch 39/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1666 - acc: 1.0000 - val_loss: 2.1719 - val_acc: 0.9872\n",
      "Epoch 40/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1666 - acc: 1.0000 - val_loss: 2.1730 - val_acc: 0.9872\n",
      "Epoch 41/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1666 - acc: 1.0000 - val_loss: 2.1736 - val_acc: 0.9872\n",
      "Epoch 42/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1666 - acc: 1.0000 - val_loss: 2.1737 - val_acc: 1.0000\n",
      "Epoch 43/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1666 - acc: 1.0000 - val_loss: 2.1727 - val_acc: 1.0000\n",
      "Epoch 44/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1666 - acc: 1.0000 - val_loss: 2.1707 - val_acc: 1.0000\n",
      "Epoch 45/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1750 - val_acc: 0.9872\n",
      "Epoch 46/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1727 - val_acc: 1.0000\n",
      "Epoch 47/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1728 - val_acc: 1.0000\n",
      "Epoch 48/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1714 - val_acc: 0.9872\n",
      "Epoch 49/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1718 - val_acc: 0.9872\n",
      "Epoch 50/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 0.9972 - val_loss: 2.1715 - val_acc: 0.9872\n",
      "Epoch 51/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1723 - val_acc: 1.0000\n",
      "Epoch 52/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1740 - val_acc: 0.9615\n",
      "Epoch 53/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1706 - val_acc: 0.9872\n",
      "Epoch 54/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1724 - val_acc: 1.0000\n",
      "Epoch 55/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1738 - val_acc: 0.9872\n",
      "Epoch 56/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1723 - val_acc: 0.9872\n",
      "Epoch 57/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1721 - val_acc: 1.0000\n",
      "Epoch 58/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1722 - val_acc: 0.9872\n",
      "Epoch 59/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1717 - val_acc: 0.9872\n",
      "Epoch 60/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1716 - val_acc: 0.9872\n",
      "Epoch 61/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1722 - val_acc: 1.0000\n",
      "Epoch 62/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1724 - val_acc: 0.9872\n",
      "Epoch 63/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1728 - val_acc: 0.9872\n",
      "Epoch 64/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1727 - val_acc: 0.9872\n",
      "Epoch 65/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1719 - val_acc: 0.9872\n",
      "Epoch 66/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1719 - val_acc: 1.0000\n",
      "Epoch 67/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1708 - val_acc: 0.9872\n",
      "Epoch 68/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1731 - val_acc: 0.9872\n",
      "Epoch 69/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1726 - val_acc: 0.9872\n",
      "Epoch 70/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1724 - val_acc: 1.0000\n",
      "Epoch 71/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1734 - val_acc: 0.9872\n",
      "Epoch 72/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1737 - val_acc: 0.9872\n",
      "Epoch 73/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1729 - val_acc: 0.9872\n",
      "Epoch 74/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1726 - val_acc: 0.9872\n",
      "Epoch 75/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1727 - val_acc: 0.9872\n",
      "Epoch 76/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1731 - val_acc: 1.0000\n",
      "Epoch 77/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1721 - val_acc: 0.9872\n",
      "Epoch 78/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1730 - val_acc: 0.9872\n",
      "Epoch 79/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1718 - val_acc: 0.9872\n",
      "Epoch 80/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1733 - val_acc: 0.9872\n",
      "Epoch 81/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1724 - val_acc: 0.9872\n",
      "Epoch 82/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1716 - val_acc: 0.9872\n",
      "Epoch 83/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1722 - val_acc: 0.9872\n",
      "Epoch 84/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1722 - val_acc: 0.9872\n",
      "Epoch 85/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1726 - val_acc: 0.9872\n",
      "Epoch 86/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1725 - val_acc: 0.9872\n",
      "Epoch 87/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1721 - val_acc: 0.9872\n",
      "Epoch 88/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1724 - val_acc: 0.9872\n",
      "Epoch 89/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1721 - val_acc: 1.0000\n",
      "Epoch 90/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1717 - val_acc: 0.9872\n",
      "Epoch 91/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1718 - val_acc: 0.9872\n",
      "Epoch 92/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1724 - val_acc: 0.9872\n",
      "Epoch 93/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1727 - val_acc: 0.9872\n",
      "Epoch 94/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1724 - val_acc: 0.9872\n",
      "Epoch 95/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1726 - val_acc: 0.9872\n",
      "Epoch 96/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1713 - val_acc: 0.9872\n",
      "Epoch 97/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1715 - val_acc: 0.9872\n",
      "Epoch 98/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1714 - val_acc: 1.0000\n",
      "Epoch 99/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1713 - val_acc: 1.0000\n",
      "Epoch 100/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1719 - val_acc: 0.9872\n",
      "78/78 [==============================] - 0s 347us/step\n",
      "Score for fold 5: loss of 2.170259041663928; acc of 98.71794871794873%\n",
      "Train on 702 samples, validate on 78 samples\n",
      "Epoch 1/100\n",
      "702/702 [==============================] - 9s 13ms/step - loss: 2.4773 - acc: 0.2821 - val_loss: 2.3721 - val_acc: 0.3718\n",
      "Epoch 2/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2897 - acc: 0.6097 - val_loss: 2.2614 - val_acc: 0.6923\n",
      "Epoch 3/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2471 - acc: 0.7464 - val_loss: 2.2562 - val_acc: 0.6923\n",
      "Epoch 4/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2180 - acc: 0.8376 - val_loss: 2.2274 - val_acc: 0.8205\n",
      "Epoch 5/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1986 - acc: 0.8989 - val_loss: 2.2722 - val_acc: 0.7308\n",
      "Epoch 6/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2000 - acc: 0.9174 - val_loss: 2.1995 - val_acc: 0.9231\n",
      "Epoch 7/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1898 - acc: 0.9302 - val_loss: 2.2007 - val_acc: 0.9487\n",
      "Epoch 8/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1814 - acc: 0.9601 - val_loss: 2.2185 - val_acc: 0.7821\n",
      "Epoch 9/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1853 - acc: 0.9430 - val_loss: 2.2030 - val_acc: 0.9103\n",
      "Epoch 10/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1761 - acc: 0.9687 - val_loss: 2.2078 - val_acc: 0.7821\n",
      "Epoch 11/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1805 - acc: 0.9701 - val_loss: 2.1830 - val_acc: 0.9231\n",
      "Epoch 12/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1736 - acc: 0.9858 - val_loss: 2.2121 - val_acc: 0.8205\n",
      "Epoch 13/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1785 - acc: 0.9644 - val_loss: 2.1800 - val_acc: 0.9872\n",
      "Epoch 14/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1729 - acc: 0.9829 - val_loss: 2.1783 - val_acc: 0.9744\n",
      "Epoch 15/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1731 - acc: 0.9815 - val_loss: 2.1786 - val_acc: 0.9487\n",
      "Epoch 16/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1705 - acc: 0.9943 - val_loss: 2.1824 - val_acc: 0.9615\n",
      "Epoch 17/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1697 - acc: 0.9972 - val_loss: 2.2044 - val_acc: 0.8590\n",
      "Epoch 18/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1745 - acc: 0.9758 - val_loss: 2.1837 - val_acc: 0.9359\n",
      "Epoch 19/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1689 - acc: 0.9957 - val_loss: 2.1738 - val_acc: 0.9744\n",
      "Epoch 20/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1705 - acc: 0.9943 - val_loss: 2.1898 - val_acc: 0.9487\n",
      "Epoch 21/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1694 - acc: 0.9972 - val_loss: 2.2212 - val_acc: 0.9359\n",
      "Epoch 22/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1720 - acc: 0.9915 - val_loss: 2.1771 - val_acc: 0.9872\n",
      "Epoch 23/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1682 - acc: 0.9986 - val_loss: 2.1761 - val_acc: 0.9615\n",
      "Epoch 24/100\n",
      "702/702 [==============================] - 3s 5ms/step - loss: 2.1701 - acc: 0.9929 - val_loss: 2.1754 - val_acc: 0.9744\n",
      "Epoch 25/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1684 - acc: 0.9943 - val_loss: 2.1799 - val_acc: 0.9487\n",
      "Epoch 26/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1677 - acc: 0.9957 - val_loss: 2.1750 - val_acc: 0.9744\n",
      "Epoch 27/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1672 - acc: 0.9986 - val_loss: 2.1739 - val_acc: 0.9744\n",
      "Epoch 28/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1678 - acc: 0.9957 - val_loss: 2.1789 - val_acc: 0.9615\n",
      "Epoch 29/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1672 - acc: 0.9986 - val_loss: 2.1717 - val_acc: 0.9872\n",
      "Epoch 30/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1671 - acc: 1.0000 - val_loss: 2.1721 - val_acc: 0.9872\n",
      "Epoch 31/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1679 - acc: 0.9986 - val_loss: 2.1850 - val_acc: 0.9487\n",
      "Epoch 32/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1676 - acc: 0.9986 - val_loss: 2.1726 - val_acc: 0.9872\n",
      "Epoch 33/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1669 - acc: 0.9972 - val_loss: 2.1721 - val_acc: 0.9744\n",
      "Epoch 34/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1670 - acc: 0.9972 - val_loss: 2.1732 - val_acc: 0.9744\n",
      "Epoch 35/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1666 - acc: 1.0000 - val_loss: 2.1728 - val_acc: 0.9872\n",
      "Epoch 36/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1667 - acc: 1.0000 - val_loss: 2.1723 - val_acc: 0.9872\n",
      "Epoch 37/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1666 - acc: 1.0000 - val_loss: 2.1726 - val_acc: 0.9615\n",
      "Epoch 38/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1669 - acc: 0.9972 - val_loss: 2.1718 - val_acc: 0.9872\n",
      "Epoch 39/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1666 - acc: 1.0000 - val_loss: 2.1709 - val_acc: 0.9872\n",
      "Epoch 40/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1718 - val_acc: 0.9744\n",
      "Epoch 41/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1713 - val_acc: 0.9872\n",
      "Epoch 42/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1718 - val_acc: 0.9744\n",
      "Epoch 43/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1714 - val_acc: 0.9744\n",
      "Epoch 44/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1712 - val_acc: 0.9872\n",
      "Epoch 45/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1708 - val_acc: 0.9872\n",
      "Epoch 46/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1706 - val_acc: 0.9744\n",
      "Epoch 47/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1704 - val_acc: 0.9872\n",
      "Epoch 48/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1704 - val_acc: 0.9744\n",
      "Epoch 49/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1707 - val_acc: 0.9744\n",
      "Epoch 50/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1706 - val_acc: 0.9872\n",
      "Epoch 51/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1709 - val_acc: 0.9872\n",
      "Epoch 52/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1702 - val_acc: 0.9872\n",
      "Epoch 53/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 0.9972 - val_loss: 2.1706 - val_acc: 0.9744\n",
      "Epoch 54/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1703 - val_acc: 0.9744\n",
      "Epoch 55/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1703 - val_acc: 0.9744\n",
      "Epoch 56/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1703 - val_acc: 0.9744\n",
      "Epoch 57/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1705 - val_acc: 0.9744\n",
      "Epoch 58/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1708 - val_acc: 0.9872\n",
      "Epoch 59/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1703 - val_acc: 0.9744\n",
      "Epoch 60/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1709 - val_acc: 0.9872\n",
      "Epoch 61/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1704 - val_acc: 0.9744\n",
      "Epoch 62/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1706 - val_acc: 0.9872\n",
      "Epoch 63/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1706 - val_acc: 0.9744\n",
      "Epoch 64/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1703 - val_acc: 0.9872\n",
      "Epoch 65/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1701 - val_acc: 0.9872\n",
      "Epoch 66/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 0.9986 - val_loss: 2.1704 - val_acc: 0.9872\n",
      "Epoch 67/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1706 - val_acc: 0.9744\n",
      "Epoch 68/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1706 - val_acc: 0.9872\n",
      "Epoch 69/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1706 - val_acc: 0.9744\n",
      "Epoch 70/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1706 - val_acc: 0.9744\n",
      "Epoch 71/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1704 - val_acc: 0.9744\n",
      "Epoch 72/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1705 - val_acc: 0.9744\n",
      "Epoch 73/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1704 - val_acc: 0.9744\n",
      "Epoch 74/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1707 - val_acc: 0.9872\n",
      "Epoch 75/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1706 - val_acc: 0.9744\n",
      "Epoch 76/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1705 - val_acc: 0.9744\n",
      "Epoch 77/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1706 - val_acc: 0.9744\n",
      "Epoch 78/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1701 - val_acc: 0.9744\n",
      "Epoch 79/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1704 - val_acc: 0.9744\n",
      "Epoch 80/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1704 - val_acc: 0.9744\n",
      "Epoch 81/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1706 - val_acc: 0.9744\n",
      "Epoch 82/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1708 - val_acc: 0.9872\n",
      "Epoch 83/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1706 - val_acc: 0.9872\n",
      "Epoch 84/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1705 - val_acc: 0.9872\n",
      "Epoch 85/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1707 - val_acc: 0.9744\n",
      "Epoch 86/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1705 - val_acc: 0.9744\n",
      "Epoch 87/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1705 - val_acc: 0.9744\n",
      "Epoch 88/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1706 - val_acc: 0.9872\n",
      "Epoch 89/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1706 - val_acc: 0.9744\n",
      "Epoch 90/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1706 - val_acc: 0.9872\n",
      "Epoch 91/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1706 - val_acc: 0.9872\n",
      "Epoch 92/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1707 - val_acc: 0.9744\n",
      "Epoch 93/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1707 - val_acc: 0.9872\n",
      "Epoch 94/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1705 - val_acc: 0.9744\n",
      "Epoch 95/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1709 - val_acc: 0.9872\n",
      "Epoch 96/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1707 - val_acc: 0.9744\n",
      "Epoch 97/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1706 - val_acc: 0.9744\n",
      "Epoch 98/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1707 - val_acc: 0.9744\n",
      "Epoch 99/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1707 - val_acc: 0.9744\n",
      "Epoch 100/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1707 - val_acc: 0.9744\n",
      "78/78 [==============================] - 0s 351us/step\n",
      "Score for fold 6: loss of 2.1711090161250186; acc of 97.43589697740018%\n",
      "Train on 702 samples, validate on 78 samples\n",
      "Epoch 1/100\n",
      "702/702 [==============================] - 9s 13ms/step - loss: 2.5089 - acc: 0.2806 - val_loss: 2.3179 - val_acc: 0.5641\n",
      "Epoch 2/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.3042 - acc: 0.5755 - val_loss: 2.3687 - val_acc: 0.5385\n",
      "Epoch 3/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2472 - acc: 0.7550 - val_loss: 2.2511 - val_acc: 0.8333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2307 - acc: 0.7991 - val_loss: 2.2273 - val_acc: 0.8333\n",
      "Epoch 5/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2064 - acc: 0.8960 - val_loss: 2.2092 - val_acc: 0.8718\n",
      "Epoch 6/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2000 - acc: 0.8803 - val_loss: 2.2285 - val_acc: 0.7949\n",
      "Epoch 7/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1891 - acc: 0.9373 - val_loss: 2.1893 - val_acc: 0.8718\n",
      "Epoch 8/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1874 - acc: 0.9316 - val_loss: 2.1876 - val_acc: 0.8974\n",
      "Epoch 9/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1807 - acc: 0.9615 - val_loss: 2.2019 - val_acc: 0.7692\n",
      "Epoch 10/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1776 - acc: 0.9715 - val_loss: 2.1938 - val_acc: 0.9231\n",
      "Epoch 11/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1746 - acc: 0.9815 - val_loss: 2.1957 - val_acc: 0.9359\n",
      "Epoch 12/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1760 - acc: 0.9758 - val_loss: 2.1854 - val_acc: 0.9744\n",
      "Epoch 13/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1737 - acc: 0.9786 - val_loss: 2.1916 - val_acc: 0.8974\n",
      "Epoch 14/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1728 - acc: 0.9915 - val_loss: 2.1806 - val_acc: 0.9744\n",
      "Epoch 15/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1739 - acc: 0.9886 - val_loss: 2.1849 - val_acc: 0.9615\n",
      "Epoch 16/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1760 - acc: 0.9801 - val_loss: 2.1806 - val_acc: 0.9615\n",
      "Epoch 17/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1701 - acc: 0.9929 - val_loss: 2.1997 - val_acc: 0.9487\n",
      "Epoch 18/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1718 - acc: 0.9886 - val_loss: 2.1972 - val_acc: 0.8590\n",
      "Epoch 19/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1711 - acc: 0.9872 - val_loss: 2.1827 - val_acc: 0.9615\n",
      "Epoch 20/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1718 - acc: 0.9900 - val_loss: 2.1831 - val_acc: 0.9744\n",
      "Epoch 21/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1687 - acc: 0.9943 - val_loss: 2.1777 - val_acc: 0.9744\n",
      "Epoch 22/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1687 - acc: 0.9972 - val_loss: 2.1781 - val_acc: 0.9744\n",
      "Epoch 23/100\n",
      "702/702 [==============================] - 3s 5ms/step - loss: 2.1688 - acc: 0.9915 - val_loss: 2.1809 - val_acc: 0.9615\n",
      "Epoch 24/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1677 - acc: 0.9986 - val_loss: 2.1798 - val_acc: 0.9615\n",
      "Epoch 25/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1673 - acc: 0.9986 - val_loss: 2.1803 - val_acc: 0.9744\n",
      "Epoch 26/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1674 - acc: 1.0000 - val_loss: 2.1786 - val_acc: 0.9744\n",
      "Epoch 27/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1671 - acc: 1.0000 - val_loss: 2.1751 - val_acc: 0.9744\n",
      "Epoch 28/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1679 - acc: 0.9957 - val_loss: 2.1800 - val_acc: 0.9744\n",
      "Epoch 29/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1670 - acc: 1.0000 - val_loss: 2.1786 - val_acc: 0.9744\n",
      "Epoch 30/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1668 - acc: 1.0000 - val_loss: 2.1790 - val_acc: 0.9744\n",
      "Epoch 31/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1677 - acc: 1.0000 - val_loss: 2.1805 - val_acc: 0.9615\n",
      "Epoch 32/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1669 - acc: 1.0000 - val_loss: 2.1764 - val_acc: 0.9744\n",
      "Epoch 33/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1668 - acc: 1.0000 - val_loss: 2.1797 - val_acc: 0.9744\n",
      "Epoch 34/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1666 - acc: 1.0000 - val_loss: 2.1780 - val_acc: 0.9744\n",
      "Epoch 35/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1771 - val_acc: 0.9744\n",
      "Epoch 36/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1799 - val_acc: 0.9744\n",
      "Epoch 37/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1666 - acc: 1.0000 - val_loss: 2.1778 - val_acc: 0.9744\n",
      "Epoch 38/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1666 - acc: 1.0000 - val_loss: 2.1722 - val_acc: 0.9872\n",
      "Epoch 39/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1765 - val_acc: 0.9744\n",
      "Epoch 40/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1668 - acc: 0.9986 - val_loss: 2.1773 - val_acc: 0.9744\n",
      "Epoch 41/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1778 - val_acc: 0.9744\n",
      "Epoch 42/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1782 - val_acc: 0.9744\n",
      "Epoch 43/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1764 - val_acc: 0.9744\n",
      "Epoch 44/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1785 - val_acc: 0.9744\n",
      "Epoch 45/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1766 - val_acc: 0.9744\n",
      "Epoch 46/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1785 - val_acc: 0.9744\n",
      "Epoch 47/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1794 - val_acc: 0.9615\n",
      "Epoch 48/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1803 - val_acc: 0.9744\n",
      "Epoch 49/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1793 - val_acc: 0.9744\n",
      "Epoch 50/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1732 - val_acc: 0.9872\n",
      "Epoch 51/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1796 - val_acc: 0.9744\n",
      "Epoch 52/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1765 - val_acc: 0.9744\n",
      "Epoch 53/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1789 - val_acc: 0.9744\n",
      "Epoch 54/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1786 - val_acc: 0.9744\n",
      "Epoch 55/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1774 - val_acc: 0.9744\n",
      "Epoch 56/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1790 - val_acc: 0.9744\n",
      "Epoch 57/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1774 - val_acc: 0.9744\n",
      "Epoch 58/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1781 - val_acc: 0.9744\n",
      "Epoch 59/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1776 - val_acc: 0.9744\n",
      "Epoch 60/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1797 - val_acc: 0.9744\n",
      "Epoch 61/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1786 - val_acc: 0.9744\n",
      "Epoch 62/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1762 - val_acc: 0.9744\n",
      "Epoch 63/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1799 - val_acc: 0.9744\n",
      "Epoch 64/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1781 - val_acc: 0.9744\n",
      "Epoch 65/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1783 - val_acc: 0.9744\n",
      "Epoch 66/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1779 - val_acc: 0.9744\n",
      "Epoch 67/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1777 - val_acc: 0.9744\n",
      "Epoch 68/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1780 - val_acc: 0.9744\n",
      "Epoch 69/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1781 - val_acc: 0.9744\n",
      "Epoch 70/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1781 - val_acc: 0.9744\n",
      "Epoch 71/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1781 - val_acc: 0.9744\n",
      "Epoch 72/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1772 - val_acc: 0.9744\n",
      "Epoch 73/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1777 - val_acc: 0.9744\n",
      "Epoch 74/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1778 - val_acc: 0.9744\n",
      "Epoch 75/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1778 - val_acc: 0.9744\n",
      "Epoch 76/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1779 - val_acc: 0.9744\n",
      "Epoch 77/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1776 - val_acc: 0.9744\n",
      "Epoch 78/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1781 - val_acc: 0.9744\n",
      "Epoch 79/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1782 - val_acc: 0.9744\n",
      "Epoch 80/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1781 - val_acc: 0.9744\n",
      "Epoch 81/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1787 - val_acc: 0.9744\n",
      "Epoch 82/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1780 - val_acc: 0.9744\n",
      "Epoch 83/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1784 - val_acc: 0.9744\n",
      "Epoch 84/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1780 - val_acc: 0.9744\n",
      "Epoch 85/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1770 - val_acc: 0.9744\n",
      "Epoch 86/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1782 - val_acc: 0.9744\n",
      "Epoch 87/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1773 - val_acc: 0.9744\n",
      "Epoch 88/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1764 - val_acc: 0.9744\n",
      "Epoch 89/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1772 - val_acc: 0.9744\n",
      "Epoch 90/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1778 - val_acc: 0.9744\n",
      "Epoch 91/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1784 - val_acc: 0.9744\n",
      "Epoch 92/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1778 - val_acc: 0.9744\n",
      "Epoch 93/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1774 - val_acc: 0.9744\n",
      "Epoch 94/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1780 - val_acc: 0.9744\n",
      "Epoch 95/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1777 - val_acc: 0.9744\n",
      "Epoch 96/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1778 - val_acc: 0.9744\n",
      "Epoch 97/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1778 - val_acc: 0.9744\n",
      "Epoch 98/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1776 - val_acc: 0.9744\n",
      "Epoch 99/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1783 - val_acc: 0.9744\n",
      "Epoch 100/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1780 - val_acc: 0.9744\n",
      "78/78 [==============================] - 0s 347us/step\n",
      "Score for fold 7: loss of 2.1772637244982596; acc of 97.43589697740018%\n",
      "Train on 702 samples, validate on 78 samples\n",
      "Epoch 1/100\n",
      "702/702 [==============================] - 9s 13ms/step - loss: 2.4903 - acc: 0.2692 - val_loss: 2.4337 - val_acc: 0.5000\n",
      "Epoch 2/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.3149 - acc: 0.5655 - val_loss: 2.2857 - val_acc: 0.6667\n",
      "Epoch 3/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2665 - acc: 0.7108 - val_loss: 2.2517 - val_acc: 0.6410\n",
      "Epoch 4/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2267 - acc: 0.8063 - val_loss: 2.2234 - val_acc: 0.8462\n",
      "Epoch 5/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2051 - acc: 0.8875 - val_loss: 2.2243 - val_acc: 0.7821\n",
      "Epoch 6/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1960 - acc: 0.9074 - val_loss: 2.2514 - val_acc: 0.6795\n",
      "Epoch 7/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1934 - acc: 0.9231 - val_loss: 2.2081 - val_acc: 0.8846\n",
      "Epoch 8/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1809 - acc: 0.9473 - val_loss: 2.2035 - val_acc: 0.8846\n",
      "Epoch 9/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1820 - acc: 0.9658 - val_loss: 2.1937 - val_acc: 0.8590\n",
      "Epoch 10/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1821 - acc: 0.9615 - val_loss: 2.1837 - val_acc: 0.9359\n",
      "Epoch 11/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1772 - acc: 0.9744 - val_loss: 2.1899 - val_acc: 0.8974\n",
      "Epoch 12/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1765 - acc: 0.9801 - val_loss: 2.1940 - val_acc: 0.9231\n",
      "Epoch 13/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1752 - acc: 0.9758 - val_loss: 2.1787 - val_acc: 0.9744\n",
      "Epoch 14/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1722 - acc: 0.9858 - val_loss: 2.1799 - val_acc: 0.9487\n",
      "Epoch 15/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1772 - acc: 0.9701 - val_loss: 2.1829 - val_acc: 0.9615\n",
      "Epoch 16/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1722 - acc: 0.9843 - val_loss: 2.2145 - val_acc: 0.9103\n",
      "Epoch 17/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1698 - acc: 0.9972 - val_loss: 2.1876 - val_acc: 0.9103\n",
      "Epoch 18/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1720 - acc: 0.9915 - val_loss: 2.1837 - val_acc: 0.9615\n",
      "Epoch 19/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1715 - acc: 0.9900 - val_loss: 2.1886 - val_acc: 0.9231\n",
      "Epoch 20/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1712 - acc: 0.9843 - val_loss: 2.1916 - val_acc: 0.9487\n",
      "Epoch 21/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1690 - acc: 0.9929 - val_loss: 2.1831 - val_acc: 0.9615\n",
      "Epoch 22/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1722 - acc: 0.9886 - val_loss: 2.1758 - val_acc: 0.9744\n",
      "Epoch 23/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1690 - acc: 0.9886 - val_loss: 2.1829 - val_acc: 0.9359\n",
      "Epoch 24/100\n",
      "702/702 [==============================] - 3s 5ms/step - loss: 2.1730 - acc: 0.9900 - val_loss: 2.1785 - val_acc: 0.9615\n",
      "Epoch 25/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1678 - acc: 0.9972 - val_loss: 2.1743 - val_acc: 0.9872\n",
      "Epoch 26/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1679 - acc: 0.9972 - val_loss: 2.1789 - val_acc: 0.9744\n",
      "Epoch 27/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1681 - acc: 0.9943 - val_loss: 2.1805 - val_acc: 0.9744\n",
      "Epoch 28/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1675 - acc: 0.9957 - val_loss: 2.1755 - val_acc: 0.9615\n",
      "Epoch 29/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1673 - acc: 0.9986 - val_loss: 2.1807 - val_acc: 0.9744\n",
      "Epoch 30/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1674 - acc: 0.9972 - val_loss: 2.1807 - val_acc: 0.9744\n",
      "Epoch 31/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1671 - acc: 0.9972 - val_loss: 2.1807 - val_acc: 0.9615\n",
      "Epoch 32/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1671 - acc: 0.9986 - val_loss: 2.1800 - val_acc: 0.9744\n",
      "Epoch 33/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1669 - acc: 0.9986 - val_loss: 2.1795 - val_acc: 0.9744\n",
      "Epoch 34/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1667 - acc: 1.0000 - val_loss: 2.1787 - val_acc: 0.9744\n",
      "Epoch 35/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1678 - acc: 0.9957 - val_loss: 2.1731 - val_acc: 0.9872\n",
      "Epoch 36/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1668 - acc: 1.0000 - val_loss: 2.1749 - val_acc: 0.9744\n",
      "Epoch 37/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1773 - val_acc: 0.9744\n",
      "Epoch 38/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1666 - acc: 0.9986 - val_loss: 2.1754 - val_acc: 0.9744\n",
      "Epoch 39/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1666 - acc: 0.9986 - val_loss: 2.1760 - val_acc: 0.9744\n",
      "Epoch 40/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1803 - val_acc: 0.9487\n",
      "Epoch 41/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1666 - acc: 0.9986 - val_loss: 2.1765 - val_acc: 0.9744\n",
      "Epoch 42/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1756 - val_acc: 0.9615\n",
      "Epoch 43/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1668 - acc: 0.9986 - val_loss: 2.1723 - val_acc: 0.9615\n",
      "Epoch 44/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1735 - val_acc: 0.9744\n",
      "Epoch 45/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 0.9986 - val_loss: 2.1735 - val_acc: 0.9615\n",
      "Epoch 46/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1732 - val_acc: 0.9744\n",
      "Epoch 47/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1725 - val_acc: 0.9744\n",
      "Epoch 48/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 0.9986 - val_loss: 2.1803 - val_acc: 0.9615\n",
      "Epoch 49/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1731 - val_acc: 0.9615\n",
      "Epoch 50/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1768 - val_acc: 0.9744\n",
      "Epoch 51/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1769 - val_acc: 0.9744\n",
      "Epoch 52/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1760 - val_acc: 0.9744\n",
      "Epoch 53/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 0.9986 - val_loss: 2.1803 - val_acc: 0.9615\n",
      "Epoch 54/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1728 - val_acc: 0.9744\n",
      "Epoch 55/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1743 - val_acc: 0.9744\n",
      "Epoch 56/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1726 - val_acc: 0.9744\n",
      "Epoch 57/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1744 - val_acc: 0.9615\n",
      "Epoch 58/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1740 - val_acc: 0.9744\n",
      "Epoch 59/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1727 - val_acc: 0.9744\n",
      "Epoch 60/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1731 - val_acc: 0.9744\n",
      "Epoch 61/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1736 - val_acc: 0.9744\n",
      "Epoch 62/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1742 - val_acc: 0.9744\n",
      "Epoch 63/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1796 - val_acc: 0.9615\n",
      "Epoch 64/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1760 - val_acc: 0.9744\n",
      "Epoch 65/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1750 - val_acc: 0.9615\n",
      "Epoch 66/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1728 - val_acc: 0.9744\n",
      "Epoch 67/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1742 - val_acc: 0.9744\n",
      "Epoch 68/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1732 - val_acc: 0.9744\n",
      "Epoch 69/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1752 - val_acc: 0.9744\n",
      "Epoch 70/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1784 - val_acc: 0.9744\n",
      "Epoch 71/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1727 - val_acc: 0.9744\n",
      "Epoch 72/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1732 - val_acc: 0.9744\n",
      "Epoch 73/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1750 - val_acc: 0.9744\n",
      "Epoch 74/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1730 - val_acc: 0.9744\n",
      "Epoch 75/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1749 - val_acc: 0.9744\n",
      "Epoch 76/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1725 - val_acc: 0.9744\n",
      "Epoch 77/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1726 - val_acc: 0.9744\n",
      "Epoch 78/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1738 - val_acc: 0.9744\n",
      "Epoch 79/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1739 - val_acc: 0.9744\n",
      "Epoch 80/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1741 - val_acc: 0.9744\n",
      "Epoch 81/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1716 - val_acc: 0.9744\n",
      "Epoch 82/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1738 - val_acc: 0.9744\n",
      "Epoch 83/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1733 - val_acc: 0.9744\n",
      "Epoch 84/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1732 - val_acc: 0.9744\n",
      "Epoch 85/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1738 - val_acc: 0.9744\n",
      "Epoch 86/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1729 - val_acc: 0.9744\n",
      "Epoch 87/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1738 - val_acc: 0.9744\n",
      "Epoch 88/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1737 - val_acc: 0.9744\n",
      "Epoch 89/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1746 - val_acc: 0.9744\n",
      "Epoch 90/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1739 - val_acc: 0.9744\n",
      "Epoch 91/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1738 - val_acc: 0.9744\n",
      "Epoch 92/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1734 - val_acc: 0.9744\n",
      "Epoch 93/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1745 - val_acc: 0.9744\n",
      "Epoch 94/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1741 - val_acc: 0.9744\n",
      "Epoch 95/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1748 - val_acc: 0.9744\n",
      "Epoch 96/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1744 - val_acc: 0.9744\n",
      "Epoch 97/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1737 - val_acc: 0.9744\n",
      "Epoch 98/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1737 - val_acc: 0.9744\n",
      "Epoch 99/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1744 - val_acc: 0.9744\n",
      "Epoch 100/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1744 - val_acc: 0.9744\n",
      "78/78 [==============================] - 0s 341us/step\n",
      "Score for fold 8: loss of 2.1743046320401707; acc of 97.43589743589743%\n",
      "Train on 702 samples, validate on 78 samples\n",
      "Epoch 1/100\n",
      "702/702 [==============================] - 10s 14ms/step - loss: 2.5078 - acc: 0.2536 - val_loss: 2.4093 - val_acc: 0.4103\n",
      "Epoch 2/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.3063 - acc: 0.5726 - val_loss: 2.2632 - val_acc: 0.7308\n",
      "Epoch 3/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2458 - acc: 0.7507 - val_loss: 2.2405 - val_acc: 0.6538\n",
      "Epoch 4/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2157 - acc: 0.8177 - val_loss: 2.2520 - val_acc: 0.7564\n",
      "Epoch 5/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1964 - acc: 0.8903 - val_loss: 2.2295 - val_acc: 0.8846\n",
      "Epoch 6/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1921 - acc: 0.9217 - val_loss: 2.2013 - val_acc: 0.9359\n",
      "Epoch 7/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1941 - acc: 0.9217 - val_loss: 2.1901 - val_acc: 0.9487\n",
      "Epoch 8/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1778 - acc: 0.9801 - val_loss: 2.2144 - val_acc: 0.8718\n",
      "Epoch 9/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1799 - acc: 0.9615 - val_loss: 2.1885 - val_acc: 0.9744\n",
      "Epoch 10/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1823 - acc: 0.9544 - val_loss: 2.1871 - val_acc: 0.9615\n",
      "Epoch 11/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1742 - acc: 0.9786 - val_loss: 2.1872 - val_acc: 0.9615\n",
      "Epoch 12/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1758 - acc: 0.9744 - val_loss: 2.1844 - val_acc: 0.8974\n",
      "Epoch 13/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1715 - acc: 0.9829 - val_loss: 2.2060 - val_acc: 0.9359\n",
      "Epoch 14/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1739 - acc: 0.9829 - val_loss: 2.1859 - val_acc: 0.9359\n",
      "Epoch 15/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1708 - acc: 0.9900 - val_loss: 2.1834 - val_acc: 0.9359\n",
      "Epoch 16/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1703 - acc: 0.9858 - val_loss: 2.1815 - val_acc: 0.9615\n",
      "Epoch 17/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1711 - acc: 0.9957 - val_loss: 2.1988 - val_acc: 0.9487\n",
      "Epoch 18/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1721 - acc: 0.9943 - val_loss: 2.1809 - val_acc: 0.9744\n",
      "Epoch 19/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1698 - acc: 0.9900 - val_loss: 2.1830 - val_acc: 0.9744\n",
      "Epoch 20/100\n",
      "702/702 [==============================] - 4s 5ms/step - loss: 2.1679 - acc: 0.9986 - val_loss: 2.1816 - val_acc: 0.9744\n",
      "Epoch 21/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1674 - acc: 0.9986 - val_loss: 2.1799 - val_acc: 0.9744\n",
      "Epoch 22/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1681 - acc: 0.9986 - val_loss: 2.1788 - val_acc: 0.9744\n",
      "Epoch 23/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1678 - acc: 0.9957 - val_loss: 2.1795 - val_acc: 0.9615\n",
      "Epoch 24/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1669 - acc: 1.0000 - val_loss: 2.1794 - val_acc: 0.9615\n",
      "Epoch 25/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1689 - acc: 0.9972 - val_loss: 2.1804 - val_acc: 0.9744\n",
      "Epoch 26/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1668 - acc: 1.0000 - val_loss: 2.1811 - val_acc: 0.9615\n",
      "Epoch 27/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1670 - acc: 0.9986 - val_loss: 2.1802 - val_acc: 0.9615\n",
      "Epoch 28/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1674 - acc: 0.9986 - val_loss: 2.1789 - val_acc: 0.9744\n",
      "Epoch 29/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1669 - acc: 0.9986 - val_loss: 2.1797 - val_acc: 0.9744\n",
      "Epoch 30/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1666 - acc: 1.0000 - val_loss: 2.1791 - val_acc: 0.9744\n",
      "Epoch 31/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1791 - val_acc: 0.9615\n",
      "Epoch 32/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1788 - val_acc: 0.9744\n",
      "Epoch 33/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1791 - val_acc: 0.9744\n",
      "Epoch 34/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1666 - acc: 1.0000 - val_loss: 2.1812 - val_acc: 0.9615\n",
      "Epoch 35/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1667 - acc: 1.0000 - val_loss: 2.1790 - val_acc: 0.9744\n",
      "Epoch 36/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1793 - val_acc: 0.9744\n",
      "Epoch 37/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1792 - val_acc: 0.9744\n",
      "Epoch 38/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1791 - val_acc: 0.9615\n",
      "Epoch 39/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1786 - val_acc: 0.9744\n",
      "Epoch 40/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1796 - val_acc: 0.9744\n",
      "Epoch 41/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1785 - val_acc: 0.9744\n",
      "Epoch 42/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1788 - val_acc: 0.9744\n",
      "Epoch 43/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1791 - val_acc: 0.9744\n",
      "Epoch 44/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1781 - val_acc: 0.9744\n",
      "Epoch 45/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1789 - val_acc: 0.9744\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1789 - val_acc: 0.9744\n",
      "Epoch 47/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1788 - val_acc: 0.9744\n",
      "Epoch 48/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1791 - val_acc: 0.9744\n",
      "Epoch 49/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1791 - val_acc: 0.9744\n",
      "Epoch 50/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1789 - val_acc: 0.9744\n",
      "Epoch 51/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1789 - val_acc: 0.9744\n",
      "Epoch 52/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1787 - val_acc: 0.9744\n",
      "Epoch 53/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1789 - val_acc: 0.9744\n",
      "Epoch 54/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1789 - val_acc: 0.9744\n",
      "Epoch 55/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1791 - val_acc: 0.9744\n",
      "Epoch 56/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1790 - val_acc: 0.9744\n",
      "Epoch 57/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1785 - val_acc: 0.9744\n",
      "Epoch 58/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1787 - val_acc: 0.9744\n",
      "Epoch 59/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1790 - val_acc: 0.9744\n",
      "Epoch 60/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1788 - val_acc: 0.9744\n",
      "Epoch 61/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1791 - val_acc: 0.9744\n",
      "Epoch 62/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1791 - val_acc: 0.9744\n",
      "Epoch 63/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1792 - val_acc: 0.9744\n",
      "Epoch 64/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1793 - val_acc: 0.9744\n",
      "Epoch 65/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1788 - val_acc: 0.9744\n",
      "Epoch 66/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1791 - val_acc: 0.9744\n",
      "Epoch 67/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1786 - val_acc: 0.9744\n",
      "Epoch 68/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1790 - val_acc: 0.9744\n",
      "Epoch 69/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1791 - val_acc: 0.9744\n",
      "Epoch 70/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1789 - val_acc: 0.9744\n",
      "Epoch 71/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1788 - val_acc: 0.9744\n",
      "Epoch 72/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1790 - val_acc: 0.9744\n",
      "Epoch 73/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1787 - val_acc: 0.9744\n",
      "Epoch 74/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1789 - val_acc: 0.9744\n",
      "Epoch 75/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1786 - val_acc: 0.9744\n",
      "Epoch 76/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1786 - val_acc: 0.9744\n",
      "Epoch 77/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1788 - val_acc: 0.9744\n",
      "Epoch 78/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1788 - val_acc: 0.9744\n",
      "Epoch 79/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1791 - val_acc: 0.9744\n",
      "Epoch 80/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1787 - val_acc: 0.9744\n",
      "Epoch 81/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1789 - val_acc: 0.9744\n",
      "Epoch 82/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1788 - val_acc: 0.9744\n",
      "Epoch 83/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1789 - val_acc: 0.9744\n",
      "Epoch 84/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1789 - val_acc: 0.9744\n",
      "Epoch 85/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1787 - val_acc: 0.9744\n",
      "Epoch 86/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1789 - val_acc: 0.9744\n",
      "Epoch 87/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1789 - val_acc: 0.9744\n",
      "Epoch 88/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1789 - val_acc: 0.9744\n",
      "Epoch 89/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1788 - val_acc: 0.9744\n",
      "Epoch 90/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1788 - val_acc: 0.9744\n",
      "Epoch 91/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1788 - val_acc: 0.9744\n",
      "Epoch 92/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1787 - val_acc: 0.9744\n",
      "Epoch 93/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1787 - val_acc: 0.9744\n",
      "Epoch 94/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1787 - val_acc: 0.9744\n",
      "Epoch 95/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1788 - val_acc: 0.9744\n",
      "Epoch 96/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1787 - val_acc: 0.9744\n",
      "Epoch 97/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1786 - val_acc: 0.9744\n",
      "Epoch 98/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1786 - val_acc: 0.9744\n",
      "Epoch 99/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1787 - val_acc: 0.9744\n",
      "Epoch 100/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1788 - val_acc: 0.9744\n",
      "78/78 [==============================] - 0s 327us/step\n",
      "Score for fold 9: loss of 2.178708791732788; acc of 97.43589697740018%\n",
      "Train on 702 samples, validate on 78 samples\n",
      "Epoch 1/100\n",
      "702/702 [==============================] - 10s 14ms/step - loss: 2.5015 - acc: 0.2521 - val_loss: 2.4136 - val_acc: 0.3718\n",
      "Epoch 2/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.3204 - acc: 0.5584 - val_loss: 2.3049 - val_acc: 0.5385\n",
      "Epoch 3/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2488 - acc: 0.7194 - val_loss: 2.2841 - val_acc: 0.6026\n",
      "Epoch 4/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2263 - acc: 0.7906 - val_loss: 2.2011 - val_acc: 0.8974\n",
      "Epoch 5/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2082 - acc: 0.8405 - val_loss: 2.2394 - val_acc: 0.8333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2024 - acc: 0.9003 - val_loss: 2.2809 - val_acc: 0.7949\n",
      "Epoch 7/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1914 - acc: 0.9373 - val_loss: 2.2147 - val_acc: 0.9231\n",
      "Epoch 8/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1957 - acc: 0.9160 - val_loss: 2.1871 - val_acc: 0.8974\n",
      "Epoch 9/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1822 - acc: 0.9615 - val_loss: 2.2079 - val_acc: 0.8590\n",
      "Epoch 10/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1782 - acc: 0.9829 - val_loss: 2.1795 - val_acc: 0.9487\n",
      "Epoch 11/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1817 - acc: 0.9687 - val_loss: 2.1826 - val_acc: 0.8846\n",
      "Epoch 12/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1771 - acc: 0.9729 - val_loss: 2.1800 - val_acc: 0.9487\n",
      "Epoch 13/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1780 - acc: 0.9758 - val_loss: 2.1759 - val_acc: 0.9487\n",
      "Epoch 14/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1734 - acc: 0.9915 - val_loss: 2.1755 - val_acc: 0.9359\n",
      "Epoch 15/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1721 - acc: 0.9929 - val_loss: 2.2637 - val_acc: 0.7051\n",
      "Epoch 16/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1746 - acc: 0.9843 - val_loss: 2.1833 - val_acc: 0.9487\n",
      "Epoch 17/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1727 - acc: 0.9843 - val_loss: 2.1810 - val_acc: 0.8718\n",
      "Epoch 18/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1713 - acc: 0.9929 - val_loss: 2.1790 - val_acc: 0.9231\n",
      "Epoch 19/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1738 - acc: 0.9858 - val_loss: 2.1725 - val_acc: 0.9615\n",
      "Epoch 20/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1705 - acc: 0.9900 - val_loss: 2.1871 - val_acc: 0.9231\n",
      "Epoch 21/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1703 - acc: 0.9957 - val_loss: 2.1728 - val_acc: 0.9744\n",
      "Epoch 22/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1718 - acc: 0.9929 - val_loss: 2.1735 - val_acc: 0.9615\n",
      "Epoch 23/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1699 - acc: 0.9943 - val_loss: 2.1770 - val_acc: 0.9359\n",
      "Epoch 24/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1686 - acc: 0.9972 - val_loss: 2.1776 - val_acc: 0.9615\n",
      "Epoch 25/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1705 - acc: 0.9943 - val_loss: 2.1733 - val_acc: 0.9615\n",
      "Epoch 26/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1686 - acc: 0.9986 - val_loss: 2.1787 - val_acc: 0.8974\n",
      "Epoch 27/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1695 - acc: 0.9943 - val_loss: 2.1716 - val_acc: 0.9872\n",
      "Epoch 28/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1720 - acc: 0.9900 - val_loss: 2.1748 - val_acc: 0.9744\n",
      "Epoch 29/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1678 - acc: 0.9957 - val_loss: 2.1706 - val_acc: 0.9744\n",
      "Epoch 30/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1679 - acc: 0.9986 - val_loss: 2.1721 - val_acc: 0.9615\n",
      "Epoch 31/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1675 - acc: 1.0000 - val_loss: 2.1745 - val_acc: 0.9487\n",
      "Epoch 32/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1686 - acc: 0.9929 - val_loss: 2.1732 - val_acc: 0.9615\n",
      "Epoch 33/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1672 - acc: 1.0000 - val_loss: 2.1731 - val_acc: 0.9487\n",
      "Epoch 34/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1674 - acc: 0.9986 - val_loss: 2.1787 - val_acc: 0.9231\n",
      "Epoch 35/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1672 - acc: 0.9957 - val_loss: 2.1723 - val_acc: 0.9744\n",
      "Epoch 36/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1674 - acc: 1.0000 - val_loss: 2.1817 - val_acc: 0.9231\n",
      "Epoch 37/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1674 - acc: 0.9986 - val_loss: 2.1739 - val_acc: 0.9487\n",
      "Epoch 38/100\n",
      "702/702 [==============================] - 4s 5ms/step - loss: 2.1679 - acc: 0.9943 - val_loss: 2.1701 - val_acc: 0.9872\n",
      "Epoch 39/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1666 - acc: 1.0000 - val_loss: 2.1710 - val_acc: 0.9872\n",
      "Epoch 40/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1667 - acc: 1.0000 - val_loss: 2.1723 - val_acc: 0.9487\n",
      "Epoch 41/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1718 - val_acc: 0.9615\n",
      "Epoch 42/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1667 - acc: 1.0000 - val_loss: 2.1704 - val_acc: 0.9744\n",
      "Epoch 43/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1680 - acc: 0.9972 - val_loss: 2.1701 - val_acc: 0.9744\n",
      "Epoch 44/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1723 - val_acc: 0.9744\n",
      "Epoch 45/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1667 - acc: 1.0000 - val_loss: 2.1695 - val_acc: 0.9744\n",
      "Epoch 46/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1700 - val_acc: 0.9872\n",
      "Epoch 47/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1666 - acc: 1.0000 - val_loss: 2.1698 - val_acc: 0.9872\n",
      "Epoch 48/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1698 - val_acc: 1.0000\n",
      "Epoch 49/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1731 - val_acc: 0.9744\n",
      "Epoch 50/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1666 - acc: 1.0000 - val_loss: 2.1698 - val_acc: 0.9872\n",
      "Epoch 51/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1667 - acc: 1.0000 - val_loss: 2.1696 - val_acc: 0.9872\n",
      "Epoch 52/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1691 - val_acc: 0.9872\n",
      "Epoch 53/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1672 - acc: 0.9986 - val_loss: 2.1710 - val_acc: 0.9744\n",
      "Epoch 54/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1701 - val_acc: 0.9744\n",
      "Epoch 55/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1717 - val_acc: 0.9615\n",
      "Epoch 56/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1670 - acc: 0.9986 - val_loss: 2.1682 - val_acc: 1.0000\n",
      "Epoch 57/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1699 - val_acc: 0.9744\n",
      "Epoch 58/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1693 - val_acc: 0.9744\n",
      "Epoch 59/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1712 - val_acc: 0.9872\n",
      "Epoch 60/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1686 - val_acc: 1.0000\n",
      "Epoch 61/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1683 - val_acc: 0.9872\n",
      "Epoch 62/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1733 - val_acc: 0.9744\n",
      "Epoch 63/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1666 - acc: 1.0000 - val_loss: 2.1686 - val_acc: 0.9872\n",
      "Epoch 64/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1685 - val_acc: 0.9872\n",
      "Epoch 65/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1697 - val_acc: 0.9744\n",
      "Epoch 66/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1718 - val_acc: 0.9744\n",
      "Epoch 67/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1693 - val_acc: 0.9872\n",
      "Epoch 68/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1683 - val_acc: 1.0000\n",
      "Epoch 69/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1690 - val_acc: 1.0000\n",
      "Epoch 70/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 0.9986 - val_loss: 2.1693 - val_acc: 0.9744\n",
      "Epoch 71/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1686 - val_acc: 0.9872\n",
      "Epoch 72/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1692 - val_acc: 0.9744\n",
      "Epoch 73/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1683 - val_acc: 0.9872\n",
      "Epoch 74/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1695 - val_acc: 0.9744\n",
      "Epoch 75/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1680 - val_acc: 1.0000\n",
      "Epoch 76/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1684 - val_acc: 0.9872\n",
      "Epoch 77/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1683 - val_acc: 1.0000\n",
      "Epoch 78/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1682 - val_acc: 1.0000\n",
      "Epoch 79/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 0.9986 - val_loss: 2.1689 - val_acc: 0.9744\n",
      "Epoch 80/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1690 - val_acc: 0.9744\n",
      "Epoch 81/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1683 - val_acc: 1.0000\n",
      "Epoch 82/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1682 - val_acc: 1.0000\n",
      "Epoch 83/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1687 - val_acc: 0.9744\n",
      "Epoch 84/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1684 - val_acc: 1.0000\n",
      "Epoch 85/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1683 - val_acc: 1.0000\n",
      "Epoch 86/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1687 - val_acc: 0.9872\n",
      "Epoch 87/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1689 - val_acc: 0.9872\n",
      "Epoch 88/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1686 - val_acc: 0.9744\n",
      "Epoch 89/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1689 - val_acc: 0.9872\n",
      "Epoch 90/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1691 - val_acc: 0.9744\n",
      "Epoch 91/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1683 - val_acc: 0.9872\n",
      "Epoch 92/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1684 - val_acc: 1.0000\n",
      "Epoch 93/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1686 - val_acc: 0.9744\n",
      "Epoch 94/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1683 - val_acc: 1.0000\n",
      "Epoch 95/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1684 - val_acc: 0.9744\n",
      "Epoch 96/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1685 - val_acc: 1.0000\n",
      "Epoch 97/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1678 - val_acc: 1.0000\n",
      "Epoch 98/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1681 - val_acc: 1.0000\n",
      "Epoch 99/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1690 - val_acc: 1.0000\n",
      "Epoch 100/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1683 - val_acc: 0.9872\n",
      "78/78 [==============================] - 0s 312us/step\n",
      "Score for fold 10: loss of 2.1687303750942917; acc of 98.71794871794873%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1296x1296 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m 4       \u001b[0m | \u001b[0m 97.82   \u001b[0m | \u001b[0m 0.9184  \u001b[0m | \u001b[0m 7.419   \u001b[0m |\n",
      "Train on 702 samples, validate on 78 samples\n",
      "Epoch 1/100\n",
      "702/702 [==============================] - 10s 14ms/step - loss: 2.4902 - acc: 0.2949 - val_loss: 2.5180 - val_acc: 0.2564\n",
      "Epoch 2/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.3067 - acc: 0.5698 - val_loss: 2.2972 - val_acc: 0.5769\n",
      "Epoch 3/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2453 - acc: 0.7336 - val_loss: 2.2551 - val_acc: 0.7436\n",
      "Epoch 4/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2267 - acc: 0.8348 - val_loss: 2.2183 - val_acc: 0.8205\n",
      "Epoch 5/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2131 - acc: 0.8632 - val_loss: 2.2035 - val_acc: 0.8462\n",
      "Epoch 6/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1990 - acc: 0.9031 - val_loss: 2.2032 - val_acc: 0.7821\n",
      "Epoch 7/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1874 - acc: 0.9402 - val_loss: 2.2086 - val_acc: 0.8077\n",
      "Epoch 8/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1880 - acc: 0.9416 - val_loss: 2.1945 - val_acc: 0.8846\n",
      "Epoch 9/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1845 - acc: 0.9444 - val_loss: 2.2038 - val_acc: 0.8846\n",
      "Epoch 10/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1791 - acc: 0.9615 - val_loss: 2.1978 - val_acc: 0.9103\n",
      "Epoch 11/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1756 - acc: 0.9744 - val_loss: 2.1850 - val_acc: 0.9103\n",
      "Epoch 12/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1740 - acc: 0.9801 - val_loss: 2.1975 - val_acc: 0.8846\n",
      "Epoch 13/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1775 - acc: 0.9658 - val_loss: 2.1957 - val_acc: 0.9103\n",
      "Epoch 14/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1733 - acc: 0.9801 - val_loss: 2.2240 - val_acc: 0.8590\n",
      "Epoch 15/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1738 - acc: 0.9872 - val_loss: 2.1830 - val_acc: 0.9487\n",
      "Epoch 16/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1721 - acc: 0.9872 - val_loss: 2.1996 - val_acc: 0.9487\n",
      "Epoch 17/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1752 - acc: 0.9815 - val_loss: 2.1853 - val_acc: 0.9231\n",
      "Epoch 18/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1705 - acc: 0.9943 - val_loss: 2.1931 - val_acc: 0.9103\n",
      "Epoch 19/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1719 - acc: 0.9900 - val_loss: 2.1897 - val_acc: 0.8590\n",
      "Epoch 20/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1717 - acc: 0.9872 - val_loss: 2.1878 - val_acc: 0.9359\n",
      "Epoch 21/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1690 - acc: 0.9972 - val_loss: 2.1815 - val_acc: 0.9487\n",
      "Epoch 22/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1692 - acc: 0.9972 - val_loss: 2.2009 - val_acc: 0.9103\n",
      "Epoch 23/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1699 - acc: 0.9943 - val_loss: 2.1858 - val_acc: 0.9359\n",
      "Epoch 24/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1689 - acc: 0.9986 - val_loss: 2.1812 - val_acc: 0.9359\n",
      "Epoch 25/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1722 - acc: 0.9815 - val_loss: 2.1809 - val_acc: 0.9359\n",
      "Epoch 26/100\n",
      "702/702 [==============================] - 4s 5ms/step - loss: 2.1701 - acc: 0.9929 - val_loss: 2.1793 - val_acc: 0.9487\n",
      "Epoch 27/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1674 - acc: 0.9986 - val_loss: 2.1834 - val_acc: 0.9487\n",
      "Epoch 28/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1674 - acc: 0.9986 - val_loss: 2.1794 - val_acc: 0.9359\n",
      "Epoch 29/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1676 - acc: 0.9943 - val_loss: 2.1814 - val_acc: 0.9231\n",
      "Epoch 30/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1674 - acc: 0.9986 - val_loss: 2.1802 - val_acc: 0.9487\n",
      "Epoch 31/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1677 - acc: 0.9986 - val_loss: 2.1808 - val_acc: 0.9359\n",
      "Epoch 32/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1670 - acc: 0.9986 - val_loss: 2.1771 - val_acc: 0.9487\n",
      "Epoch 33/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1672 - acc: 0.9986 - val_loss: 2.1816 - val_acc: 0.9359\n",
      "Epoch 34/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1674 - acc: 0.9957 - val_loss: 2.1777 - val_acc: 0.9615\n",
      "Epoch 35/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1668 - acc: 1.0000 - val_loss: 2.1770 - val_acc: 0.9487\n",
      "Epoch 36/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1668 - acc: 1.0000 - val_loss: 2.1785 - val_acc: 0.9487\n",
      "Epoch 37/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1671 - acc: 1.0000 - val_loss: 2.1785 - val_acc: 0.9487\n",
      "Epoch 38/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1667 - acc: 1.0000 - val_loss: 2.1832 - val_acc: 0.9359\n",
      "Epoch 39/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1669 - acc: 0.9986 - val_loss: 2.1784 - val_acc: 0.9487\n",
      "Epoch 40/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1667 - acc: 1.0000 - val_loss: 2.1789 - val_acc: 0.9359\n",
      "Epoch 41/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1669 - acc: 1.0000 - val_loss: 2.1806 - val_acc: 0.9487\n",
      "Epoch 42/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1669 - acc: 0.9986 - val_loss: 2.1786 - val_acc: 0.9615\n",
      "Epoch 43/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1666 - acc: 1.0000 - val_loss: 2.1767 - val_acc: 0.9487\n",
      "Epoch 44/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1668 - acc: 1.0000 - val_loss: 2.1795 - val_acc: 0.9487\n",
      "Epoch 45/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1669 - acc: 0.9986 - val_loss: 2.1794 - val_acc: 0.9487\n",
      "Epoch 46/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1666 - acc: 1.0000 - val_loss: 2.1814 - val_acc: 0.9487\n",
      "Epoch 47/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1785 - val_acc: 0.9487\n",
      "Epoch 48/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1768 - val_acc: 0.9487\n",
      "Epoch 49/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1779 - val_acc: 0.9487\n",
      "Epoch 50/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1790 - val_acc: 0.9487\n",
      "Epoch 51/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1783 - val_acc: 0.9615\n",
      "Epoch 52/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1666 - acc: 0.9986 - val_loss: 2.1760 - val_acc: 0.9615\n",
      "Epoch 53/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1759 - val_acc: 0.9487\n",
      "Epoch 54/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1775 - val_acc: 0.9615\n",
      "Epoch 55/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1766 - val_acc: 0.9615\n",
      "Epoch 56/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1763 - val_acc: 0.9615\n",
      "Epoch 57/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1762 - val_acc: 0.9487\n",
      "Epoch 58/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1765 - val_acc: 0.9615\n",
      "Epoch 59/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1764 - val_acc: 0.9487\n",
      "Epoch 60/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1761 - val_acc: 0.9615\n",
      "Epoch 61/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1776 - val_acc: 0.9487\n",
      "Epoch 62/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1766 - val_acc: 0.9359\n",
      "Epoch 63/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1783 - val_acc: 0.9487\n",
      "Epoch 64/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1772 - val_acc: 0.9487\n",
      "Epoch 65/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1770 - val_acc: 0.9487\n",
      "Epoch 66/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1775 - val_acc: 0.9487\n",
      "Epoch 67/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1769 - val_acc: 0.9487\n",
      "Epoch 68/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1761 - val_acc: 0.9487\n",
      "Epoch 69/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1769 - val_acc: 0.9487\n",
      "Epoch 70/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1770 - val_acc: 0.9487\n",
      "Epoch 71/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1756 - val_acc: 0.9487\n",
      "Epoch 72/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1768 - val_acc: 0.9487\n",
      "Epoch 73/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1757 - val_acc: 0.9487\n",
      "Epoch 74/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1752 - val_acc: 0.9615\n",
      "Epoch 75/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1759 - val_acc: 0.9615\n",
      "Epoch 76/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1756 - val_acc: 0.9487\n",
      "Epoch 77/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1758 - val_acc: 0.9615\n",
      "Epoch 78/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1757 - val_acc: 0.9487\n",
      "Epoch 79/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1759 - val_acc: 0.9615\n",
      "Epoch 80/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1758 - val_acc: 0.9615\n",
      "Epoch 81/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1762 - val_acc: 0.9615\n",
      "Epoch 82/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1762 - val_acc: 0.9487\n",
      "Epoch 83/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1758 - val_acc: 0.9487\n",
      "Epoch 84/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1754 - val_acc: 0.9487\n",
      "Epoch 85/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1755 - val_acc: 0.9487\n",
      "Epoch 86/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1753 - val_acc: 0.9487\n",
      "Epoch 87/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1757 - val_acc: 0.9359\n",
      "Epoch 88/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1755 - val_acc: 0.9487\n",
      "Epoch 89/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1754 - val_acc: 0.9487\n",
      "Epoch 90/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1755 - val_acc: 0.9487\n",
      "Epoch 91/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1755 - val_acc: 0.9487\n",
      "Epoch 92/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1756 - val_acc: 0.9487\n",
      "Epoch 93/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1754 - val_acc: 0.9487\n",
      "Epoch 94/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1755 - val_acc: 0.9487\n",
      "Epoch 95/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1752 - val_acc: 0.9487\n",
      "Epoch 96/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1754 - val_acc: 0.9487\n",
      "Epoch 97/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1754 - val_acc: 0.9487\n",
      "Epoch 98/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1754 - val_acc: 0.9487\n",
      "Epoch 99/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1754 - val_acc: 0.9487\n",
      "Epoch 100/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1754 - val_acc: 0.9487\n",
      "78/78 [==============================] - 0s 313us/step\n",
      "Score for fold 1: loss of 2.1755520747258115; acc of 94.87179487179486%\n",
      "Train on 702 samples, validate on 78 samples\n",
      "Epoch 1/100\n",
      "702/702 [==============================] - 10s 15ms/step - loss: 2.4798 - acc: 0.2707 - val_loss: 2.3499 - val_acc: 0.5256\n",
      "Epoch 2/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.3145 - acc: 0.5926 - val_loss: 2.3356 - val_acc: 0.4359\n",
      "Epoch 3/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2442 - acc: 0.7479 - val_loss: 2.2667 - val_acc: 0.6538\n",
      "Epoch 4/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2243 - acc: 0.8390 - val_loss: 2.2429 - val_acc: 0.6923\n",
      "Epoch 5/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2135 - acc: 0.8462 - val_loss: 2.2367 - val_acc: 0.6795\n",
      "Epoch 6/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1893 - acc: 0.9345 - val_loss: 2.2154 - val_acc: 0.7692\n",
      "Epoch 7/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1868 - acc: 0.9259 - val_loss: 2.2015 - val_acc: 0.8205\n",
      "Epoch 8/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1849 - acc: 0.9359 - val_loss: 2.2449 - val_acc: 0.7692\n",
      "Epoch 9/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1841 - acc: 0.9530 - val_loss: 2.1888 - val_acc: 0.8846\n",
      "Epoch 10/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1813 - acc: 0.9587 - val_loss: 2.1946 - val_acc: 0.9744\n",
      "Epoch 11/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1777 - acc: 0.9801 - val_loss: 2.1940 - val_acc: 0.7949\n",
      "Epoch 12/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1752 - acc: 0.9715 - val_loss: 2.1855 - val_acc: 0.9487\n",
      "Epoch 13/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1758 - acc: 0.9858 - val_loss: 2.1797 - val_acc: 0.9615\n",
      "Epoch 14/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1801 - acc: 0.9729 - val_loss: 2.1800 - val_acc: 0.8846\n",
      "Epoch 15/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1737 - acc: 0.9872 - val_loss: 2.1822 - val_acc: 0.9487\n",
      "Epoch 16/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1710 - acc: 0.9929 - val_loss: 2.1744 - val_acc: 0.9615\n",
      "Epoch 17/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1737 - acc: 0.9872 - val_loss: 2.1752 - val_acc: 0.9744\n",
      "Epoch 18/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1711 - acc: 0.9957 - val_loss: 2.1796 - val_acc: 0.9615\n",
      "Epoch 19/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1700 - acc: 0.9957 - val_loss: 2.1793 - val_acc: 0.9615\n",
      "Epoch 20/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1698 - acc: 0.9886 - val_loss: 2.1776 - val_acc: 0.9744\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/100\n",
      "702/702 [==============================] - 4s 5ms/step - loss: 2.1694 - acc: 0.9972 - val_loss: 2.1717 - val_acc: 0.9744\n",
      "Epoch 22/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1679 - acc: 0.9986 - val_loss: 2.1702 - val_acc: 0.9744\n",
      "Epoch 23/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1687 - acc: 0.9972 - val_loss: 2.1704 - val_acc: 1.0000\n",
      "Epoch 24/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1679 - acc: 0.9986 - val_loss: 2.1709 - val_acc: 1.0000\n",
      "Epoch 25/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1677 - acc: 0.9986 - val_loss: 2.1712 - val_acc: 0.9872\n",
      "Epoch 26/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1674 - acc: 0.9972 - val_loss: 2.1708 - val_acc: 0.9872\n",
      "Epoch 27/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1672 - acc: 0.9986 - val_loss: 2.1739 - val_acc: 0.9744\n",
      "Epoch 28/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1677 - acc: 0.9986 - val_loss: 2.1718 - val_acc: 0.9615\n",
      "Epoch 29/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1672 - acc: 1.0000 - val_loss: 2.1718 - val_acc: 0.9615\n",
      "Epoch 30/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1671 - acc: 1.0000 - val_loss: 2.1722 - val_acc: 0.9615\n",
      "Epoch 31/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1679 - acc: 0.9972 - val_loss: 2.1710 - val_acc: 1.0000\n",
      "Epoch 32/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1668 - acc: 1.0000 - val_loss: 2.1703 - val_acc: 1.0000\n",
      "Epoch 33/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1669 - acc: 1.0000 - val_loss: 2.1716 - val_acc: 0.9744\n",
      "Epoch 34/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1669 - acc: 0.9986 - val_loss: 2.1786 - val_acc: 0.9615\n",
      "Epoch 35/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1673 - acc: 0.9986 - val_loss: 2.1691 - val_acc: 1.0000\n",
      "Epoch 36/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1666 - acc: 1.0000 - val_loss: 2.1704 - val_acc: 0.9872\n",
      "Epoch 37/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1666 - acc: 1.0000 - val_loss: 2.1691 - val_acc: 0.9872\n",
      "Epoch 38/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1666 - acc: 1.0000 - val_loss: 2.1705 - val_acc: 0.9744\n",
      "Epoch 39/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1687 - val_acc: 1.0000\n",
      "Epoch 40/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1666 - acc: 1.0000 - val_loss: 2.1689 - val_acc: 1.0000\n",
      "Epoch 41/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1694 - val_acc: 1.0000\n",
      "Epoch 42/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1666 - acc: 1.0000 - val_loss: 2.1707 - val_acc: 0.9872\n",
      "Epoch 43/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1668 - acc: 0.9986 - val_loss: 2.1686 - val_acc: 1.0000\n",
      "Epoch 44/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1690 - val_acc: 1.0000\n",
      "Epoch 45/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1691 - val_acc: 1.0000\n",
      "Epoch 46/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1686 - val_acc: 1.0000\n",
      "Epoch 47/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1690 - val_acc: 1.0000\n",
      "Epoch 48/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1693 - val_acc: 1.0000\n",
      "Epoch 49/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1703 - val_acc: 1.0000\n",
      "Epoch 50/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1693 - val_acc: 1.0000\n",
      "Epoch 51/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1692 - val_acc: 1.0000\n",
      "Epoch 52/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1692 - val_acc: 1.0000\n",
      "Epoch 53/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1683 - val_acc: 1.0000\n",
      "Epoch 54/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1684 - val_acc: 1.0000\n",
      "Epoch 55/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1684 - val_acc: 1.0000\n",
      "Epoch 56/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1680 - val_acc: 1.0000\n",
      "Epoch 57/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1682 - val_acc: 1.0000\n",
      "Epoch 58/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1686 - val_acc: 1.0000\n",
      "Epoch 59/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1681 - val_acc: 1.0000\n",
      "Epoch 60/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1683 - val_acc: 1.0000\n",
      "Epoch 61/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1683 - val_acc: 1.0000\n",
      "Epoch 62/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1683 - val_acc: 1.0000\n",
      "Epoch 63/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1686 - val_acc: 1.0000\n",
      "Epoch 64/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1685 - val_acc: 1.0000\n",
      "Epoch 65/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1688 - val_acc: 1.0000\n",
      "Epoch 66/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1685 - val_acc: 1.0000\n",
      "Epoch 67/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1682 - val_acc: 1.0000\n",
      "Epoch 68/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1684 - val_acc: 1.0000\n",
      "Epoch 69/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1684 - val_acc: 1.0000\n",
      "Epoch 70/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1690 - val_acc: 1.0000\n",
      "Epoch 71/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1682 - val_acc: 1.0000\n",
      "Epoch 72/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1688 - val_acc: 1.0000\n",
      "Epoch 73/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1686 - val_acc: 1.0000\n",
      "Epoch 74/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1685 - val_acc: 1.0000\n",
      "Epoch 75/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1683 - val_acc: 1.0000\n",
      "Epoch 76/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1686 - val_acc: 1.0000\n",
      "Epoch 77/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1685 - val_acc: 1.0000\n",
      "Epoch 78/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1682 - val_acc: 1.0000\n",
      "Epoch 79/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1685 - val_acc: 1.0000\n",
      "Epoch 80/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1683 - val_acc: 1.0000\n",
      "Epoch 81/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1686 - val_acc: 1.0000\n",
      "Epoch 82/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1682 - val_acc: 1.0000\n",
      "Epoch 83/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1683 - val_acc: 1.0000\n",
      "Epoch 84/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1683 - val_acc: 1.0000\n",
      "Epoch 85/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1685 - val_acc: 1.0000\n",
      "Epoch 86/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1684 - val_acc: 0.9872\n",
      "Epoch 87/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1685 - val_acc: 1.0000\n",
      "Epoch 88/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1684 - val_acc: 1.0000\n",
      "Epoch 89/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1684 - val_acc: 1.0000\n",
      "Epoch 90/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1683 - val_acc: 1.0000\n",
      "Epoch 91/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1683 - val_acc: 1.0000\n",
      "Epoch 92/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1683 - val_acc: 1.0000\n",
      "Epoch 93/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1684 - val_acc: 1.0000\n",
      "Epoch 94/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1683 - val_acc: 1.0000\n",
      "Epoch 95/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1685 - val_acc: 1.0000\n",
      "Epoch 96/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1683 - val_acc: 1.0000\n",
      "Epoch 97/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1684 - val_acc: 1.0000\n",
      "Epoch 98/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1684 - val_acc: 1.0000\n",
      "Epoch 99/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1683 - val_acc: 1.0000\n",
      "Epoch 100/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1683 - val_acc: 1.0000\n",
      "78/78 [==============================] - 0s 325us/step\n",
      "Score for fold 2: loss of 2.168387364118527; acc of 100.0%\n",
      "Train on 702 samples, validate on 78 samples\n",
      "Epoch 1/100\n",
      "702/702 [==============================] - 10s 15ms/step - loss: 2.5077 - acc: 0.2635 - val_loss: 2.4954 - val_acc: 0.3077\n",
      "Epoch 2/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.3102 - acc: 0.5670 - val_loss: 2.2501 - val_acc: 0.7051\n",
      "Epoch 3/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2510 - acc: 0.7422 - val_loss: 2.3090 - val_acc: 0.5769\n",
      "Epoch 4/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2249 - acc: 0.8319 - val_loss: 2.2738 - val_acc: 0.7051\n",
      "Epoch 5/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2092 - acc: 0.9017 - val_loss: 2.2347 - val_acc: 0.7821\n",
      "Epoch 6/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1939 - acc: 0.8960 - val_loss: 2.1921 - val_acc: 0.9487\n",
      "Epoch 7/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1892 - acc: 0.9330 - val_loss: 2.2325 - val_acc: 0.8462\n",
      "Epoch 8/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1904 - acc: 0.9316 - val_loss: 2.1941 - val_acc: 0.9487\n",
      "Epoch 9/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1815 - acc: 0.9658 - val_loss: 2.1825 - val_acc: 0.9615\n",
      "Epoch 10/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1798 - acc: 0.9658 - val_loss: 2.1807 - val_acc: 0.9615\n",
      "Epoch 11/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1813 - acc: 0.9687 - val_loss: 2.1851 - val_acc: 0.9744\n",
      "Epoch 12/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1735 - acc: 0.9758 - val_loss: 2.2036 - val_acc: 0.8846\n",
      "Epoch 13/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1787 - acc: 0.9744 - val_loss: 2.1800 - val_acc: 0.9872\n",
      "Epoch 14/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1733 - acc: 0.9858 - val_loss: 2.1817 - val_acc: 1.0000\n",
      "Epoch 15/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1759 - acc: 0.9729 - val_loss: 2.1937 - val_acc: 0.9231\n",
      "Epoch 16/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1730 - acc: 0.9858 - val_loss: 2.1805 - val_acc: 0.9487\n",
      "Epoch 17/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1727 - acc: 0.9886 - val_loss: 2.1805 - val_acc: 0.9487\n",
      "Epoch 18/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1706 - acc: 0.9915 - val_loss: 2.1779 - val_acc: 0.9744\n",
      "Epoch 19/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1707 - acc: 0.9900 - val_loss: 2.1765 - val_acc: 0.9872\n",
      "Epoch 20/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1713 - acc: 0.9872 - val_loss: 2.1719 - val_acc: 1.0000\n",
      "Epoch 21/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1703 - acc: 0.9900 - val_loss: 2.1822 - val_acc: 0.9872\n",
      "Epoch 22/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1707 - acc: 0.9929 - val_loss: 2.1746 - val_acc: 0.9872\n",
      "Epoch 23/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1684 - acc: 0.9972 - val_loss: 2.1771 - val_acc: 0.9872\n",
      "Epoch 24/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1732 - acc: 0.9815 - val_loss: 2.1768 - val_acc: 0.9872\n",
      "Epoch 25/100\n",
      "702/702 [==============================] - 4s 5ms/step - loss: 2.1697 - acc: 0.9915 - val_loss: 2.1794 - val_acc: 0.9744\n",
      "Epoch 26/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1679 - acc: 0.9972 - val_loss: 2.1742 - val_acc: 0.9744\n",
      "Epoch 27/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1676 - acc: 0.9986 - val_loss: 2.1716 - val_acc: 1.0000\n",
      "Epoch 28/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1679 - acc: 0.9972 - val_loss: 2.1704 - val_acc: 1.0000\n",
      "Epoch 29/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1671 - acc: 0.9986 - val_loss: 2.1718 - val_acc: 1.0000\n",
      "Epoch 30/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1683 - acc: 0.9972 - val_loss: 2.1708 - val_acc: 1.0000\n",
      "Epoch 31/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1670 - acc: 0.9986 - val_loss: 2.1698 - val_acc: 1.0000\n",
      "Epoch 32/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1677 - acc: 0.9929 - val_loss: 2.1715 - val_acc: 0.9872\n",
      "Epoch 33/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1671 - acc: 1.0000 - val_loss: 2.1730 - val_acc: 0.9872\n",
      "Epoch 34/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1671 - acc: 0.9986 - val_loss: 2.1727 - val_acc: 1.0000\n",
      "Epoch 35/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1671 - acc: 0.9986 - val_loss: 2.1757 - val_acc: 0.9872\n",
      "Epoch 36/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1667 - acc: 1.0000 - val_loss: 2.1700 - val_acc: 1.0000\n",
      "Epoch 37/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1666 - acc: 1.0000 - val_loss: 2.1692 - val_acc: 1.0000\n",
      "Epoch 38/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1666 - acc: 1.0000 - val_loss: 2.1712 - val_acc: 0.9872\n",
      "Epoch 39/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1718 - val_acc: 1.0000\n",
      "Epoch 40/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1666 - acc: 1.0000 - val_loss: 2.1764 - val_acc: 0.9872\n",
      "Epoch 41/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1669 - acc: 0.9972 - val_loss: 2.1734 - val_acc: 0.9872\n",
      "Epoch 42/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1705 - val_acc: 1.0000\n",
      "Epoch 43/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1715 - val_acc: 1.0000\n",
      "Epoch 44/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1666 - acc: 1.0000 - val_loss: 2.1711 - val_acc: 1.0000\n",
      "Epoch 45/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1689 - val_acc: 1.0000\n",
      "Epoch 46/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1686 - val_acc: 1.0000\n",
      "Epoch 47/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1693 - val_acc: 1.0000\n",
      "Epoch 48/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1696 - val_acc: 1.0000\n",
      "Epoch 49/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1718 - val_acc: 0.9872\n",
      "Epoch 50/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1697 - val_acc: 1.0000\n",
      "Epoch 51/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1694 - val_acc: 1.0000\n",
      "Epoch 52/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 0.9986 - val_loss: 2.1691 - val_acc: 1.0000\n",
      "Epoch 53/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1698 - val_acc: 1.0000\n",
      "Epoch 54/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1701 - val_acc: 1.0000\n",
      "Epoch 55/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1711 - val_acc: 1.0000\n",
      "Epoch 56/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1704 - val_acc: 0.9872\n",
      "Epoch 57/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1698 - val_acc: 1.0000\n",
      "Epoch 58/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1697 - val_acc: 1.0000\n",
      "Epoch 59/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1700 - val_acc: 1.0000\n",
      "Epoch 60/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1695 - val_acc: 1.0000\n",
      "Epoch 61/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1710 - val_acc: 1.0000\n",
      "Epoch 62/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1722 - val_acc: 0.9872\n",
      "Epoch 63/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1690 - val_acc: 1.0000\n",
      "Epoch 64/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1699 - val_acc: 1.0000\n",
      "Epoch 65/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1691 - val_acc: 1.0000\n",
      "Epoch 66/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1694 - val_acc: 1.0000\n",
      "Epoch 67/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1700 - val_acc: 1.0000\n",
      "Epoch 68/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1698 - val_acc: 1.0000\n",
      "Epoch 69/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1708 - val_acc: 1.0000\n",
      "Epoch 70/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1708 - val_acc: 1.0000\n",
      "Epoch 71/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1700 - val_acc: 1.0000\n",
      "Epoch 72/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1702 - val_acc: 1.0000\n",
      "Epoch 73/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1703 - val_acc: 1.0000\n",
      "Epoch 74/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1704 - val_acc: 1.0000\n",
      "Epoch 75/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1700 - val_acc: 1.0000\n",
      "Epoch 76/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1699 - val_acc: 1.0000\n",
      "Epoch 77/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1701 - val_acc: 1.0000\n",
      "Epoch 78/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1697 - val_acc: 1.0000\n",
      "Epoch 79/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1695 - val_acc: 1.0000\n",
      "Epoch 80/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1700 - val_acc: 1.0000\n",
      "Epoch 81/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1708 - val_acc: 1.0000\n",
      "Epoch 82/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1704 - val_acc: 1.0000\n",
      "Epoch 83/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1700 - val_acc: 1.0000\n",
      "Epoch 84/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1698 - val_acc: 1.0000\n",
      "Epoch 85/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1698 - val_acc: 1.0000\n",
      "Epoch 86/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1700 - val_acc: 1.0000\n",
      "Epoch 87/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1703 - val_acc: 1.0000\n",
      "Epoch 88/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1703 - val_acc: 1.0000\n",
      "Epoch 89/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1700 - val_acc: 1.0000\n",
      "Epoch 90/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1698 - val_acc: 1.0000\n",
      "Epoch 91/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1704 - val_acc: 1.0000\n",
      "Epoch 92/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1701 - val_acc: 1.0000\n",
      "Epoch 93/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1700 - val_acc: 1.0000\n",
      "Epoch 94/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1706 - val_acc: 1.0000\n",
      "Epoch 95/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1702 - val_acc: 1.0000\n",
      "Epoch 96/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1705 - val_acc: 1.0000\n",
      "Epoch 97/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1707 - val_acc: 1.0000\n",
      "Epoch 98/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1700 - val_acc: 1.0000\n",
      "Epoch 99/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1702 - val_acc: 1.0000\n",
      "Epoch 100/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1703 - val_acc: 1.0000\n",
      "78/78 [==============================] - 0s 346us/step\n",
      "Score for fold 3: loss of 2.1699153765653953; acc of 100.0%\n",
      "Train on 702 samples, validate on 78 samples\n",
      "Epoch 1/100\n",
      "702/702 [==============================] - 11s 15ms/step - loss: 2.4920 - acc: 0.2792 - val_loss: 2.3601 - val_acc: 0.3974\n",
      "Epoch 2/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "702/702 [==============================] - 1s 1ms/step - loss: 2.3154 - acc: 0.5456 - val_loss: 2.2714 - val_acc: 0.7051\n",
      "Epoch 3/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2532 - acc: 0.7422 - val_loss: 2.2325 - val_acc: 0.8718\n",
      "Epoch 4/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2265 - acc: 0.8405 - val_loss: 2.2488 - val_acc: 0.7692\n",
      "Epoch 5/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2091 - acc: 0.8618 - val_loss: 2.1978 - val_acc: 0.8205\n",
      "Epoch 6/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1974 - acc: 0.9031 - val_loss: 2.2153 - val_acc: 0.8718\n",
      "Epoch 7/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1951 - acc: 0.9031 - val_loss: 2.2728 - val_acc: 0.7949\n",
      "Epoch 8/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1891 - acc: 0.9345 - val_loss: 2.1851 - val_acc: 0.9103\n",
      "Epoch 9/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1830 - acc: 0.9672 - val_loss: 2.2097 - val_acc: 0.8590\n",
      "Epoch 10/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1793 - acc: 0.9644 - val_loss: 2.1757 - val_acc: 0.9615\n",
      "Epoch 11/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1851 - acc: 0.9615 - val_loss: 2.1759 - val_acc: 0.9744\n",
      "Epoch 12/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1738 - acc: 0.9872 - val_loss: 2.1757 - val_acc: 0.8846\n",
      "Epoch 13/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1787 - acc: 0.9615 - val_loss: 2.1731 - val_acc: 0.9872\n",
      "Epoch 14/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1834 - acc: 0.9815 - val_loss: 2.1756 - val_acc: 0.9615\n",
      "Epoch 15/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1720 - acc: 0.9900 - val_loss: 2.1792 - val_acc: 0.9359\n",
      "Epoch 16/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1728 - acc: 0.9744 - val_loss: 2.1724 - val_acc: 0.9872\n",
      "Epoch 17/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1753 - acc: 0.9829 - val_loss: 2.1785 - val_acc: 0.9615\n",
      "Epoch 18/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1703 - acc: 0.9986 - val_loss: 2.1792 - val_acc: 0.9359\n",
      "Epoch 19/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1717 - acc: 0.9900 - val_loss: 2.1789 - val_acc: 0.8846\n",
      "Epoch 20/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1749 - acc: 0.9900 - val_loss: 2.1794 - val_acc: 0.9615\n",
      "Epoch 21/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1708 - acc: 0.9915 - val_loss: 2.1713 - val_acc: 0.9744\n",
      "Epoch 22/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1729 - acc: 0.9872 - val_loss: 2.1718 - val_acc: 0.9744\n",
      "Epoch 23/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1710 - acc: 0.9943 - val_loss: 2.1847 - val_acc: 0.9744\n",
      "Epoch 24/100\n",
      "702/702 [==============================] - 4s 6ms/step - loss: 2.1690 - acc: 0.9986 - val_loss: 2.1707 - val_acc: 0.9744\n",
      "Epoch 25/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1679 - acc: 0.9986 - val_loss: 2.1697 - val_acc: 1.0000\n",
      "Epoch 26/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1680 - acc: 0.9972 - val_loss: 2.1691 - val_acc: 0.9872\n",
      "Epoch 27/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1688 - acc: 0.9957 - val_loss: 2.1698 - val_acc: 0.9872\n",
      "Epoch 28/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1676 - acc: 0.9972 - val_loss: 2.1706 - val_acc: 0.9744\n",
      "Epoch 29/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1679 - acc: 0.9957 - val_loss: 2.1694 - val_acc: 0.9744\n",
      "Epoch 30/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1672 - acc: 0.9972 - val_loss: 2.1688 - val_acc: 0.9872\n",
      "Epoch 31/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1674 - acc: 0.9986 - val_loss: 2.1717 - val_acc: 0.9744\n",
      "Epoch 32/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1676 - acc: 0.9986 - val_loss: 2.1694 - val_acc: 0.9744\n",
      "Epoch 33/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1670 - acc: 1.0000 - val_loss: 2.1701 - val_acc: 1.0000\n",
      "Epoch 34/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1680 - acc: 0.9957 - val_loss: 2.1694 - val_acc: 0.9744\n",
      "Epoch 35/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1669 - acc: 1.0000 - val_loss: 2.1687 - val_acc: 1.0000\n",
      "Epoch 36/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1673 - acc: 0.9986 - val_loss: 2.1691 - val_acc: 0.9872\n",
      "Epoch 37/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1667 - acc: 1.0000 - val_loss: 2.1689 - val_acc: 1.0000\n",
      "Epoch 38/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1667 - acc: 0.9986 - val_loss: 2.1684 - val_acc: 0.9872\n",
      "Epoch 39/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1668 - acc: 1.0000 - val_loss: 2.1697 - val_acc: 0.9872\n",
      "Epoch 40/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1667 - acc: 1.0000 - val_loss: 2.1694 - val_acc: 1.0000\n",
      "Epoch 41/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1666 - acc: 1.0000 - val_loss: 2.1685 - val_acc: 1.0000\n",
      "Epoch 42/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1666 - acc: 1.0000 - val_loss: 2.1688 - val_acc: 1.0000\n",
      "Epoch 43/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1666 - acc: 1.0000 - val_loss: 2.1702 - val_acc: 0.9615\n",
      "Epoch 44/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1667 - acc: 0.9986 - val_loss: 2.1693 - val_acc: 0.9872\n",
      "Epoch 45/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1666 - acc: 1.0000 - val_loss: 2.1694 - val_acc: 1.0000\n",
      "Epoch 46/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1666 - acc: 1.0000 - val_loss: 2.1678 - val_acc: 1.0000\n",
      "Epoch 47/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1687 - val_acc: 0.9872\n",
      "Epoch 48/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1682 - val_acc: 1.0000\n",
      "Epoch 49/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1675 - val_acc: 1.0000\n",
      "Epoch 50/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1678 - val_acc: 1.0000\n",
      "Epoch 51/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1677 - val_acc: 1.0000\n",
      "Epoch 52/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1682 - val_acc: 1.0000\n",
      "Epoch 53/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1684 - val_acc: 1.0000\n",
      "Epoch 54/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1679 - val_acc: 1.0000\n",
      "Epoch 55/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1685 - val_acc: 1.0000\n",
      "Epoch 56/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1682 - val_acc: 1.0000\n",
      "Epoch 57/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1681 - val_acc: 1.0000\n",
      "Epoch 58/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1681 - val_acc: 1.0000\n",
      "Epoch 59/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1683 - val_acc: 1.0000\n",
      "Epoch 60/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1682 - val_acc: 1.0000\n",
      "Epoch 61/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1682 - val_acc: 0.9872\n",
      "Epoch 62/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1686 - val_acc: 0.9872\n",
      "Epoch 63/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1679 - val_acc: 1.0000\n",
      "Epoch 64/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1686 - val_acc: 0.9872\n",
      "Epoch 65/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1686 - val_acc: 0.9872\n",
      "Epoch 66/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1682 - val_acc: 1.0000\n",
      "Epoch 67/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1680 - val_acc: 1.0000\n",
      "Epoch 68/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1680 - val_acc: 1.0000\n",
      "Epoch 69/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1686 - val_acc: 0.9872\n",
      "Epoch 70/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1685 - val_acc: 0.9872\n",
      "Epoch 71/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1679 - val_acc: 1.0000\n",
      "Epoch 72/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1680 - val_acc: 1.0000\n",
      "Epoch 73/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1680 - val_acc: 0.9872\n",
      "Epoch 74/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1682 - val_acc: 1.0000\n",
      "Epoch 75/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1683 - val_acc: 0.9872\n",
      "Epoch 76/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1682 - val_acc: 1.0000\n",
      "Epoch 77/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1678 - val_acc: 1.0000\n",
      "Epoch 78/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1683 - val_acc: 0.9872\n",
      "Epoch 79/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1679 - val_acc: 1.0000\n",
      "Epoch 80/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1677 - val_acc: 1.0000\n",
      "Epoch 81/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1681 - val_acc: 1.0000\n",
      "Epoch 82/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1677 - val_acc: 1.0000\n",
      "Epoch 83/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1679 - val_acc: 1.0000\n",
      "Epoch 84/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1679 - val_acc: 1.0000\n",
      "Epoch 85/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1680 - val_acc: 1.0000\n",
      "Epoch 86/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1683 - val_acc: 0.9872\n",
      "Epoch 87/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1681 - val_acc: 1.0000\n",
      "Epoch 88/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1677 - val_acc: 1.0000\n",
      "Epoch 89/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1681 - val_acc: 1.0000\n",
      "Epoch 90/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1681 - val_acc: 0.9872\n",
      "Epoch 91/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1680 - val_acc: 1.0000\n",
      "Epoch 92/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1681 - val_acc: 0.9872\n",
      "Epoch 93/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1679 - val_acc: 1.0000\n",
      "Epoch 94/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1680 - val_acc: 1.0000\n",
      "Epoch 95/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1678 - val_acc: 1.0000\n",
      "Epoch 96/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1682 - val_acc: 0.9872\n",
      "Epoch 97/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1680 - val_acc: 1.0000\n",
      "Epoch 98/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1680 - val_acc: 1.0000\n",
      "Epoch 99/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1679 - val_acc: 1.0000\n",
      "Epoch 100/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1680 - val_acc: 1.0000\n",
      "78/78 [==============================] - 0s 387us/step\n",
      "Score for fold 4: loss of 2.1686152066939917; acc of 98.71794825945145%\n",
      "Train on 702 samples, validate on 78 samples\n",
      "Epoch 1/100\n",
      "702/702 [==============================] - 11s 16ms/step - loss: 2.4811 - acc: 0.2963 - val_loss: 2.4190 - val_acc: 0.2821\n",
      "Epoch 2/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2921 - acc: 0.6368 - val_loss: 2.3500 - val_acc: 0.4359\n",
      "Epoch 3/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2556 - acc: 0.7236 - val_loss: 2.2381 - val_acc: 0.7949\n",
      "Epoch 4/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2150 - acc: 0.8419 - val_loss: 2.2338 - val_acc: 0.8205\n",
      "Epoch 5/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2112 - acc: 0.8519 - val_loss: 2.2389 - val_acc: 0.7436\n",
      "Epoch 6/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1963 - acc: 0.9131 - val_loss: 2.1987 - val_acc: 0.9103\n",
      "Epoch 7/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1907 - acc: 0.9259 - val_loss: 2.2047 - val_acc: 0.9359\n",
      "Epoch 8/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1843 - acc: 0.9473 - val_loss: 2.2361 - val_acc: 0.8846\n",
      "Epoch 9/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1832 - acc: 0.9430 - val_loss: 2.2033 - val_acc: 0.8333\n",
      "Epoch 10/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1815 - acc: 0.9416 - val_loss: 2.1859 - val_acc: 0.8846\n",
      "Epoch 11/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1756 - acc: 0.9758 - val_loss: 2.1912 - val_acc: 0.8846\n",
      "Epoch 12/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1746 - acc: 0.9801 - val_loss: 2.1852 - val_acc: 0.9103\n",
      "Epoch 13/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1789 - acc: 0.9658 - val_loss: 2.1747 - val_acc: 0.9615\n",
      "Epoch 14/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1719 - acc: 0.9872 - val_loss: 2.1977 - val_acc: 0.9744\n",
      "Epoch 15/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1728 - acc: 0.9886 - val_loss: 2.1840 - val_acc: 0.9744\n",
      "Epoch 16/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1740 - acc: 0.9786 - val_loss: 2.1842 - val_acc: 0.9231\n",
      "Epoch 17/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1696 - acc: 0.9986 - val_loss: 2.1793 - val_acc: 0.9744\n",
      "Epoch 18/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1714 - acc: 0.9872 - val_loss: 2.1782 - val_acc: 0.9615\n",
      "Epoch 19/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1802 - acc: 0.9744 - val_loss: 2.1739 - val_acc: 0.9872\n",
      "Epoch 20/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1694 - acc: 0.9943 - val_loss: 2.1822 - val_acc: 0.9615\n",
      "Epoch 21/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1703 - acc: 0.9886 - val_loss: 2.1839 - val_acc: 0.9231\n",
      "Epoch 22/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1703 - acc: 0.9900 - val_loss: 2.1736 - val_acc: 0.9615\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1684 - acc: 0.9972 - val_loss: 2.1716 - val_acc: 0.9744\n",
      "Epoch 24/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1703 - acc: 0.9915 - val_loss: 2.1848 - val_acc: 0.9615\n",
      "Epoch 25/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1684 - acc: 0.9943 - val_loss: 2.1713 - val_acc: 1.0000\n",
      "Epoch 26/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1689 - acc: 0.9972 - val_loss: 2.1764 - val_acc: 0.9744\n",
      "Epoch 27/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1689 - acc: 0.9886 - val_loss: 2.1799 - val_acc: 0.9359\n",
      "Epoch 28/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1702 - acc: 0.9843 - val_loss: 2.1700 - val_acc: 1.0000\n",
      "Epoch 29/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1680 - acc: 0.9929 - val_loss: 2.1826 - val_acc: 0.9615\n",
      "Epoch 30/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1678 - acc: 0.9957 - val_loss: 2.1696 - val_acc: 0.9872\n",
      "Epoch 31/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1681 - acc: 0.9972 - val_loss: 2.1789 - val_acc: 0.9744\n",
      "Epoch 32/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1702 - acc: 0.9943 - val_loss: 2.1726 - val_acc: 0.9744\n",
      "Epoch 33/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1671 - acc: 0.9986 - val_loss: 2.1716 - val_acc: 0.9744\n",
      "Epoch 34/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1673 - acc: 1.0000 - val_loss: 2.1711 - val_acc: 0.9744\n",
      "Epoch 35/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1690 - acc: 0.9900 - val_loss: 2.1845 - val_acc: 0.8974\n",
      "Epoch 36/100\n",
      "702/702 [==============================] - 4s 6ms/step - loss: 2.1674 - acc: 0.9957 - val_loss: 2.1779 - val_acc: 0.9487\n",
      "Epoch 37/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1668 - acc: 0.9986 - val_loss: 2.1702 - val_acc: 0.9872\n",
      "Epoch 38/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1667 - acc: 1.0000 - val_loss: 2.1744 - val_acc: 0.9872\n",
      "Epoch 39/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1666 - acc: 1.0000 - val_loss: 2.1724 - val_acc: 0.9872\n",
      "Epoch 40/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1676 - acc: 0.9972 - val_loss: 2.1712 - val_acc: 0.9872\n",
      "Epoch 41/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1667 - acc: 0.9986 - val_loss: 2.1708 - val_acc: 0.9872\n",
      "Epoch 42/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1666 - acc: 1.0000 - val_loss: 2.1734 - val_acc: 0.9872\n",
      "Epoch 43/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1667 - acc: 1.0000 - val_loss: 2.1703 - val_acc: 0.9872\n",
      "Epoch 44/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1667 - acc: 0.9986 - val_loss: 2.1722 - val_acc: 0.9744\n",
      "Epoch 45/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1668 - acc: 1.0000 - val_loss: 2.1700 - val_acc: 1.0000\n",
      "Epoch 46/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1666 - acc: 1.0000 - val_loss: 2.1696 - val_acc: 1.0000\n",
      "Epoch 47/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1691 - val_acc: 1.0000\n",
      "Epoch 48/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1689 - val_acc: 1.0000\n",
      "Epoch 49/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1670 - acc: 0.9986 - val_loss: 2.1703 - val_acc: 0.9872\n",
      "Epoch 50/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1690 - val_acc: 0.9744\n",
      "Epoch 51/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1689 - val_acc: 1.0000\n",
      "Epoch 52/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1696 - val_acc: 0.9872\n",
      "Epoch 53/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1703 - val_acc: 0.9872\n",
      "Epoch 54/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1686 - val_acc: 1.0000\n",
      "Epoch 55/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1689 - val_acc: 1.0000\n",
      "Epoch 56/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1695 - val_acc: 1.0000\n",
      "Epoch 57/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1712 - val_acc: 1.0000\n",
      "Epoch 58/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1686 - val_acc: 1.0000\n",
      "Epoch 59/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1697 - val_acc: 1.0000\n",
      "Epoch 60/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1699 - val_acc: 1.0000\n",
      "Epoch 61/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1716 - val_acc: 1.0000\n",
      "Epoch 62/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1704 - val_acc: 1.0000\n",
      "Epoch 63/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1713 - val_acc: 1.0000\n",
      "Epoch 64/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1703 - val_acc: 0.9872\n",
      "Epoch 65/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1712 - val_acc: 0.9744\n",
      "Epoch 66/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1694 - val_acc: 1.0000\n",
      "Epoch 67/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1694 - val_acc: 0.9872\n",
      "Epoch 68/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1691 - val_acc: 0.9872\n",
      "Epoch 69/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1690 - val_acc: 1.0000\n",
      "Epoch 70/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1695 - val_acc: 1.0000\n",
      "Epoch 71/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1685 - val_acc: 1.0000\n",
      "Epoch 72/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1685 - val_acc: 0.9872\n",
      "Epoch 73/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1688 - val_acc: 0.9872\n",
      "Epoch 74/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1690 - val_acc: 0.9872\n",
      "Epoch 75/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1682 - val_acc: 1.0000\n",
      "Epoch 76/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1683 - val_acc: 0.9872\n",
      "Epoch 77/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1686 - val_acc: 1.0000\n",
      "Epoch 78/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1683 - val_acc: 1.0000\n",
      "Epoch 79/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1686 - val_acc: 1.0000\n",
      "Epoch 80/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1684 - val_acc: 1.0000\n",
      "Epoch 81/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1683 - val_acc: 1.0000\n",
      "Epoch 82/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1686 - val_acc: 1.0000\n",
      "Epoch 83/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1682 - val_acc: 1.0000\n",
      "Epoch 84/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1682 - val_acc: 1.0000\n",
      "Epoch 85/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1684 - val_acc: 1.0000\n",
      "Epoch 86/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1683 - val_acc: 1.0000\n",
      "Epoch 87/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1682 - val_acc: 1.0000\n",
      "Epoch 88/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1683 - val_acc: 1.0000\n",
      "Epoch 89/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1684 - val_acc: 1.0000\n",
      "Epoch 90/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1686 - val_acc: 1.0000\n",
      "Epoch 91/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1686 - val_acc: 1.0000\n",
      "Epoch 92/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1684 - val_acc: 1.0000\n",
      "Epoch 93/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1682 - val_acc: 1.0000\n",
      "Epoch 94/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1684 - val_acc: 1.0000\n",
      "Epoch 95/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1682 - val_acc: 1.0000\n",
      "Epoch 96/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1684 - val_acc: 1.0000\n",
      "Epoch 97/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1683 - val_acc: 1.0000\n",
      "Epoch 98/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1681 - val_acc: 1.0000\n",
      "Epoch 99/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1682 - val_acc: 1.0000\n",
      "Epoch 100/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1683 - val_acc: 1.0000\n",
      "78/78 [==============================] - 0s 367us/step\n",
      "Score for fold 5: loss of 2.168160010606815; acc of 100.0%\n",
      "Train on 702 samples, validate on 78 samples\n",
      "Epoch 1/100\n",
      "702/702 [==============================] - 11s 16ms/step - loss: 2.5048 - acc: 0.3048 - val_loss: 2.3824 - val_acc: 0.4615\n",
      "Epoch 2/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.3206 - acc: 0.5698 - val_loss: 2.2812 - val_acc: 0.6795\n",
      "Epoch 3/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2429 - acc: 0.7564 - val_loss: 2.2322 - val_acc: 0.8205\n",
      "Epoch 4/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2159 - acc: 0.8490 - val_loss: 2.2589 - val_acc: 0.7051\n",
      "Epoch 5/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1992 - acc: 0.9017 - val_loss: 2.2175 - val_acc: 0.8205\n",
      "Epoch 6/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1894 - acc: 0.9259 - val_loss: 2.2002 - val_acc: 0.9359\n",
      "Epoch 7/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1893 - acc: 0.9074 - val_loss: 2.1899 - val_acc: 0.9487\n",
      "Epoch 8/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1820 - acc: 0.9587 - val_loss: 2.1823 - val_acc: 0.9103\n",
      "Epoch 9/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1836 - acc: 0.9501 - val_loss: 2.1794 - val_acc: 0.9487\n",
      "Epoch 10/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1786 - acc: 0.9459 - val_loss: 2.1808 - val_acc: 0.9744\n",
      "Epoch 11/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1751 - acc: 0.9687 - val_loss: 2.2165 - val_acc: 0.9103\n",
      "Epoch 12/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1759 - acc: 0.9701 - val_loss: 2.1766 - val_acc: 0.9359\n",
      "Epoch 13/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1739 - acc: 0.9872 - val_loss: 2.2263 - val_acc: 0.8077\n",
      "Epoch 14/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1752 - acc: 0.9801 - val_loss: 2.1765 - val_acc: 0.9744\n",
      "Epoch 15/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1785 - acc: 0.9587 - val_loss: 2.1789 - val_acc: 0.9744\n",
      "Epoch 16/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1704 - acc: 0.9843 - val_loss: 2.1772 - val_acc: 0.9872\n",
      "Epoch 17/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1718 - acc: 0.9829 - val_loss: 2.2065 - val_acc: 0.8333\n",
      "Epoch 18/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1711 - acc: 0.9858 - val_loss: 2.1730 - val_acc: 0.9872\n",
      "Epoch 19/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1718 - acc: 0.9900 - val_loss: 2.1778 - val_acc: 0.9744\n",
      "Epoch 20/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1697 - acc: 0.9900 - val_loss: 2.1759 - val_acc: 0.9487\n",
      "Epoch 21/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1744 - acc: 0.9858 - val_loss: 2.1732 - val_acc: 0.9872\n",
      "Epoch 22/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1692 - acc: 0.9957 - val_loss: 2.1757 - val_acc: 0.9872\n",
      "Epoch 23/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1698 - acc: 0.9900 - val_loss: 2.1725 - val_acc: 0.9872\n",
      "Epoch 24/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1700 - acc: 0.9843 - val_loss: 2.1720 - val_acc: 0.9872\n",
      "Epoch 25/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1685 - acc: 0.9972 - val_loss: 2.1756 - val_acc: 0.9615\n",
      "Epoch 26/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1710 - acc: 0.9915 - val_loss: 2.1718 - val_acc: 0.9872\n",
      "Epoch 27/100\n",
      "702/702 [==============================] - 4s 6ms/step - loss: 2.1679 - acc: 0.9986 - val_loss: 2.1701 - val_acc: 0.9744\n",
      "Epoch 28/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1674 - acc: 0.9972 - val_loss: 2.1743 - val_acc: 0.9487\n",
      "Epoch 29/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1676 - acc: 0.9986 - val_loss: 2.1729 - val_acc: 0.9744\n",
      "Epoch 30/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1674 - acc: 0.9986 - val_loss: 2.1706 - val_acc: 1.0000\n",
      "Epoch 31/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1673 - acc: 0.9986 - val_loss: 2.1702 - val_acc: 1.0000\n",
      "Epoch 32/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1674 - acc: 0.9943 - val_loss: 2.1755 - val_acc: 0.9744\n",
      "Epoch 33/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1677 - acc: 0.9972 - val_loss: 2.1706 - val_acc: 0.9872\n",
      "Epoch 34/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1667 - acc: 1.0000 - val_loss: 2.1714 - val_acc: 0.9872\n",
      "Epoch 35/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1670 - acc: 0.9986 - val_loss: 2.1749 - val_acc: 0.9744\n",
      "Epoch 36/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1671 - acc: 0.9972 - val_loss: 2.1767 - val_acc: 0.9744\n",
      "Epoch 37/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1671 - acc: 0.9986 - val_loss: 2.1705 - val_acc: 1.0000\n",
      "Epoch 38/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1673 - acc: 0.9986 - val_loss: 2.1713 - val_acc: 1.0000\n",
      "Epoch 39/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1671 - acc: 1.0000 - val_loss: 2.1703 - val_acc: 0.9872\n",
      "Epoch 40/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1706 - val_acc: 1.0000\n",
      "Epoch 41/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1672 - acc: 0.9957 - val_loss: 2.1727 - val_acc: 1.0000\n",
      "Epoch 42/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1666 - acc: 1.0000 - val_loss: 2.1708 - val_acc: 0.9872\n",
      "Epoch 43/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1695 - val_acc: 0.9872\n",
      "Epoch 44/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1701 - val_acc: 1.0000\n",
      "Epoch 45/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1670 - acc: 0.9972 - val_loss: 2.1702 - val_acc: 0.9872\n",
      "Epoch 46/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1706 - val_acc: 0.9872\n",
      "Epoch 47/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1704 - val_acc: 0.9872\n",
      "Epoch 48/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1710 - val_acc: 0.9872\n",
      "Epoch 49/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1702 - val_acc: 0.9872\n",
      "Epoch 50/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1701 - val_acc: 0.9872\n",
      "Epoch 51/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1728 - val_acc: 0.9744\n",
      "Epoch 52/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1694 - val_acc: 1.0000\n",
      "Epoch 53/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1694 - val_acc: 1.0000\n",
      "Epoch 54/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1696 - val_acc: 0.9872\n",
      "Epoch 55/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1693 - val_acc: 1.0000\n",
      "Epoch 56/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1695 - val_acc: 1.0000\n",
      "Epoch 57/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1693 - val_acc: 1.0000\n",
      "Epoch 58/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1695 - val_acc: 0.9872\n",
      "Epoch 59/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1701 - val_acc: 0.9872\n",
      "Epoch 60/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1697 - val_acc: 0.9872\n",
      "Epoch 61/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1692 - val_acc: 1.0000\n",
      "Epoch 62/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1699 - val_acc: 0.9872\n",
      "Epoch 63/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1694 - val_acc: 1.0000\n",
      "Epoch 64/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1703 - val_acc: 0.9872\n",
      "Epoch 65/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1696 - val_acc: 0.9872\n",
      "Epoch 66/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1694 - val_acc: 1.0000\n",
      "Epoch 67/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1694 - val_acc: 0.9872\n",
      "Epoch 68/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1693 - val_acc: 1.0000\n",
      "Epoch 69/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1692 - val_acc: 0.9872\n",
      "Epoch 70/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1699 - val_acc: 0.9872\n",
      "Epoch 71/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1696 - val_acc: 0.9872\n",
      "Epoch 72/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1703 - val_acc: 0.9872\n",
      "Epoch 73/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1699 - val_acc: 0.9872\n",
      "Epoch 74/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1697 - val_acc: 0.9872\n",
      "Epoch 75/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1701 - val_acc: 0.9872\n",
      "Epoch 76/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1695 - val_acc: 0.9872\n",
      "Epoch 77/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1698 - val_acc: 0.9872\n",
      "Epoch 78/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1697 - val_acc: 0.9872\n",
      "Epoch 79/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1699 - val_acc: 0.9872\n",
      "Epoch 80/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1700 - val_acc: 0.9872\n",
      "Epoch 81/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1692 - val_acc: 1.0000\n",
      "Epoch 82/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1691 - val_acc: 1.0000\n",
      "Epoch 83/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1695 - val_acc: 1.0000\n",
      "Epoch 84/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1692 - val_acc: 1.0000\n",
      "Epoch 85/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1693 - val_acc: 1.0000\n",
      "Epoch 86/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1694 - val_acc: 0.9872\n",
      "Epoch 87/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1695 - val_acc: 0.9872\n",
      "Epoch 88/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1695 - val_acc: 0.9872\n",
      "Epoch 89/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1695 - val_acc: 0.9872\n",
      "Epoch 90/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1694 - val_acc: 1.0000\n",
      "Epoch 91/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1699 - val_acc: 0.9872\n",
      "Epoch 92/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1696 - val_acc: 0.9872\n",
      "Epoch 93/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1698 - val_acc: 0.9872\n",
      "Epoch 94/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1695 - val_acc: 0.9872\n",
      "Epoch 95/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1695 - val_acc: 0.9872\n",
      "Epoch 96/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1697 - val_acc: 0.9872\n",
      "Epoch 97/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1697 - val_acc: 0.9872\n",
      "Epoch 98/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1697 - val_acc: 0.9872\n",
      "Epoch 99/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1695 - val_acc: 1.0000\n",
      "Epoch 100/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1696 - val_acc: 0.9872\n",
      "78/78 [==============================] - 0s 386us/step\n",
      "Score for fold 6: loss of 2.169548982228988; acc of 98.71794825945145%\n",
      "Train on 702 samples, validate on 78 samples\n",
      "Epoch 1/100\n",
      "702/702 [==============================] - 11s 16ms/step - loss: 2.5068 - acc: 0.2265 - val_loss: 2.3816 - val_acc: 0.3718\n",
      "Epoch 2/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.3020 - acc: 0.5741 - val_loss: 2.2923 - val_acc: 0.6282\n",
      "Epoch 3/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2528 - acc: 0.6994 - val_loss: 2.2287 - val_acc: 0.7179\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2203 - acc: 0.8177 - val_loss: 2.2207 - val_acc: 0.7821\n",
      "Epoch 5/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2043 - acc: 0.8960 - val_loss: 2.2462 - val_acc: 0.7308\n",
      "Epoch 6/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1987 - acc: 0.9017 - val_loss: 2.2645 - val_acc: 0.8077\n",
      "Epoch 7/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1874 - acc: 0.9359 - val_loss: 2.1881 - val_acc: 0.8718\n",
      "Epoch 8/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1867 - acc: 0.9544 - val_loss: 2.1828 - val_acc: 0.9744\n",
      "Epoch 9/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1825 - acc: 0.9544 - val_loss: 2.1891 - val_acc: 0.9487\n",
      "Epoch 10/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1770 - acc: 0.9772 - val_loss: 2.1824 - val_acc: 0.9744\n",
      "Epoch 11/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1803 - acc: 0.9644 - val_loss: 2.1846 - val_acc: 0.8590\n",
      "Epoch 12/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1747 - acc: 0.9601 - val_loss: 2.1865 - val_acc: 0.8974\n",
      "Epoch 13/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1767 - acc: 0.9715 - val_loss: 2.1866 - val_acc: 0.9615\n",
      "Epoch 14/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1742 - acc: 0.9843 - val_loss: 2.1803 - val_acc: 0.9231\n",
      "Epoch 15/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1764 - acc: 0.9858 - val_loss: 2.1815 - val_acc: 0.9487\n",
      "Epoch 16/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1725 - acc: 0.9900 - val_loss: 2.1824 - val_acc: 0.9615\n",
      "Epoch 17/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1729 - acc: 0.9915 - val_loss: 2.1865 - val_acc: 0.9487\n",
      "Epoch 18/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1701 - acc: 0.9957 - val_loss: 2.1861 - val_acc: 0.9103\n",
      "Epoch 19/100\n",
      "702/702 [==============================] - 4s 6ms/step - loss: 2.1758 - acc: 0.9630 - val_loss: 2.1827 - val_acc: 0.9615\n",
      "Epoch 20/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1682 - acc: 0.9986 - val_loss: 2.1825 - val_acc: 0.9487\n",
      "Epoch 21/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1678 - acc: 0.9957 - val_loss: 2.1815 - val_acc: 0.9487\n",
      "Epoch 22/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1688 - acc: 0.9943 - val_loss: 2.1785 - val_acc: 0.9744\n",
      "Epoch 23/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1678 - acc: 1.0000 - val_loss: 2.1768 - val_acc: 0.9744\n",
      "Epoch 24/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1675 - acc: 0.9972 - val_loss: 2.1817 - val_acc: 0.9744\n",
      "Epoch 25/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1676 - acc: 0.9972 - val_loss: 2.1840 - val_acc: 0.9487\n",
      "Epoch 26/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1676 - acc: 0.9972 - val_loss: 2.1904 - val_acc: 0.9615\n",
      "Epoch 27/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1677 - acc: 0.9986 - val_loss: 2.1802 - val_acc: 0.9744\n",
      "Epoch 28/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1672 - acc: 1.0000 - val_loss: 2.1809 - val_acc: 0.9487\n",
      "Epoch 29/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1675 - acc: 0.9972 - val_loss: 2.1787 - val_acc: 0.9231\n",
      "Epoch 30/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1668 - acc: 1.0000 - val_loss: 2.1792 - val_acc: 0.9744\n",
      "Epoch 31/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1668 - acc: 0.9986 - val_loss: 2.1823 - val_acc: 0.9615\n",
      "Epoch 32/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1667 - acc: 1.0000 - val_loss: 2.1789 - val_acc: 0.9359\n",
      "Epoch 33/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1668 - acc: 0.9972 - val_loss: 2.1770 - val_acc: 0.9744\n",
      "Epoch 34/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1668 - acc: 1.0000 - val_loss: 2.1800 - val_acc: 0.9744\n",
      "Epoch 35/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1666 - acc: 1.0000 - val_loss: 2.1798 - val_acc: 0.9744\n",
      "Epoch 36/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1666 - acc: 1.0000 - val_loss: 2.1801 - val_acc: 0.9744\n",
      "Epoch 37/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1667 - acc: 1.0000 - val_loss: 2.1827 - val_acc: 0.9744\n",
      "Epoch 38/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1801 - val_acc: 0.9744\n",
      "Epoch 39/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1666 - acc: 1.0000 - val_loss: 2.1779 - val_acc: 0.9744\n",
      "Epoch 40/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1799 - val_acc: 0.9744\n",
      "Epoch 41/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1793 - val_acc: 0.9744\n",
      "Epoch 42/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1805 - val_acc: 0.9744\n",
      "Epoch 43/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1789 - val_acc: 0.9744\n",
      "Epoch 44/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1807 - val_acc: 0.9744\n",
      "Epoch 45/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1785 - val_acc: 0.9744\n",
      "Epoch 46/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1758 - val_acc: 0.9744\n",
      "Epoch 47/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1806 - val_acc: 0.9744\n",
      "Epoch 48/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1806 - val_acc: 0.9744\n",
      "Epoch 49/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1757 - val_acc: 0.9744\n",
      "Epoch 50/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1803 - val_acc: 0.9744\n",
      "Epoch 51/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1775 - val_acc: 0.9744\n",
      "Epoch 52/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1770 - val_acc: 0.9744\n",
      "Epoch 53/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1785 - val_acc: 0.9744\n",
      "Epoch 54/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1754 - val_acc: 0.9744\n",
      "Epoch 55/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1772 - val_acc: 0.9744\n",
      "Epoch 56/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1794 - val_acc: 0.9744\n",
      "Epoch 57/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1799 - val_acc: 0.9744\n",
      "Epoch 58/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1792 - val_acc: 0.9744\n",
      "Epoch 59/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1784 - val_acc: 0.9744\n",
      "Epoch 60/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1767 - val_acc: 0.9744\n",
      "Epoch 61/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1794 - val_acc: 0.9744\n",
      "Epoch 62/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1793 - val_acc: 0.9744\n",
      "Epoch 63/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1786 - val_acc: 0.9744\n",
      "Epoch 64/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1767 - val_acc: 0.9744\n",
      "Epoch 65/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1779 - val_acc: 0.9744\n",
      "Epoch 66/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1775 - val_acc: 0.9744\n",
      "Epoch 67/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1796 - val_acc: 0.9744\n",
      "Epoch 68/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1786 - val_acc: 0.9744\n",
      "Epoch 69/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1790 - val_acc: 0.9744\n",
      "Epoch 70/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1766 - val_acc: 0.9744\n",
      "Epoch 71/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1780 - val_acc: 0.9744\n",
      "Epoch 72/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1770 - val_acc: 0.9744\n",
      "Epoch 73/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1781 - val_acc: 0.9744\n",
      "Epoch 74/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1792 - val_acc: 0.9744\n",
      "Epoch 75/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1770 - val_acc: 0.9744\n",
      "Epoch 76/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1774 - val_acc: 0.9744\n",
      "Epoch 77/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1775 - val_acc: 0.9744\n",
      "Epoch 78/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1780 - val_acc: 0.9744\n",
      "Epoch 79/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1788 - val_acc: 0.9744\n",
      "Epoch 80/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1786 - val_acc: 0.9744\n",
      "Epoch 81/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1769 - val_acc: 0.9744\n",
      "Epoch 82/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1792 - val_acc: 0.9744\n",
      "Epoch 83/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1790 - val_acc: 0.9744\n",
      "Epoch 84/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1784 - val_acc: 0.9744\n",
      "Epoch 85/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1785 - val_acc: 0.9744\n",
      "Epoch 86/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1787 - val_acc: 0.9744\n",
      "Epoch 87/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1775 - val_acc: 0.9744\n",
      "Epoch 88/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1785 - val_acc: 0.9744\n",
      "Epoch 89/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1791 - val_acc: 0.9744\n",
      "Epoch 90/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1784 - val_acc: 0.9744\n",
      "Epoch 91/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1799 - val_acc: 0.9744\n",
      "Epoch 92/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1788 - val_acc: 0.9744\n",
      "Epoch 93/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1789 - val_acc: 0.9744\n",
      "Epoch 94/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1792 - val_acc: 0.9744\n",
      "Epoch 95/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1789 - val_acc: 0.9744\n",
      "Epoch 96/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1802 - val_acc: 0.9744\n",
      "Epoch 97/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1791 - val_acc: 0.9744\n",
      "Epoch 98/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1790 - val_acc: 0.9744\n",
      "Epoch 99/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1780 - val_acc: 0.9744\n",
      "Epoch 100/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1785 - val_acc: 0.9744\n",
      "78/78 [==============================] - 0s 353us/step\n",
      "Score for fold 7: loss of 2.178351640701294; acc of 97.43589697740018%\n",
      "Train on 702 samples, validate on 78 samples\n",
      "Epoch 1/100\n",
      "702/702 [==============================] - 12s 17ms/step - loss: 2.4901 - acc: 0.2521 - val_loss: 2.3338 - val_acc: 0.5128\n",
      "Epoch 2/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.3131 - acc: 0.5356 - val_loss: 2.2913 - val_acc: 0.5385\n",
      "Epoch 3/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2534 - acc: 0.7308 - val_loss: 2.2756 - val_acc: 0.6410\n",
      "Epoch 4/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2183 - acc: 0.8319 - val_loss: 2.3119 - val_acc: 0.7436\n",
      "Epoch 5/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2101 - acc: 0.8803 - val_loss: 2.2301 - val_acc: 0.7692\n",
      "Epoch 6/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1952 - acc: 0.8974 - val_loss: 2.2059 - val_acc: 0.8462\n",
      "Epoch 7/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1968 - acc: 0.9060 - val_loss: 2.1992 - val_acc: 0.9103\n",
      "Epoch 8/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1798 - acc: 0.9630 - val_loss: 2.2027 - val_acc: 0.8462\n",
      "Epoch 9/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1843 - acc: 0.9473 - val_loss: 2.1897 - val_acc: 0.9231\n",
      "Epoch 10/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1778 - acc: 0.9772 - val_loss: 2.2197 - val_acc: 0.8590\n",
      "Epoch 11/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1775 - acc: 0.9601 - val_loss: 2.1917 - val_acc: 0.9231\n",
      "Epoch 12/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1768 - acc: 0.9772 - val_loss: 2.1903 - val_acc: 0.8846\n",
      "Epoch 13/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1739 - acc: 0.9801 - val_loss: 2.1886 - val_acc: 0.9103\n",
      "Epoch 14/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1754 - acc: 0.9786 - val_loss: 2.1998 - val_acc: 0.9103\n",
      "Epoch 15/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1707 - acc: 0.9915 - val_loss: 2.1914 - val_acc: 0.9359\n",
      "Epoch 16/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1709 - acc: 0.9915 - val_loss: 2.1849 - val_acc: 0.9487\n",
      "Epoch 17/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1713 - acc: 0.9786 - val_loss: 2.1811 - val_acc: 0.9615\n",
      "Epoch 18/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1753 - acc: 0.9729 - val_loss: 2.1831 - val_acc: 0.9359\n",
      "Epoch 19/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1686 - acc: 0.9986 - val_loss: 2.1816 - val_acc: 0.9744\n",
      "Epoch 20/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1694 - acc: 0.9957 - val_loss: 2.1887 - val_acc: 0.9231\n",
      "Epoch 21/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1691 - acc: 0.9929 - val_loss: 2.1849 - val_acc: 0.9487\n",
      "Epoch 22/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1703 - acc: 0.9858 - val_loss: 2.1816 - val_acc: 0.9744\n",
      "Epoch 23/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1683 - acc: 0.9943 - val_loss: 2.1898 - val_acc: 0.9615\n",
      "Epoch 24/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1696 - acc: 0.9957 - val_loss: 2.1897 - val_acc: 0.9359\n",
      "Epoch 25/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1686 - acc: 0.9986 - val_loss: 2.1835 - val_acc: 0.9487\n",
      "Epoch 26/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1680 - acc: 0.9972 - val_loss: 2.1788 - val_acc: 0.9872\n",
      "Epoch 27/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1690 - acc: 0.9900 - val_loss: 2.1844 - val_acc: 0.9103\n",
      "Epoch 28/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1685 - acc: 0.9943 - val_loss: 2.1769 - val_acc: 0.9744\n",
      "Epoch 29/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1678 - acc: 0.9986 - val_loss: 2.2112 - val_acc: 0.8590\n",
      "Epoch 30/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1680 - acc: 0.9943 - val_loss: 2.1775 - val_acc: 0.9744\n",
      "Epoch 31/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1685 - acc: 0.9986 - val_loss: 2.1883 - val_acc: 0.9487\n",
      "Epoch 32/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1674 - acc: 1.0000 - val_loss: 2.1799 - val_acc: 0.9615\n",
      "Epoch 33/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1671 - acc: 1.0000 - val_loss: 2.1795 - val_acc: 0.9744\n",
      "Epoch 34/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1687 - acc: 0.9972 - val_loss: 2.1754 - val_acc: 0.9744\n",
      "Epoch 35/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1668 - acc: 1.0000 - val_loss: 2.1809 - val_acc: 0.9744\n",
      "Epoch 36/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1672 - acc: 0.9972 - val_loss: 2.1760 - val_acc: 0.9744\n",
      "Epoch 37/100\n",
      "702/702 [==============================] - 4s 6ms/step - loss: 2.1674 - acc: 0.9957 - val_loss: 2.1785 - val_acc: 0.9231\n",
      "Epoch 38/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1675 - acc: 0.9972 - val_loss: 2.1754 - val_acc: 0.9872\n",
      "Epoch 39/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1666 - acc: 1.0000 - val_loss: 2.1744 - val_acc: 0.9744\n",
      "Epoch 40/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1765 - val_acc: 0.9744\n",
      "Epoch 41/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1674 - acc: 0.9972 - val_loss: 2.1810 - val_acc: 0.9744\n",
      "Epoch 42/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1805 - val_acc: 0.9744\n",
      "Epoch 43/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1770 - val_acc: 0.9487\n",
      "Epoch 44/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1675 - acc: 0.9972 - val_loss: 2.1790 - val_acc: 0.9744\n",
      "Epoch 45/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1797 - val_acc: 0.9744\n",
      "Epoch 46/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1771 - val_acc: 0.9744\n",
      "Epoch 47/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1795 - val_acc: 0.9487\n",
      "Epoch 48/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1760 - val_acc: 0.9744\n",
      "Epoch 49/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1781 - val_acc: 0.9744\n",
      "Epoch 50/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1729 - val_acc: 0.9872\n",
      "Epoch 51/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1770 - val_acc: 0.9744\n",
      "Epoch 52/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1780 - val_acc: 0.9744\n",
      "Epoch 53/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1792 - val_acc: 0.9744\n",
      "Epoch 54/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1783 - val_acc: 0.9744\n",
      "Epoch 55/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1788 - val_acc: 0.9744\n",
      "Epoch 56/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1788 - val_acc: 0.9744\n",
      "Epoch 57/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1795 - val_acc: 0.9744\n",
      "Epoch 58/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1765 - val_acc: 0.9744\n",
      "Epoch 59/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1788 - val_acc: 0.9744\n",
      "Epoch 60/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1785 - val_acc: 0.9744\n",
      "Epoch 61/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1766 - val_acc: 0.9744\n",
      "Epoch 62/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1776 - val_acc: 0.9744\n",
      "Epoch 63/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1745 - val_acc: 0.9744\n",
      "Epoch 64/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1772 - val_acc: 0.9744\n",
      "Epoch 65/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1789 - val_acc: 0.9744\n",
      "Epoch 66/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1778 - val_acc: 0.9744\n",
      "Epoch 67/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1754 - val_acc: 0.9744\n",
      "Epoch 68/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1762 - val_acc: 0.9744\n",
      "Epoch 69/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1774 - val_acc: 0.9744\n",
      "Epoch 70/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1764 - val_acc: 0.9744\n",
      "Epoch 71/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1778 - val_acc: 0.9744\n",
      "Epoch 72/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1766 - val_acc: 0.9744\n",
      "Epoch 73/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1773 - val_acc: 0.9744\n",
      "Epoch 74/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1778 - val_acc: 0.9744\n",
      "Epoch 75/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1785 - val_acc: 0.9744\n",
      "Epoch 76/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1780 - val_acc: 0.9744\n",
      "Epoch 77/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1783 - val_acc: 0.9744\n",
      "Epoch 78/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1770 - val_acc: 0.9744\n",
      "Epoch 79/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1783 - val_acc: 0.9744\n",
      "Epoch 80/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1780 - val_acc: 0.9744\n",
      "Epoch 81/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1782 - val_acc: 0.9744\n",
      "Epoch 82/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1780 - val_acc: 0.9744\n",
      "Epoch 83/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1783 - val_acc: 0.9744\n",
      "Epoch 84/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1775 - val_acc: 0.9744\n",
      "Epoch 85/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1773 - val_acc: 0.9744\n",
      "Epoch 86/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1782 - val_acc: 0.9744\n",
      "Epoch 87/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1776 - val_acc: 0.9744\n",
      "Epoch 88/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1785 - val_acc: 0.9744\n",
      "Epoch 89/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1776 - val_acc: 0.9744\n",
      "Epoch 90/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1756 - val_acc: 0.9744\n",
      "Epoch 91/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1771 - val_acc: 0.9744\n",
      "Epoch 92/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1783 - val_acc: 0.9744\n",
      "Epoch 93/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1772 - val_acc: 0.9744\n",
      "Epoch 94/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1781 - val_acc: 0.9744\n",
      "Epoch 95/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1767 - val_acc: 0.9744\n",
      "Epoch 96/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1778 - val_acc: 0.9744\n",
      "Epoch 97/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1783 - val_acc: 0.9744\n",
      "Epoch 98/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1776 - val_acc: 0.9744\n",
      "Epoch 99/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1772 - val_acc: 0.9744\n",
      "Epoch 100/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1774 - val_acc: 0.9744\n",
      "78/78 [==============================] - 0s 341us/step\n",
      "Score for fold 8: loss of 2.176848393220168; acc of 97.43589743589743%\n",
      "Train on 702 samples, validate on 78 samples\n",
      "Epoch 1/100\n",
      "702/702 [==============================] - 12s 17ms/step - loss: 2.4951 - acc: 0.2407 - val_loss: 2.3557 - val_acc: 0.4359\n",
      "Epoch 2/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2965 - acc: 0.6068 - val_loss: 2.2369 - val_acc: 0.7692\n",
      "Epoch 3/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2442 - acc: 0.7436 - val_loss: 2.2322 - val_acc: 0.8590\n",
      "Epoch 4/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2287 - acc: 0.8333 - val_loss: 2.2025 - val_acc: 0.8462\n",
      "Epoch 5/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2017 - acc: 0.8803 - val_loss: 2.2008 - val_acc: 0.8718\n",
      "Epoch 6/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1913 - acc: 0.9103 - val_loss: 2.1934 - val_acc: 0.9359\n",
      "Epoch 7/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1867 - acc: 0.9416 - val_loss: 2.1903 - val_acc: 0.9744\n",
      "Epoch 8/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1845 - acc: 0.9473 - val_loss: 2.2231 - val_acc: 0.8590\n",
      "Epoch 9/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1809 - acc: 0.9701 - val_loss: 2.1971 - val_acc: 0.9359\n",
      "Epoch 10/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1793 - acc: 0.9672 - val_loss: 2.1952 - val_acc: 0.9359\n",
      "Epoch 11/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1779 - acc: 0.9772 - val_loss: 2.1865 - val_acc: 0.8718\n",
      "Epoch 12/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1726 - acc: 0.9858 - val_loss: 2.1829 - val_acc: 0.9487\n",
      "Epoch 13/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1759 - acc: 0.9786 - val_loss: 2.1815 - val_acc: 0.9744\n",
      "Epoch 14/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1763 - acc: 0.9772 - val_loss: 2.1847 - val_acc: 0.9744\n",
      "Epoch 15/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1716 - acc: 0.9858 - val_loss: 2.1853 - val_acc: 0.9615\n",
      "Epoch 16/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1700 - acc: 0.9900 - val_loss: 2.1809 - val_acc: 0.9744\n",
      "Epoch 17/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1691 - acc: 0.9972 - val_loss: 2.1803 - val_acc: 0.9744\n",
      "Epoch 18/100\n",
      "702/702 [==============================] - 4s 6ms/step - loss: 2.1710 - acc: 0.9886 - val_loss: 2.1794 - val_acc: 0.9615\n",
      "Epoch 19/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1676 - acc: 0.9986 - val_loss: 2.1788 - val_acc: 0.9615\n",
      "Epoch 20/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1681 - acc: 0.9986 - val_loss: 2.1825 - val_acc: 0.9615\n",
      "Epoch 21/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1679 - acc: 1.0000 - val_loss: 2.1820 - val_acc: 0.9615\n",
      "Epoch 22/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1673 - acc: 1.0000 - val_loss: 2.1780 - val_acc: 0.9615\n",
      "Epoch 23/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1676 - acc: 1.0000 - val_loss: 2.1786 - val_acc: 0.9744\n",
      "Epoch 24/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1684 - acc: 0.9943 - val_loss: 2.1802 - val_acc: 0.9615\n",
      "Epoch 25/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1675 - acc: 0.9986 - val_loss: 2.1793 - val_acc: 0.9744\n",
      "Epoch 26/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1672 - acc: 0.9986 - val_loss: 2.1784 - val_acc: 0.9615\n",
      "Epoch 27/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1670 - acc: 1.0000 - val_loss: 2.1794 - val_acc: 0.9487\n",
      "Epoch 28/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1680 - acc: 0.9986 - val_loss: 2.1774 - val_acc: 0.9744\n",
      "Epoch 29/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1667 - acc: 1.0000 - val_loss: 2.1783 - val_acc: 0.9744\n",
      "Epoch 30/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1668 - acc: 0.9972 - val_loss: 2.1803 - val_acc: 0.9615\n",
      "Epoch 31/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1669 - acc: 1.0000 - val_loss: 2.1787 - val_acc: 0.9744\n",
      "Epoch 32/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1666 - acc: 1.0000 - val_loss: 2.1785 - val_acc: 0.9744\n",
      "Epoch 33/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1666 - acc: 1.0000 - val_loss: 2.1787 - val_acc: 0.9744\n",
      "Epoch 34/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1668 - acc: 1.0000 - val_loss: 2.1782 - val_acc: 0.9744\n",
      "Epoch 35/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1666 - acc: 1.0000 - val_loss: 2.1778 - val_acc: 0.9744\n",
      "Epoch 36/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1666 - acc: 1.0000 - val_loss: 2.1786 - val_acc: 0.9487\n",
      "Epoch 37/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1666 - acc: 1.0000 - val_loss: 2.1781 - val_acc: 0.9744\n",
      "Epoch 38/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1666 - acc: 1.0000 - val_loss: 2.1788 - val_acc: 0.9744\n",
      "Epoch 39/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1667 - acc: 1.0000 - val_loss: 2.1789 - val_acc: 0.9744\n",
      "Epoch 40/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1782 - val_acc: 0.9615\n",
      "Epoch 41/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1787 - val_acc: 0.9744\n",
      "Epoch 42/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1782 - val_acc: 0.9744\n",
      "Epoch 43/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1779 - val_acc: 0.9744\n",
      "Epoch 44/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1785 - val_acc: 0.9615\n",
      "Epoch 45/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1778 - val_acc: 0.9744\n",
      "Epoch 46/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1783 - val_acc: 0.9744\n",
      "Epoch 47/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1777 - val_acc: 0.9744\n",
      "Epoch 48/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1780 - val_acc: 0.9744\n",
      "Epoch 49/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1776 - val_acc: 0.9744\n",
      "Epoch 50/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1776 - val_acc: 0.9744\n",
      "Epoch 51/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1776 - val_acc: 0.9744\n",
      "Epoch 52/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1776 - val_acc: 0.9744\n",
      "Epoch 53/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1780 - val_acc: 0.9744\n",
      "Epoch 54/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1777 - val_acc: 0.9744\n",
      "Epoch 55/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1779 - val_acc: 0.9744\n",
      "Epoch 56/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1778 - val_acc: 0.9744\n",
      "Epoch 57/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1780 - val_acc: 0.9744\n",
      "Epoch 58/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1780 - val_acc: 0.9744\n",
      "Epoch 59/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1771 - val_acc: 0.9744\n",
      "Epoch 60/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1779 - val_acc: 0.9744\n",
      "Epoch 61/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1778 - val_acc: 0.9744\n",
      "Epoch 62/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1781 - val_acc: 0.9744\n",
      "Epoch 63/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1777 - val_acc: 0.9744\n",
      "Epoch 64/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1780 - val_acc: 0.9744\n",
      "Epoch 65/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1781 - val_acc: 0.9744\n",
      "Epoch 66/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1778 - val_acc: 0.9744\n",
      "Epoch 67/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1778 - val_acc: 0.9744\n",
      "Epoch 68/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1778 - val_acc: 0.9744\n",
      "Epoch 69/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1780 - val_acc: 0.9744\n",
      "Epoch 70/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1775 - val_acc: 0.9744\n",
      "Epoch 71/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1779 - val_acc: 0.9744\n",
      "Epoch 72/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1776 - val_acc: 0.9744\n",
      "Epoch 73/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1779 - val_acc: 0.9744\n",
      "Epoch 74/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1779 - val_acc: 0.9744\n",
      "Epoch 75/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1777 - val_acc: 0.9744\n",
      "Epoch 76/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1778 - val_acc: 0.9744\n",
      "Epoch 77/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1777 - val_acc: 0.9744\n",
      "Epoch 78/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1778 - val_acc: 0.9744\n",
      "Epoch 79/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1777 - val_acc: 0.9744\n",
      "Epoch 80/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1777 - val_acc: 0.9744\n",
      "Epoch 81/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1778 - val_acc: 0.9744\n",
      "Epoch 82/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1779 - val_acc: 0.9744\n",
      "Epoch 83/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1778 - val_acc: 0.9744\n",
      "Epoch 84/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1779 - val_acc: 0.9744\n",
      "Epoch 85/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1778 - val_acc: 0.9744\n",
      "Epoch 86/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1780 - val_acc: 0.9744\n",
      "Epoch 87/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1778 - val_acc: 0.9744\n",
      "Epoch 88/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1777 - val_acc: 0.9744\n",
      "Epoch 89/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1778 - val_acc: 0.9744\n",
      "Epoch 90/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1778 - val_acc: 0.9744\n",
      "Epoch 91/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1777 - val_acc: 0.9744\n",
      "Epoch 92/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1777 - val_acc: 0.9744\n",
      "Epoch 93/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1777 - val_acc: 0.9744\n",
      "Epoch 94/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1777 - val_acc: 0.9744\n",
      "Epoch 95/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1778 - val_acc: 0.9744\n",
      "Epoch 96/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1777 - val_acc: 0.9744\n",
      "Epoch 97/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1777 - val_acc: 0.9744\n",
      "Epoch 98/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1779 - val_acc: 0.9744\n",
      "Epoch 99/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1777 - val_acc: 0.9744\n",
      "Epoch 100/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1778 - val_acc: 0.9744\n",
      "78/78 [==============================] - 0s 378us/step\n",
      "Score for fold 9: loss of 2.1777663964491625; acc of 97.43589697740018%\n",
      "Train on 702 samples, validate on 78 samples\n",
      "Epoch 1/100\n",
      "702/702 [==============================] - 12s 17ms/step - loss: 2.5154 - acc: 0.2550 - val_loss: 2.4516 - val_acc: 0.5000\n",
      "Epoch 2/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.3291 - acc: 0.5613 - val_loss: 2.3204 - val_acc: 0.4487\n",
      "Epoch 3/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2474 - acc: 0.7365 - val_loss: 2.2761 - val_acc: 0.7179\n",
      "Epoch 4/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2241 - acc: 0.8305 - val_loss: 2.2563 - val_acc: 0.7179\n",
      "Epoch 5/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2136 - acc: 0.8689 - val_loss: 2.2108 - val_acc: 0.8590\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2038 - acc: 0.8746 - val_loss: 2.1979 - val_acc: 0.9103\n",
      "Epoch 7/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1851 - acc: 0.9516 - val_loss: 2.1930 - val_acc: 0.9231\n",
      "Epoch 8/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1887 - acc: 0.9416 - val_loss: 2.1783 - val_acc: 0.9744\n",
      "Epoch 9/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1871 - acc: 0.9302 - val_loss: 2.1923 - val_acc: 0.8718\n",
      "Epoch 10/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1783 - acc: 0.9615 - val_loss: 2.1748 - val_acc: 0.9744\n",
      "Epoch 11/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1765 - acc: 0.9772 - val_loss: 2.1773 - val_acc: 0.9872\n",
      "Epoch 12/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1824 - acc: 0.9601 - val_loss: 2.1764 - val_acc: 0.9487\n",
      "Epoch 13/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1722 - acc: 0.9900 - val_loss: 2.2042 - val_acc: 0.8205\n",
      "Epoch 14/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1773 - acc: 0.9672 - val_loss: 2.1746 - val_acc: 0.9744\n",
      "Epoch 15/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1720 - acc: 0.9829 - val_loss: 2.1749 - val_acc: 0.9487\n",
      "Epoch 16/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1750 - acc: 0.9815 - val_loss: 2.1976 - val_acc: 0.8974\n",
      "Epoch 17/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1716 - acc: 0.9872 - val_loss: 2.1775 - val_acc: 0.9615\n",
      "Epoch 18/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1729 - acc: 0.9929 - val_loss: 2.1905 - val_acc: 0.9615\n",
      "Epoch 19/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1709 - acc: 0.9872 - val_loss: 2.1721 - val_acc: 0.9615\n",
      "Epoch 20/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1738 - acc: 0.9815 - val_loss: 2.1745 - val_acc: 0.9744\n",
      "Epoch 21/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1696 - acc: 0.9915 - val_loss: 2.1757 - val_acc: 0.9744\n",
      "Epoch 22/100\n",
      "702/702 [==============================] - 4s 6ms/step - loss: 2.1726 - acc: 0.9858 - val_loss: 2.1762 - val_acc: 0.9231\n",
      "Epoch 23/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1684 - acc: 0.9986 - val_loss: 2.1720 - val_acc: 0.9744\n",
      "Epoch 24/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1685 - acc: 0.9957 - val_loss: 2.1788 - val_acc: 0.9615\n",
      "Epoch 25/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1687 - acc: 0.9943 - val_loss: 2.1743 - val_acc: 0.9744\n",
      "Epoch 26/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1696 - acc: 0.9972 - val_loss: 2.1717 - val_acc: 0.9744\n",
      "Epoch 27/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1678 - acc: 0.9986 - val_loss: 2.1718 - val_acc: 0.9744\n",
      "Epoch 28/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1677 - acc: 0.9986 - val_loss: 2.1949 - val_acc: 0.9487\n",
      "Epoch 29/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1702 - acc: 0.9943 - val_loss: 2.1721 - val_acc: 0.9744\n",
      "Epoch 30/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1680 - acc: 0.9972 - val_loss: 2.1722 - val_acc: 0.9744\n",
      "Epoch 31/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1676 - acc: 0.9900 - val_loss: 2.1886 - val_acc: 0.9359\n",
      "Epoch 32/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1690 - acc: 0.9915 - val_loss: 2.1721 - val_acc: 0.9744\n",
      "Epoch 33/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1674 - acc: 0.9972 - val_loss: 2.1710 - val_acc: 0.9872\n",
      "Epoch 34/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1670 - acc: 0.9986 - val_loss: 2.1722 - val_acc: 0.9744\n",
      "Epoch 35/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1670 - acc: 0.9986 - val_loss: 2.1713 - val_acc: 0.9872\n",
      "Epoch 36/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1670 - acc: 0.9972 - val_loss: 2.1720 - val_acc: 0.9872\n",
      "Epoch 37/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1668 - acc: 0.9986 - val_loss: 2.1723 - val_acc: 0.9487\n",
      "Epoch 38/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1669 - acc: 0.9986 - val_loss: 2.1711 - val_acc: 0.9744\n",
      "Epoch 39/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1668 - acc: 0.9986 - val_loss: 2.1720 - val_acc: 0.9744\n",
      "Epoch 40/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1668 - acc: 0.9986 - val_loss: 2.1700 - val_acc: 0.9872\n",
      "Epoch 41/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1667 - acc: 1.0000 - val_loss: 2.1698 - val_acc: 0.9872\n",
      "Epoch 42/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1668 - acc: 1.0000 - val_loss: 2.1703 - val_acc: 0.9872\n",
      "Epoch 43/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1666 - acc: 1.0000 - val_loss: 2.1695 - val_acc: 0.9872\n",
      "Epoch 44/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1666 - acc: 1.0000 - val_loss: 2.1701 - val_acc: 0.9872\n",
      "Epoch 45/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1712 - val_acc: 0.9744\n",
      "Epoch 46/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1709 - val_acc: 0.9744\n",
      "Epoch 47/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1712 - val_acc: 0.9744\n",
      "Epoch 48/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1690 - val_acc: 1.0000\n",
      "Epoch 49/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1666 - acc: 1.0000 - val_loss: 2.1717 - val_acc: 0.9872\n",
      "Epoch 50/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1719 - val_acc: 0.9615\n",
      "Epoch 51/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1702 - val_acc: 0.9744\n",
      "Epoch 52/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1711 - val_acc: 0.9615\n",
      "Epoch 53/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1700 - val_acc: 0.9872\n",
      "Epoch 54/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1712 - val_acc: 0.9872\n",
      "Epoch 55/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1694 - val_acc: 1.0000\n",
      "Epoch 56/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1694 - val_acc: 0.9872\n",
      "Epoch 57/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1690 - val_acc: 0.9872\n",
      "Epoch 58/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1723 - val_acc: 0.9744\n",
      "Epoch 59/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 0.9986 - val_loss: 2.1697 - val_acc: 0.9872\n",
      "Epoch 60/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1700 - val_acc: 0.9872\n",
      "Epoch 61/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1701 - val_acc: 0.9872\n",
      "Epoch 62/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1700 - val_acc: 0.9872\n",
      "Epoch 63/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1693 - val_acc: 0.9872\n",
      "Epoch 64/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1691 - val_acc: 1.0000\n",
      "Epoch 65/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1691 - val_acc: 1.0000\n",
      "Epoch 66/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1700 - val_acc: 0.9872\n",
      "Epoch 67/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1687 - val_acc: 0.9872\n",
      "Epoch 68/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1692 - val_acc: 0.9872\n",
      "Epoch 69/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1694 - val_acc: 0.9872\n",
      "Epoch 70/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1690 - val_acc: 1.0000\n",
      "Epoch 71/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1692 - val_acc: 0.9872\n",
      "Epoch 72/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1694 - val_acc: 0.9872\n",
      "Epoch 73/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1710 - val_acc: 0.9744\n",
      "Epoch 74/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1691 - val_acc: 0.9872\n",
      "Epoch 75/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1697 - val_acc: 0.9872\n",
      "Epoch 76/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1698 - val_acc: 0.9872\n",
      "Epoch 77/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1699 - val_acc: 0.9744\n",
      "Epoch 78/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1692 - val_acc: 0.9872\n",
      "Epoch 79/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1694 - val_acc: 0.9872\n",
      "Epoch 80/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1690 - val_acc: 0.9872\n",
      "Epoch 81/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1692 - val_acc: 0.9872\n",
      "Epoch 82/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1689 - val_acc: 0.9872\n",
      "Epoch 83/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1692 - val_acc: 0.9872\n",
      "Epoch 84/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1693 - val_acc: 0.9872\n",
      "Epoch 85/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1690 - val_acc: 1.0000\n",
      "Epoch 86/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1688 - val_acc: 1.0000\n",
      "Epoch 87/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1688 - val_acc: 0.9872\n",
      "Epoch 88/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1693 - val_acc: 0.9872\n",
      "Epoch 89/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1690 - val_acc: 1.0000\n",
      "Epoch 90/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1694 - val_acc: 1.0000\n",
      "Epoch 91/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1689 - val_acc: 0.9872\n",
      "Epoch 92/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1688 - val_acc: 1.0000\n",
      "Epoch 93/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1694 - val_acc: 0.9872\n",
      "Epoch 94/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1689 - val_acc: 0.9872\n",
      "Epoch 95/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1689 - val_acc: 1.0000\n",
      "Epoch 96/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1687 - val_acc: 1.0000\n",
      "Epoch 97/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1687 - val_acc: 0.9872\n",
      "Epoch 98/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1689 - val_acc: 1.0000\n",
      "Epoch 99/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1688 - val_acc: 0.9872\n",
      "Epoch 100/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1687 - val_acc: 1.0000\n",
      "78/78 [==============================] - 0s 359us/step\n",
      "Score for fold 10: loss of 2.1687088440626097; acc of 100.0%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1296x1296 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[95m 5       \u001b[0m | \u001b[95m 98.46   \u001b[0m | \u001b[95m 0.9108  \u001b[0m | \u001b[95m 7.582   \u001b[0m |\n",
      "Train on 702 samples, validate on 78 samples\n",
      "Epoch 1/100\n",
      "702/702 [==============================] - 12s 18ms/step - loss: 2.5173 - acc: 0.2350 - val_loss: 2.3444 - val_acc: 0.5128\n",
      "Epoch 2/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.3024 - acc: 0.5769 - val_loss: 2.2559 - val_acc: 0.6282\n",
      "Epoch 3/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2426 - acc: 0.7450 - val_loss: 2.2709 - val_acc: 0.6282\n",
      "Epoch 4/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2215 - acc: 0.8376 - val_loss: 2.2100 - val_acc: 0.8718\n",
      "Epoch 5/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2073 - acc: 0.8590 - val_loss: 2.1978 - val_acc: 0.8590\n",
      "Epoch 6/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1915 - acc: 0.9188 - val_loss: 2.2555 - val_acc: 0.8462\n",
      "Epoch 7/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1913 - acc: 0.9217 - val_loss: 2.2040 - val_acc: 0.8718\n",
      "Epoch 8/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1853 - acc: 0.9416 - val_loss: 2.1952 - val_acc: 0.8846\n",
      "Epoch 9/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1861 - acc: 0.9416 - val_loss: 2.1981 - val_acc: 0.9231\n",
      "Epoch 10/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1768 - acc: 0.9587 - val_loss: 2.1946 - val_acc: 0.8846\n",
      "Epoch 11/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1768 - acc: 0.9744 - val_loss: 2.2173 - val_acc: 0.8590\n",
      "Epoch 12/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1768 - acc: 0.9786 - val_loss: 2.1819 - val_acc: 0.9359\n",
      "Epoch 13/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1751 - acc: 0.9687 - val_loss: 2.1957 - val_acc: 0.8718\n",
      "Epoch 14/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1748 - acc: 0.9744 - val_loss: 2.1839 - val_acc: 0.9487\n",
      "Epoch 15/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1713 - acc: 0.9886 - val_loss: 2.1850 - val_acc: 0.9231\n",
      "Epoch 16/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1745 - acc: 0.9715 - val_loss: 2.1847 - val_acc: 0.9487\n",
      "Epoch 17/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1730 - acc: 0.9886 - val_loss: 2.1828 - val_acc: 0.9359\n",
      "Epoch 18/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1697 - acc: 0.9943 - val_loss: 2.1795 - val_acc: 0.9359\n",
      "Epoch 19/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1713 - acc: 0.9843 - val_loss: 2.1844 - val_acc: 0.8974\n",
      "Epoch 20/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1712 - acc: 0.9886 - val_loss: 2.1819 - val_acc: 0.9359\n",
      "Epoch 21/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1717 - acc: 0.9886 - val_loss: 2.1893 - val_acc: 0.8974\n",
      "Epoch 22/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1710 - acc: 0.9929 - val_loss: 2.1821 - val_acc: 0.9359\n",
      "Epoch 23/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1701 - acc: 0.9929 - val_loss: 2.1810 - val_acc: 0.9487\n",
      "Epoch 24/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1683 - acc: 0.9929 - val_loss: 2.1809 - val_acc: 0.9487\n",
      "Epoch 25/100\n",
      "702/702 [==============================] - 5s 7ms/step - loss: 2.1701 - acc: 0.9929 - val_loss: 2.1922 - val_acc: 0.9359\n",
      "Epoch 26/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1688 - acc: 0.9972 - val_loss: 2.1805 - val_acc: 0.9359\n",
      "Epoch 27/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1675 - acc: 0.9986 - val_loss: 2.1853 - val_acc: 0.9487\n",
      "Epoch 28/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1679 - acc: 0.9943 - val_loss: 2.1811 - val_acc: 0.9359\n",
      "Epoch 29/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1675 - acc: 0.9943 - val_loss: 2.1795 - val_acc: 0.9487\n",
      "Epoch 30/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1675 - acc: 0.9972 - val_loss: 2.1810 - val_acc: 0.9487\n",
      "Epoch 31/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1676 - acc: 0.9986 - val_loss: 2.1787 - val_acc: 0.9487\n",
      "Epoch 32/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1670 - acc: 1.0000 - val_loss: 2.1846 - val_acc: 0.9359\n",
      "Epoch 33/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1673 - acc: 0.9972 - val_loss: 2.1795 - val_acc: 0.9487\n",
      "Epoch 34/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1681 - acc: 0.9972 - val_loss: 2.1787 - val_acc: 0.9487\n",
      "Epoch 35/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1667 - acc: 1.0000 - val_loss: 2.1783 - val_acc: 0.9487\n",
      "Epoch 36/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1666 - acc: 1.0000 - val_loss: 2.1801 - val_acc: 0.9487\n",
      "Epoch 37/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1667 - acc: 0.9986 - val_loss: 2.1789 - val_acc: 0.9359\n",
      "Epoch 38/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1666 - acc: 1.0000 - val_loss: 2.1788 - val_acc: 0.9487\n",
      "Epoch 39/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1666 - acc: 1.0000 - val_loss: 2.1800 - val_acc: 0.9487\n",
      "Epoch 40/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1666 - acc: 1.0000 - val_loss: 2.1782 - val_acc: 0.9487\n",
      "Epoch 41/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1666 - acc: 1.0000 - val_loss: 2.1904 - val_acc: 0.9359\n",
      "Epoch 42/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1780 - val_acc: 0.9487\n",
      "Epoch 43/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1795 - val_acc: 0.9359\n",
      "Epoch 44/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1821 - val_acc: 0.9487\n",
      "Epoch 45/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1792 - val_acc: 0.9487\n",
      "Epoch 46/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1767 - val_acc: 0.9487\n",
      "Epoch 47/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1804 - val_acc: 0.9359\n",
      "Epoch 48/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1667 - acc: 1.0000 - val_loss: 2.1775 - val_acc: 0.9487\n",
      "Epoch 49/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1772 - val_acc: 0.9487\n",
      "Epoch 50/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1777 - val_acc: 0.9487\n",
      "Epoch 51/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1804 - val_acc: 0.9487\n",
      "Epoch 52/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1813 - val_acc: 0.9231\n",
      "Epoch 53/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1800 - val_acc: 0.9487\n",
      "Epoch 54/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1785 - val_acc: 0.9487\n",
      "Epoch 55/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1810 - val_acc: 0.9487\n",
      "Epoch 56/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1785 - val_acc: 0.9487\n",
      "Epoch 57/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1787 - val_acc: 0.9487\n",
      "Epoch 58/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1790 - val_acc: 0.9487\n",
      "Epoch 59/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1791 - val_acc: 0.9487\n",
      "Epoch 60/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1785 - val_acc: 0.9487\n",
      "Epoch 61/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1797 - val_acc: 0.9487\n",
      "Epoch 62/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1782 - val_acc: 0.9487\n",
      "Epoch 63/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1784 - val_acc: 0.9487\n",
      "Epoch 64/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1778 - val_acc: 0.9487\n",
      "Epoch 65/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1794 - val_acc: 0.9487\n",
      "Epoch 66/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1780 - val_acc: 0.9487\n",
      "Epoch 67/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1779 - val_acc: 0.9487\n",
      "Epoch 68/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1777 - val_acc: 0.9487\n",
      "Epoch 69/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1791 - val_acc: 0.9487\n",
      "Epoch 70/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1770 - val_acc: 0.9487\n",
      "Epoch 71/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1776 - val_acc: 0.9487\n",
      "Epoch 72/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1772 - val_acc: 0.9487\n",
      "Epoch 73/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1771 - val_acc: 0.9487\n",
      "Epoch 74/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1772 - val_acc: 0.9487\n",
      "Epoch 75/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1771 - val_acc: 0.9487\n",
      "Epoch 76/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1775 - val_acc: 0.9487\n",
      "Epoch 77/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1774 - val_acc: 0.9487\n",
      "Epoch 78/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1776 - val_acc: 0.9487\n",
      "Epoch 79/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1783 - val_acc: 0.9487\n",
      "Epoch 80/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1776 - val_acc: 0.9487\n",
      "Epoch 81/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1777 - val_acc: 0.9487\n",
      "Epoch 82/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1775 - val_acc: 0.9487\n",
      "Epoch 83/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1774 - val_acc: 0.9487\n",
      "Epoch 84/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1780 - val_acc: 0.9487\n",
      "Epoch 85/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1777 - val_acc: 0.9487\n",
      "Epoch 86/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1779 - val_acc: 0.9487\n",
      "Epoch 87/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1785 - val_acc: 0.9487\n",
      "Epoch 88/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1782 - val_acc: 0.9487\n",
      "Epoch 89/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1780 - val_acc: 0.9487\n",
      "Epoch 90/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1779 - val_acc: 0.9487\n",
      "Epoch 91/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1776 - val_acc: 0.9487\n",
      "Epoch 92/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1780 - val_acc: 0.9487\n",
      "Epoch 93/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1776 - val_acc: 0.9487\n",
      "Epoch 94/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1777 - val_acc: 0.9487\n",
      "Epoch 95/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1772 - val_acc: 0.9487\n",
      "Epoch 96/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1774 - val_acc: 0.9487\n",
      "Epoch 97/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1778 - val_acc: 0.9487\n",
      "Epoch 98/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1775 - val_acc: 0.9487\n",
      "Epoch 99/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1774 - val_acc: 0.9487\n",
      "Epoch 100/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1772 - val_acc: 0.9487\n",
      "78/78 [==============================] - 0s 324us/step\n",
      "Score for fold 1: loss of 2.17731813284067; acc of 94.87179487179486%\n",
      "Train on 702 samples, validate on 78 samples\n",
      "Epoch 1/100\n",
      "702/702 [==============================] - 13s 18ms/step - loss: 2.4651 - acc: 0.3063 - val_loss: 2.3876 - val_acc: 0.3590\n",
      "Epoch 2/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2939 - acc: 0.6254 - val_loss: 2.2812 - val_acc: 0.7051\n",
      "Epoch 3/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2514 - acc: 0.7650 - val_loss: 2.2343 - val_acc: 0.7949\n",
      "Epoch 4/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2106 - acc: 0.8661 - val_loss: 2.2238 - val_acc: 0.8462\n",
      "Epoch 5/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2127 - acc: 0.8618 - val_loss: 2.2204 - val_acc: 0.8333\n",
      "Epoch 6/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1906 - acc: 0.9330 - val_loss: 2.2024 - val_acc: 0.8846\n",
      "Epoch 7/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1867 - acc: 0.9330 - val_loss: 2.2231 - val_acc: 0.8333\n",
      "Epoch 8/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1823 - acc: 0.9530 - val_loss: 2.1972 - val_acc: 0.8590\n",
      "Epoch 9/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1832 - acc: 0.9473 - val_loss: 2.1805 - val_acc: 0.9615\n",
      "Epoch 10/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1787 - acc: 0.9658 - val_loss: 2.2066 - val_acc: 0.8590\n",
      "Epoch 11/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1766 - acc: 0.9786 - val_loss: 2.1781 - val_acc: 0.9487\n",
      "Epoch 12/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1761 - acc: 0.9786 - val_loss: 2.1844 - val_acc: 0.9615\n",
      "Epoch 13/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1738 - acc: 0.9886 - val_loss: 2.1778 - val_acc: 0.9231\n",
      "Epoch 14/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1756 - acc: 0.9815 - val_loss: 2.1739 - val_acc: 1.0000\n",
      "Epoch 15/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1707 - acc: 0.9929 - val_loss: 2.1780 - val_acc: 0.9231\n",
      "Epoch 16/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1733 - acc: 0.9815 - val_loss: 2.1767 - val_acc: 0.9615\n",
      "Epoch 17/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1714 - acc: 0.9886 - val_loss: 2.1806 - val_acc: 0.9615\n",
      "Epoch 18/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1705 - acc: 0.9943 - val_loss: 2.1759 - val_acc: 0.9487\n",
      "Epoch 19/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1736 - acc: 0.9829 - val_loss: 2.1721 - val_acc: 0.9872\n",
      "Epoch 20/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1690 - acc: 0.9943 - val_loss: 2.1934 - val_acc: 0.8846\n",
      "Epoch 21/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1709 - acc: 0.9872 - val_loss: 2.1718 - val_acc: 0.9744\n",
      "Epoch 22/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1717 - acc: 0.9929 - val_loss: 2.1709 - val_acc: 0.9744\n",
      "Epoch 23/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1687 - acc: 0.9915 - val_loss: 2.1740 - val_acc: 0.9359\n",
      "Epoch 24/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1686 - acc: 0.9957 - val_loss: 2.1742 - val_acc: 0.9487\n",
      "Epoch 25/100\n",
      "702/702 [==============================] - 5s 6ms/step - loss: 2.1729 - acc: 0.9729 - val_loss: 2.1711 - val_acc: 0.9744\n",
      "Epoch 26/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1678 - acc: 0.9986 - val_loss: 2.1740 - val_acc: 0.9744\n",
      "Epoch 27/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1681 - acc: 0.9972 - val_loss: 2.1709 - val_acc: 0.9744\n",
      "Epoch 28/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1676 - acc: 0.9986 - val_loss: 2.1701 - val_acc: 0.9744\n",
      "Epoch 29/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1691 - acc: 0.9943 - val_loss: 2.1694 - val_acc: 1.0000\n",
      "Epoch 30/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1676 - acc: 0.9957 - val_loss: 2.1697 - val_acc: 0.9872\n",
      "Epoch 31/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1674 - acc: 0.9986 - val_loss: 2.1693 - val_acc: 1.0000\n",
      "Epoch 32/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1677 - acc: 0.9957 - val_loss: 2.1695 - val_acc: 1.0000\n",
      "Epoch 33/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1674 - acc: 0.9972 - val_loss: 2.1696 - val_acc: 0.9872\n",
      "Epoch 34/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1675 - acc: 0.9972 - val_loss: 2.1696 - val_acc: 1.0000\n",
      "Epoch 35/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1678 - acc: 0.9943 - val_loss: 2.1690 - val_acc: 1.0000\n",
      "Epoch 36/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1667 - acc: 1.0000 - val_loss: 2.1693 - val_acc: 0.9744\n",
      "Epoch 37/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1666 - acc: 1.0000 - val_loss: 2.1691 - val_acc: 1.0000\n",
      "Epoch 38/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1668 - acc: 1.0000 - val_loss: 2.1690 - val_acc: 1.0000\n",
      "Epoch 39/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1667 - acc: 1.0000 - val_loss: 2.1690 - val_acc: 1.0000\n",
      "Epoch 40/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1668 - acc: 0.9986 - val_loss: 2.1685 - val_acc: 1.0000\n",
      "Epoch 41/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1687 - val_acc: 0.9872\n",
      "Epoch 42/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1666 - acc: 1.0000 - val_loss: 2.1711 - val_acc: 0.9744\n",
      "Epoch 43/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1666 - acc: 1.0000 - val_loss: 2.1689 - val_acc: 0.9872\n",
      "Epoch 44/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1690 - val_acc: 1.0000\n",
      "Epoch 45/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1667 - acc: 1.0000 - val_loss: 2.1686 - val_acc: 1.0000\n",
      "Epoch 46/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1666 - acc: 1.0000 - val_loss: 2.1686 - val_acc: 1.0000\n",
      "Epoch 47/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1687 - val_acc: 0.9872\n",
      "Epoch 48/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1686 - val_acc: 1.0000\n",
      "Epoch 49/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1686 - val_acc: 0.9872\n",
      "Epoch 50/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1684 - val_acc: 0.9872\n",
      "Epoch 51/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1687 - val_acc: 1.0000\n",
      "Epoch 52/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1692 - val_acc: 0.9872\n",
      "Epoch 53/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1684 - val_acc: 1.0000\n",
      "Epoch 54/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1687 - val_acc: 0.9872\n",
      "Epoch 55/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1685 - val_acc: 1.0000\n",
      "Epoch 56/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1681 - val_acc: 1.0000\n",
      "Epoch 57/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1682 - val_acc: 1.0000\n",
      "Epoch 58/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1683 - val_acc: 1.0000\n",
      "Epoch 59/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1682 - val_acc: 1.0000\n",
      "Epoch 60/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1684 - val_acc: 1.0000\n",
      "Epoch 61/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1684 - val_acc: 0.9744\n",
      "Epoch 62/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1681 - val_acc: 1.0000\n",
      "Epoch 63/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1683 - val_acc: 1.0000\n",
      "Epoch 64/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1685 - val_acc: 0.9872\n",
      "Epoch 65/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1683 - val_acc: 0.9872\n",
      "Epoch 66/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1680 - val_acc: 1.0000\n",
      "Epoch 67/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1681 - val_acc: 1.0000\n",
      "Epoch 68/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1681 - val_acc: 1.0000\n",
      "Epoch 69/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1681 - val_acc: 1.0000\n",
      "Epoch 70/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1680 - val_acc: 1.0000\n",
      "Epoch 71/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1679 - val_acc: 1.0000\n",
      "Epoch 72/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1681 - val_acc: 1.0000\n",
      "Epoch 73/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1682 - val_acc: 0.9872\n",
      "Epoch 74/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1682 - val_acc: 1.0000\n",
      "Epoch 75/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1679 - val_acc: 0.9872\n",
      "Epoch 76/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1680 - val_acc: 1.0000\n",
      "Epoch 77/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1681 - val_acc: 1.0000\n",
      "Epoch 78/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1682 - val_acc: 0.9872\n",
      "Epoch 79/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1681 - val_acc: 1.0000\n",
      "Epoch 80/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1682 - val_acc: 0.9872\n",
      "Epoch 81/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1681 - val_acc: 0.9872\n",
      "Epoch 82/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1681 - val_acc: 1.0000\n",
      "Epoch 83/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1682 - val_acc: 0.9872\n",
      "Epoch 84/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1682 - val_acc: 1.0000\n",
      "Epoch 85/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1682 - val_acc: 0.9872\n",
      "Epoch 86/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1681 - val_acc: 1.0000\n",
      "Epoch 87/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1683 - val_acc: 0.9872\n",
      "Epoch 88/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1681 - val_acc: 0.9872\n",
      "Epoch 89/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1682 - val_acc: 0.9872\n",
      "Epoch 90/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1683 - val_acc: 0.9872\n",
      "Epoch 91/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1682 - val_acc: 0.9872\n",
      "Epoch 92/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1681 - val_acc: 1.0000\n",
      "Epoch 93/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1681 - val_acc: 1.0000\n",
      "Epoch 94/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1681 - val_acc: 0.9872\n",
      "Epoch 95/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1682 - val_acc: 0.9872\n",
      "Epoch 96/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1682 - val_acc: 0.9872\n",
      "Epoch 97/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1681 - val_acc: 0.9872\n",
      "Epoch 98/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1681 - val_acc: 0.9872\n",
      "Epoch 99/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1681 - val_acc: 0.9872\n",
      "Epoch 100/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1681 - val_acc: 0.9872\n",
      "78/78 [==============================] - 0s 321us/step\n",
      "Score for fold 2: loss of 2.168176553188226; acc of 98.71794871794873%\n",
      "Train on 702 samples, validate on 78 samples\n",
      "Epoch 1/100\n",
      "702/702 [==============================] - 13s 18ms/step - loss: 2.4978 - acc: 0.2479 - val_loss: 2.4146 - val_acc: 0.3590\n",
      "Epoch 2/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.3092 - acc: 0.5570 - val_loss: 2.4703 - val_acc: 0.3205\n",
      "Epoch 3/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2554 - acc: 0.7194 - val_loss: 2.2528 - val_acc: 0.6282\n",
      "Epoch 4/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2211 - acc: 0.8034 - val_loss: 2.2218 - val_acc: 0.8718\n",
      "Epoch 5/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2168 - acc: 0.8462 - val_loss: 2.2309 - val_acc: 0.8205\n",
      "Epoch 6/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1948 - acc: 0.9003 - val_loss: 2.1911 - val_acc: 0.9359\n",
      "Epoch 7/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1875 - acc: 0.9217 - val_loss: 2.2150 - val_acc: 0.8077\n",
      "Epoch 8/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1908 - acc: 0.9202 - val_loss: 2.2054 - val_acc: 0.9103\n",
      "Epoch 9/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1769 - acc: 0.9744 - val_loss: 2.1857 - val_acc: 0.9359\n",
      "Epoch 10/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1786 - acc: 0.9729 - val_loss: 2.1807 - val_acc: 0.9359\n",
      "Epoch 11/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1763 - acc: 0.9601 - val_loss: 2.1815 - val_acc: 0.9615\n",
      "Epoch 12/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1805 - acc: 0.9658 - val_loss: 2.1803 - val_acc: 0.9487\n",
      "Epoch 13/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1715 - acc: 0.9872 - val_loss: 2.1945 - val_acc: 0.9359\n",
      "Epoch 14/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1781 - acc: 0.9701 - val_loss: 2.1792 - val_acc: 0.9744\n",
      "Epoch 15/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1712 - acc: 0.9900 - val_loss: 2.1821 - val_acc: 0.9744\n",
      "Epoch 16/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1733 - acc: 0.9900 - val_loss: 2.1822 - val_acc: 0.9744\n",
      "Epoch 17/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1709 - acc: 0.9929 - val_loss: 2.1755 - val_acc: 0.9487\n",
      "Epoch 18/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1697 - acc: 0.9943 - val_loss: 2.1914 - val_acc: 0.9615\n",
      "Epoch 19/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1716 - acc: 0.9872 - val_loss: 2.1764 - val_acc: 0.9615\n",
      "Epoch 20/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1690 - acc: 0.9915 - val_loss: 2.1781 - val_acc: 0.9359\n",
      "Epoch 21/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1725 - acc: 0.9900 - val_loss: 2.1844 - val_acc: 0.9744\n",
      "Epoch 22/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1692 - acc: 0.9900 - val_loss: 2.1772 - val_acc: 0.9744\n",
      "Epoch 23/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1700 - acc: 0.9957 - val_loss: 2.1779 - val_acc: 0.9744\n",
      "Epoch 24/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1682 - acc: 0.9929 - val_loss: 2.1809 - val_acc: 0.9872\n",
      "Epoch 25/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1686 - acc: 0.9929 - val_loss: 2.1734 - val_acc: 0.9872\n",
      "Epoch 26/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1705 - acc: 0.9915 - val_loss: 2.1712 - val_acc: 0.9615\n",
      "Epoch 27/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1673 - acc: 1.0000 - val_loss: 2.1759 - val_acc: 0.9872\n",
      "Epoch 28/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1679 - acc: 0.9986 - val_loss: 2.1697 - val_acc: 0.9872\n",
      "Epoch 29/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1686 - acc: 0.9986 - val_loss: 2.1864 - val_acc: 1.0000\n",
      "Epoch 30/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1681 - acc: 0.9972 - val_loss: 2.1732 - val_acc: 1.0000\n",
      "Epoch 31/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1685 - acc: 0.9972 - val_loss: 2.1687 - val_acc: 1.0000\n",
      "Epoch 32/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1672 - acc: 1.0000 - val_loss: 2.1740 - val_acc: 0.9872\n",
      "Epoch 33/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1672 - acc: 0.9986 - val_loss: 2.1704 - val_acc: 0.9872\n",
      "Epoch 34/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1688 - acc: 0.9972 - val_loss: 2.1712 - val_acc: 1.0000\n",
      "Epoch 35/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1673 - acc: 1.0000 - val_loss: 2.1724 - val_acc: 0.9872\n",
      "Epoch 36/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1670 - acc: 1.0000 - val_loss: 2.1720 - val_acc: 0.9872\n",
      "Epoch 37/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1672 - acc: 0.9986 - val_loss: 2.1759 - val_acc: 0.9872\n",
      "Epoch 38/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1699 - acc: 0.9943 - val_loss: 2.1707 - val_acc: 0.9872\n",
      "Epoch 39/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1672 - acc: 0.9986 - val_loss: 2.1713 - val_acc: 0.9872\n",
      "Epoch 40/100\n",
      "702/702 [==============================] - 5s 7ms/step - loss: 2.1671 - acc: 1.0000 - val_loss: 2.1779 - val_acc: 0.9872\n",
      "Epoch 41/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1667 - acc: 1.0000 - val_loss: 2.1707 - val_acc: 0.9872\n",
      "Epoch 42/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1666 - acc: 1.0000 - val_loss: 2.1732 - val_acc: 1.0000\n",
      "Epoch 43/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1666 - acc: 1.0000 - val_loss: 2.1789 - val_acc: 0.9615\n",
      "Epoch 44/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1673 - acc: 0.9986 - val_loss: 2.1699 - val_acc: 0.9872\n",
      "Epoch 45/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1693 - val_acc: 0.9872\n",
      "Epoch 46/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1709 - val_acc: 0.9872\n",
      "Epoch 47/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1719 - val_acc: 0.9744\n",
      "Epoch 48/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1670 - acc: 0.9986 - val_loss: 2.1713 - val_acc: 0.9872\n",
      "Epoch 49/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1693 - val_acc: 0.9872\n",
      "Epoch 50/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1666 - acc: 1.0000 - val_loss: 2.1703 - val_acc: 0.9872\n",
      "Epoch 51/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1678 - val_acc: 1.0000\n",
      "Epoch 52/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1715 - val_acc: 0.9872\n",
      "Epoch 53/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1679 - val_acc: 1.0000\n",
      "Epoch 54/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1700 - val_acc: 0.9872\n",
      "Epoch 55/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1703 - val_acc: 0.9872\n",
      "Epoch 56/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1753 - val_acc: 0.9744\n",
      "Epoch 57/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 0.9986 - val_loss: 2.1694 - val_acc: 0.9872\n",
      "Epoch 58/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1711 - val_acc: 0.9872\n",
      "Epoch 59/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1711 - val_acc: 1.0000\n",
      "Epoch 60/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1719 - val_acc: 0.9872\n",
      "Epoch 61/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1685 - val_acc: 1.0000\n",
      "Epoch 62/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1686 - val_acc: 1.0000\n",
      "Epoch 63/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1690 - val_acc: 0.9872\n",
      "Epoch 64/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1692 - val_acc: 1.0000\n",
      "Epoch 65/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1686 - val_acc: 0.9872\n",
      "Epoch 66/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1691 - val_acc: 0.9872\n",
      "Epoch 67/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1696 - val_acc: 0.9872\n",
      "Epoch 68/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1682 - val_acc: 1.0000\n",
      "Epoch 69/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1696 - val_acc: 0.9872\n",
      "Epoch 70/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1693 - val_acc: 0.9872\n",
      "Epoch 71/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1684 - val_acc: 1.0000\n",
      "Epoch 72/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1686 - val_acc: 0.9872\n",
      "Epoch 73/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1695 - val_acc: 0.9872\n",
      "Epoch 74/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1696 - val_acc: 0.9872\n",
      "Epoch 75/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1687 - val_acc: 0.9872\n",
      "Epoch 76/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1693 - val_acc: 0.9872\n",
      "Epoch 77/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1688 - val_acc: 1.0000\n",
      "Epoch 78/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1684 - val_acc: 1.0000\n",
      "Epoch 79/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1684 - val_acc: 1.0000\n",
      "Epoch 80/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1699 - val_acc: 0.9872\n",
      "Epoch 81/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1686 - val_acc: 1.0000\n",
      "Epoch 82/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1691 - val_acc: 1.0000\n",
      "Epoch 83/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1692 - val_acc: 1.0000\n",
      "Epoch 84/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1694 - val_acc: 0.9872\n",
      "Epoch 85/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1690 - val_acc: 1.0000\n",
      "Epoch 86/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1688 - val_acc: 0.9872\n",
      "Epoch 87/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1692 - val_acc: 1.0000\n",
      "Epoch 88/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1694 - val_acc: 1.0000\n",
      "Epoch 89/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1686 - val_acc: 1.0000\n",
      "Epoch 90/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1696 - val_acc: 0.9872\n",
      "Epoch 91/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1685 - val_acc: 1.0000\n",
      "Epoch 92/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1692 - val_acc: 1.0000\n",
      "Epoch 93/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1688 - val_acc: 1.0000\n",
      "Epoch 94/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1688 - val_acc: 1.0000\n",
      "Epoch 95/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1688 - val_acc: 1.0000\n",
      "Epoch 96/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1691 - val_acc: 1.0000\n",
      "Epoch 97/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1690 - val_acc: 0.9872\n",
      "Epoch 98/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1688 - val_acc: 1.0000\n",
      "Epoch 99/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1685 - val_acc: 1.0000\n",
      "Epoch 100/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1693 - val_acc: 1.0000\n",
      "78/78 [==============================] - 0s 323us/step\n",
      "Score for fold 3: loss of 2.16942844635401; acc of 100.0%\n",
      "Train on 702 samples, validate on 78 samples\n",
      "Epoch 1/100\n",
      "702/702 [==============================] - 13s 19ms/step - loss: 2.4854 - acc: 0.3219 - val_loss: 2.3747 - val_acc: 0.3333\n",
      "Epoch 2/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "702/702 [==============================] - 1s 1ms/step - loss: 2.3360 - acc: 0.5399 - val_loss: 2.2818 - val_acc: 0.6538\n",
      "Epoch 3/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2537 - acc: 0.7151 - val_loss: 2.2691 - val_acc: 0.7051\n",
      "Epoch 4/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2225 - acc: 0.8177 - val_loss: 2.2455 - val_acc: 0.7436\n",
      "Epoch 5/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2102 - acc: 0.8746 - val_loss: 2.2006 - val_acc: 0.8846\n",
      "Epoch 6/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2039 - acc: 0.8946 - val_loss: 2.2010 - val_acc: 0.8077\n",
      "Epoch 7/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1866 - acc: 0.9459 - val_loss: 2.1962 - val_acc: 0.8718\n",
      "Epoch 8/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1914 - acc: 0.9373 - val_loss: 2.1796 - val_acc: 0.9744\n",
      "Epoch 9/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1781 - acc: 0.9601 - val_loss: 2.2200 - val_acc: 0.7949\n",
      "Epoch 10/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1799 - acc: 0.9516 - val_loss: 2.1979 - val_acc: 0.8590\n",
      "Epoch 11/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1772 - acc: 0.9843 - val_loss: 2.1738 - val_acc: 0.9487\n",
      "Epoch 12/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1757 - acc: 0.9872 - val_loss: 2.1803 - val_acc: 0.9487\n",
      "Epoch 13/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1755 - acc: 0.9815 - val_loss: 2.1740 - val_acc: 0.9744\n",
      "Epoch 14/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1736 - acc: 0.9858 - val_loss: 2.1748 - val_acc: 0.9744\n",
      "Epoch 15/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1719 - acc: 0.9815 - val_loss: 2.1880 - val_acc: 0.9231\n",
      "Epoch 16/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1718 - acc: 0.9943 - val_loss: 2.1754 - val_acc: 0.9615\n",
      "Epoch 17/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1697 - acc: 0.9943 - val_loss: 2.1716 - val_acc: 0.9487\n",
      "Epoch 18/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1749 - acc: 0.9801 - val_loss: 2.1737 - val_acc: 0.9359\n",
      "Epoch 19/100\n",
      "702/702 [==============================] - 5s 7ms/step - loss: 2.1691 - acc: 0.9986 - val_loss: 2.1717 - val_acc: 0.9744\n",
      "Epoch 20/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1687 - acc: 0.9972 - val_loss: 2.1736 - val_acc: 0.9615\n",
      "Epoch 21/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1682 - acc: 0.9972 - val_loss: 2.1711 - val_acc: 1.0000\n",
      "Epoch 22/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1679 - acc: 0.9986 - val_loss: 2.1694 - val_acc: 0.9615\n",
      "Epoch 23/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1680 - acc: 0.9957 - val_loss: 2.1707 - val_acc: 0.9744\n",
      "Epoch 24/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1679 - acc: 0.9986 - val_loss: 2.1700 - val_acc: 0.9872\n",
      "Epoch 25/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1682 - acc: 0.9957 - val_loss: 2.1711 - val_acc: 0.9615\n",
      "Epoch 26/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1682 - acc: 0.9943 - val_loss: 2.1702 - val_acc: 0.9872\n",
      "Epoch 27/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1672 - acc: 0.9986 - val_loss: 2.1701 - val_acc: 0.9615\n",
      "Epoch 28/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1673 - acc: 0.9986 - val_loss: 2.1696 - val_acc: 0.9872\n",
      "Epoch 29/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1672 - acc: 0.9986 - val_loss: 2.1689 - val_acc: 0.9872\n",
      "Epoch 30/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1680 - acc: 0.9972 - val_loss: 2.1687 - val_acc: 1.0000\n",
      "Epoch 31/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1672 - acc: 1.0000 - val_loss: 2.1714 - val_acc: 0.9872\n",
      "Epoch 32/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1670 - acc: 1.0000 - val_loss: 2.1685 - val_acc: 1.0000\n",
      "Epoch 33/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1666 - acc: 1.0000 - val_loss: 2.1685 - val_acc: 0.9744\n",
      "Epoch 34/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1666 - acc: 1.0000 - val_loss: 2.1681 - val_acc: 1.0000\n",
      "Epoch 35/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1667 - acc: 1.0000 - val_loss: 2.1691 - val_acc: 0.9872\n",
      "Epoch 36/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1666 - acc: 1.0000 - val_loss: 2.1743 - val_acc: 0.9744\n",
      "Epoch 37/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1671 - acc: 0.9986 - val_loss: 2.1682 - val_acc: 0.9872\n",
      "Epoch 38/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1680 - val_acc: 1.0000\n",
      "Epoch 39/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1683 - val_acc: 0.9872\n",
      "Epoch 40/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1702 - val_acc: 0.9872\n",
      "Epoch 41/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1685 - val_acc: 1.0000\n",
      "Epoch 42/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1666 - acc: 1.0000 - val_loss: 2.1685 - val_acc: 0.9872\n",
      "Epoch 43/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1680 - val_acc: 0.9872\n",
      "Epoch 44/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1682 - val_acc: 0.9872\n",
      "Epoch 45/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1686 - val_acc: 0.9872\n",
      "Epoch 46/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1685 - val_acc: 0.9872\n",
      "Epoch 47/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1698 - val_acc: 0.9872\n",
      "Epoch 48/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1681 - val_acc: 0.9872\n",
      "Epoch 49/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1684 - val_acc: 0.9872\n",
      "Epoch 50/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1678 - val_acc: 1.0000\n",
      "Epoch 51/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1676 - val_acc: 1.0000\n",
      "Epoch 52/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1675 - val_acc: 1.0000\n",
      "Epoch 53/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1680 - val_acc: 0.9872\n",
      "Epoch 54/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1675 - val_acc: 1.0000\n",
      "Epoch 55/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1685 - val_acc: 0.9872\n",
      "Epoch 56/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1681 - val_acc: 0.9872\n",
      "Epoch 57/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1676 - val_acc: 1.0000\n",
      "Epoch 58/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1676 - val_acc: 0.9872\n",
      "Epoch 59/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1677 - val_acc: 0.9872\n",
      "Epoch 60/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1681 - val_acc: 0.9872\n",
      "Epoch 61/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1679 - val_acc: 1.0000\n",
      "Epoch 62/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1679 - val_acc: 1.0000\n",
      "Epoch 63/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1676 - val_acc: 1.0000\n",
      "Epoch 64/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1676 - val_acc: 1.0000\n",
      "Epoch 65/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1678 - val_acc: 1.0000\n",
      "Epoch 66/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1678 - val_acc: 1.0000\n",
      "Epoch 67/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1679 - val_acc: 0.9872\n",
      "Epoch 68/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1677 - val_acc: 1.0000\n",
      "Epoch 69/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1677 - val_acc: 1.0000\n",
      "Epoch 70/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1675 - val_acc: 1.0000\n",
      "Epoch 71/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1676 - val_acc: 1.0000\n",
      "Epoch 72/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1676 - val_acc: 1.0000\n",
      "Epoch 73/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1674 - val_acc: 1.0000\n",
      "Epoch 74/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1674 - val_acc: 1.0000\n",
      "Epoch 75/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1677 - val_acc: 0.9872\n",
      "Epoch 76/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1675 - val_acc: 1.0000\n",
      "Epoch 77/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1675 - val_acc: 1.0000\n",
      "Epoch 78/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1677 - val_acc: 1.0000\n",
      "Epoch 79/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1676 - val_acc: 1.0000\n",
      "Epoch 80/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1676 - val_acc: 1.0000\n",
      "Epoch 81/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1674 - val_acc: 1.0000\n",
      "Epoch 82/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1675 - val_acc: 1.0000\n",
      "Epoch 83/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1675 - val_acc: 1.0000\n",
      "Epoch 84/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1675 - val_acc: 1.0000\n",
      "Epoch 85/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1674 - val_acc: 1.0000\n",
      "Epoch 86/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1673 - val_acc: 1.0000\n",
      "Epoch 87/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1675 - val_acc: 1.0000\n",
      "Epoch 88/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1674 - val_acc: 1.0000\n",
      "Epoch 89/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1675 - val_acc: 1.0000\n",
      "Epoch 90/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1675 - val_acc: 1.0000\n",
      "Epoch 91/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1675 - val_acc: 1.0000\n",
      "Epoch 92/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1675 - val_acc: 1.0000\n",
      "Epoch 93/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1675 - val_acc: 1.0000\n",
      "Epoch 94/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1674 - val_acc: 1.0000\n",
      "Epoch 95/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1674 - val_acc: 1.0000\n",
      "Epoch 96/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1674 - val_acc: 1.0000\n",
      "Epoch 97/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1674 - val_acc: 1.0000\n",
      "Epoch 98/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1675 - val_acc: 1.0000\n",
      "Epoch 99/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1674 - val_acc: 1.0000\n",
      "Epoch 100/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1675 - val_acc: 1.0000\n",
      "78/78 [==============================] - 0s 333us/step\n",
      "Score for fold 4: loss of 2.167764902114868; acc of 98.71794825945145%\n",
      "Train on 702 samples, validate on 78 samples\n",
      "Epoch 1/100\n",
      "702/702 [==============================] - 13s 19ms/step - loss: 2.4843 - acc: 0.3105 - val_loss: 2.3685 - val_acc: 0.4744\n",
      "Epoch 2/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.3335 - acc: 0.5285 - val_loss: 2.2883 - val_acc: 0.5769\n",
      "Epoch 3/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2452 - acc: 0.7450 - val_loss: 2.2574 - val_acc: 0.7179\n",
      "Epoch 4/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2148 - acc: 0.8476 - val_loss: 2.2589 - val_acc: 0.7692\n",
      "Epoch 5/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2120 - acc: 0.8561 - val_loss: 2.2085 - val_acc: 0.8462\n",
      "Epoch 6/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1990 - acc: 0.8989 - val_loss: 2.2021 - val_acc: 0.8974\n",
      "Epoch 7/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1907 - acc: 0.9373 - val_loss: 2.1974 - val_acc: 0.8846\n",
      "Epoch 8/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1881 - acc: 0.9316 - val_loss: 2.2083 - val_acc: 0.8846\n",
      "Epoch 9/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1825 - acc: 0.9601 - val_loss: 2.1954 - val_acc: 0.8846\n",
      "Epoch 10/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1811 - acc: 0.9516 - val_loss: 2.1887 - val_acc: 0.9487\n",
      "Epoch 11/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1789 - acc: 0.9715 - val_loss: 2.2270 - val_acc: 0.7949\n",
      "Epoch 12/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1772 - acc: 0.9786 - val_loss: 2.1841 - val_acc: 0.9359\n",
      "Epoch 13/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1747 - acc: 0.9815 - val_loss: 2.1805 - val_acc: 0.9615\n",
      "Epoch 14/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1748 - acc: 0.9886 - val_loss: 2.2028 - val_acc: 0.8974\n",
      "Epoch 15/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1739 - acc: 0.9858 - val_loss: 2.2141 - val_acc: 0.8846\n",
      "Epoch 16/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1724 - acc: 0.9900 - val_loss: 2.1774 - val_acc: 0.9744\n",
      "Epoch 17/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1746 - acc: 0.9801 - val_loss: 2.1780 - val_acc: 0.9615\n",
      "Epoch 18/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1703 - acc: 0.9929 - val_loss: 2.1751 - val_acc: 0.9744\n",
      "Epoch 19/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1701 - acc: 0.9972 - val_loss: 2.2690 - val_acc: 0.7179\n",
      "Epoch 20/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1710 - acc: 0.9886 - val_loss: 2.1745 - val_acc: 0.9744\n",
      "Epoch 21/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1719 - acc: 0.9886 - val_loss: 2.1743 - val_acc: 0.9744\n",
      "Epoch 22/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1725 - acc: 0.9815 - val_loss: 2.1827 - val_acc: 0.9615\n",
      "Epoch 23/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1707 - acc: 0.9972 - val_loss: 2.1762 - val_acc: 1.0000\n",
      "Epoch 24/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1696 - acc: 0.9943 - val_loss: 2.1743 - val_acc: 0.9744\n",
      "Epoch 25/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1685 - acc: 0.9986 - val_loss: 2.1728 - val_acc: 0.9744\n",
      "Epoch 26/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1683 - acc: 0.9929 - val_loss: 2.1761 - val_acc: 0.9744\n",
      "Epoch 27/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1716 - acc: 0.9943 - val_loss: 2.1815 - val_acc: 0.9615\n",
      "Epoch 28/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1680 - acc: 1.0000 - val_loss: 2.1727 - val_acc: 0.9872\n",
      "Epoch 29/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1701 - acc: 0.9929 - val_loss: 2.1768 - val_acc: 0.9615\n",
      "Epoch 30/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1683 - acc: 0.9972 - val_loss: 2.1709 - val_acc: 0.9872\n",
      "Epoch 31/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1673 - acc: 1.0000 - val_loss: 2.1733 - val_acc: 0.9615\n",
      "Epoch 32/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1692 - acc: 0.9957 - val_loss: 2.1716 - val_acc: 0.9872\n",
      "Epoch 33/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1671 - acc: 1.0000 - val_loss: 2.1751 - val_acc: 0.9744\n",
      "Epoch 34/100\n",
      "702/702 [==============================] - 5s 7ms/step - loss: 2.1672 - acc: 0.9986 - val_loss: 2.1808 - val_acc: 0.9487\n",
      "Epoch 35/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1687 - acc: 0.9943 - val_loss: 2.1754 - val_acc: 0.9744\n",
      "Epoch 36/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1668 - acc: 1.0000 - val_loss: 2.1715 - val_acc: 0.9872\n",
      "Epoch 37/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1669 - acc: 0.9957 - val_loss: 2.1712 - val_acc: 0.9744\n",
      "Epoch 38/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1670 - acc: 0.9972 - val_loss: 2.1720 - val_acc: 0.9744\n",
      "Epoch 39/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1667 - acc: 1.0000 - val_loss: 2.1724 - val_acc: 0.9872\n",
      "Epoch 40/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1667 - acc: 1.0000 - val_loss: 2.1714 - val_acc: 0.9872\n",
      "Epoch 41/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1667 - acc: 1.0000 - val_loss: 2.1716 - val_acc: 0.9872\n",
      "Epoch 42/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1667 - acc: 1.0000 - val_loss: 2.1747 - val_acc: 0.9872\n",
      "Epoch 43/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1669 - acc: 1.0000 - val_loss: 2.1712 - val_acc: 0.9744\n",
      "Epoch 44/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1683 - acc: 0.9972 - val_loss: 2.1706 - val_acc: 0.9744\n",
      "Epoch 45/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1709 - val_acc: 0.9872\n",
      "Epoch 46/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1709 - val_acc: 0.9872\n",
      "Epoch 47/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1706 - val_acc: 0.9872\n",
      "Epoch 48/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1711 - val_acc: 0.9744\n",
      "Epoch 49/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1707 - val_acc: 0.9744\n",
      "Epoch 50/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1708 - val_acc: 0.9744\n",
      "Epoch 51/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1721 - val_acc: 0.9872\n",
      "Epoch 52/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1707 - val_acc: 0.9872\n",
      "Epoch 53/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1681 - acc: 0.9972 - val_loss: 2.1704 - val_acc: 0.9872\n",
      "Epoch 54/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1705 - val_acc: 0.9872\n",
      "Epoch 55/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1703 - val_acc: 0.9872\n",
      "Epoch 56/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1702 - val_acc: 0.9872\n",
      "Epoch 57/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1704 - val_acc: 0.9872\n",
      "Epoch 58/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1711 - val_acc: 0.9872\n",
      "Epoch 59/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1709 - val_acc: 0.9872\n",
      "Epoch 60/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1707 - val_acc: 0.9872\n",
      "Epoch 61/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1711 - val_acc: 0.9872\n",
      "Epoch 62/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1715 - val_acc: 0.9872\n",
      "Epoch 63/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1704 - val_acc: 0.9872\n",
      "Epoch 64/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1704 - val_acc: 0.9872\n",
      "Epoch 65/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1705 - val_acc: 0.9872\n",
      "Epoch 66/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1707 - val_acc: 0.9872\n",
      "Epoch 67/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1704 - val_acc: 0.9872\n",
      "Epoch 68/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1704 - val_acc: 0.9872\n",
      "Epoch 69/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1703 - val_acc: 0.9872\n",
      "Epoch 70/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1704 - val_acc: 0.9872\n",
      "Epoch 71/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1704 - val_acc: 0.9872\n",
      "Epoch 72/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1707 - val_acc: 0.9872\n",
      "Epoch 73/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1706 - val_acc: 0.9872\n",
      "Epoch 74/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1705 - val_acc: 0.9872\n",
      "Epoch 75/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1702 - val_acc: 0.9872\n",
      "Epoch 76/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1704 - val_acc: 0.9872\n",
      "Epoch 77/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1704 - val_acc: 0.9872\n",
      "Epoch 78/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1700 - val_acc: 0.9872\n",
      "Epoch 79/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1704 - val_acc: 0.9872\n",
      "Epoch 80/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1706 - val_acc: 0.9872\n",
      "Epoch 81/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1704 - val_acc: 0.9872\n",
      "Epoch 82/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1705 - val_acc: 0.9872\n",
      "Epoch 83/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1705 - val_acc: 0.9872\n",
      "Epoch 84/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1705 - val_acc: 0.9872\n",
      "Epoch 85/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1704 - val_acc: 0.9872\n",
      "Epoch 86/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1703 - val_acc: 0.9872\n",
      "Epoch 87/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1704 - val_acc: 0.9872\n",
      "Epoch 88/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1704 - val_acc: 0.9872\n",
      "Epoch 89/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1703 - val_acc: 0.9872\n",
      "Epoch 90/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1703 - val_acc: 0.9872\n",
      "Epoch 91/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1706 - val_acc: 0.9872\n",
      "Epoch 92/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1704 - val_acc: 0.9872\n",
      "Epoch 93/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1705 - val_acc: 0.9872\n",
      "Epoch 94/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1705 - val_acc: 0.9872\n",
      "Epoch 95/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1704 - val_acc: 0.9872\n",
      "Epoch 96/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1705 - val_acc: 0.9872\n",
      "Epoch 97/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1705 - val_acc: 0.9872\n",
      "Epoch 98/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1705 - val_acc: 0.9872\n",
      "Epoch 99/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1705 - val_acc: 0.9872\n",
      "Epoch 100/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1706 - val_acc: 0.9872\n",
      "78/78 [==============================] - 0s 330us/step\n",
      "Score for fold 5: loss of 2.1705691936688547; acc of 98.71794871794873%\n",
      "Train on 702 samples, validate on 78 samples\n",
      "Epoch 1/100\n",
      "702/702 [==============================] - 14s 19ms/step - loss: 2.4892 - acc: 0.2407 - val_loss: 2.4815 - val_acc: 0.2436\n",
      "Epoch 2/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.3277 - acc: 0.5199 - val_loss: 2.2780 - val_acc: 0.5385\n",
      "Epoch 3/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2458 - acc: 0.7407 - val_loss: 2.2931 - val_acc: 0.5897\n",
      "Epoch 4/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2265 - acc: 0.7835 - val_loss: 2.2283 - val_acc: 0.7692\n",
      "Epoch 5/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2020 - acc: 0.8974 - val_loss: 2.2030 - val_acc: 0.8718\n",
      "Epoch 6/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2041 - acc: 0.9046 - val_loss: 2.2463 - val_acc: 0.7949\n",
      "Epoch 7/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1882 - acc: 0.9444 - val_loss: 2.2097 - val_acc: 0.8077\n",
      "Epoch 8/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1872 - acc: 0.9345 - val_loss: 2.1990 - val_acc: 0.9103\n",
      "Epoch 9/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1860 - acc: 0.9302 - val_loss: 2.1788 - val_acc: 0.9615\n",
      "Epoch 10/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1794 - acc: 0.9658 - val_loss: 2.2205 - val_acc: 0.7949\n",
      "Epoch 11/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1775 - acc: 0.9801 - val_loss: 2.1962 - val_acc: 0.8974\n",
      "Epoch 12/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1758 - acc: 0.9815 - val_loss: 2.1845 - val_acc: 0.8974\n",
      "Epoch 13/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1775 - acc: 0.9644 - val_loss: 2.1758 - val_acc: 0.9872\n",
      "Epoch 14/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1846 - acc: 0.9573 - val_loss: 2.1769 - val_acc: 0.9487\n",
      "Epoch 15/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1727 - acc: 0.9829 - val_loss: 2.1741 - val_acc: 0.9615\n",
      "Epoch 16/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1734 - acc: 0.9829 - val_loss: 2.1747 - val_acc: 0.9487\n",
      "Epoch 17/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1729 - acc: 0.9758 - val_loss: 2.2128 - val_acc: 0.8846\n",
      "Epoch 18/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1718 - acc: 0.9886 - val_loss: 2.1725 - val_acc: 0.9744\n",
      "Epoch 19/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1727 - acc: 0.9929 - val_loss: 2.1752 - val_acc: 0.9744\n",
      "Epoch 20/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1700 - acc: 0.9900 - val_loss: 2.2078 - val_acc: 0.8846\n",
      "Epoch 21/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1714 - acc: 0.9886 - val_loss: 2.1860 - val_acc: 0.9359\n",
      "Epoch 22/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1701 - acc: 0.9915 - val_loss: 2.1712 - val_acc: 0.9744\n",
      "Epoch 23/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1700 - acc: 0.9915 - val_loss: 2.1747 - val_acc: 0.9744\n",
      "Epoch 24/100\n",
      "702/702 [==============================] - 5s 7ms/step - loss: 2.1700 - acc: 0.9886 - val_loss: 2.1736 - val_acc: 0.9231\n",
      "Epoch 25/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1682 - acc: 0.9900 - val_loss: 2.1714 - val_acc: 0.9744\n",
      "Epoch 26/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1676 - acc: 0.9986 - val_loss: 2.1730 - val_acc: 0.9744\n",
      "Epoch 27/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1677 - acc: 0.9986 - val_loss: 2.1709 - val_acc: 0.9872\n",
      "Epoch 28/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1686 - acc: 0.9957 - val_loss: 2.1706 - val_acc: 0.9744\n",
      "Epoch 29/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1672 - acc: 0.9986 - val_loss: 2.1725 - val_acc: 0.9744\n",
      "Epoch 30/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1677 - acc: 0.9972 - val_loss: 2.1712 - val_acc: 0.9872\n",
      "Epoch 31/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1671 - acc: 0.9986 - val_loss: 2.1715 - val_acc: 0.9744\n",
      "Epoch 32/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1681 - acc: 0.9972 - val_loss: 2.1703 - val_acc: 0.9744\n",
      "Epoch 33/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1671 - acc: 0.9986 - val_loss: 2.1711 - val_acc: 0.9872\n",
      "Epoch 34/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1678 - acc: 0.9986 - val_loss: 2.1725 - val_acc: 0.9744\n",
      "Epoch 35/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1667 - acc: 1.0000 - val_loss: 2.1706 - val_acc: 0.9744\n",
      "Epoch 36/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1666 - acc: 1.0000 - val_loss: 2.1727 - val_acc: 0.9615\n",
      "Epoch 37/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1666 - acc: 0.9986 - val_loss: 2.1705 - val_acc: 0.9744\n",
      "Epoch 38/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1717 - val_acc: 0.9872\n",
      "Epoch 39/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1668 - acc: 0.9986 - val_loss: 2.1702 - val_acc: 0.9744\n",
      "Epoch 40/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1704 - val_acc: 0.9744\n",
      "Epoch 41/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1705 - val_acc: 0.9744\n",
      "Epoch 42/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1666 - acc: 0.9986 - val_loss: 2.1709 - val_acc: 0.9744\n",
      "Epoch 43/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1714 - val_acc: 0.9744\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1707 - val_acc: 0.9872\n",
      "Epoch 45/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1695 - val_acc: 0.9744\n",
      "Epoch 46/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1703 - val_acc: 0.9744\n",
      "Epoch 47/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1712 - val_acc: 0.9744\n",
      "Epoch 48/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1702 - val_acc: 0.9744\n",
      "Epoch 49/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1695 - val_acc: 0.9744\n",
      "Epoch 50/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1697 - val_acc: 0.9744\n",
      "Epoch 51/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1704 - val_acc: 0.9744\n",
      "Epoch 52/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1667 - acc: 1.0000 - val_loss: 2.1705 - val_acc: 0.9744\n",
      "Epoch 53/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1699 - val_acc: 0.9744\n",
      "Epoch 54/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1694 - val_acc: 0.9744\n",
      "Epoch 55/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1696 - val_acc: 0.9744\n",
      "Epoch 56/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1703 - val_acc: 0.9872\n",
      "Epoch 57/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1703 - val_acc: 0.9744\n",
      "Epoch 58/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1701 - val_acc: 0.9744\n",
      "Epoch 59/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1698 - val_acc: 0.9744\n",
      "Epoch 60/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1696 - val_acc: 0.9744\n",
      "Epoch 61/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1703 - val_acc: 0.9744\n",
      "Epoch 62/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1696 - val_acc: 0.9744\n",
      "Epoch 63/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1698 - val_acc: 0.9744\n",
      "Epoch 64/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 0.9986 - val_loss: 2.1702 - val_acc: 0.9744\n",
      "Epoch 65/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1698 - val_acc: 0.9744\n",
      "Epoch 66/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1695 - val_acc: 0.9744\n",
      "Epoch 67/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1697 - val_acc: 0.9744\n",
      "Epoch 68/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1697 - val_acc: 0.9744\n",
      "Epoch 69/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1698 - val_acc: 0.9744\n",
      "Epoch 70/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1700 - val_acc: 0.9744\n",
      "Epoch 71/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1698 - val_acc: 0.9744\n",
      "Epoch 72/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1706 - val_acc: 0.9744\n",
      "Epoch 73/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1702 - val_acc: 0.9744\n",
      "Epoch 74/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1697 - val_acc: 0.9744\n",
      "Epoch 75/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1701 - val_acc: 0.9744\n",
      "Epoch 76/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1702 - val_acc: 0.9744\n",
      "Epoch 77/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1698 - val_acc: 0.9744\n",
      "Epoch 78/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1701 - val_acc: 0.9744\n",
      "Epoch 79/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1698 - val_acc: 0.9744\n",
      "Epoch 80/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1702 - val_acc: 0.9744\n",
      "Epoch 81/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1703 - val_acc: 0.9744\n",
      "Epoch 82/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1699 - val_acc: 0.9744\n",
      "Epoch 83/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1699 - val_acc: 0.9744\n",
      "Epoch 84/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1700 - val_acc: 0.9744\n",
      "Epoch 85/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1698 - val_acc: 0.9744\n",
      "Epoch 86/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1702 - val_acc: 0.9744\n",
      "Epoch 87/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1701 - val_acc: 0.9744\n",
      "Epoch 88/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1700 - val_acc: 0.9744\n",
      "Epoch 89/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1699 - val_acc: 0.9744\n",
      "Epoch 90/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1700 - val_acc: 0.9744\n",
      "Epoch 91/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1702 - val_acc: 0.9744\n",
      "Epoch 92/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1702 - val_acc: 0.9744\n",
      "Epoch 93/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1700 - val_acc: 0.9744\n",
      "Epoch 94/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1697 - val_acc: 0.9744\n",
      "Epoch 95/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1698 - val_acc: 0.9744\n",
      "Epoch 96/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1697 - val_acc: 0.9744\n",
      "Epoch 97/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1699 - val_acc: 0.9744\n",
      "Epoch 98/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1699 - val_acc: 0.9744\n",
      "Epoch 99/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1700 - val_acc: 0.9744\n",
      "Epoch 100/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1699 - val_acc: 0.9744\n",
      "78/78 [==============================] - 0s 405us/step\n",
      "Score for fold 6: loss of 2.169799444002983; acc of 97.43589697740018%\n",
      "Train on 702 samples, validate on 78 samples\n",
      "Epoch 1/100\n",
      "702/702 [==============================] - 14s 20ms/step - loss: 2.5097 - acc: 0.2835 - val_loss: 2.3719 - val_acc: 0.4744\n",
      "Epoch 2/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.3203 - acc: 0.5570 - val_loss: 2.3276 - val_acc: 0.5256\n",
      "Epoch 3/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2644 - acc: 0.7080 - val_loss: 2.2291 - val_acc: 0.8462\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2349 - acc: 0.8063 - val_loss: 2.2176 - val_acc: 0.8205\n",
      "Epoch 5/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2028 - acc: 0.8846 - val_loss: 2.2189 - val_acc: 0.8077\n",
      "Epoch 6/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1961 - acc: 0.9231 - val_loss: 2.2002 - val_acc: 0.9231\n",
      "Epoch 7/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1967 - acc: 0.9088 - val_loss: 2.1869 - val_acc: 0.9487\n",
      "Epoch 8/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1817 - acc: 0.9601 - val_loss: 2.2124 - val_acc: 0.8974\n",
      "Epoch 9/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1841 - acc: 0.9501 - val_loss: 2.1941 - val_acc: 0.9231\n",
      "Epoch 10/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1840 - acc: 0.9544 - val_loss: 2.1915 - val_acc: 0.9487\n",
      "Epoch 11/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1788 - acc: 0.9687 - val_loss: 2.1806 - val_acc: 0.9615\n",
      "Epoch 12/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1770 - acc: 0.9858 - val_loss: 2.1899 - val_acc: 0.9359\n",
      "Epoch 13/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1725 - acc: 0.9858 - val_loss: 2.2105 - val_acc: 0.8590\n",
      "Epoch 14/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1738 - acc: 0.9772 - val_loss: 2.1761 - val_acc: 0.9615\n",
      "Epoch 15/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1713 - acc: 0.9843 - val_loss: 2.1816 - val_acc: 0.9231\n",
      "Epoch 16/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1750 - acc: 0.9801 - val_loss: 2.1764 - val_acc: 0.9744\n",
      "Epoch 17/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1734 - acc: 0.9915 - val_loss: 2.1784 - val_acc: 0.9615\n",
      "Epoch 18/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1703 - acc: 0.9943 - val_loss: 2.1776 - val_acc: 0.9615\n",
      "Epoch 19/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1727 - acc: 0.9801 - val_loss: 2.1776 - val_acc: 0.9615\n",
      "Epoch 20/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1716 - acc: 0.9943 - val_loss: 2.1778 - val_acc: 0.9615\n",
      "Epoch 21/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1697 - acc: 0.9957 - val_loss: 2.1809 - val_acc: 0.9487\n",
      "Epoch 22/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1707 - acc: 0.9900 - val_loss: 2.1869 - val_acc: 0.9487\n",
      "Epoch 23/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1688 - acc: 0.9957 - val_loss: 2.1780 - val_acc: 0.9487\n",
      "Epoch 24/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1713 - acc: 0.9929 - val_loss: 2.1805 - val_acc: 0.9615\n",
      "Epoch 25/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1681 - acc: 0.9957 - val_loss: 2.1778 - val_acc: 0.9487\n",
      "Epoch 26/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1716 - acc: 0.9801 - val_loss: 2.1802 - val_acc: 0.9487\n",
      "Epoch 27/100\n",
      "702/702 [==============================] - 5s 7ms/step - loss: 2.1678 - acc: 0.9972 - val_loss: 2.1770 - val_acc: 0.9744\n",
      "Epoch 28/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1674 - acc: 0.9986 - val_loss: 2.1770 - val_acc: 0.9744\n",
      "Epoch 29/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1671 - acc: 1.0000 - val_loss: 2.1818 - val_acc: 0.9615\n",
      "Epoch 30/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1671 - acc: 1.0000 - val_loss: 2.1766 - val_acc: 0.9487\n",
      "Epoch 31/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1674 - acc: 0.9957 - val_loss: 2.1757 - val_acc: 0.9615\n",
      "Epoch 32/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1668 - acc: 1.0000 - val_loss: 2.1826 - val_acc: 0.9615\n",
      "Epoch 33/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1668 - acc: 1.0000 - val_loss: 2.1779 - val_acc: 0.9744\n",
      "Epoch 34/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1674 - acc: 0.9972 - val_loss: 2.1785 - val_acc: 0.9744\n",
      "Epoch 35/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1666 - acc: 1.0000 - val_loss: 2.1744 - val_acc: 0.9744\n",
      "Epoch 36/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1669 - acc: 1.0000 - val_loss: 2.1734 - val_acc: 0.9615\n",
      "Epoch 37/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1666 - acc: 1.0000 - val_loss: 2.1754 - val_acc: 0.9744\n",
      "Epoch 38/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1666 - acc: 0.9986 - val_loss: 2.1745 - val_acc: 0.9615\n",
      "Epoch 39/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1764 - val_acc: 0.9615\n",
      "Epoch 40/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1746 - val_acc: 0.9744\n",
      "Epoch 41/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1777 - val_acc: 0.9744\n",
      "Epoch 42/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 0.9972 - val_loss: 2.1774 - val_acc: 0.9615\n",
      "Epoch 43/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1764 - val_acc: 0.9744\n",
      "Epoch 44/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1749 - val_acc: 0.9744\n",
      "Epoch 45/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1773 - val_acc: 0.9615\n",
      "Epoch 46/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1757 - val_acc: 0.9615\n",
      "Epoch 47/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1790 - val_acc: 0.9615\n",
      "Epoch 48/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1778 - val_acc: 0.9615\n",
      "Epoch 49/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1767 - val_acc: 0.9615\n",
      "Epoch 50/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1760 - val_acc: 0.9615\n",
      "Epoch 51/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1755 - val_acc: 0.9744\n",
      "Epoch 52/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1752 - val_acc: 0.9615\n",
      "Epoch 53/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1764 - val_acc: 0.9744\n",
      "Epoch 54/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1734 - val_acc: 0.9872\n",
      "Epoch 55/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1765 - val_acc: 0.9744\n",
      "Epoch 56/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1755 - val_acc: 0.9744\n",
      "Epoch 57/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1828 - val_acc: 0.9744\n",
      "Epoch 58/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1754 - val_acc: 0.9744\n",
      "Epoch 59/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1757 - val_acc: 0.9744\n",
      "Epoch 60/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1754 - val_acc: 0.9744\n",
      "Epoch 61/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1751 - val_acc: 0.9744\n",
      "Epoch 62/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1752 - val_acc: 0.9615\n",
      "Epoch 63/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1743 - val_acc: 0.9872\n",
      "Epoch 64/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1747 - val_acc: 0.9872\n",
      "Epoch 65/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1751 - val_acc: 0.9872\n",
      "Epoch 66/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1735 - val_acc: 0.9872\n",
      "Epoch 67/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1756 - val_acc: 0.9744\n",
      "Epoch 68/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1750 - val_acc: 0.9615\n",
      "Epoch 69/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1749 - val_acc: 0.9744\n",
      "Epoch 70/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1741 - val_acc: 0.9872\n",
      "Epoch 71/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1739 - val_acc: 0.9872\n",
      "Epoch 72/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1752 - val_acc: 0.9744\n",
      "Epoch 73/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1741 - val_acc: 0.9872\n",
      "Epoch 74/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1747 - val_acc: 0.9744\n",
      "Epoch 75/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1747 - val_acc: 0.9744\n",
      "Epoch 76/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1758 - val_acc: 0.9615\n",
      "Epoch 77/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1735 - val_acc: 0.9872\n",
      "Epoch 78/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1756 - val_acc: 0.9744\n",
      "Epoch 79/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1750 - val_acc: 0.9872\n",
      "Epoch 80/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1749 - val_acc: 0.9744\n",
      "Epoch 81/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1750 - val_acc: 0.9744\n",
      "Epoch 82/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1786 - val_acc: 0.9744\n",
      "Epoch 83/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1754 - val_acc: 0.9744\n",
      "Epoch 84/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1750 - val_acc: 0.9872\n",
      "Epoch 85/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1754 - val_acc: 0.9744\n",
      "Epoch 86/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1754 - val_acc: 0.9744\n",
      "Epoch 87/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1752 - val_acc: 0.9615\n",
      "Epoch 88/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1748 - val_acc: 0.9872\n",
      "Epoch 89/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1745 - val_acc: 0.9872\n",
      "Epoch 90/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1746 - val_acc: 0.9872\n",
      "Epoch 91/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1750 - val_acc: 0.9872\n",
      "Epoch 92/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1745 - val_acc: 0.9872\n",
      "Epoch 93/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1761 - val_acc: 0.9744\n",
      "Epoch 94/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1754 - val_acc: 0.9615\n",
      "Epoch 95/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1748 - val_acc: 0.9744\n",
      "Epoch 96/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1745 - val_acc: 0.9872\n",
      "Epoch 97/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1746 - val_acc: 0.9872\n",
      "Epoch 98/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1742 - val_acc: 0.9872\n",
      "Epoch 99/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1746 - val_acc: 0.9872\n",
      "Epoch 100/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1750 - val_acc: 0.9744\n",
      "78/78 [==============================] - 0s 356us/step\n",
      "Score for fold 7: loss of 2.1742989283341627; acc of 97.43589697740018%\n",
      "Train on 702 samples, validate on 78 samples\n",
      "Epoch 1/100\n",
      "702/702 [==============================] - 15s 21ms/step - loss: 2.4890 - acc: 0.2493 - val_loss: 2.3547 - val_acc: 0.4615\n",
      "Epoch 2/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.3177 - acc: 0.5684 - val_loss: 2.3532 - val_acc: 0.5385\n",
      "Epoch 3/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2484 - acc: 0.7350 - val_loss: 2.3874 - val_acc: 0.5513\n",
      "Epoch 4/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2425 - acc: 0.7692 - val_loss: 2.2283 - val_acc: 0.7821\n",
      "Epoch 5/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2030 - acc: 0.8632 - val_loss: 2.2157 - val_acc: 0.8333\n",
      "Epoch 6/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1991 - acc: 0.9031 - val_loss: 2.2546 - val_acc: 0.8333\n",
      "Epoch 7/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1862 - acc: 0.9430 - val_loss: 2.2230 - val_acc: 0.7821\n",
      "Epoch 8/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1842 - acc: 0.9530 - val_loss: 2.1984 - val_acc: 0.8077\n",
      "Epoch 9/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1850 - acc: 0.9487 - val_loss: 2.2238 - val_acc: 0.8333\n",
      "Epoch 10/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1766 - acc: 0.9644 - val_loss: 2.1950 - val_acc: 0.8846\n",
      "Epoch 11/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1786 - acc: 0.9487 - val_loss: 2.2098 - val_acc: 0.8333\n",
      "Epoch 12/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1759 - acc: 0.9687 - val_loss: 2.1855 - val_acc: 0.9359\n",
      "Epoch 13/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1784 - acc: 0.9615 - val_loss: 2.2023 - val_acc: 0.8718\n",
      "Epoch 14/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1727 - acc: 0.9886 - val_loss: 2.1899 - val_acc: 0.8846\n",
      "Epoch 15/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1705 - acc: 0.9929 - val_loss: 2.1895 - val_acc: 0.8974\n",
      "Epoch 16/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1769 - acc: 0.9858 - val_loss: 2.1852 - val_acc: 0.9359\n",
      "Epoch 17/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1722 - acc: 0.9886 - val_loss: 2.1867 - val_acc: 0.9103\n",
      "Epoch 18/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1709 - acc: 0.9943 - val_loss: 2.2096 - val_acc: 0.8462\n",
      "Epoch 19/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1734 - acc: 0.9858 - val_loss: 2.1933 - val_acc: 0.9231\n",
      "Epoch 20/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1689 - acc: 0.9972 - val_loss: 2.1844 - val_acc: 0.9231\n",
      "Epoch 21/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1684 - acc: 0.9986 - val_loss: 2.1815 - val_acc: 0.9103\n",
      "Epoch 22/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1695 - acc: 0.9929 - val_loss: 2.1939 - val_acc: 0.8846\n",
      "Epoch 23/100\n",
      "702/702 [==============================] - 5s 7ms/step - loss: 2.1713 - acc: 0.9886 - val_loss: 2.1857 - val_acc: 0.9103\n",
      "Epoch 24/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1684 - acc: 0.9972 - val_loss: 2.1800 - val_acc: 0.9487\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1675 - acc: 0.9986 - val_loss: 2.1871 - val_acc: 0.9615\n",
      "Epoch 26/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1684 - acc: 0.9972 - val_loss: 2.1800 - val_acc: 0.9615\n",
      "Epoch 27/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1675 - acc: 0.9972 - val_loss: 2.1941 - val_acc: 0.9103\n",
      "Epoch 28/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1682 - acc: 0.9972 - val_loss: 2.1797 - val_acc: 0.9487\n",
      "Epoch 29/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1676 - acc: 0.9986 - val_loss: 2.1860 - val_acc: 0.9231\n",
      "Epoch 30/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1672 - acc: 0.9972 - val_loss: 2.1850 - val_acc: 0.9231\n",
      "Epoch 31/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1670 - acc: 1.0000 - val_loss: 2.1856 - val_acc: 0.9487\n",
      "Epoch 32/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1682 - acc: 0.9943 - val_loss: 2.1833 - val_acc: 0.9487\n",
      "Epoch 33/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1668 - acc: 1.0000 - val_loss: 2.1794 - val_acc: 0.9615\n",
      "Epoch 34/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1671 - acc: 1.0000 - val_loss: 2.1829 - val_acc: 0.9615\n",
      "Epoch 35/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1668 - acc: 1.0000 - val_loss: 2.1798 - val_acc: 0.9359\n",
      "Epoch 36/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1672 - acc: 1.0000 - val_loss: 2.1818 - val_acc: 0.9487\n",
      "Epoch 37/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1666 - acc: 1.0000 - val_loss: 2.1753 - val_acc: 0.9615\n",
      "Epoch 38/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1666 - acc: 1.0000 - val_loss: 2.1811 - val_acc: 0.9359\n",
      "Epoch 39/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1807 - val_acc: 0.9359\n",
      "Epoch 40/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1666 - acc: 1.0000 - val_loss: 2.1832 - val_acc: 0.9359\n",
      "Epoch 41/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1803 - val_acc: 0.9615\n",
      "Epoch 42/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1807 - val_acc: 0.9231\n",
      "Epoch 43/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1666 - acc: 0.9986 - val_loss: 2.1798 - val_acc: 0.9615\n",
      "Epoch 44/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1802 - val_acc: 0.9615\n",
      "Epoch 45/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1818 - val_acc: 0.9487\n",
      "Epoch 46/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1807 - val_acc: 0.9487\n",
      "Epoch 47/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1807 - val_acc: 0.9359\n",
      "Epoch 48/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1753 - val_acc: 0.9487\n",
      "Epoch 49/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1776 - val_acc: 0.9487\n",
      "Epoch 50/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1811 - val_acc: 0.9487\n",
      "Epoch 51/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1794 - val_acc: 0.9359\n",
      "Epoch 52/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1751 - val_acc: 0.9615\n",
      "Epoch 53/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1908 - val_acc: 0.9103\n",
      "Epoch 54/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1825 - val_acc: 0.9359\n",
      "Epoch 55/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1819 - val_acc: 0.9487\n",
      "Epoch 56/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1815 - val_acc: 0.9615\n",
      "Epoch 57/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1806 - val_acc: 0.9615\n",
      "Epoch 58/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1792 - val_acc: 0.9615\n",
      "Epoch 59/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1798 - val_acc: 0.9487\n",
      "Epoch 60/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1782 - val_acc: 0.9487\n",
      "Epoch 61/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1820 - val_acc: 0.9615\n",
      "Epoch 62/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1780 - val_acc: 0.9615\n",
      "Epoch 63/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1776 - val_acc: 0.9615\n",
      "Epoch 64/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1783 - val_acc: 0.9615\n",
      "Epoch 65/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1789 - val_acc: 0.9615\n",
      "Epoch 66/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1785 - val_acc: 0.9615\n",
      "Epoch 67/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1798 - val_acc: 0.9615\n",
      "Epoch 68/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1789 - val_acc: 0.9615\n",
      "Epoch 69/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1791 - val_acc: 0.9615\n",
      "Epoch 70/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1782 - val_acc: 0.9615\n",
      "Epoch 71/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1776 - val_acc: 0.9615\n",
      "Epoch 72/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1781 - val_acc: 0.9615\n",
      "Epoch 73/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1778 - val_acc: 0.9615\n",
      "Epoch 74/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1790 - val_acc: 0.9615\n",
      "Epoch 75/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1783 - val_acc: 0.9615\n",
      "Epoch 76/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1765 - val_acc: 0.9615\n",
      "Epoch 77/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1775 - val_acc: 0.9615\n",
      "Epoch 78/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1777 - val_acc: 0.9615\n",
      "Epoch 79/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1773 - val_acc: 0.9615\n",
      "Epoch 80/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1776 - val_acc: 0.9615\n",
      "Epoch 81/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1785 - val_acc: 0.9615\n",
      "Epoch 82/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1774 - val_acc: 0.9615\n",
      "Epoch 83/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1790 - val_acc: 0.9615\n",
      "Epoch 84/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1790 - val_acc: 0.9615\n",
      "Epoch 85/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1787 - val_acc: 0.9615\n",
      "Epoch 86/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1785 - val_acc: 0.9615\n",
      "Epoch 87/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1779 - val_acc: 0.9615\n",
      "Epoch 88/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1776 - val_acc: 0.9615\n",
      "Epoch 89/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1780 - val_acc: 0.9615\n",
      "Epoch 90/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1774 - val_acc: 0.9615\n",
      "Epoch 91/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1763 - val_acc: 0.9615\n",
      "Epoch 92/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1779 - val_acc: 0.9615\n",
      "Epoch 93/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1791 - val_acc: 0.9615\n",
      "Epoch 94/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1789 - val_acc: 0.9615\n",
      "Epoch 95/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1781 - val_acc: 0.9615\n",
      "Epoch 96/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1777 - val_acc: 0.9615\n",
      "Epoch 97/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1776 - val_acc: 0.9615\n",
      "Epoch 98/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1776 - val_acc: 0.9615\n",
      "Epoch 99/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1774 - val_acc: 0.9615\n",
      "Epoch 100/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1773 - val_acc: 0.9615\n",
      "78/78 [==============================] - 0s 327us/step\n",
      "Score for fold 8: loss of 2.177157524304512; acc of 96.15384569534888%\n",
      "Train on 702 samples, validate on 78 samples\n",
      "Epoch 1/100\n",
      "702/702 [==============================] - 14s 20ms/step - loss: 2.5010 - acc: 0.2664 - val_loss: 2.3561 - val_acc: 0.5641\n",
      "Epoch 2/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2983 - acc: 0.6496 - val_loss: 2.2946 - val_acc: 0.6795\n",
      "Epoch 3/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2575 - acc: 0.7308 - val_loss: 2.2447 - val_acc: 0.6667\n",
      "Epoch 4/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2133 - acc: 0.8362 - val_loss: 2.2008 - val_acc: 0.8077\n",
      "Epoch 5/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2035 - acc: 0.8746 - val_loss: 2.2102 - val_acc: 0.8462\n",
      "Epoch 6/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1928 - acc: 0.9174 - val_loss: 2.1988 - val_acc: 0.8333\n",
      "Epoch 7/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1928 - acc: 0.9117 - val_loss: 2.1919 - val_acc: 0.9103\n",
      "Epoch 8/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1809 - acc: 0.9644 - val_loss: 2.1959 - val_acc: 0.9103\n",
      "Epoch 9/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1849 - acc: 0.9387 - val_loss: 2.1996 - val_acc: 0.8590\n",
      "Epoch 10/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1750 - acc: 0.9772 - val_loss: 2.1903 - val_acc: 0.9231\n",
      "Epoch 11/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1802 - acc: 0.9601 - val_loss: 2.1852 - val_acc: 0.9359\n",
      "Epoch 12/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1719 - acc: 0.9886 - val_loss: 2.2054 - val_acc: 0.8462\n",
      "Epoch 13/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1772 - acc: 0.9687 - val_loss: 2.1833 - val_acc: 0.9615\n",
      "Epoch 14/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1727 - acc: 0.9843 - val_loss: 2.1917 - val_acc: 0.9359\n",
      "Epoch 15/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1713 - acc: 0.9943 - val_loss: 2.1910 - val_acc: 0.9487\n",
      "Epoch 16/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1721 - acc: 0.9843 - val_loss: 2.1808 - val_acc: 0.9487\n",
      "Epoch 17/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1727 - acc: 0.9872 - val_loss: 2.1909 - val_acc: 0.9359\n",
      "Epoch 18/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1710 - acc: 0.9929 - val_loss: 2.1814 - val_acc: 0.9615\n",
      "Epoch 19/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1700 - acc: 0.9900 - val_loss: 2.2203 - val_acc: 0.8462\n",
      "Epoch 20/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1748 - acc: 0.9815 - val_loss: 2.1803 - val_acc: 0.9744\n",
      "Epoch 21/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1688 - acc: 0.9929 - val_loss: 2.1821 - val_acc: 0.9615\n",
      "Epoch 22/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1713 - acc: 0.9786 - val_loss: 2.1797 - val_acc: 0.9744\n",
      "Epoch 23/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1677 - acc: 0.9986 - val_loss: 2.1800 - val_acc: 0.9744\n",
      "Epoch 24/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1709 - acc: 0.9915 - val_loss: 2.1795 - val_acc: 0.9744\n",
      "Epoch 25/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1685 - acc: 0.9957 - val_loss: 2.1788 - val_acc: 0.9744\n",
      "Epoch 26/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1673 - acc: 1.0000 - val_loss: 2.1798 - val_acc: 0.9744\n",
      "Epoch 27/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1692 - acc: 0.9986 - val_loss: 2.1782 - val_acc: 0.9744\n",
      "Epoch 28/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1701 - acc: 0.9915 - val_loss: 2.1799 - val_acc: 0.9744\n",
      "Epoch 29/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1679 - acc: 0.9886 - val_loss: 2.1942 - val_acc: 0.9615\n",
      "Epoch 30/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1677 - acc: 0.9986 - val_loss: 2.1786 - val_acc: 0.9744\n",
      "Epoch 31/100\n",
      "702/702 [==============================] - 5s 7ms/step - loss: 2.1678 - acc: 0.9972 - val_loss: 2.1799 - val_acc: 0.9744\n",
      "Epoch 32/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1673 - acc: 0.9986 - val_loss: 2.1797 - val_acc: 0.9744\n",
      "Epoch 33/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1667 - acc: 1.0000 - val_loss: 2.1785 - val_acc: 0.9744\n",
      "Epoch 34/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1668 - acc: 1.0000 - val_loss: 2.1778 - val_acc: 0.9744\n",
      "Epoch 35/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1671 - acc: 1.0000 - val_loss: 2.1788 - val_acc: 0.9744\n",
      "Epoch 36/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1666 - acc: 1.0000 - val_loss: 2.1782 - val_acc: 0.9744\n",
      "Epoch 37/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1667 - acc: 1.0000 - val_loss: 2.1799 - val_acc: 0.9615\n",
      "Epoch 38/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1668 - acc: 1.0000 - val_loss: 2.1795 - val_acc: 0.9744\n",
      "Epoch 39/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1669 - acc: 0.9957 - val_loss: 2.1786 - val_acc: 0.9744\n",
      "Epoch 40/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1674 - acc: 1.0000 - val_loss: 2.1780 - val_acc: 0.9744\n",
      "Epoch 41/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1666 - acc: 1.0000 - val_loss: 2.1784 - val_acc: 0.9744\n",
      "Epoch 42/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1772 - val_acc: 0.9744\n",
      "Epoch 43/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1667 - acc: 1.0000 - val_loss: 2.1772 - val_acc: 0.9744\n",
      "Epoch 44/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1778 - val_acc: 0.9744\n",
      "Epoch 45/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1777 - val_acc: 0.9744\n",
      "Epoch 46/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1781 - val_acc: 0.9744\n",
      "Epoch 47/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1779 - val_acc: 0.9744\n",
      "Epoch 48/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1772 - val_acc: 0.9744\n",
      "Epoch 49/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1778 - val_acc: 0.9744\n",
      "Epoch 50/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1778 - val_acc: 0.9744\n",
      "Epoch 51/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1778 - val_acc: 0.9744\n",
      "Epoch 52/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1775 - val_acc: 0.9744\n",
      "Epoch 53/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1779 - val_acc: 0.9744\n",
      "Epoch 54/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1777 - val_acc: 0.9744\n",
      "Epoch 55/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1778 - val_acc: 0.9744\n",
      "Epoch 56/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1774 - val_acc: 0.9744\n",
      "Epoch 57/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1778 - val_acc: 0.9744\n",
      "Epoch 58/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1771 - val_acc: 0.9744\n",
      "Epoch 59/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1774 - val_acc: 0.9744\n",
      "Epoch 60/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1772 - val_acc: 0.9744\n",
      "Epoch 61/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1773 - val_acc: 0.9744\n",
      "Epoch 62/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1769 - val_acc: 0.9744\n",
      "Epoch 63/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1775 - val_acc: 0.9744\n",
      "Epoch 64/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1777 - val_acc: 0.9744\n",
      "Epoch 65/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1774 - val_acc: 0.9744\n",
      "Epoch 66/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1772 - val_acc: 0.9744\n",
      "Epoch 67/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1775 - val_acc: 0.9744\n",
      "Epoch 68/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1773 - val_acc: 0.9744\n",
      "Epoch 69/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1775 - val_acc: 0.9744\n",
      "Epoch 70/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1772 - val_acc: 0.9744\n",
      "Epoch 71/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1779 - val_acc: 0.9744\n",
      "Epoch 72/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1772 - val_acc: 0.9744\n",
      "Epoch 73/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1774 - val_acc: 0.9744\n",
      "Epoch 74/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1773 - val_acc: 0.9744\n",
      "Epoch 75/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1774 - val_acc: 0.9744\n",
      "Epoch 76/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1773 - val_acc: 0.9744\n",
      "Epoch 77/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1772 - val_acc: 0.9744\n",
      "Epoch 78/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1773 - val_acc: 0.9744\n",
      "Epoch 79/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1775 - val_acc: 0.9744\n",
      "Epoch 80/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1773 - val_acc: 0.9744\n",
      "Epoch 81/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1773 - val_acc: 0.9744\n",
      "Epoch 82/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1773 - val_acc: 0.9744\n",
      "Epoch 83/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1771 - val_acc: 0.9744\n",
      "Epoch 84/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1773 - val_acc: 0.9744\n",
      "Epoch 85/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1772 - val_acc: 0.9744\n",
      "Epoch 86/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1774 - val_acc: 0.9744\n",
      "Epoch 87/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1773 - val_acc: 0.9744\n",
      "Epoch 88/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1772 - val_acc: 0.9744\n",
      "Epoch 89/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1772 - val_acc: 0.9744\n",
      "Epoch 90/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1773 - val_acc: 0.9744\n",
      "Epoch 91/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1773 - val_acc: 0.9744\n",
      "Epoch 92/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1772 - val_acc: 0.9744\n",
      "Epoch 93/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1773 - val_acc: 0.9744\n",
      "Epoch 94/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1773 - val_acc: 0.9744\n",
      "Epoch 95/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1772 - val_acc: 0.9744\n",
      "Epoch 96/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1772 - val_acc: 0.9744\n",
      "Epoch 97/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1773 - val_acc: 0.9744\n",
      "Epoch 98/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1773 - val_acc: 0.9744\n",
      "Epoch 99/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1775 - val_acc: 0.9744\n",
      "Epoch 100/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1772 - val_acc: 0.9744\n",
      "78/78 [==============================] - 0s 337us/step\n",
      "Score for fold 9: loss of 2.1772072315216064; acc of 97.43589697740018%\n",
      "Train on 702 samples, validate on 78 samples\n",
      "Epoch 1/100\n",
      "702/702 [==============================] - 15s 21ms/step - loss: 2.4700 - acc: 0.2835 - val_loss: 2.5222 - val_acc: 0.2436\n",
      "Epoch 2/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2897 - acc: 0.6083 - val_loss: 2.2872 - val_acc: 0.5513\n",
      "Epoch 3/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2424 - acc: 0.7564 - val_loss: 2.2548 - val_acc: 0.7692\n",
      "Epoch 4/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2225 - acc: 0.8348 - val_loss: 2.2016 - val_acc: 0.8590\n",
      "Epoch 5/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2030 - acc: 0.8903 - val_loss: 2.2111 - val_acc: 0.9359\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1909 - acc: 0.9231 - val_loss: 2.2480 - val_acc: 0.7436\n",
      "Epoch 7/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1882 - acc: 0.9359 - val_loss: 2.1809 - val_acc: 0.9359\n",
      "Epoch 8/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1816 - acc: 0.9672 - val_loss: 2.2101 - val_acc: 0.8462\n",
      "Epoch 9/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1843 - acc: 0.9530 - val_loss: 2.1888 - val_acc: 0.9487\n",
      "Epoch 10/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1759 - acc: 0.9744 - val_loss: 2.1784 - val_acc: 0.9359\n",
      "Epoch 11/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1798 - acc: 0.9615 - val_loss: 2.1783 - val_acc: 0.9615\n",
      "Epoch 12/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1741 - acc: 0.9843 - val_loss: 2.1800 - val_acc: 0.9487\n",
      "Epoch 13/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1729 - acc: 0.9858 - val_loss: 2.1780 - val_acc: 0.9744\n",
      "Epoch 14/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1755 - acc: 0.9687 - val_loss: 2.1854 - val_acc: 0.9487\n",
      "Epoch 15/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1730 - acc: 0.9843 - val_loss: 2.1754 - val_acc: 1.0000\n",
      "Epoch 16/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1732 - acc: 0.9900 - val_loss: 2.1931 - val_acc: 0.9231\n",
      "Epoch 17/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1719 - acc: 0.9900 - val_loss: 2.1729 - val_acc: 0.9744\n",
      "Epoch 18/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1700 - acc: 0.9886 - val_loss: 2.1751 - val_acc: 0.9615\n",
      "Epoch 19/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1705 - acc: 0.9915 - val_loss: 2.1801 - val_acc: 0.9872\n",
      "Epoch 20/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1714 - acc: 0.9900 - val_loss: 2.1753 - val_acc: 0.9872\n",
      "Epoch 21/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1704 - acc: 0.9929 - val_loss: 2.1720 - val_acc: 0.9744\n",
      "Epoch 22/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1687 - acc: 0.9957 - val_loss: 2.1885 - val_acc: 0.8974\n",
      "Epoch 23/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1710 - acc: 0.9943 - val_loss: 2.2462 - val_acc: 0.7564\n",
      "Epoch 24/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1719 - acc: 0.9843 - val_loss: 2.1738 - val_acc: 0.9744\n",
      "Epoch 25/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1681 - acc: 0.9986 - val_loss: 2.1720 - val_acc: 0.9872\n",
      "Epoch 26/100\n",
      "702/702 [==============================] - 5s 7ms/step - loss: 2.1698 - acc: 0.9943 - val_loss: 2.1846 - val_acc: 0.9487\n",
      "Epoch 27/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1689 - acc: 0.9929 - val_loss: 2.1719 - val_acc: 0.9615\n",
      "Epoch 28/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1673 - acc: 0.9986 - val_loss: 2.1711 - val_acc: 0.9872\n",
      "Epoch 29/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1676 - acc: 0.9900 - val_loss: 2.1721 - val_acc: 0.9744\n",
      "Epoch 30/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1672 - acc: 0.9986 - val_loss: 2.1709 - val_acc: 0.9872\n",
      "Epoch 31/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1684 - acc: 0.9957 - val_loss: 2.1735 - val_acc: 0.9744\n",
      "Epoch 32/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1670 - acc: 1.0000 - val_loss: 2.1708 - val_acc: 0.9872\n",
      "Epoch 33/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1672 - acc: 0.9972 - val_loss: 2.1705 - val_acc: 1.0000\n",
      "Epoch 34/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1668 - acc: 1.0000 - val_loss: 2.1760 - val_acc: 0.9615\n",
      "Epoch 35/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1676 - acc: 0.9986 - val_loss: 2.1702 - val_acc: 1.0000\n",
      "Epoch 36/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1683 - acc: 0.9972 - val_loss: 2.1694 - val_acc: 0.9872\n",
      "Epoch 37/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1666 - acc: 1.0000 - val_loss: 2.1710 - val_acc: 0.9872\n",
      "Epoch 38/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1685 - val_acc: 1.0000\n",
      "Epoch 39/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1694 - val_acc: 1.0000\n",
      "Epoch 40/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1666 - acc: 1.0000 - val_loss: 2.1731 - val_acc: 0.9744\n",
      "Epoch 41/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1699 - val_acc: 0.9872\n",
      "Epoch 42/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1667 - acc: 0.9986 - val_loss: 2.1692 - val_acc: 1.0000\n",
      "Epoch 43/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1683 - val_acc: 1.0000\n",
      "Epoch 44/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1666 - acc: 1.0000 - val_loss: 2.1684 - val_acc: 1.0000\n",
      "Epoch 45/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1709 - val_acc: 1.0000\n",
      "Epoch 46/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1732 - val_acc: 0.9872\n",
      "Epoch 47/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1697 - val_acc: 0.9872\n",
      "Epoch 48/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1693 - val_acc: 0.9872\n",
      "Epoch 49/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 0.9986 - val_loss: 2.1689 - val_acc: 1.0000\n",
      "Epoch 50/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1690 - val_acc: 1.0000\n",
      "Epoch 51/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1710 - val_acc: 0.9744\n",
      "Epoch 52/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1692 - val_acc: 0.9872\n",
      "Epoch 53/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1685 - val_acc: 1.0000\n",
      "Epoch 54/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1699 - val_acc: 0.9872\n",
      "Epoch 55/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1695 - val_acc: 0.9872\n",
      "Epoch 56/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1690 - val_acc: 0.9872\n",
      "Epoch 57/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1687 - val_acc: 0.9872\n",
      "Epoch 58/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1688 - val_acc: 0.9872\n",
      "Epoch 59/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1685 - val_acc: 0.9872\n",
      "Epoch 60/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1688 - val_acc: 0.9872\n",
      "Epoch 61/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1689 - val_acc: 0.9872\n",
      "Epoch 62/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1684 - val_acc: 0.9872\n",
      "Epoch 63/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1685 - val_acc: 1.0000\n",
      "Epoch 64/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1683 - val_acc: 0.9872\n",
      "Epoch 65/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1687 - val_acc: 0.9872\n",
      "Epoch 66/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1690 - val_acc: 0.9872\n",
      "Epoch 67/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1691 - val_acc: 0.9872\n",
      "Epoch 68/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1687 - val_acc: 0.9872\n",
      "Epoch 69/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1684 - val_acc: 0.9872\n",
      "Epoch 70/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1686 - val_acc: 1.0000\n",
      "Epoch 71/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1683 - val_acc: 0.9872\n",
      "Epoch 72/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1684 - val_acc: 1.0000\n",
      "Epoch 73/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1683 - val_acc: 1.0000\n",
      "Epoch 74/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1683 - val_acc: 0.9872\n",
      "Epoch 75/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1686 - val_acc: 0.9872\n",
      "Epoch 76/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1685 - val_acc: 0.9872\n",
      "Epoch 77/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1681 - val_acc: 0.9872\n",
      "Epoch 78/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1684 - val_acc: 0.9872\n",
      "Epoch 79/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1685 - val_acc: 0.9872\n",
      "Epoch 80/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1684 - val_acc: 0.9872\n",
      "Epoch 81/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1685 - val_acc: 0.9872\n",
      "Epoch 82/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1682 - val_acc: 1.0000\n",
      "Epoch 83/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1682 - val_acc: 0.9872\n",
      "Epoch 84/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1683 - val_acc: 0.9872\n",
      "Epoch 85/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1682 - val_acc: 0.9872\n",
      "Epoch 86/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1684 - val_acc: 0.9872\n",
      "Epoch 87/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1683 - val_acc: 0.9872\n",
      "Epoch 88/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1685 - val_acc: 0.9872\n",
      "Epoch 89/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1683 - val_acc: 0.9872\n",
      "Epoch 90/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1685 - val_acc: 0.9872\n",
      "Epoch 91/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1683 - val_acc: 0.9872\n",
      "Epoch 92/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1684 - val_acc: 0.9872\n",
      "Epoch 93/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1683 - val_acc: 0.9872\n",
      "Epoch 94/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1683 - val_acc: 0.9872\n",
      "Epoch 95/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1682 - val_acc: 0.9872\n",
      "Epoch 96/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1683 - val_acc: 0.9872\n",
      "Epoch 97/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1683 - val_acc: 0.9872\n",
      "Epoch 98/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1683 - val_acc: 0.9872\n",
      "Epoch 99/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1682 - val_acc: 0.9872\n",
      "Epoch 100/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1682 - val_acc: 0.9872\n",
      "78/78 [==============================] - 0s 381us/step\n",
      "Score for fold 10: loss of 2.1682851131145773; acc of 98.71794871794873%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1296x1296 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m 6       \u001b[0m | \u001b[0m 97.82   \u001b[0m | \u001b[0m 0.9822  \u001b[0m | \u001b[0m 9.621   \u001b[0m |\n",
      "Train on 702 samples, validate on 78 samples\n",
      "Epoch 1/100\n",
      "702/702 [==============================] - 15s 22ms/step - loss: 2.5094 - acc: 0.2934 - val_loss: 2.3234 - val_acc: 0.5385\n",
      "Epoch 2/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2988 - acc: 0.5997 - val_loss: 2.3254 - val_acc: 0.4487\n",
      "Epoch 3/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2436 - acc: 0.7450 - val_loss: 2.2863 - val_acc: 0.6667\n",
      "Epoch 4/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2226 - acc: 0.8405 - val_loss: 2.2655 - val_acc: 0.7308\n",
      "Epoch 5/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1993 - acc: 0.9145 - val_loss: 2.2136 - val_acc: 0.8718\n",
      "Epoch 6/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1980 - acc: 0.9074 - val_loss: 2.1932 - val_acc: 0.8718\n",
      "Epoch 7/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1910 - acc: 0.9359 - val_loss: 2.1976 - val_acc: 0.8462\n",
      "Epoch 8/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1815 - acc: 0.9487 - val_loss: 2.1931 - val_acc: 0.9231\n",
      "Epoch 9/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1818 - acc: 0.9715 - val_loss: 2.1950 - val_acc: 0.9231\n",
      "Epoch 10/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1803 - acc: 0.9615 - val_loss: 2.2497 - val_acc: 0.7692\n",
      "Epoch 11/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1755 - acc: 0.9829 - val_loss: 2.1870 - val_acc: 0.9487\n",
      "Epoch 12/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1777 - acc: 0.9815 - val_loss: 2.2008 - val_acc: 0.8974\n",
      "Epoch 13/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1734 - acc: 0.9801 - val_loss: 2.1870 - val_acc: 0.9487\n",
      "Epoch 14/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1756 - acc: 0.9843 - val_loss: 2.1881 - val_acc: 0.9359\n",
      "Epoch 15/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1721 - acc: 0.9915 - val_loss: 2.1921 - val_acc: 0.8974\n",
      "Epoch 16/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1720 - acc: 0.9886 - val_loss: 2.1836 - val_acc: 0.9231\n",
      "Epoch 17/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1713 - acc: 0.9872 - val_loss: 2.1861 - val_acc: 0.8974\n",
      "Epoch 18/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1733 - acc: 0.9772 - val_loss: 2.1844 - val_acc: 0.9487\n",
      "Epoch 19/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1688 - acc: 0.9986 - val_loss: 2.1872 - val_acc: 0.9231\n",
      "Epoch 20/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1701 - acc: 0.9900 - val_loss: 2.1827 - val_acc: 0.9359\n",
      "Epoch 21/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1713 - acc: 0.9900 - val_loss: 2.1815 - val_acc: 0.9487\n",
      "Epoch 22/100\n",
      "702/702 [==============================] - 6s 8ms/step - loss: 2.1713 - acc: 0.9886 - val_loss: 2.1808 - val_acc: 0.9487\n",
      "Epoch 23/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1685 - acc: 0.9986 - val_loss: 2.1828 - val_acc: 0.9487\n",
      "Epoch 24/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1682 - acc: 0.9972 - val_loss: 2.1806 - val_acc: 0.9487\n",
      "Epoch 25/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1697 - acc: 0.9915 - val_loss: 2.1826 - val_acc: 0.9487\n",
      "Epoch 26/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1678 - acc: 0.9972 - val_loss: 2.1801 - val_acc: 0.9615\n",
      "Epoch 27/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1678 - acc: 0.9986 - val_loss: 2.1808 - val_acc: 0.9487\n",
      "Epoch 28/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1674 - acc: 0.9986 - val_loss: 2.1923 - val_acc: 0.8974\n",
      "Epoch 29/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1678 - acc: 0.9972 - val_loss: 2.1814 - val_acc: 0.9359\n",
      "Epoch 30/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1675 - acc: 0.9986 - val_loss: 2.1805 - val_acc: 0.9487\n",
      "Epoch 31/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1674 - acc: 0.9986 - val_loss: 2.1803 - val_acc: 0.9487\n",
      "Epoch 32/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1674 - acc: 0.9972 - val_loss: 2.1801 - val_acc: 0.9487\n",
      "Epoch 33/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1669 - acc: 0.9986 - val_loss: 2.1797 - val_acc: 0.9487\n",
      "Epoch 34/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1676 - acc: 0.9986 - val_loss: 2.1875 - val_acc: 0.9231\n",
      "Epoch 35/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1672 - acc: 0.9972 - val_loss: 2.1812 - val_acc: 0.9487\n",
      "Epoch 36/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1676 - acc: 0.9972 - val_loss: 2.1878 - val_acc: 0.9359\n",
      "Epoch 37/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1673 - acc: 0.9986 - val_loss: 2.1804 - val_acc: 0.9487\n",
      "Epoch 38/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1667 - acc: 1.0000 - val_loss: 2.1801 - val_acc: 0.9487\n",
      "Epoch 39/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1666 - acc: 1.0000 - val_loss: 2.1804 - val_acc: 0.9487\n",
      "Epoch 40/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1666 - acc: 1.0000 - val_loss: 2.1799 - val_acc: 0.9359\n",
      "Epoch 41/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1667 - acc: 0.9986 - val_loss: 2.1802 - val_acc: 0.9359\n",
      "Epoch 42/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1668 - acc: 1.0000 - val_loss: 2.1796 - val_acc: 0.9487\n",
      "Epoch 43/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1666 - acc: 1.0000 - val_loss: 2.1797 - val_acc: 0.9487\n",
      "Epoch 44/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1813 - val_acc: 0.9359\n",
      "Epoch 45/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1666 - acc: 1.0000 - val_loss: 2.1797 - val_acc: 0.9487\n",
      "Epoch 46/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1795 - val_acc: 0.9487\n",
      "Epoch 47/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1788 - val_acc: 0.9359\n",
      "Epoch 48/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1791 - val_acc: 0.9487\n",
      "Epoch 49/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1781 - val_acc: 0.9487\n",
      "Epoch 50/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1802 - val_acc: 0.9487\n",
      "Epoch 51/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1799 - val_acc: 0.9487\n",
      "Epoch 52/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1808 - val_acc: 0.9487\n",
      "Epoch 53/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1795 - val_acc: 0.9487\n",
      "Epoch 54/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1800 - val_acc: 0.9487\n",
      "Epoch 55/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1797 - val_acc: 0.9487\n",
      "Epoch 56/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1796 - val_acc: 0.9487\n",
      "Epoch 57/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1803 - val_acc: 0.9487\n",
      "Epoch 58/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1794 - val_acc: 0.9487\n",
      "Epoch 59/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1803 - val_acc: 0.9487\n",
      "Epoch 60/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1789 - val_acc: 0.9487\n",
      "Epoch 61/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1799 - val_acc: 0.9487\n",
      "Epoch 62/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1774 - val_acc: 0.9487\n",
      "Epoch 63/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1784 - val_acc: 0.9487\n",
      "Epoch 64/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1784 - val_acc: 0.9487\n",
      "Epoch 65/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1793 - val_acc: 0.9487\n",
      "Epoch 66/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1792 - val_acc: 0.9487\n",
      "Epoch 67/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1772 - val_acc: 0.9487\n",
      "Epoch 68/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1786 - val_acc: 0.9487\n",
      "Epoch 69/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1791 - val_acc: 0.9487\n",
      "Epoch 70/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1785 - val_acc: 0.9487\n",
      "Epoch 71/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1794 - val_acc: 0.9487\n",
      "Epoch 72/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1793 - val_acc: 0.9487\n",
      "Epoch 73/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1790 - val_acc: 0.9487\n",
      "Epoch 74/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1796 - val_acc: 0.9487\n",
      "Epoch 75/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1795 - val_acc: 0.9487\n",
      "Epoch 76/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1795 - val_acc: 0.9487\n",
      "Epoch 77/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1789 - val_acc: 0.9487\n",
      "Epoch 78/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1789 - val_acc: 0.9487\n",
      "Epoch 79/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1790 - val_acc: 0.9487\n",
      "Epoch 80/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1792 - val_acc: 0.9487\n",
      "Epoch 81/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1798 - val_acc: 0.9487\n",
      "Epoch 82/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1788 - val_acc: 0.9487\n",
      "Epoch 83/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1790 - val_acc: 0.9487\n",
      "Epoch 84/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1790 - val_acc: 0.9487\n",
      "Epoch 85/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1792 - val_acc: 0.9487\n",
      "Epoch 86/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1789 - val_acc: 0.9487\n",
      "Epoch 87/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1788 - val_acc: 0.9487\n",
      "Epoch 88/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1793 - val_acc: 0.9487\n",
      "Epoch 89/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1791 - val_acc: 0.9487\n",
      "Epoch 90/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1796 - val_acc: 0.9487\n",
      "Epoch 91/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1788 - val_acc: 0.9487\n",
      "Epoch 92/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1787 - val_acc: 0.9487\n",
      "Epoch 93/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1789 - val_acc: 0.9487\n",
      "Epoch 94/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1789 - val_acc: 0.9487\n",
      "Epoch 95/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1789 - val_acc: 0.9487\n",
      "Epoch 96/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1788 - val_acc: 0.9487\n",
      "Epoch 97/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1788 - val_acc: 0.9487\n",
      "Epoch 98/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1786 - val_acc: 0.9487\n",
      "Epoch 99/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1786 - val_acc: 0.9487\n",
      "Epoch 100/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1789 - val_acc: 0.9487\n",
      "78/78 [==============================] - 0s 363us/step\n",
      "Score for fold 1: loss of 2.1790759685711985; acc of 94.87179487179486%\n",
      "Train on 702 samples, validate on 78 samples\n",
      "Epoch 1/100\n",
      "702/702 [==============================] - 15s 22ms/step - loss: 2.5082 - acc: 0.2806 - val_loss: 2.4851 - val_acc: 0.2051\n",
      "Epoch 2/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.3167 - acc: 0.5627 - val_loss: 2.3003 - val_acc: 0.6154\n",
      "Epoch 3/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2517 - acc: 0.7251 - val_loss: 2.3164 - val_acc: 0.5256\n",
      "Epoch 4/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2471 - acc: 0.7464 - val_loss: 2.2843 - val_acc: 0.7308\n",
      "Epoch 5/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2057 - acc: 0.8689 - val_loss: 2.2196 - val_acc: 0.7692\n",
      "Epoch 6/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2003 - acc: 0.8903 - val_loss: 2.2431 - val_acc: 0.7949\n",
      "Epoch 7/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1929 - acc: 0.9202 - val_loss: 2.2610 - val_acc: 0.6410\n",
      "Epoch 8/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1885 - acc: 0.9160 - val_loss: 2.2002 - val_acc: 0.8462\n",
      "Epoch 9/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1840 - acc: 0.9316 - val_loss: 2.1922 - val_acc: 0.8846\n",
      "Epoch 10/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1801 - acc: 0.9758 - val_loss: 2.1866 - val_acc: 0.8590\n",
      "Epoch 11/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1803 - acc: 0.9558 - val_loss: 2.1916 - val_acc: 0.9231\n",
      "Epoch 12/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1756 - acc: 0.9872 - val_loss: 2.2072 - val_acc: 0.8718\n",
      "Epoch 13/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1741 - acc: 0.9829 - val_loss: 2.2062 - val_acc: 0.8974\n",
      "Epoch 14/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1768 - acc: 0.9786 - val_loss: 2.1897 - val_acc: 0.8974\n",
      "Epoch 15/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1714 - acc: 0.9943 - val_loss: 2.1813 - val_acc: 0.9359\n",
      "Epoch 16/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1749 - acc: 0.9900 - val_loss: 2.1770 - val_acc: 0.9744\n",
      "Epoch 17/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1736 - acc: 0.9900 - val_loss: 2.1868 - val_acc: 0.9103\n",
      "Epoch 18/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1704 - acc: 0.9929 - val_loss: 2.1795 - val_acc: 0.9615\n",
      "Epoch 19/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1707 - acc: 0.9929 - val_loss: 2.1819 - val_acc: 0.9359\n",
      "Epoch 20/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1714 - acc: 0.9872 - val_loss: 2.1798 - val_acc: 0.9359\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1715 - acc: 0.9915 - val_loss: 2.1784 - val_acc: 0.9615\n",
      "Epoch 22/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1703 - acc: 0.9929 - val_loss: 2.1873 - val_acc: 0.9615\n",
      "Epoch 23/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1696 - acc: 0.9886 - val_loss: 2.1748 - val_acc: 0.9744\n",
      "Epoch 24/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1701 - acc: 0.9872 - val_loss: 2.1829 - val_acc: 0.9359\n",
      "Epoch 25/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1693 - acc: 0.9929 - val_loss: 2.1770 - val_acc: 0.9231\n",
      "Epoch 26/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1699 - acc: 0.9900 - val_loss: 2.1817 - val_acc: 0.9359\n",
      "Epoch 27/100\n",
      "702/702 [==============================] - 5s 8ms/step - loss: 2.1710 - acc: 0.9943 - val_loss: 2.1739 - val_acc: 0.9615\n",
      "Epoch 28/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1679 - acc: 0.9972 - val_loss: 2.1737 - val_acc: 0.9744\n",
      "Epoch 29/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1676 - acc: 0.9943 - val_loss: 2.1737 - val_acc: 0.9615\n",
      "Epoch 30/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1679 - acc: 0.9929 - val_loss: 2.1736 - val_acc: 0.9615\n",
      "Epoch 31/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1673 - acc: 0.9986 - val_loss: 2.1717 - val_acc: 0.9744\n",
      "Epoch 32/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1675 - acc: 0.9986 - val_loss: 2.1721 - val_acc: 0.9872\n",
      "Epoch 33/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1670 - acc: 1.0000 - val_loss: 2.1723 - val_acc: 0.9615\n",
      "Epoch 34/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1672 - acc: 1.0000 - val_loss: 2.1702 - val_acc: 1.0000\n",
      "Epoch 35/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1671 - acc: 1.0000 - val_loss: 2.1725 - val_acc: 0.9744\n",
      "Epoch 36/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1674 - acc: 1.0000 - val_loss: 2.1726 - val_acc: 0.9744\n",
      "Epoch 37/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1668 - acc: 1.0000 - val_loss: 2.1706 - val_acc: 0.9744\n",
      "Epoch 38/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1669 - acc: 0.9986 - val_loss: 2.1714 - val_acc: 1.0000\n",
      "Epoch 39/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1670 - acc: 0.9986 - val_loss: 2.1844 - val_acc: 0.9487\n",
      "Epoch 40/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1686 - acc: 0.9957 - val_loss: 2.1759 - val_acc: 0.9615\n",
      "Epoch 41/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1675 - acc: 0.9957 - val_loss: 2.1717 - val_acc: 0.9872\n",
      "Epoch 42/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1668 - acc: 1.0000 - val_loss: 2.1706 - val_acc: 0.9744\n",
      "Epoch 43/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1673 - acc: 1.0000 - val_loss: 2.1748 - val_acc: 0.9744\n",
      "Epoch 44/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1667 - acc: 1.0000 - val_loss: 2.1702 - val_acc: 1.0000\n",
      "Epoch 45/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1667 - acc: 1.0000 - val_loss: 2.1749 - val_acc: 0.9487\n",
      "Epoch 46/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1666 - acc: 1.0000 - val_loss: 2.1728 - val_acc: 0.9744\n",
      "Epoch 47/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1668 - acc: 0.9986 - val_loss: 2.1721 - val_acc: 0.9744\n",
      "Epoch 48/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1701 - val_acc: 0.9872\n",
      "Epoch 49/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1707 - val_acc: 0.9872\n",
      "Epoch 50/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1708 - val_acc: 0.9872\n",
      "Epoch 51/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1666 - acc: 1.0000 - val_loss: 2.1707 - val_acc: 0.9615\n",
      "Epoch 52/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1671 - acc: 0.9986 - val_loss: 2.1689 - val_acc: 0.9744\n",
      "Epoch 53/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1704 - val_acc: 0.9744\n",
      "Epoch 54/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1666 - acc: 0.9986 - val_loss: 2.1710 - val_acc: 0.9744\n",
      "Epoch 55/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1729 - val_acc: 0.9615\n",
      "Epoch 56/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1694 - val_acc: 0.9615\n",
      "Epoch 57/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1689 - val_acc: 1.0000\n",
      "Epoch 58/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1704 - val_acc: 1.0000\n",
      "Epoch 59/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1698 - val_acc: 0.9872\n",
      "Epoch 60/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1689 - val_acc: 1.0000\n",
      "Epoch 61/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1692 - val_acc: 0.9872\n",
      "Epoch 62/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1683 - val_acc: 1.0000\n",
      "Epoch 63/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1689 - val_acc: 0.9744\n",
      "Epoch 64/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1700 - val_acc: 0.9872\n",
      "Epoch 65/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1680 - val_acc: 1.0000\n",
      "Epoch 66/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1703 - val_acc: 0.9872\n",
      "Epoch 67/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1684 - val_acc: 1.0000\n",
      "Epoch 68/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1695 - val_acc: 0.9872\n",
      "Epoch 69/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1687 - val_acc: 0.9744\n",
      "Epoch 70/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1691 - val_acc: 0.9872\n",
      "Epoch 71/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1689 - val_acc: 0.9872\n",
      "Epoch 72/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1689 - val_acc: 0.9872\n",
      "Epoch 73/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1690 - val_acc: 1.0000\n",
      "Epoch 74/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1692 - val_acc: 1.0000\n",
      "Epoch 75/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1689 - val_acc: 0.9872\n",
      "Epoch 76/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1693 - val_acc: 0.9872\n",
      "Epoch 77/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1693 - val_acc: 0.9872\n",
      "Epoch 78/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1694 - val_acc: 0.9872\n",
      "Epoch 79/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1690 - val_acc: 0.9744\n",
      "Epoch 80/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1694 - val_acc: 0.9872\n",
      "Epoch 81/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1692 - val_acc: 0.9872\n",
      "Epoch 82/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1691 - val_acc: 0.9872\n",
      "Epoch 83/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1691 - val_acc: 0.9872\n",
      "Epoch 84/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1688 - val_acc: 0.9872\n",
      "Epoch 85/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1706 - val_acc: 0.9872\n",
      "Epoch 86/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1694 - val_acc: 0.9872\n",
      "Epoch 87/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1691 - val_acc: 0.9872\n",
      "Epoch 88/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1690 - val_acc: 0.9872\n",
      "Epoch 89/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1693 - val_acc: 0.9872\n",
      "Epoch 90/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1695 - val_acc: 0.9872\n",
      "Epoch 91/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1687 - val_acc: 0.9872\n",
      "Epoch 92/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1692 - val_acc: 0.9872\n",
      "Epoch 93/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1691 - val_acc: 0.9872\n",
      "Epoch 94/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1695 - val_acc: 0.9872\n",
      "Epoch 95/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1694 - val_acc: 0.9872\n",
      "Epoch 96/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1692 - val_acc: 0.9872\n",
      "Epoch 97/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1697 - val_acc: 0.9872\n",
      "Epoch 98/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1696 - val_acc: 0.9872\n",
      "Epoch 99/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1694 - val_acc: 0.9872\n",
      "Epoch 100/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1697 - val_acc: 0.9872\n",
      "78/78 [==============================] - 0s 329us/step\n",
      "Score for fold 2: loss of 2.1700840363135705; acc of 98.71794871794873%\n",
      "Train on 702 samples, validate on 78 samples\n",
      "Epoch 1/100\n",
      "702/702 [==============================] - 15s 22ms/step - loss: 2.4817 - acc: 0.2949 - val_loss: 2.3995 - val_acc: 0.3462\n",
      "Epoch 2/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.3098 - acc: 0.5442 - val_loss: 2.4411 - val_acc: 0.4103\n",
      "Epoch 3/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2559 - acc: 0.7308 - val_loss: 2.2188 - val_acc: 0.8333\n",
      "Epoch 4/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2270 - acc: 0.8234 - val_loss: 2.2585 - val_acc: 0.7821\n",
      "Epoch 5/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2040 - acc: 0.8974 - val_loss: 2.2132 - val_acc: 0.8077\n",
      "Epoch 6/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2004 - acc: 0.8860 - val_loss: 2.2335 - val_acc: 0.7692\n",
      "Epoch 7/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1884 - acc: 0.9160 - val_loss: 2.2051 - val_acc: 0.8718\n",
      "Epoch 8/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1821 - acc: 0.9701 - val_loss: 2.1952 - val_acc: 0.8974\n",
      "Epoch 9/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1851 - acc: 0.9459 - val_loss: 2.2280 - val_acc: 0.7821\n",
      "Epoch 10/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1823 - acc: 0.9558 - val_loss: 2.2152 - val_acc: 0.8718\n",
      "Epoch 11/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1784 - acc: 0.9672 - val_loss: 2.1813 - val_acc: 0.9615\n",
      "Epoch 12/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1880 - acc: 0.9687 - val_loss: 2.1800 - val_acc: 0.9615\n",
      "Epoch 13/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1745 - acc: 0.9815 - val_loss: 2.2170 - val_acc: 0.8205\n",
      "Epoch 14/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1762 - acc: 0.9701 - val_loss: 2.1824 - val_acc: 0.9487\n",
      "Epoch 15/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1770 - acc: 0.9701 - val_loss: 2.1845 - val_acc: 0.9487\n",
      "Epoch 16/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1712 - acc: 0.9929 - val_loss: 2.1834 - val_acc: 0.9103\n",
      "Epoch 17/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1754 - acc: 0.9815 - val_loss: 2.1943 - val_acc: 0.9231\n",
      "Epoch 18/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1735 - acc: 0.9886 - val_loss: 2.1831 - val_acc: 0.9744\n",
      "Epoch 19/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1731 - acc: 0.9843 - val_loss: 2.1890 - val_acc: 0.9744\n",
      "Epoch 20/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1708 - acc: 0.9943 - val_loss: 2.1924 - val_acc: 0.9615\n",
      "Epoch 21/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1735 - acc: 0.9872 - val_loss: 2.1784 - val_acc: 0.9231\n",
      "Epoch 22/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1698 - acc: 0.9943 - val_loss: 2.1949 - val_acc: 0.9487\n",
      "Epoch 23/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1706 - acc: 0.9929 - val_loss: 2.2189 - val_acc: 0.8974\n",
      "Epoch 24/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1704 - acc: 0.9972 - val_loss: 2.1775 - val_acc: 0.9615\n",
      "Epoch 25/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1685 - acc: 0.9986 - val_loss: 2.1840 - val_acc: 0.9487\n",
      "Epoch 26/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1712 - acc: 0.9872 - val_loss: 2.1812 - val_acc: 0.9872\n",
      "Epoch 27/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1704 - acc: 0.9915 - val_loss: 2.1834 - val_acc: 1.0000\n",
      "Epoch 28/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1687 - acc: 0.9972 - val_loss: 2.1871 - val_acc: 0.9744\n",
      "Epoch 29/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1691 - acc: 0.9972 - val_loss: 2.1839 - val_acc: 0.9872\n",
      "Epoch 30/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1675 - acc: 0.9986 - val_loss: 2.1787 - val_acc: 0.9615\n",
      "Epoch 31/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1680 - acc: 0.9986 - val_loss: 2.1892 - val_acc: 0.9487\n",
      "Epoch 32/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1688 - acc: 0.9972 - val_loss: 2.1804 - val_acc: 1.0000\n",
      "Epoch 33/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1677 - acc: 0.9972 - val_loss: 2.1835 - val_acc: 0.9872\n",
      "Epoch 34/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1672 - acc: 0.9986 - val_loss: 2.1897 - val_acc: 0.9487\n",
      "Epoch 35/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1676 - acc: 0.9986 - val_loss: 2.1888 - val_acc: 0.9487\n",
      "Epoch 36/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1687 - acc: 0.9957 - val_loss: 2.1814 - val_acc: 0.9872\n",
      "Epoch 37/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1670 - acc: 1.0000 - val_loss: 2.1799 - val_acc: 0.9872\n",
      "Epoch 38/100\n",
      "702/702 [==============================] - 5s 8ms/step - loss: 2.1687 - acc: 0.9943 - val_loss: 2.1771 - val_acc: 1.0000\n",
      "Epoch 39/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1668 - acc: 1.0000 - val_loss: 2.1749 - val_acc: 0.9615\n",
      "Epoch 40/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1668 - acc: 1.0000 - val_loss: 2.1827 - val_acc: 1.0000\n",
      "Epoch 41/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1666 - acc: 1.0000 - val_loss: 2.1886 - val_acc: 0.9744\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1671 - acc: 0.9986 - val_loss: 2.1850 - val_acc: 0.9744\n",
      "Epoch 43/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1669 - acc: 0.9972 - val_loss: 2.1865 - val_acc: 1.0000\n",
      "Epoch 44/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1666 - acc: 1.0000 - val_loss: 2.1837 - val_acc: 1.0000\n",
      "Epoch 45/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1842 - val_acc: 0.9872\n",
      "Epoch 46/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1675 - acc: 1.0000 - val_loss: 2.1783 - val_acc: 1.0000\n",
      "Epoch 47/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1836 - val_acc: 0.9872\n",
      "Epoch 48/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1667 - acc: 0.9986 - val_loss: 2.1815 - val_acc: 0.9872\n",
      "Epoch 49/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1847 - val_acc: 1.0000\n",
      "Epoch 50/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1820 - val_acc: 0.9872\n",
      "Epoch 51/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1791 - val_acc: 0.9872\n",
      "Epoch 52/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1810 - val_acc: 1.0000\n",
      "Epoch 53/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1810 - val_acc: 0.9872\n",
      "Epoch 54/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 0.9986 - val_loss: 2.1786 - val_acc: 0.9872\n",
      "Epoch 55/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 0.9986 - val_loss: 2.1769 - val_acc: 0.9872\n",
      "Epoch 56/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1835 - val_acc: 0.9872\n",
      "Epoch 57/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1789 - val_acc: 0.9872\n",
      "Epoch 58/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1772 - val_acc: 0.9872\n",
      "Epoch 59/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1778 - val_acc: 0.9872\n",
      "Epoch 60/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1818 - val_acc: 0.9872\n",
      "Epoch 61/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1787 - val_acc: 0.9872\n",
      "Epoch 62/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1837 - val_acc: 0.9872\n",
      "Epoch 63/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1789 - val_acc: 0.9872\n",
      "Epoch 64/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1847 - val_acc: 0.9744\n",
      "Epoch 65/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1802 - val_acc: 0.9872\n",
      "Epoch 66/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1793 - val_acc: 0.9872\n",
      "Epoch 67/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1870 - val_acc: 0.9872\n",
      "Epoch 68/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1805 - val_acc: 0.9872\n",
      "Epoch 69/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1810 - val_acc: 0.9872\n",
      "Epoch 70/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1820 - val_acc: 0.9872\n",
      "Epoch 71/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1809 - val_acc: 1.0000\n",
      "Epoch 72/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1794 - val_acc: 1.0000\n",
      "Epoch 73/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1850 - val_acc: 1.0000\n",
      "Epoch 74/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1797 - val_acc: 0.9872\n",
      "Epoch 75/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1771 - val_acc: 0.9872\n",
      "Epoch 76/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1827 - val_acc: 0.9872\n",
      "Epoch 77/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1801 - val_acc: 0.9872\n",
      "Epoch 78/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1784 - val_acc: 0.9872\n",
      "Epoch 79/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1799 - val_acc: 0.9872\n",
      "Epoch 80/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1813 - val_acc: 0.9872\n",
      "Epoch 81/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1849 - val_acc: 0.9872\n",
      "Epoch 82/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1872 - val_acc: 0.9744\n",
      "Epoch 83/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1852 - val_acc: 0.9744\n",
      "Epoch 84/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1873 - val_acc: 0.9744\n",
      "Epoch 85/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1847 - val_acc: 0.9744\n",
      "Epoch 86/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1867 - val_acc: 0.9872\n",
      "Epoch 87/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1837 - val_acc: 0.9744\n",
      "Epoch 88/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1864 - val_acc: 0.9744\n",
      "Epoch 89/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1854 - val_acc: 0.9744\n",
      "Epoch 90/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1845 - val_acc: 0.9744\n",
      "Epoch 91/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1849 - val_acc: 0.9744\n",
      "Epoch 92/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1858 - val_acc: 0.9744\n",
      "Epoch 93/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1861 - val_acc: 0.9744\n",
      "Epoch 94/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1834 - val_acc: 0.9744\n",
      "Epoch 95/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1821 - val_acc: 0.9744\n",
      "Epoch 96/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1865 - val_acc: 0.9744\n",
      "Epoch 97/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1863 - val_acc: 0.9744\n",
      "Epoch 98/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1874 - val_acc: 0.9744\n",
      "Epoch 99/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1878 - val_acc: 0.9615\n",
      "Epoch 100/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1857 - val_acc: 0.9744\n",
      "78/78 [==============================] - 0s 351us/step\n",
      "Score for fold 3: loss of 2.1853630787287; acc of 97.43589697740018%\n",
      "Train on 702 samples, validate on 78 samples\n",
      "Epoch 1/100\n",
      "702/702 [==============================] - 16s 22ms/step - loss: 2.4944 - acc: 0.2692 - val_loss: 2.4217 - val_acc: 0.4231\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.3146 - acc: 0.5712 - val_loss: 2.3181 - val_acc: 0.6667\n",
      "Epoch 3/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.2478 - acc: 0.7536 - val_loss: 2.2312 - val_acc: 0.7692\n",
      "Epoch 4/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2155 - acc: 0.8248 - val_loss: 2.2331 - val_acc: 0.7692\n",
      "Epoch 5/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.2065 - acc: 0.8789 - val_loss: 2.1933 - val_acc: 0.9487\n",
      "Epoch 6/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1947 - acc: 0.9160 - val_loss: 2.1917 - val_acc: 0.9231\n",
      "Epoch 7/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1895 - acc: 0.9373 - val_loss: 2.2231 - val_acc: 0.8205\n",
      "Epoch 8/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1870 - acc: 0.9573 - val_loss: 2.1776 - val_acc: 1.0000\n",
      "Epoch 9/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1806 - acc: 0.9672 - val_loss: 2.2472 - val_acc: 0.7692\n",
      "Epoch 10/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1798 - acc: 0.9558 - val_loss: 2.1760 - val_acc: 0.9615\n",
      "Epoch 11/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1805 - acc: 0.9601 - val_loss: 2.1937 - val_acc: 0.8205\n",
      "Epoch 12/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1764 - acc: 0.9729 - val_loss: 2.1942 - val_acc: 0.9487\n",
      "Epoch 13/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1754 - acc: 0.9786 - val_loss: 2.1771 - val_acc: 0.9615\n",
      "Epoch 14/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1726 - acc: 0.9900 - val_loss: 2.1798 - val_acc: 0.9487\n",
      "Epoch 15/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1731 - acc: 0.9815 - val_loss: 2.1802 - val_acc: 0.9359\n",
      "Epoch 16/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1741 - acc: 0.9858 - val_loss: 2.1988 - val_acc: 0.9359\n",
      "Epoch 17/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1722 - acc: 0.9929 - val_loss: 2.1705 - val_acc: 0.9872\n",
      "Epoch 18/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1705 - acc: 0.9872 - val_loss: 2.1722 - val_acc: 0.9615\n",
      "Epoch 19/100\n",
      "702/702 [==============================] - 6s 8ms/step - loss: 2.1720 - acc: 0.9929 - val_loss: 2.1913 - val_acc: 0.9103\n",
      "Epoch 20/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1695 - acc: 0.9915 - val_loss: 2.1686 - val_acc: 1.0000\n",
      "Epoch 21/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1684 - acc: 0.9986 - val_loss: 2.1708 - val_acc: 0.9615\n",
      "Epoch 22/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1683 - acc: 0.9986 - val_loss: 2.1701 - val_acc: 0.9744\n",
      "Epoch 23/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1678 - acc: 0.9986 - val_loss: 2.1695 - val_acc: 0.9872\n",
      "Epoch 24/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1680 - acc: 0.9986 - val_loss: 2.1699 - val_acc: 0.9872\n",
      "Epoch 25/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1677 - acc: 0.9986 - val_loss: 2.1697 - val_acc: 0.9872\n",
      "Epoch 26/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1677 - acc: 0.9986 - val_loss: 2.1702 - val_acc: 0.9872\n",
      "Epoch 27/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1698 - acc: 0.9929 - val_loss: 2.1688 - val_acc: 1.0000\n",
      "Epoch 28/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1671 - acc: 0.9986 - val_loss: 2.1728 - val_acc: 0.9744\n",
      "Epoch 29/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1671 - acc: 0.9986 - val_loss: 2.1698 - val_acc: 0.9872\n",
      "Epoch 30/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1674 - acc: 0.9972 - val_loss: 2.1695 - val_acc: 0.9872\n",
      "Epoch 31/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1669 - acc: 1.0000 - val_loss: 2.1692 - val_acc: 1.0000\n",
      "Epoch 32/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1674 - acc: 0.9986 - val_loss: 2.1691 - val_acc: 0.9872\n",
      "Epoch 33/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1668 - acc: 0.9986 - val_loss: 2.1690 - val_acc: 0.9872\n",
      "Epoch 34/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1667 - acc: 1.0000 - val_loss: 2.1697 - val_acc: 0.9744\n",
      "Epoch 35/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1667 - acc: 1.0000 - val_loss: 2.1727 - val_acc: 0.9872\n",
      "Epoch 36/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1668 - acc: 1.0000 - val_loss: 2.1681 - val_acc: 0.9872\n",
      "Epoch 37/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1667 - acc: 0.9986 - val_loss: 2.1692 - val_acc: 0.9872\n",
      "Epoch 38/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1666 - acc: 1.0000 - val_loss: 2.1688 - val_acc: 0.9744\n",
      "Epoch 39/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1666 - acc: 1.0000 - val_loss: 2.1688 - val_acc: 0.9872\n",
      "Epoch 40/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1692 - val_acc: 0.9872\n",
      "Epoch 41/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1685 - val_acc: 0.9872\n",
      "Epoch 42/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1690 - val_acc: 0.9872\n",
      "Epoch 43/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1688 - val_acc: 0.9872\n",
      "Epoch 44/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1686 - val_acc: 0.9872\n",
      "Epoch 45/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1689 - val_acc: 0.9872\n",
      "Epoch 46/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1685 - val_acc: 0.9872\n",
      "Epoch 47/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1688 - val_acc: 0.9872\n",
      "Epoch 48/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1684 - val_acc: 0.9872\n",
      "Epoch 49/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 0.9972 - val_loss: 2.1682 - val_acc: 0.9872\n",
      "Epoch 50/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1677 - val_acc: 1.0000\n",
      "Epoch 51/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1683 - val_acc: 0.9872\n",
      "Epoch 52/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1683 - val_acc: 0.9872\n",
      "Epoch 53/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1681 - val_acc: 0.9872\n",
      "Epoch 54/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1684 - val_acc: 0.9872\n",
      "Epoch 55/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1684 - val_acc: 0.9872\n",
      "Epoch 56/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1689 - val_acc: 0.9872\n",
      "Epoch 57/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1680 - val_acc: 0.9872\n",
      "Epoch 58/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1686 - val_acc: 0.9872\n",
      "Epoch 59/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1683 - val_acc: 0.9872\n",
      "Epoch 60/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1682 - val_acc: 0.9872\n",
      "Epoch 61/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1682 - val_acc: 0.9872\n",
      "Epoch 62/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1685 - val_acc: 0.9872\n",
      "Epoch 63/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1684 - val_acc: 0.9872\n",
      "Epoch 64/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1684 - val_acc: 0.9872\n",
      "Epoch 65/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1685 - val_acc: 0.9872\n",
      "Epoch 66/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1681 - val_acc: 0.9872\n",
      "Epoch 67/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1688 - val_acc: 0.9872\n",
      "Epoch 68/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1685 - val_acc: 0.9872\n",
      "Epoch 69/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1681 - val_acc: 0.9872\n",
      "Epoch 70/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1681 - val_acc: 0.9872\n",
      "Epoch 71/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1684 - val_acc: 0.9872\n",
      "Epoch 72/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1684 - val_acc: 0.9872\n",
      "Epoch 73/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1682 - val_acc: 0.9872\n",
      "Epoch 74/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1682 - val_acc: 0.9872\n",
      "Epoch 75/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1683 - val_acc: 0.9872\n",
      "Epoch 76/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1679 - val_acc: 0.9872\n",
      "Epoch 77/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1679 - val_acc: 0.9872\n",
      "Epoch 78/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1683 - val_acc: 0.9872\n",
      "Epoch 79/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1678 - val_acc: 0.9872\n",
      "Epoch 80/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1682 - val_acc: 0.9872\n",
      "Epoch 81/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1681 - val_acc: 0.9872\n",
      "Epoch 82/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1683 - val_acc: 0.9872\n",
      "Epoch 83/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1681 - val_acc: 0.9872\n",
      "Epoch 84/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1684 - val_acc: 0.9872\n",
      "Epoch 85/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1682 - val_acc: 0.9872\n",
      "Epoch 86/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1684 - val_acc: 0.9872\n",
      "Epoch 87/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1683 - val_acc: 0.9872\n",
      "Epoch 88/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1681 - val_acc: 0.9872\n",
      "Epoch 89/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1682 - val_acc: 0.9872\n",
      "Epoch 90/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1682 - val_acc: 0.9872\n",
      "Epoch 91/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1683 - val_acc: 0.9872\n",
      "Epoch 92/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1682 - val_acc: 0.9872\n",
      "Epoch 93/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1681 - val_acc: 0.9872\n",
      "Epoch 94/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1682 - val_acc: 0.9872\n",
      "Epoch 95/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1683 - val_acc: 0.9872\n",
      "Epoch 96/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1682 - val_acc: 0.9872\n",
      "Epoch 97/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1684 - val_acc: 0.9872\n",
      "Epoch 98/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1683 - val_acc: 0.9872\n",
      "Epoch 99/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1682 - val_acc: 0.9872\n",
      "Epoch 100/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1682 - val_acc: 0.9872\n",
      "78/78 [==============================] - 0s 371us/step\n",
      "Score for fold 4: loss of 2.16859578474974; acc of 98.71794825945145%\n",
      "Train on 702 samples, validate on 78 samples\n",
      "Epoch 1/100\n",
      "702/702 [==============================] - 16s 23ms/step - loss: 2.5137 - acc: 0.2493 - val_loss: 2.3721 - val_acc: 0.3846\n",
      "Epoch 2/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.3241 - acc: 0.5014 - val_loss: 2.3744 - val_acc: 0.4231\n",
      "Epoch 3/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2622 - acc: 0.7123 - val_loss: 2.2893 - val_acc: 0.6154\n",
      "Epoch 4/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2312 - acc: 0.7835 - val_loss: 2.2319 - val_acc: 0.8590\n",
      "Epoch 5/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2081 - acc: 0.8547 - val_loss: 2.2228 - val_acc: 0.7692\n",
      "Epoch 6/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1973 - acc: 0.8846 - val_loss: 2.2018 - val_acc: 0.8846\n",
      "Epoch 7/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1936 - acc: 0.9217 - val_loss: 2.1984 - val_acc: 0.9231\n",
      "Epoch 8/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1844 - acc: 0.9430 - val_loss: 2.2223 - val_acc: 0.8846\n",
      "Epoch 9/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1819 - acc: 0.9430 - val_loss: 2.1922 - val_acc: 0.8846\n",
      "Epoch 10/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1795 - acc: 0.9729 - val_loss: 2.1806 - val_acc: 0.9744\n",
      "Epoch 11/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1752 - acc: 0.9815 - val_loss: 2.2126 - val_acc: 0.8077\n",
      "Epoch 12/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1804 - acc: 0.9501 - val_loss: 2.2662 - val_acc: 0.8077\n",
      "Epoch 13/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1764 - acc: 0.9744 - val_loss: 2.1813 - val_acc: 0.9103\n",
      "Epoch 14/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1749 - acc: 0.9744 - val_loss: 2.1938 - val_acc: 0.9231\n",
      "Epoch 15/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1728 - acc: 0.9843 - val_loss: 2.1769 - val_acc: 0.9615\n",
      "Epoch 16/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1730 - acc: 0.9900 - val_loss: 2.1756 - val_acc: 0.9872\n",
      "Epoch 17/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1743 - acc: 0.9801 - val_loss: 2.1785 - val_acc: 0.9615\n",
      "Epoch 18/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1727 - acc: 0.9886 - val_loss: 2.1741 - val_acc: 0.9872\n",
      "Epoch 19/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1708 - acc: 0.9872 - val_loss: 2.1734 - val_acc: 0.9872\n",
      "Epoch 20/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1744 - acc: 0.9843 - val_loss: 2.1813 - val_acc: 0.9744\n",
      "Epoch 21/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1693 - acc: 0.9929 - val_loss: 2.1743 - val_acc: 0.9872\n",
      "Epoch 22/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1700 - acc: 0.9957 - val_loss: 2.1715 - val_acc: 0.9872\n",
      "Epoch 23/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1705 - acc: 0.9929 - val_loss: 2.1707 - val_acc: 1.0000\n",
      "Epoch 24/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1694 - acc: 0.9943 - val_loss: 2.1717 - val_acc: 0.9872\n",
      "Epoch 25/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1688 - acc: 0.9957 - val_loss: 2.1743 - val_acc: 0.9615\n",
      "Epoch 26/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1712 - acc: 0.9900 - val_loss: 2.1716 - val_acc: 0.9872\n",
      "Epoch 27/100\n",
      "702/702 [==============================] - ETA: 0s - loss: 2.1676 - acc: 0.998 - 1s 1ms/step - loss: 2.1676 - acc: 0.9986 - val_loss: 2.1729 - val_acc: 0.9872\n",
      "Epoch 28/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1680 - acc: 0.9972 - val_loss: 2.1771 - val_acc: 0.9359\n",
      "Epoch 29/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1681 - acc: 0.9957 - val_loss: 2.1872 - val_acc: 0.8974\n",
      "Epoch 30/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1706 - acc: 0.9929 - val_loss: 2.1723 - val_acc: 0.9744\n",
      "Epoch 31/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1685 - acc: 0.9943 - val_loss: 2.1727 - val_acc: 0.9872\n",
      "Epoch 32/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1674 - acc: 1.0000 - val_loss: 2.1723 - val_acc: 0.9615\n",
      "Epoch 33/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1675 - acc: 0.9972 - val_loss: 2.1705 - val_acc: 0.9872\n",
      "Epoch 34/100\n",
      "702/702 [==============================] - 6s 8ms/step - loss: 2.1678 - acc: 0.9915 - val_loss: 2.1742 - val_acc: 0.9615\n",
      "Epoch 35/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1673 - acc: 0.9986 - val_loss: 2.1707 - val_acc: 0.9744\n",
      "Epoch 36/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1668 - acc: 1.0000 - val_loss: 2.1700 - val_acc: 0.9872\n",
      "Epoch 37/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1674 - acc: 0.9986 - val_loss: 2.1697 - val_acc: 1.0000\n",
      "Epoch 38/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1667 - acc: 1.0000 - val_loss: 2.1750 - val_acc: 0.9744\n",
      "Epoch 39/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1672 - acc: 0.9986 - val_loss: 2.1694 - val_acc: 1.0000\n",
      "Epoch 40/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1666 - acc: 1.0000 - val_loss: 2.1697 - val_acc: 0.9872\n",
      "Epoch 41/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1669 - acc: 0.9986 - val_loss: 2.1723 - val_acc: 0.9744\n",
      "Epoch 42/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1669 - acc: 1.0000 - val_loss: 2.1698 - val_acc: 0.9872\n",
      "Epoch 43/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1667 - acc: 0.9986 - val_loss: 2.1785 - val_acc: 0.9872\n",
      "Epoch 44/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1670 - acc: 0.9972 - val_loss: 2.1706 - val_acc: 0.9744\n",
      "Epoch 45/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1691 - val_acc: 0.9872\n",
      "Epoch 46/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1666 - acc: 0.9986 - val_loss: 2.1690 - val_acc: 0.9744\n",
      "Epoch 47/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1688 - val_acc: 1.0000\n",
      "Epoch 48/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1700 - val_acc: 0.9872\n",
      "Epoch 49/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1666 - acc: 1.0000 - val_loss: 2.1698 - val_acc: 0.9744\n",
      "Epoch 50/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1702 - val_acc: 0.9744\n",
      "Epoch 51/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1696 - val_acc: 0.9872\n",
      "Epoch 52/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1689 - val_acc: 0.9872\n",
      "Epoch 53/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1693 - val_acc: 0.9872\n",
      "Epoch 54/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1712 - val_acc: 0.9744\n",
      "Epoch 55/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1688 - val_acc: 0.9872\n",
      "Epoch 56/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1693 - val_acc: 1.0000\n",
      "Epoch 57/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1693 - val_acc: 0.9872\n",
      "Epoch 58/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1704 - val_acc: 1.0000\n",
      "Epoch 59/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1693 - val_acc: 0.9872\n",
      "Epoch 60/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1688 - val_acc: 1.0000\n",
      "Epoch 61/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1695 - val_acc: 1.0000\n",
      "Epoch 62/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 0.9986 - val_loss: 2.1687 - val_acc: 1.0000\n",
      "Epoch 63/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1687 - val_acc: 1.0000\n",
      "Epoch 64/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1699 - val_acc: 1.0000\n",
      "Epoch 65/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1687 - val_acc: 1.0000\n",
      "Epoch 66/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1697 - val_acc: 1.0000\n",
      "Epoch 67/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1688 - val_acc: 1.0000\n",
      "Epoch 68/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1686 - val_acc: 1.0000\n",
      "Epoch 69/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1686 - val_acc: 0.9872\n",
      "Epoch 70/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1685 - val_acc: 1.0000\n",
      "Epoch 71/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1686 - val_acc: 1.0000\n",
      "Epoch 72/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1682 - val_acc: 0.9872\n",
      "Epoch 73/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1684 - val_acc: 0.9872\n",
      "Epoch 74/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1688 - val_acc: 0.9872\n",
      "Epoch 75/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1684 - val_acc: 1.0000\n",
      "Epoch 76/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1685 - val_acc: 1.0000\n",
      "Epoch 77/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1685 - val_acc: 0.9872\n",
      "Epoch 78/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1685 - val_acc: 1.0000\n",
      "Epoch 79/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1687 - val_acc: 1.0000\n",
      "Epoch 80/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1687 - val_acc: 1.0000\n",
      "Epoch 81/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1687 - val_acc: 0.9872\n",
      "Epoch 82/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1685 - val_acc: 1.0000\n",
      "Epoch 83/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1685 - val_acc: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 84/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1685 - val_acc: 1.0000\n",
      "Epoch 85/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1685 - val_acc: 1.0000\n",
      "Epoch 86/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1684 - val_acc: 1.0000\n",
      "Epoch 87/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1683 - val_acc: 1.0000\n",
      "Epoch 88/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1686 - val_acc: 1.0000\n",
      "Epoch 89/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1682 - val_acc: 1.0000\n",
      "Epoch 90/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1682 - val_acc: 1.0000\n",
      "Epoch 91/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1682 - val_acc: 1.0000\n",
      "Epoch 92/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1684 - val_acc: 1.0000\n",
      "Epoch 93/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1683 - val_acc: 1.0000\n",
      "Epoch 94/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1683 - val_acc: 1.0000\n",
      "Epoch 95/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1684 - val_acc: 1.0000\n",
      "Epoch 96/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1683 - val_acc: 1.0000\n",
      "Epoch 97/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1683 - val_acc: 1.0000\n",
      "Epoch 98/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1683 - val_acc: 1.0000\n",
      "Epoch 99/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1684 - val_acc: 1.0000\n",
      "Epoch 100/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1684 - val_acc: 0.9872\n",
      "78/78 [==============================] - 0s 334us/step\n",
      "Score for fold 5: loss of 2.168427913616865; acc of 100.0%\n",
      "Train on 702 samples, validate on 78 samples\n",
      "Epoch 1/100\n",
      "702/702 [==============================] - 16s 23ms/step - loss: 2.4913 - acc: 0.2650 - val_loss: 2.3451 - val_acc: 0.5256\n",
      "Epoch 2/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.3035 - acc: 0.5826 - val_loss: 2.2662 - val_acc: 0.7308\n",
      "Epoch 3/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2420 - acc: 0.7607 - val_loss: 2.2733 - val_acc: 0.6026\n",
      "Epoch 4/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2198 - acc: 0.8405 - val_loss: 2.2311 - val_acc: 0.7308\n",
      "Epoch 5/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2048 - acc: 0.8618 - val_loss: 2.2207 - val_acc: 0.8462\n",
      "Epoch 6/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1936 - acc: 0.9217 - val_loss: 2.2276 - val_acc: 0.7821\n",
      "Epoch 7/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1910 - acc: 0.9288 - val_loss: 2.1992 - val_acc: 0.8077\n",
      "Epoch 8/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1830 - acc: 0.9416 - val_loss: 2.1906 - val_acc: 0.9744\n",
      "Epoch 9/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1802 - acc: 0.9772 - val_loss: 2.1905 - val_acc: 0.9487\n",
      "Epoch 10/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1828 - acc: 0.9729 - val_loss: 2.2107 - val_acc: 0.9487\n",
      "Epoch 11/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1804 - acc: 0.9687 - val_loss: 2.1800 - val_acc: 0.9744\n",
      "Epoch 12/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1765 - acc: 0.9729 - val_loss: 2.1824 - val_acc: 0.9744\n",
      "Epoch 13/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1753 - acc: 0.9815 - val_loss: 2.1882 - val_acc: 0.9487\n",
      "Epoch 14/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1753 - acc: 0.9858 - val_loss: 2.1788 - val_acc: 0.9744\n",
      "Epoch 15/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1760 - acc: 0.9843 - val_loss: 2.1776 - val_acc: 0.9744\n",
      "Epoch 16/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1725 - acc: 0.9900 - val_loss: 2.1811 - val_acc: 0.9615\n",
      "Epoch 17/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1782 - acc: 0.9900 - val_loss: 2.1780 - val_acc: 0.9744\n",
      "Epoch 18/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1721 - acc: 0.9886 - val_loss: 2.1752 - val_acc: 0.9744\n",
      "Epoch 19/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1718 - acc: 0.9872 - val_loss: 2.1781 - val_acc: 0.9872\n",
      "Epoch 20/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1713 - acc: 0.9900 - val_loss: 2.1856 - val_acc: 0.9744\n",
      "Epoch 21/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1702 - acc: 0.9915 - val_loss: 2.1892 - val_acc: 0.9744\n",
      "Epoch 22/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1692 - acc: 0.9972 - val_loss: 2.1743 - val_acc: 0.9872\n",
      "Epoch 23/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1731 - acc: 0.9900 - val_loss: 2.1739 - val_acc: 0.9615\n",
      "Epoch 24/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1679 - acc: 0.9986 - val_loss: 2.1748 - val_acc: 0.9744\n",
      "Epoch 25/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1704 - acc: 0.9929 - val_loss: 2.1807 - val_acc: 0.9872\n",
      "Epoch 26/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1690 - acc: 0.9943 - val_loss: 2.1788 - val_acc: 0.9615\n",
      "Epoch 27/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1690 - acc: 0.9943 - val_loss: 2.1785 - val_acc: 0.9744\n",
      "Epoch 28/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1695 - acc: 0.9929 - val_loss: 2.1734 - val_acc: 0.9872\n",
      "Epoch 29/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1674 - acc: 1.0000 - val_loss: 2.1763 - val_acc: 0.9872\n",
      "Epoch 30/100\n",
      "702/702 [==============================] - 6s 8ms/step - loss: 2.1678 - acc: 0.9986 - val_loss: 2.1785 - val_acc: 0.9487\n",
      "Epoch 31/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1679 - acc: 0.9943 - val_loss: 2.1761 - val_acc: 0.9872\n",
      "Epoch 32/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1672 - acc: 0.9972 - val_loss: 2.1726 - val_acc: 0.9872\n",
      "Epoch 33/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1669 - acc: 1.0000 - val_loss: 2.1846 - val_acc: 0.9744\n",
      "Epoch 34/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1669 - acc: 0.9986 - val_loss: 2.1725 - val_acc: 0.9872\n",
      "Epoch 35/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1668 - acc: 1.0000 - val_loss: 2.1752 - val_acc: 0.9872\n",
      "Epoch 36/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1668 - acc: 1.0000 - val_loss: 2.1719 - val_acc: 0.9872\n",
      "Epoch 37/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1673 - acc: 0.9986 - val_loss: 2.1716 - val_acc: 0.9872\n",
      "Epoch 38/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1667 - acc: 1.0000 - val_loss: 2.1711 - val_acc: 0.9872\n",
      "Epoch 39/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1669 - acc: 1.0000 - val_loss: 2.1726 - val_acc: 0.9872\n",
      "Epoch 40/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1668 - acc: 1.0000 - val_loss: 2.1722 - val_acc: 0.9872\n",
      "Epoch 41/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1708 - val_acc: 0.9872\n",
      "Epoch 42/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1707 - val_acc: 0.9872\n",
      "Epoch 43/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1666 - acc: 1.0000 - val_loss: 2.1719 - val_acc: 0.9615\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 0.9986 - val_loss: 2.1726 - val_acc: 0.9872\n",
      "Epoch 45/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1720 - val_acc: 0.9872\n",
      "Epoch 46/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1667 - acc: 1.0000 - val_loss: 2.1706 - val_acc: 0.9872\n",
      "Epoch 47/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1714 - val_acc: 0.9872\n",
      "Epoch 48/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1725 - val_acc: 0.9872\n",
      "Epoch 49/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1741 - val_acc: 0.9872\n",
      "Epoch 50/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1667 - acc: 0.9957 - val_loss: 2.1717 - val_acc: 0.9872\n",
      "Epoch 51/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1705 - val_acc: 0.9872\n",
      "Epoch 52/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1709 - val_acc: 0.9872\n",
      "Epoch 53/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1705 - val_acc: 0.9872\n",
      "Epoch 54/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1720 - val_acc: 0.9872\n",
      "Epoch 55/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1715 - val_acc: 0.9872\n",
      "Epoch 56/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1744 - val_acc: 0.9872\n",
      "Epoch 57/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1732 - val_acc: 0.9872\n",
      "Epoch 58/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1719 - val_acc: 0.9872\n",
      "Epoch 59/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1715 - val_acc: 0.9872\n",
      "Epoch 60/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1714 - val_acc: 0.9872\n",
      "Epoch 61/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1717 - val_acc: 0.9872\n",
      "Epoch 62/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1713 - val_acc: 0.9872\n",
      "Epoch 63/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1712 - val_acc: 0.9872\n",
      "Epoch 64/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1709 - val_acc: 0.9872\n",
      "Epoch 65/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1712 - val_acc: 0.9872\n",
      "Epoch 66/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1713 - val_acc: 0.9872\n",
      "Epoch 67/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1715 - val_acc: 0.9872\n",
      "Epoch 68/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1715 - val_acc: 0.9872\n",
      "Epoch 69/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1711 - val_acc: 0.9872\n",
      "Epoch 70/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1717 - val_acc: 0.9872\n",
      "Epoch 71/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1712 - val_acc: 0.9872\n",
      "Epoch 72/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1710 - val_acc: 0.9872\n",
      "Epoch 73/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1707 - val_acc: 0.9872\n",
      "Epoch 74/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1718 - val_acc: 0.9872\n",
      "Epoch 75/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1715 - val_acc: 0.9872\n",
      "Epoch 76/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1710 - val_acc: 0.9872\n",
      "Epoch 77/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1722 - val_acc: 0.9872\n",
      "Epoch 78/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1710 - val_acc: 0.9872\n",
      "Epoch 79/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1704 - val_acc: 0.9872\n",
      "Epoch 80/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1711 - val_acc: 0.9872\n",
      "Epoch 81/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1708 - val_acc: 0.9872\n",
      "Epoch 82/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1710 - val_acc: 0.9872\n",
      "Epoch 83/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1711 - val_acc: 0.9872\n",
      "Epoch 84/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1703 - val_acc: 0.9872\n",
      "Epoch 85/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1707 - val_acc: 0.9872\n",
      "Epoch 86/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1705 - val_acc: 0.9872\n",
      "Epoch 87/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1708 - val_acc: 0.9872\n",
      "Epoch 88/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1708 - val_acc: 0.9872\n",
      "Epoch 89/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1709 - val_acc: 0.9872\n",
      "Epoch 90/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1710 - val_acc: 0.9872\n",
      "Epoch 91/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1708 - val_acc: 0.9872\n",
      "Epoch 92/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1712 - val_acc: 0.9872\n",
      "Epoch 93/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1708 - val_acc: 0.9872\n",
      "Epoch 94/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1710 - val_acc: 0.9872\n",
      "Epoch 95/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1710 - val_acc: 0.9872\n",
      "Epoch 96/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1709 - val_acc: 0.9872\n",
      "Epoch 97/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1712 - val_acc: 0.9872\n",
      "Epoch 98/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1708 - val_acc: 0.9872\n",
      "Epoch 99/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1711 - val_acc: 0.9872\n",
      "Epoch 100/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1709 - val_acc: 0.9872\n",
      "78/78 [==============================] - 0s 343us/step\n",
      "Score for fold 6: loss of 2.171052009631426; acc of 98.71794825945145%\n",
      "Train on 702 samples, validate on 78 samples\n",
      "Epoch 1/100\n",
      "702/702 [==============================] - 17s 24ms/step - loss: 2.5043 - acc: 0.2650 - val_loss: 2.3917 - val_acc: 0.4359\n",
      "Epoch 2/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2995 - acc: 0.6382 - val_loss: 2.2659 - val_acc: 0.6538\n",
      "Epoch 3/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2417 - acc: 0.7692 - val_loss: 2.2174 - val_acc: 0.7692\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2151 - acc: 0.8462 - val_loss: 2.2367 - val_acc: 0.7564\n",
      "Epoch 5/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2096 - acc: 0.8618 - val_loss: 2.2178 - val_acc: 0.7821\n",
      "Epoch 6/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1924 - acc: 0.9231 - val_loss: 2.1988 - val_acc: 0.8205\n",
      "Epoch 7/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1861 - acc: 0.9459 - val_loss: 2.1940 - val_acc: 0.8846\n",
      "Epoch 8/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1843 - acc: 0.9459 - val_loss: 2.1818 - val_acc: 0.9359\n",
      "Epoch 9/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1789 - acc: 0.9615 - val_loss: 2.1929 - val_acc: 0.8974\n",
      "Epoch 10/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1762 - acc: 0.9786 - val_loss: 2.1930 - val_acc: 0.9744\n",
      "Epoch 11/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1736 - acc: 0.9829 - val_loss: 2.2598 - val_acc: 0.8333\n",
      "Epoch 12/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1771 - acc: 0.9815 - val_loss: 2.1899 - val_acc: 0.9487\n",
      "Epoch 13/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1730 - acc: 0.9886 - val_loss: 2.1827 - val_acc: 0.9615\n",
      "Epoch 14/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1720 - acc: 0.9886 - val_loss: 2.2016 - val_acc: 0.8974\n",
      "Epoch 15/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1718 - acc: 0.9886 - val_loss: 2.1879 - val_acc: 0.9359\n",
      "Epoch 16/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1718 - acc: 0.9801 - val_loss: 2.1888 - val_acc: 0.9487\n",
      "Epoch 17/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1708 - acc: 0.9915 - val_loss: 2.1798 - val_acc: 0.9231\n",
      "Epoch 18/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1720 - acc: 0.9872 - val_loss: 2.1798 - val_acc: 0.9487\n",
      "Epoch 19/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1718 - acc: 0.9929 - val_loss: 2.1800 - val_acc: 0.9744\n",
      "Epoch 20/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1701 - acc: 0.9943 - val_loss: 2.1799 - val_acc: 0.9744\n",
      "Epoch 21/100\n",
      "702/702 [==============================] - 6s 9ms/step - loss: 2.1691 - acc: 0.9915 - val_loss: 2.1829 - val_acc: 0.9487\n",
      "Epoch 22/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1684 - acc: 0.9972 - val_loss: 2.1831 - val_acc: 0.9615\n",
      "Epoch 23/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1678 - acc: 0.9957 - val_loss: 2.1808 - val_acc: 0.9615\n",
      "Epoch 24/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1678 - acc: 0.9986 - val_loss: 2.1789 - val_acc: 0.9359\n",
      "Epoch 25/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1672 - acc: 1.0000 - val_loss: 2.1805 - val_acc: 0.9615\n",
      "Epoch 26/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1671 - acc: 0.9986 - val_loss: 2.1809 - val_acc: 0.9615\n",
      "Epoch 27/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1671 - acc: 0.9986 - val_loss: 2.1809 - val_acc: 0.9615\n",
      "Epoch 28/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1669 - acc: 1.0000 - val_loss: 2.1776 - val_acc: 0.9615\n",
      "Epoch 29/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1674 - acc: 0.9972 - val_loss: 2.1777 - val_acc: 0.9615\n",
      "Epoch 30/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1668 - acc: 1.0000 - val_loss: 2.1797 - val_acc: 0.9615\n",
      "Epoch 31/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1668 - acc: 1.0000 - val_loss: 2.1798 - val_acc: 0.9231\n",
      "Epoch 32/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1666 - acc: 1.0000 - val_loss: 2.1781 - val_acc: 0.9615\n",
      "Epoch 33/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1777 - val_acc: 0.9744\n",
      "Epoch 34/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1795 - val_acc: 0.9615\n",
      "Epoch 35/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1666 - acc: 1.0000 - val_loss: 2.1804 - val_acc: 0.9615\n",
      "Epoch 36/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1666 - acc: 1.0000 - val_loss: 2.1770 - val_acc: 0.9615\n",
      "Epoch 37/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1788 - val_acc: 0.9615\n",
      "Epoch 38/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1776 - val_acc: 0.9487\n",
      "Epoch 39/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1800 - val_acc: 0.9615\n",
      "Epoch 40/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1812 - val_acc: 0.9615\n",
      "Epoch 41/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1777 - val_acc: 0.9615\n",
      "Epoch 42/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1767 - val_acc: 0.9615\n",
      "Epoch 43/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1778 - val_acc: 0.9615\n",
      "Epoch 44/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1754 - val_acc: 0.9615\n",
      "Epoch 45/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1778 - val_acc: 0.9615\n",
      "Epoch 46/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1785 - val_acc: 0.9615\n",
      "Epoch 47/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1753 - val_acc: 0.9744\n",
      "Epoch 48/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1804 - val_acc: 0.9615\n",
      "Epoch 49/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1793 - val_acc: 0.9615\n",
      "Epoch 50/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1785 - val_acc: 0.9615\n",
      "Epoch 51/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1760 - val_acc: 0.9615\n",
      "Epoch 52/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1786 - val_acc: 0.9615\n",
      "Epoch 53/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1802 - val_acc: 0.9615\n",
      "Epoch 54/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1790 - val_acc: 0.9615\n",
      "Epoch 55/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1782 - val_acc: 0.9615\n",
      "Epoch 56/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1790 - val_acc: 0.9615\n",
      "Epoch 57/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1805 - val_acc: 0.9615\n",
      "Epoch 58/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1794 - val_acc: 0.9615\n",
      "Epoch 59/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1771 - val_acc: 0.9615\n",
      "Epoch 60/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1787 - val_acc: 0.9615\n",
      "Epoch 61/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1801 - val_acc: 0.9615\n",
      "Epoch 62/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1791 - val_acc: 0.9615\n",
      "Epoch 63/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1770 - val_acc: 0.9615\n",
      "Epoch 64/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1788 - val_acc: 0.9615\n",
      "Epoch 65/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1775 - val_acc: 0.9615\n",
      "Epoch 66/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1780 - val_acc: 0.9615\n",
      "Epoch 67/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1808 - val_acc: 0.9615\n",
      "Epoch 68/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1788 - val_acc: 0.9615\n",
      "Epoch 69/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1777 - val_acc: 0.9615\n",
      "Epoch 70/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1789 - val_acc: 0.9615\n",
      "Epoch 71/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1776 - val_acc: 0.9615\n",
      "Epoch 72/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1782 - val_acc: 0.9615\n",
      "Epoch 73/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1785 - val_acc: 0.9615\n",
      "Epoch 74/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1779 - val_acc: 0.9615\n",
      "Epoch 75/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1777 - val_acc: 0.9615\n",
      "Epoch 76/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1774 - val_acc: 0.9615\n",
      "Epoch 77/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1783 - val_acc: 0.9615\n",
      "Epoch 78/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1774 - val_acc: 0.9615\n",
      "Epoch 79/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1784 - val_acc: 0.9615\n",
      "Epoch 80/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1781 - val_acc: 0.9615\n",
      "Epoch 81/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1790 - val_acc: 0.9615\n",
      "Epoch 82/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1778 - val_acc: 0.9615\n",
      "Epoch 83/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1787 - val_acc: 0.9615\n",
      "Epoch 84/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1780 - val_acc: 0.9615\n",
      "Epoch 85/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1775 - val_acc: 0.9615\n",
      "Epoch 86/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1788 - val_acc: 0.9615\n",
      "Epoch 87/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1785 - val_acc: 0.9615\n",
      "Epoch 88/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1781 - val_acc: 0.9615\n",
      "Epoch 89/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1776 - val_acc: 0.9615\n",
      "Epoch 90/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1789 - val_acc: 0.9615\n",
      "Epoch 91/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1786 - val_acc: 0.9615\n",
      "Epoch 92/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1784 - val_acc: 0.9615\n",
      "Epoch 93/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1781 - val_acc: 0.9615\n",
      "Epoch 94/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1783 - val_acc: 0.9615\n",
      "Epoch 95/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1782 - val_acc: 0.9615\n",
      "Epoch 96/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1786 - val_acc: 0.9615\n",
      "Epoch 97/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1783 - val_acc: 0.9615\n",
      "Epoch 98/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1784 - val_acc: 0.9615\n",
      "Epoch 99/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1785 - val_acc: 0.9615\n",
      "Epoch 100/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1778 - val_acc: 0.9615\n",
      "78/78 [==============================] - 0s 435us/step\n",
      "Score for fold 7: loss of 2.1753813853630652; acc of 96.15384630667859%\n",
      "Train on 702 samples, validate on 78 samples\n",
      "Epoch 1/100\n",
      "702/702 [==============================] - 17s 25ms/step - loss: 2.4954 - acc: 0.2593 - val_loss: 2.4615 - val_acc: 0.2692\n",
      "Epoch 2/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.3213 - acc: 0.5413 - val_loss: 2.2981 - val_acc: 0.4872\n",
      "Epoch 3/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2440 - acc: 0.7322 - val_loss: 2.2511 - val_acc: 0.6795\n",
      "Epoch 4/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2210 - acc: 0.8177 - val_loss: 2.2453 - val_acc: 0.8462\n",
      "Epoch 5/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2025 - acc: 0.8860 - val_loss: 2.2830 - val_acc: 0.6410\n",
      "Epoch 6/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1990 - acc: 0.9174 - val_loss: 2.1971 - val_acc: 0.9359\n",
      "Epoch 7/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1847 - acc: 0.9558 - val_loss: 2.3006 - val_acc: 0.6282\n",
      "Epoch 8/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1889 - acc: 0.9501 - val_loss: 2.1937 - val_acc: 0.9359\n",
      "Epoch 9/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1800 - acc: 0.9630 - val_loss: 2.2029 - val_acc: 0.8462\n",
      "Epoch 10/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1774 - acc: 0.9729 - val_loss: 2.1843 - val_acc: 0.9615\n",
      "Epoch 11/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1796 - acc: 0.9530 - val_loss: 2.2005 - val_acc: 0.9103\n",
      "Epoch 12/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1767 - acc: 0.9786 - val_loss: 2.2039 - val_acc: 0.9359\n",
      "Epoch 13/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1726 - acc: 0.9900 - val_loss: 2.1861 - val_acc: 0.9231\n",
      "Epoch 14/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1755 - acc: 0.9801 - val_loss: 2.1830 - val_acc: 0.9744\n",
      "Epoch 15/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1708 - acc: 0.9929 - val_loss: 2.1973 - val_acc: 0.8974\n",
      "Epoch 16/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1717 - acc: 0.9872 - val_loss: 2.1811 - val_acc: 0.9615\n",
      "Epoch 17/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1741 - acc: 0.9872 - val_loss: 2.1893 - val_acc: 0.9872\n",
      "Epoch 18/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1700 - acc: 0.9943 - val_loss: 2.1957 - val_acc: 0.8974\n",
      "Epoch 19/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1717 - acc: 0.9872 - val_loss: 2.1865 - val_acc: 0.9615\n",
      "Epoch 20/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1718 - acc: 0.9858 - val_loss: 2.1871 - val_acc: 0.9615\n",
      "Epoch 21/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1691 - acc: 0.9986 - val_loss: 2.1840 - val_acc: 0.9359\n",
      "Epoch 22/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1732 - acc: 0.9801 - val_loss: 2.1974 - val_acc: 0.9359\n",
      "Epoch 23/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1691 - acc: 0.9957 - val_loss: 2.1827 - val_acc: 0.9487\n",
      "Epoch 24/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1693 - acc: 0.9957 - val_loss: 2.1815 - val_acc: 0.9744\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1683 - acc: 0.9957 - val_loss: 2.1819 - val_acc: 0.9487\n",
      "Epoch 26/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1707 - acc: 0.9886 - val_loss: 2.1832 - val_acc: 0.9487\n",
      "Epoch 27/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1680 - acc: 0.9972 - val_loss: 2.1790 - val_acc: 0.9744\n",
      "Epoch 28/100\n",
      "702/702 [==============================] - 6s 8ms/step - loss: 2.1678 - acc: 0.9972 - val_loss: 2.1994 - val_acc: 0.8974\n",
      "Epoch 29/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1682 - acc: 0.9972 - val_loss: 2.1761 - val_acc: 0.9744\n",
      "Epoch 30/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1678 - acc: 0.9972 - val_loss: 2.1861 - val_acc: 0.9487\n",
      "Epoch 31/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1675 - acc: 0.9957 - val_loss: 2.1798 - val_acc: 0.9744\n",
      "Epoch 32/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1672 - acc: 0.9972 - val_loss: 2.1813 - val_acc: 0.9872\n",
      "Epoch 33/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1670 - acc: 0.9986 - val_loss: 2.1793 - val_acc: 0.9744\n",
      "Epoch 34/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1670 - acc: 1.0000 - val_loss: 2.1791 - val_acc: 0.9615\n",
      "Epoch 35/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1669 - acc: 1.0000 - val_loss: 2.1796 - val_acc: 0.9744\n",
      "Epoch 36/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1668 - acc: 1.0000 - val_loss: 2.1754 - val_acc: 0.9744\n",
      "Epoch 37/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1683 - acc: 0.9957 - val_loss: 2.1819 - val_acc: 0.9615\n",
      "Epoch 38/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1666 - acc: 1.0000 - val_loss: 2.1753 - val_acc: 0.9744\n",
      "Epoch 39/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1666 - acc: 1.0000 - val_loss: 2.1789 - val_acc: 0.9744\n",
      "Epoch 40/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1746 - val_acc: 0.9744\n",
      "Epoch 41/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1668 - acc: 1.0000 - val_loss: 2.1807 - val_acc: 0.9744\n",
      "Epoch 42/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1778 - val_acc: 0.9744\n",
      "Epoch 43/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1802 - val_acc: 0.9744\n",
      "Epoch 44/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1753 - val_acc: 0.9744\n",
      "Epoch 45/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1750 - val_acc: 0.9744\n",
      "Epoch 46/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1771 - val_acc: 0.9744\n",
      "Epoch 47/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1767 - val_acc: 0.9744\n",
      "Epoch 48/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1750 - val_acc: 0.9744\n",
      "Epoch 49/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1748 - val_acc: 0.9744\n",
      "Epoch 50/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1771 - val_acc: 0.9744\n",
      "Epoch 51/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1778 - val_acc: 0.9744\n",
      "Epoch 52/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1783 - val_acc: 0.9744\n",
      "Epoch 53/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1767 - val_acc: 0.9744\n",
      "Epoch 54/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1811 - val_acc: 0.9615\n",
      "Epoch 55/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1779 - val_acc: 0.9744\n",
      "Epoch 56/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1786 - val_acc: 0.9744\n",
      "Epoch 57/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1764 - val_acc: 0.9744\n",
      "Epoch 58/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 0.9986 - val_loss: 2.1754 - val_acc: 0.9744\n",
      "Epoch 59/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1764 - val_acc: 0.9744\n",
      "Epoch 60/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1764 - val_acc: 0.9744\n",
      "Epoch 61/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1759 - val_acc: 0.9744\n",
      "Epoch 62/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1756 - val_acc: 0.9744\n",
      "Epoch 63/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1769 - val_acc: 0.9744\n",
      "Epoch 64/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1779 - val_acc: 0.9744\n",
      "Epoch 65/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1811 - val_acc: 0.9615\n",
      "Epoch 66/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1772 - val_acc: 0.9744\n",
      "Epoch 67/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1738 - val_acc: 0.9744\n",
      "Epoch 68/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1765 - val_acc: 0.9744\n",
      "Epoch 69/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1772 - val_acc: 0.9744\n",
      "Epoch 70/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1769 - val_acc: 0.9744\n",
      "Epoch 71/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1780 - val_acc: 0.9744\n",
      "Epoch 72/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1737 - val_acc: 0.9744\n",
      "Epoch 73/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1778 - val_acc: 0.9744\n",
      "Epoch 74/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1781 - val_acc: 0.9744\n",
      "Epoch 75/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1764 - val_acc: 0.9744\n",
      "Epoch 76/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1779 - val_acc: 0.9744\n",
      "Epoch 77/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1763 - val_acc: 0.9744\n",
      "Epoch 78/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1773 - val_acc: 0.9744\n",
      "Epoch 79/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1769 - val_acc: 0.9744\n",
      "Epoch 80/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1780 - val_acc: 0.9744\n",
      "Epoch 81/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1762 - val_acc: 0.9744\n",
      "Epoch 82/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1767 - val_acc: 0.9744\n",
      "Epoch 83/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1772 - val_acc: 0.9744\n",
      "Epoch 84/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1755 - val_acc: 0.9744\n",
      "Epoch 85/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1772 - val_acc: 0.9744\n",
      "Epoch 86/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1765 - val_acc: 0.9744\n",
      "Epoch 87/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1776 - val_acc: 0.9744\n",
      "Epoch 88/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1768 - val_acc: 0.9744\n",
      "Epoch 89/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1762 - val_acc: 0.9744\n",
      "Epoch 90/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1754 - val_acc: 0.9744\n",
      "Epoch 91/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1755 - val_acc: 0.9744\n",
      "Epoch 92/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1761 - val_acc: 0.9744\n",
      "Epoch 93/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1771 - val_acc: 0.9744\n",
      "Epoch 94/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1770 - val_acc: 0.9744\n",
      "Epoch 95/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1764 - val_acc: 0.9744\n",
      "Epoch 96/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1765 - val_acc: 0.9744\n",
      "Epoch 97/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1765 - val_acc: 0.9744\n",
      "Epoch 98/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1764 - val_acc: 0.9744\n",
      "Epoch 99/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1753 - val_acc: 0.9744\n",
      "Epoch 100/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1757 - val_acc: 0.9744\n",
      "78/78 [==============================] - 0s 418us/step\n",
      "Score for fold 8: loss of 2.175717457746848; acc of 97.43589743589743%\n",
      "Train on 702 samples, validate on 78 samples\n",
      "Epoch 1/100\n",
      "702/702 [==============================] - 17s 25ms/step - loss: 2.4832 - acc: 0.2792 - val_loss: 2.3302 - val_acc: 0.6026\n",
      "Epoch 2/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.3193 - acc: 0.5584 - val_loss: 2.2920 - val_acc: 0.5385\n",
      "Epoch 3/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2467 - acc: 0.7251 - val_loss: 2.3106 - val_acc: 0.5641\n",
      "Epoch 4/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2346 - acc: 0.7991 - val_loss: 2.2002 - val_acc: 0.9231\n",
      "Epoch 5/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.2011 - acc: 0.8946 - val_loss: 2.2002 - val_acc: 0.8590\n",
      "Epoch 6/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2049 - acc: 0.8746 - val_loss: 2.1995 - val_acc: 0.8205\n",
      "Epoch 7/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1887 - acc: 0.9444 - val_loss: 2.1892 - val_acc: 0.9359\n",
      "Epoch 8/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1860 - acc: 0.9501 - val_loss: 2.1928 - val_acc: 0.9103\n",
      "Epoch 9/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1787 - acc: 0.9516 - val_loss: 2.2046 - val_acc: 0.8974\n",
      "Epoch 10/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1794 - acc: 0.9615 - val_loss: 2.1857 - val_acc: 0.9359\n",
      "Epoch 11/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1782 - acc: 0.9801 - val_loss: 2.2284 - val_acc: 0.7436\n",
      "Epoch 12/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1748 - acc: 0.9758 - val_loss: 2.1868 - val_acc: 0.9487\n",
      "Epoch 13/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1721 - acc: 0.9801 - val_loss: 2.1826 - val_acc: 0.9744\n",
      "Epoch 14/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1749 - acc: 0.9701 - val_loss: 2.1814 - val_acc: 0.9615\n",
      "Epoch 15/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1713 - acc: 0.9815 - val_loss: 2.1827 - val_acc: 0.9744\n",
      "Epoch 16/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1702 - acc: 0.9900 - val_loss: 2.2156 - val_acc: 0.9103\n",
      "Epoch 17/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1726 - acc: 0.9858 - val_loss: 2.1807 - val_acc: 0.9615\n",
      "Epoch 18/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1698 - acc: 0.9957 - val_loss: 2.1839 - val_acc: 0.9615\n",
      "Epoch 19/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1691 - acc: 0.9872 - val_loss: 2.2302 - val_acc: 0.8462\n",
      "Epoch 20/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1724 - acc: 0.9929 - val_loss: 2.1843 - val_acc: 0.9487\n",
      "Epoch 21/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1730 - acc: 0.9929 - val_loss: 2.1797 - val_acc: 0.9744\n",
      "Epoch 22/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1688 - acc: 0.9929 - val_loss: 2.1799 - val_acc: 0.9615\n",
      "Epoch 23/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1690 - acc: 0.9972 - val_loss: 2.1870 - val_acc: 0.9615\n",
      "Epoch 24/100\n",
      "702/702 [==============================] - 6s 8ms/step - loss: 2.1700 - acc: 0.9943 - val_loss: 2.1799 - val_acc: 0.9744\n",
      "Epoch 25/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1676 - acc: 0.9972 - val_loss: 2.1794 - val_acc: 0.9744\n",
      "Epoch 26/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1671 - acc: 1.0000 - val_loss: 2.1804 - val_acc: 0.9744\n",
      "Epoch 27/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1671 - acc: 0.9972 - val_loss: 2.1785 - val_acc: 0.9744\n",
      "Epoch 28/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1669 - acc: 1.0000 - val_loss: 2.1782 - val_acc: 0.9744\n",
      "Epoch 29/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1688 - acc: 0.9957 - val_loss: 2.1769 - val_acc: 0.9872\n",
      "Epoch 30/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1669 - acc: 1.0000 - val_loss: 2.1789 - val_acc: 0.9744\n",
      "Epoch 31/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1670 - acc: 1.0000 - val_loss: 2.1784 - val_acc: 0.9744\n",
      "Epoch 32/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1667 - acc: 1.0000 - val_loss: 2.1778 - val_acc: 0.9744\n",
      "Epoch 33/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1669 - acc: 1.0000 - val_loss: 2.1776 - val_acc: 0.9744\n",
      "Epoch 34/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1667 - acc: 1.0000 - val_loss: 2.1779 - val_acc: 0.9744\n",
      "Epoch 35/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1672 - acc: 0.9986 - val_loss: 2.1775 - val_acc: 0.9744\n",
      "Epoch 36/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1672 - acc: 0.9986 - val_loss: 2.1770 - val_acc: 0.9744\n",
      "Epoch 37/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1776 - val_acc: 0.9744\n",
      "Epoch 38/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1666 - acc: 1.0000 - val_loss: 2.1776 - val_acc: 0.9744\n",
      "Epoch 39/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1666 - acc: 1.0000 - val_loss: 2.1778 - val_acc: 0.9744\n",
      "Epoch 40/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1674 - acc: 0.9986 - val_loss: 2.1783 - val_acc: 0.9744\n",
      "Epoch 41/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1756 - val_acc: 0.9872\n",
      "Epoch 42/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 0.9986 - val_loss: 2.1779 - val_acc: 0.9744\n",
      "Epoch 43/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1781 - val_acc: 0.9744\n",
      "Epoch 44/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1783 - val_acc: 0.9744\n",
      "Epoch 45/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1772 - val_acc: 0.9744\n",
      "Epoch 46/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1773 - val_acc: 0.9744\n",
      "Epoch 47/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1669 - acc: 0.9972 - val_loss: 2.1769 - val_acc: 0.9744\n",
      "Epoch 48/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1770 - val_acc: 0.9744\n",
      "Epoch 49/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1770 - val_acc: 0.9744\n",
      "Epoch 50/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1772 - val_acc: 0.9744\n",
      "Epoch 51/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1775 - val_acc: 0.9744\n",
      "Epoch 52/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1666 - acc: 1.0000 - val_loss: 2.1771 - val_acc: 0.9744\n",
      "Epoch 53/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1773 - val_acc: 0.9744\n",
      "Epoch 54/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1773 - val_acc: 0.9744\n",
      "Epoch 55/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1770 - val_acc: 0.9744\n",
      "Epoch 56/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1773 - val_acc: 0.9744\n",
      "Epoch 57/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1773 - val_acc: 0.9744\n",
      "Epoch 58/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1771 - val_acc: 0.9744\n",
      "Epoch 59/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1775 - val_acc: 0.9744\n",
      "Epoch 60/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1776 - val_acc: 0.9744\n",
      "Epoch 61/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1771 - val_acc: 0.9744\n",
      "Epoch 62/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1773 - val_acc: 0.9744\n",
      "Epoch 63/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1772 - val_acc: 0.9744\n",
      "Epoch 64/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1771 - val_acc: 0.9744\n",
      "Epoch 65/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1772 - val_acc: 0.9744\n",
      "Epoch 66/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1771 - val_acc: 0.9744\n",
      "Epoch 67/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1772 - val_acc: 0.9744\n",
      "Epoch 68/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1769 - val_acc: 0.9744\n",
      "Epoch 69/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1772 - val_acc: 0.9744\n",
      "Epoch 70/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1770 - val_acc: 0.9744\n",
      "Epoch 71/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1771 - val_acc: 0.9744\n",
      "Epoch 72/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1767 - val_acc: 0.9744\n",
      "Epoch 73/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1771 - val_acc: 0.9744\n",
      "Epoch 74/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1769 - val_acc: 0.9744\n",
      "Epoch 75/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1771 - val_acc: 0.9744\n",
      "Epoch 76/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1769 - val_acc: 0.9744\n",
      "Epoch 77/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1771 - val_acc: 0.9744\n",
      "Epoch 78/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1770 - val_acc: 0.9744\n",
      "Epoch 79/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1773 - val_acc: 0.9744\n",
      "Epoch 80/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1770 - val_acc: 0.9744\n",
      "Epoch 81/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1770 - val_acc: 0.9744\n",
      "Epoch 82/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1770 - val_acc: 0.9744\n",
      "Epoch 83/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1770 - val_acc: 0.9744\n",
      "Epoch 84/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1770 - val_acc: 0.9744\n",
      "Epoch 85/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1772 - val_acc: 0.9744\n",
      "Epoch 86/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1770 - val_acc: 0.9744\n",
      "Epoch 87/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1770 - val_acc: 0.9744\n",
      "Epoch 88/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1770 - val_acc: 0.9744\n",
      "Epoch 89/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1772 - val_acc: 0.9744\n",
      "Epoch 90/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1772 - val_acc: 0.9744\n",
      "Epoch 91/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1770 - val_acc: 0.9744\n",
      "Epoch 92/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1770 - val_acc: 0.9744\n",
      "Epoch 93/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1769 - val_acc: 0.9744\n",
      "Epoch 94/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1770 - val_acc: 0.9744\n",
      "Epoch 95/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1770 - val_acc: 0.9744\n",
      "Epoch 96/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1772 - val_acc: 0.9744\n",
      "Epoch 97/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1771 - val_acc: 0.9744\n",
      "Epoch 98/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1770 - val_acc: 0.9744\n",
      "Epoch 99/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1771 - val_acc: 0.9744\n",
      "Epoch 100/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1770 - val_acc: 0.9744\n",
      "78/78 [==============================] - 0s 356us/step\n",
      "Score for fold 9: loss of 2.1770915801708517; acc of 97.43589697740018%\n",
      "Train on 702 samples, validate on 78 samples\n",
      "Epoch 1/100\n",
      "702/702 [==============================] - 18s 25ms/step - loss: 2.5145 - acc: 0.2450 - val_loss: 2.4498 - val_acc: 0.3077\n",
      "Epoch 2/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.3147 - acc: 0.5513 - val_loss: 2.3350 - val_acc: 0.4615\n",
      "Epoch 3/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2466 - acc: 0.7593 - val_loss: 2.2354 - val_acc: 0.7179\n",
      "Epoch 4/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2237 - acc: 0.7977 - val_loss: 2.2115 - val_acc: 0.8205\n",
      "Epoch 5/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2098 - acc: 0.8447 - val_loss: 2.2013 - val_acc: 0.9231\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1926 - acc: 0.9160 - val_loss: 2.1931 - val_acc: 0.8974\n",
      "Epoch 7/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1923 - acc: 0.9145 - val_loss: 2.2046 - val_acc: 0.9231\n",
      "Epoch 8/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1840 - acc: 0.9459 - val_loss: 2.2123 - val_acc: 0.8462\n",
      "Epoch 9/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1824 - acc: 0.9558 - val_loss: 2.1855 - val_acc: 0.9487\n",
      "Epoch 10/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1806 - acc: 0.9573 - val_loss: 2.1844 - val_acc: 0.9359\n",
      "Epoch 11/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1768 - acc: 0.9715 - val_loss: 2.1817 - val_acc: 0.9359\n",
      "Epoch 12/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1750 - acc: 0.9786 - val_loss: 2.1823 - val_acc: 0.9103\n",
      "Epoch 13/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1752 - acc: 0.9815 - val_loss: 2.1800 - val_acc: 0.8974\n",
      "Epoch 14/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1771 - acc: 0.9715 - val_loss: 2.1762 - val_acc: 0.9487\n",
      "Epoch 15/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1713 - acc: 0.9900 - val_loss: 2.1754 - val_acc: 0.9487\n",
      "Epoch 16/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1733 - acc: 0.9858 - val_loss: 2.1758 - val_acc: 0.9744\n",
      "Epoch 17/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1703 - acc: 0.9886 - val_loss: 2.1751 - val_acc: 0.9744\n",
      "Epoch 18/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1716 - acc: 0.9929 - val_loss: 2.1797 - val_acc: 0.9615\n",
      "Epoch 19/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1708 - acc: 0.9929 - val_loss: 2.1832 - val_acc: 0.9487\n",
      "Epoch 20/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1744 - acc: 0.9758 - val_loss: 2.1828 - val_acc: 0.9359\n",
      "Epoch 21/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1716 - acc: 0.9915 - val_loss: 2.1737 - val_acc: 0.9872\n",
      "Epoch 22/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1682 - acc: 0.9986 - val_loss: 2.1874 - val_acc: 0.9231\n",
      "Epoch 23/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1690 - acc: 0.9929 - val_loss: 2.1774 - val_acc: 0.9872\n",
      "Epoch 24/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1720 - acc: 0.9815 - val_loss: 2.1726 - val_acc: 0.9615\n",
      "Epoch 25/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1681 - acc: 0.9986 - val_loss: 2.1759 - val_acc: 0.9615\n",
      "Epoch 26/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1687 - acc: 0.9943 - val_loss: 2.1906 - val_acc: 0.9231\n",
      "Epoch 27/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1686 - acc: 0.9929 - val_loss: 2.1836 - val_acc: 0.9487\n",
      "Epoch 28/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1676 - acc: 0.9986 - val_loss: 2.1764 - val_acc: 0.9615\n",
      "Epoch 29/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1694 - acc: 0.9900 - val_loss: 2.1764 - val_acc: 0.9615\n",
      "Epoch 30/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1694 - acc: 0.9986 - val_loss: 2.1873 - val_acc: 0.9231\n",
      "Epoch 31/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1677 - acc: 0.9986 - val_loss: 2.1752 - val_acc: 0.9615\n",
      "Epoch 32/100\n",
      "702/702 [==============================] - 6s 9ms/step - loss: 2.1671 - acc: 1.0000 - val_loss: 2.1806 - val_acc: 0.9487\n",
      "Epoch 33/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1669 - acc: 0.9986 - val_loss: 2.1731 - val_acc: 0.9744\n",
      "Epoch 34/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1668 - acc: 1.0000 - val_loss: 2.1747 - val_acc: 0.9487\n",
      "Epoch 35/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1680 - acc: 0.9972 - val_loss: 2.1714 - val_acc: 0.9615\n",
      "Epoch 36/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1667 - acc: 1.0000 - val_loss: 2.1724 - val_acc: 0.9615\n",
      "Epoch 37/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1667 - acc: 0.9972 - val_loss: 2.1724 - val_acc: 0.9744\n",
      "Epoch 38/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1669 - acc: 1.0000 - val_loss: 2.1762 - val_acc: 0.9359\n",
      "Epoch 39/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1672 - acc: 0.9986 - val_loss: 2.1738 - val_acc: 0.9615\n",
      "Epoch 40/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1666 - acc: 1.0000 - val_loss: 2.1711 - val_acc: 0.9744\n",
      "Epoch 41/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1668 - acc: 1.0000 - val_loss: 2.1722 - val_acc: 0.9615\n",
      "Epoch 42/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1671 - acc: 0.9986 - val_loss: 2.1719 - val_acc: 0.9744\n",
      "Epoch 43/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1721 - val_acc: 0.9615\n",
      "Epoch 44/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1709 - val_acc: 0.9744\n",
      "Epoch 45/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1702 - val_acc: 0.9872\n",
      "Epoch 46/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1666 - acc: 1.0000 - val_loss: 2.1694 - val_acc: 0.9872\n",
      "Epoch 47/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1666 - acc: 0.9972 - val_loss: 2.1705 - val_acc: 0.9744\n",
      "Epoch 48/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1701 - val_acc: 0.9872\n",
      "Epoch 49/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1717 - val_acc: 0.9615\n",
      "Epoch 50/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1719 - val_acc: 0.9487\n",
      "Epoch 51/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1665 - acc: 0.9972 - val_loss: 2.1697 - val_acc: 0.9872\n",
      "Epoch 52/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1729 - val_acc: 0.9615\n",
      "Epoch 53/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1699 - val_acc: 0.9744\n",
      "Epoch 54/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1702 - val_acc: 0.9744\n",
      "Epoch 55/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1719 - val_acc: 0.9744\n",
      "Epoch 56/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1703 - val_acc: 0.9744\n",
      "Epoch 57/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1718 - val_acc: 0.9744\n",
      "Epoch 58/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1691 - val_acc: 0.9872\n",
      "Epoch 59/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1697 - val_acc: 0.9744\n",
      "Epoch 60/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1694 - val_acc: 0.9872\n",
      "Epoch 61/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1701 - val_acc: 0.9744\n",
      "Epoch 62/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1687 - val_acc: 1.0000\n",
      "Epoch 63/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1709 - val_acc: 0.9615\n",
      "Epoch 64/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1701 - val_acc: 0.9744\n",
      "Epoch 65/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1701 - val_acc: 0.9615\n",
      "Epoch 66/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1700 - val_acc: 0.9744\n",
      "Epoch 67/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1695 - val_acc: 0.9872\n",
      "Epoch 68/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1697 - val_acc: 0.9872\n",
      "Epoch 69/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1687 - val_acc: 0.9872\n",
      "Epoch 70/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1696 - val_acc: 1.0000\n",
      "Epoch 71/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1696 - val_acc: 0.9872\n",
      "Epoch 72/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1691 - val_acc: 1.0000\n",
      "Epoch 73/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1681 - val_acc: 1.0000\n",
      "Epoch 74/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1698 - val_acc: 0.9615\n",
      "Epoch 75/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1688 - val_acc: 1.0000\n",
      "Epoch 76/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1694 - val_acc: 0.9872\n",
      "Epoch 77/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1691 - val_acc: 0.9872\n",
      "Epoch 78/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1707 - val_acc: 0.9872\n",
      "Epoch 79/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1700 - val_acc: 0.9744\n",
      "Epoch 80/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1693 - val_acc: 0.9744\n",
      "Epoch 81/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1696 - val_acc: 1.0000\n",
      "Epoch 82/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1695 - val_acc: 0.9872\n",
      "Epoch 83/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1697 - val_acc: 0.9872\n",
      "Epoch 84/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1696 - val_acc: 0.9744\n",
      "Epoch 85/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1688 - val_acc: 1.0000\n",
      "Epoch 86/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1700 - val_acc: 0.9744\n",
      "Epoch 87/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1695 - val_acc: 0.9744\n",
      "Epoch 88/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1689 - val_acc: 0.9872\n",
      "Epoch 89/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1686 - val_acc: 1.0000\n",
      "Epoch 90/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1692 - val_acc: 0.9744\n",
      "Epoch 91/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1690 - val_acc: 0.9872\n",
      "Epoch 92/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1686 - val_acc: 1.0000\n",
      "Epoch 93/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1688 - val_acc: 1.0000\n",
      "Epoch 94/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1696 - val_acc: 0.9744\n",
      "Epoch 95/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1694 - val_acc: 0.9872\n",
      "Epoch 96/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1699 - val_acc: 0.9744\n",
      "Epoch 97/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1689 - val_acc: 0.9872\n",
      "Epoch 98/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1691 - val_acc: 0.9744\n",
      "Epoch 99/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1692 - val_acc: 0.9744\n",
      "Epoch 100/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1700 - val_acc: 0.9744\n",
      "78/78 [==============================] - 0s 391us/step\n",
      "Score for fold 10: loss of 2.170092136431963; acc of 97.43589743589743%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1296x1296 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m 7       \u001b[0m | \u001b[0m 97.69   \u001b[0m | \u001b[0m 0.9148  \u001b[0m | \u001b[0m 9.402   \u001b[0m |\n",
      "Train on 702 samples, validate on 78 samples\n",
      "Epoch 1/100\n",
      "702/702 [==============================] - 18s 25ms/step - loss: 2.4868 - acc: 0.2821 - val_loss: 2.4245 - val_acc: 0.3718\n",
      "Epoch 2/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.3068 - acc: 0.5541 - val_loss: 2.3164 - val_acc: 0.5385\n",
      "Epoch 3/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2453 - acc: 0.7322 - val_loss: 2.2411 - val_acc: 0.7308\n",
      "Epoch 4/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2226 - acc: 0.8205 - val_loss: 2.2677 - val_acc: 0.6538\n",
      "Epoch 5/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2168 - acc: 0.8419 - val_loss: 2.2061 - val_acc: 0.8718\n",
      "Epoch 6/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1891 - acc: 0.9245 - val_loss: 2.3131 - val_acc: 0.6026\n",
      "Epoch 7/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1983 - acc: 0.8832 - val_loss: 2.1962 - val_acc: 0.9359\n",
      "Epoch 8/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1849 - acc: 0.9459 - val_loss: 2.2046 - val_acc: 0.8590\n",
      "Epoch 9/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1809 - acc: 0.9558 - val_loss: 2.2177 - val_acc: 0.7692\n",
      "Epoch 10/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1789 - acc: 0.9615 - val_loss: 2.1925 - val_acc: 0.8590\n",
      "Epoch 11/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1787 - acc: 0.9672 - val_loss: 2.1900 - val_acc: 0.9103\n",
      "Epoch 12/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1754 - acc: 0.9829 - val_loss: 2.1894 - val_acc: 0.9615\n",
      "Epoch 13/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1767 - acc: 0.9601 - val_loss: 2.2265 - val_acc: 0.8333\n",
      "Epoch 14/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1780 - acc: 0.9758 - val_loss: 2.2048 - val_acc: 0.8974\n",
      "Epoch 15/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1706 - acc: 0.9915 - val_loss: 2.1880 - val_acc: 0.8974\n",
      "Epoch 16/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1738 - acc: 0.9858 - val_loss: 2.1850 - val_acc: 0.9487\n",
      "Epoch 17/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1698 - acc: 0.9929 - val_loss: 2.1884 - val_acc: 0.8846\n",
      "Epoch 18/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1748 - acc: 0.9815 - val_loss: 2.1802 - val_acc: 0.9615\n",
      "Epoch 19/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1697 - acc: 0.9929 - val_loss: 2.2076 - val_acc: 0.8718\n",
      "Epoch 20/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1710 - acc: 0.9843 - val_loss: 2.1809 - val_acc: 0.9359\n",
      "Epoch 21/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1705 - acc: 0.9858 - val_loss: 2.1804 - val_acc: 0.9359\n",
      "Epoch 22/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1700 - acc: 0.9929 - val_loss: 2.1795 - val_acc: 0.9615\n",
      "Epoch 23/100\n",
      "702/702 [==============================] - 6s 9ms/step - loss: 2.1691 - acc: 0.9915 - val_loss: 2.1856 - val_acc: 0.9231\n",
      "Epoch 24/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1685 - acc: 0.9957 - val_loss: 2.1778 - val_acc: 0.9487\n",
      "Epoch 25/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1676 - acc: 0.9986 - val_loss: 2.1777 - val_acc: 0.9487\n",
      "Epoch 26/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1675 - acc: 0.9972 - val_loss: 2.1826 - val_acc: 0.9231\n",
      "Epoch 27/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1683 - acc: 0.9957 - val_loss: 2.1756 - val_acc: 0.9359\n",
      "Epoch 28/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1669 - acc: 1.0000 - val_loss: 2.1761 - val_acc: 0.9487\n",
      "Epoch 29/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1678 - acc: 0.9972 - val_loss: 2.1817 - val_acc: 0.9487\n",
      "Epoch 30/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1671 - acc: 1.0000 - val_loss: 2.1770 - val_acc: 0.9615\n",
      "Epoch 31/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1668 - acc: 1.0000 - val_loss: 2.1742 - val_acc: 0.9487\n",
      "Epoch 32/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1672 - acc: 0.9986 - val_loss: 2.1797 - val_acc: 0.9359\n",
      "Epoch 33/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1668 - acc: 1.0000 - val_loss: 2.1802 - val_acc: 0.9103\n",
      "Epoch 34/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1668 - acc: 0.9986 - val_loss: 2.1745 - val_acc: 0.9487\n",
      "Epoch 35/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1666 - acc: 1.0000 - val_loss: 2.1747 - val_acc: 0.9615\n",
      "Epoch 36/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1743 - val_acc: 0.9615\n",
      "Epoch 37/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1666 - acc: 1.0000 - val_loss: 2.1755 - val_acc: 0.9615\n",
      "Epoch 38/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1666 - acc: 1.0000 - val_loss: 2.1806 - val_acc: 0.9487\n",
      "Epoch 39/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1666 - acc: 1.0000 - val_loss: 2.1748 - val_acc: 0.9744\n",
      "Epoch 40/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1735 - val_acc: 0.9615\n",
      "Epoch 41/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1769 - val_acc: 0.9487\n",
      "Epoch 42/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1666 - acc: 0.9972 - val_loss: 2.1742 - val_acc: 0.9744\n",
      "Epoch 43/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1741 - val_acc: 0.9744\n",
      "Epoch 44/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1745 - val_acc: 0.9615\n",
      "Epoch 45/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1674 - acc: 0.9986 - val_loss: 2.1741 - val_acc: 0.9744\n",
      "Epoch 46/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1751 - val_acc: 0.9615\n",
      "Epoch 47/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1735 - val_acc: 0.9615\n",
      "Epoch 48/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1740 - val_acc: 0.9615\n",
      "Epoch 49/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1666 - acc: 1.0000 - val_loss: 2.1754 - val_acc: 0.9615\n",
      "Epoch 50/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1741 - val_acc: 0.9615\n",
      "Epoch 51/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1735 - val_acc: 0.9615\n",
      "Epoch 52/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1745 - val_acc: 0.9615\n",
      "Epoch 53/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1734 - val_acc: 0.9487\n",
      "Epoch 54/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1735 - val_acc: 0.9615\n",
      "Epoch 55/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1740 - val_acc: 0.9615\n",
      "Epoch 56/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1768 - val_acc: 0.9615\n",
      "Epoch 57/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1746 - val_acc: 0.9615\n",
      "Epoch 58/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1748 - val_acc: 0.9615\n",
      "Epoch 59/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1728 - val_acc: 0.9615\n",
      "Epoch 60/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1741 - val_acc: 0.9615\n",
      "Epoch 61/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1731 - val_acc: 0.9615\n",
      "Epoch 62/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1731 - val_acc: 0.9615\n",
      "Epoch 63/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1739 - val_acc: 0.9615\n",
      "Epoch 64/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1727 - val_acc: 0.9615\n",
      "Epoch 65/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1730 - val_acc: 0.9615\n",
      "Epoch 66/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1734 - val_acc: 0.9615\n",
      "Epoch 67/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1731 - val_acc: 0.9615\n",
      "Epoch 68/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1739 - val_acc: 0.9615\n",
      "Epoch 69/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1740 - val_acc: 0.9615\n",
      "Epoch 70/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1763 - val_acc: 0.9615\n",
      "Epoch 71/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1730 - val_acc: 0.9615\n",
      "Epoch 72/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1737 - val_acc: 0.9615\n",
      "Epoch 73/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1740 - val_acc: 0.9615\n",
      "Epoch 74/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1739 - val_acc: 0.9615\n",
      "Epoch 75/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1737 - val_acc: 0.9615\n",
      "Epoch 76/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1736 - val_acc: 0.9615\n",
      "Epoch 77/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1734 - val_acc: 0.9615\n",
      "Epoch 78/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1733 - val_acc: 0.9615\n",
      "Epoch 79/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1730 - val_acc: 0.9615\n",
      "Epoch 80/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1737 - val_acc: 0.9615\n",
      "Epoch 81/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1732 - val_acc: 0.9615\n",
      "Epoch 82/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1736 - val_acc: 0.9615\n",
      "Epoch 83/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1730 - val_acc: 0.9615\n",
      "Epoch 84/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1732 - val_acc: 0.9615\n",
      "Epoch 85/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1736 - val_acc: 0.9615\n",
      "Epoch 86/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1733 - val_acc: 0.9615\n",
      "Epoch 87/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1733 - val_acc: 0.9615\n",
      "Epoch 88/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1740 - val_acc: 0.9615\n",
      "Epoch 89/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1735 - val_acc: 0.9615\n",
      "Epoch 90/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1735 - val_acc: 0.9615\n",
      "Epoch 91/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1732 - val_acc: 0.9615\n",
      "Epoch 92/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1733 - val_acc: 0.9615\n",
      "Epoch 93/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1734 - val_acc: 0.9615\n",
      "Epoch 94/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1732 - val_acc: 0.9615\n",
      "Epoch 95/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1734 - val_acc: 0.9615\n",
      "Epoch 96/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1728 - val_acc: 0.9615\n",
      "Epoch 97/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1727 - val_acc: 0.9744\n",
      "Epoch 98/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1731 - val_acc: 0.9615\n",
      "Epoch 99/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1734 - val_acc: 0.9615\n",
      "Epoch 100/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1730 - val_acc: 0.9615\n",
      "78/78 [==============================] - 0s 360us/step\n",
      "Score for fold 1: loss of 2.173419897372906; acc of 96.15384615384616%\n",
      "Train on 702 samples, validate on 78 samples\n",
      "Epoch 1/100\n",
      "702/702 [==============================] - 17s 25ms/step - loss: 2.4908 - acc: 0.2593 - val_loss: 2.3717 - val_acc: 0.3718\n",
      "Epoch 2/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.3178 - acc: 0.5228 - val_loss: 2.3756 - val_acc: 0.4872\n",
      "Epoch 3/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2463 - acc: 0.7635 - val_loss: 2.2406 - val_acc: 0.8077\n",
      "Epoch 4/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2296 - acc: 0.8177 - val_loss: 2.2351 - val_acc: 0.7179\n",
      "Epoch 5/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2077 - acc: 0.8746 - val_loss: 2.2636 - val_acc: 0.6154\n",
      "Epoch 6/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2023 - acc: 0.8761 - val_loss: 2.2000 - val_acc: 0.8590\n",
      "Epoch 7/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1844 - acc: 0.9373 - val_loss: 2.2306 - val_acc: 0.7179\n",
      "Epoch 8/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1912 - acc: 0.9245 - val_loss: 2.2270 - val_acc: 0.8077\n",
      "Epoch 9/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1808 - acc: 0.9516 - val_loss: 2.4142 - val_acc: 0.7179\n",
      "Epoch 10/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1893 - acc: 0.9530 - val_loss: 2.2294 - val_acc: 0.8462\n",
      "Epoch 11/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1814 - acc: 0.9501 - val_loss: 2.1939 - val_acc: 0.8974\n",
      "Epoch 12/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1774 - acc: 0.9758 - val_loss: 2.1886 - val_acc: 0.9359\n",
      "Epoch 13/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1762 - acc: 0.9715 - val_loss: 2.1883 - val_acc: 0.9103\n",
      "Epoch 14/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1796 - acc: 0.9758 - val_loss: 2.1877 - val_acc: 0.9103\n",
      "Epoch 15/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1721 - acc: 0.9829 - val_loss: 2.1926 - val_acc: 0.8846\n",
      "Epoch 16/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1733 - acc: 0.9872 - val_loss: 2.1807 - val_acc: 0.9103\n",
      "Epoch 17/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1726 - acc: 0.9843 - val_loss: 2.1789 - val_acc: 0.9103\n",
      "Epoch 18/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1712 - acc: 0.9943 - val_loss: 2.1854 - val_acc: 0.8718\n",
      "Epoch 19/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1711 - acc: 0.9858 - val_loss: 2.1753 - val_acc: 0.9487\n",
      "Epoch 20/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1764 - acc: 0.9801 - val_loss: 2.1807 - val_acc: 0.9487\n",
      "Epoch 21/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1699 - acc: 0.9929 - val_loss: 2.1741 - val_acc: 0.9744\n",
      "Epoch 22/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1720 - acc: 0.9886 - val_loss: 2.1750 - val_acc: 0.9487\n",
      "Epoch 23/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1685 - acc: 0.9972 - val_loss: 2.2218 - val_acc: 0.8974\n",
      "Epoch 24/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1731 - acc: 0.9872 - val_loss: 2.1835 - val_acc: 0.9359\n",
      "Epoch 25/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1695 - acc: 0.9943 - val_loss: 2.1783 - val_acc: 0.8846\n",
      "Epoch 26/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1686 - acc: 0.9972 - val_loss: 2.1773 - val_acc: 0.9487\n",
      "Epoch 27/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1707 - acc: 0.9858 - val_loss: 2.1721 - val_acc: 0.9744\n",
      "Epoch 28/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1685 - acc: 0.9957 - val_loss: 2.1758 - val_acc: 0.8974\n",
      "Epoch 29/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1679 - acc: 0.9986 - val_loss: 2.1751 - val_acc: 0.9103\n",
      "Epoch 30/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1696 - acc: 0.9929 - val_loss: 2.1731 - val_acc: 0.9744\n",
      "Epoch 31/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1713 - acc: 0.9886 - val_loss: 2.1737 - val_acc: 0.9487\n",
      "Epoch 32/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1733 - acc: 0.9915 - val_loss: 2.1731 - val_acc: 0.9872\n",
      "Epoch 33/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1674 - acc: 0.9986 - val_loss: 2.1725 - val_acc: 0.9872\n",
      "Epoch 34/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1676 - acc: 0.9986 - val_loss: 2.1884 - val_acc: 0.9359\n",
      "Epoch 35/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1692 - acc: 0.9957 - val_loss: 2.1717 - val_acc: 0.9615\n",
      "Epoch 36/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1679 - acc: 0.9929 - val_loss: 2.1868 - val_acc: 0.9487\n",
      "Epoch 37/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1677 - acc: 0.9986 - val_loss: 2.1733 - val_acc: 0.9872\n",
      "Epoch 38/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1670 - acc: 1.0000 - val_loss: 2.1702 - val_acc: 1.0000\n",
      "Epoch 39/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1700 - acc: 0.9929 - val_loss: 2.1736 - val_acc: 0.9872\n",
      "Epoch 40/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1670 - acc: 1.0000 - val_loss: 2.1699 - val_acc: 1.0000\n",
      "Epoch 41/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1673 - acc: 0.9943 - val_loss: 2.1749 - val_acc: 0.9872\n",
      "Epoch 42/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1691 - acc: 0.9957 - val_loss: 2.1708 - val_acc: 0.9872\n",
      "Epoch 43/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1669 - acc: 1.0000 - val_loss: 2.1705 - val_acc: 0.9872\n",
      "Epoch 44/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1669 - acc: 1.0000 - val_loss: 2.1754 - val_acc: 0.9615\n",
      "Epoch 45/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1669 - acc: 1.0000 - val_loss: 2.1709 - val_acc: 0.9744\n",
      "Epoch 46/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1682 - acc: 0.9972 - val_loss: 2.1705 - val_acc: 1.0000\n",
      "Epoch 47/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1673 - acc: 0.9986 - val_loss: 2.1711 - val_acc: 0.9872\n",
      "Epoch 48/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1666 - acc: 1.0000 - val_loss: 2.1697 - val_acc: 1.0000\n",
      "Epoch 49/100\n",
      "702/702 [==============================] - 6s 9ms/step - loss: 2.1672 - acc: 0.9986 - val_loss: 2.1707 - val_acc: 0.9615\n",
      "Epoch 50/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1667 - acc: 1.0000 - val_loss: 2.1718 - val_acc: 0.9872\n",
      "Epoch 51/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1698 - val_acc: 0.9872\n",
      "Epoch 52/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1667 - acc: 0.9986 - val_loss: 2.1702 - val_acc: 1.0000\n",
      "Epoch 53/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1671 - acc: 0.9986 - val_loss: 2.1713 - val_acc: 0.9744\n",
      "Epoch 54/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1666 - acc: 1.0000 - val_loss: 2.1682 - val_acc: 1.0000\n",
      "Epoch 55/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1701 - val_acc: 0.9872\n",
      "Epoch 56/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1700 - val_acc: 0.9872\n",
      "Epoch 57/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1666 - acc: 1.0000 - val_loss: 2.1906 - val_acc: 0.9103\n",
      "Epoch 58/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1707 - val_acc: 1.0000\n",
      "Epoch 59/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1702 - val_acc: 0.9872\n",
      "Epoch 60/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1691 - val_acc: 1.0000\n",
      "Epoch 61/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1696 - val_acc: 0.9872\n",
      "Epoch 62/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1695 - val_acc: 0.9872\n",
      "Epoch 63/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1688 - val_acc: 1.0000\n",
      "Epoch 64/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1688 - val_acc: 0.9872\n",
      "Epoch 65/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1696 - val_acc: 1.0000\n",
      "Epoch 66/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1698 - val_acc: 0.9872\n",
      "Epoch 67/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1688 - val_acc: 1.0000\n",
      "Epoch 68/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1726 - val_acc: 0.9872\n",
      "Epoch 69/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1667 - acc: 0.9986 - val_loss: 2.1690 - val_acc: 1.0000\n",
      "Epoch 70/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1687 - val_acc: 1.0000\n",
      "Epoch 71/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1687 - val_acc: 1.0000\n",
      "Epoch 72/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1684 - val_acc: 1.0000\n",
      "Epoch 73/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1683 - val_acc: 0.9872\n",
      "Epoch 74/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1680 - val_acc: 1.0000\n",
      "Epoch 75/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1683 - val_acc: 1.0000\n",
      "Epoch 76/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1686 - val_acc: 1.0000\n",
      "Epoch 77/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1685 - val_acc: 1.0000\n",
      "Epoch 78/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1684 - val_acc: 1.0000\n",
      "Epoch 79/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1685 - val_acc: 1.0000\n",
      "Epoch 80/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1682 - val_acc: 1.0000\n",
      "Epoch 81/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1684 - val_acc: 1.0000\n",
      "Epoch 82/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1683 - val_acc: 1.0000\n",
      "Epoch 83/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1683 - val_acc: 1.0000\n",
      "Epoch 84/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1681 - val_acc: 1.0000\n",
      "Epoch 85/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1682 - val_acc: 1.0000\n",
      "Epoch 86/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1683 - val_acc: 1.0000\n",
      "Epoch 87/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1681 - val_acc: 1.0000\n",
      "Epoch 88/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1683 - val_acc: 1.0000\n",
      "Epoch 89/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1684 - val_acc: 1.0000\n",
      "Epoch 90/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1683 - val_acc: 1.0000\n",
      "Epoch 91/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1682 - val_acc: 1.0000\n",
      "Epoch 92/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1683 - val_acc: 1.0000\n",
      "Epoch 93/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1683 - val_acc: 1.0000\n",
      "Epoch 94/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1688 - val_acc: 0.9872\n",
      "Epoch 95/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1683 - val_acc: 1.0000\n",
      "Epoch 96/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1683 - val_acc: 1.0000\n",
      "Epoch 97/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1683 - val_acc: 1.0000\n",
      "Epoch 98/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1678 - val_acc: 1.0000\n",
      "Epoch 99/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1682 - val_acc: 1.0000\n",
      "Epoch 100/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1683 - val_acc: 1.0000\n",
      "78/78 [==============================] - 0s 379us/step\n",
      "Score for fold 2: loss of 2.168365723047501; acc of 100.0%\n",
      "Train on 702 samples, validate on 78 samples\n",
      "Epoch 1/100\n",
      "702/702 [==============================] - 18s 25ms/step - loss: 2.4813 - acc: 0.2721 - val_loss: 2.3631 - val_acc: 0.6282\n",
      "Epoch 2/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2975 - acc: 0.5969 - val_loss: 2.2640 - val_acc: 0.7821\n",
      "Epoch 3/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2375 - acc: 0.7493 - val_loss: 2.2165 - val_acc: 0.8718\n",
      "Epoch 4/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2107 - acc: 0.8533 - val_loss: 2.2156 - val_acc: 0.7436\n",
      "Epoch 5/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2039 - acc: 0.8846 - val_loss: 2.2204 - val_acc: 0.7821\n",
      "Epoch 6/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1985 - acc: 0.9031 - val_loss: 2.1944 - val_acc: 0.9359\n",
      "Epoch 7/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1877 - acc: 0.9501 - val_loss: 2.2028 - val_acc: 0.8333\n",
      "Epoch 8/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1833 - acc: 0.9516 - val_loss: 2.2162 - val_acc: 0.7949\n",
      "Epoch 9/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1812 - acc: 0.9501 - val_loss: 2.1889 - val_acc: 0.8974\n",
      "Epoch 10/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1806 - acc: 0.9587 - val_loss: 2.1843 - val_acc: 0.9231\n",
      "Epoch 11/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1770 - acc: 0.9615 - val_loss: 2.2122 - val_acc: 0.8333\n",
      "Epoch 12/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1792 - acc: 0.9729 - val_loss: 2.1783 - val_acc: 0.9359\n",
      "Epoch 13/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1765 - acc: 0.9786 - val_loss: 2.1949 - val_acc: 0.9744\n",
      "Epoch 14/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1724 - acc: 0.9843 - val_loss: 2.1821 - val_acc: 0.9744\n",
      "Epoch 15/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1724 - acc: 0.9872 - val_loss: 2.2226 - val_acc: 0.8077\n",
      "Epoch 16/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1746 - acc: 0.9744 - val_loss: 2.1797 - val_acc: 0.9744\n",
      "Epoch 17/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1704 - acc: 0.9900 - val_loss: 2.1892 - val_acc: 0.9359\n",
      "Epoch 18/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1737 - acc: 0.9829 - val_loss: 2.1804 - val_acc: 0.9615\n",
      "Epoch 19/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1700 - acc: 0.9929 - val_loss: 2.1806 - val_acc: 0.9487\n",
      "Epoch 20/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1695 - acc: 0.9943 - val_loss: 2.1910 - val_acc: 0.9487\n",
      "Epoch 21/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1725 - acc: 0.9886 - val_loss: 2.1783 - val_acc: 0.9744\n",
      "Epoch 22/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1684 - acc: 0.9986 - val_loss: 2.1913 - val_acc: 0.9615\n",
      "Epoch 23/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1701 - acc: 0.9929 - val_loss: 2.1735 - val_acc: 0.9744\n",
      "Epoch 24/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1698 - acc: 0.9915 - val_loss: 2.1729 - val_acc: 0.9872\n",
      "Epoch 25/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1699 - acc: 0.9929 - val_loss: 2.1794 - val_acc: 0.9744\n",
      "Epoch 26/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1691 - acc: 0.9943 - val_loss: 2.1881 - val_acc: 0.9744\n",
      "Epoch 27/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1684 - acc: 0.9986 - val_loss: 2.1725 - val_acc: 1.0000\n",
      "Epoch 28/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1697 - acc: 0.9915 - val_loss: 2.1765 - val_acc: 0.9487\n",
      "Epoch 29/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1679 - acc: 0.9986 - val_loss: 2.1724 - val_acc: 0.9872\n",
      "Epoch 30/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1676 - acc: 0.9986 - val_loss: 2.1745 - val_acc: 0.9872\n",
      "Epoch 31/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1678 - acc: 0.9957 - val_loss: 2.2111 - val_acc: 0.9615\n",
      "Epoch 32/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1701 - acc: 0.9943 - val_loss: 2.1704 - val_acc: 1.0000\n",
      "Epoch 33/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1671 - acc: 1.0000 - val_loss: 2.1713 - val_acc: 1.0000\n",
      "Epoch 34/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1732 - acc: 0.9929 - val_loss: 2.1728 - val_acc: 0.9872\n",
      "Epoch 35/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1668 - acc: 1.0000 - val_loss: 2.1768 - val_acc: 0.9744\n",
      "Epoch 36/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1669 - acc: 1.0000 - val_loss: 2.1725 - val_acc: 0.9872\n",
      "Epoch 37/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1678 - acc: 0.9957 - val_loss: 2.1803 - val_acc: 0.9359\n",
      "Epoch 38/100\n",
      "702/702 [==============================] - 6s 9ms/step - loss: 2.1672 - acc: 0.9986 - val_loss: 2.1725 - val_acc: 1.0000\n",
      "Epoch 39/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1667 - acc: 1.0000 - val_loss: 2.1707 - val_acc: 1.0000\n",
      "Epoch 40/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1666 - acc: 1.0000 - val_loss: 2.1722 - val_acc: 0.9744\n",
      "Epoch 41/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1669 - acc: 1.0000 - val_loss: 2.1726 - val_acc: 0.9872\n",
      "Epoch 42/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1712 - val_acc: 1.0000\n",
      "Epoch 43/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1668 - acc: 0.9986 - val_loss: 2.1782 - val_acc: 0.9872\n",
      "Epoch 44/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1666 - acc: 0.9986 - val_loss: 2.1693 - val_acc: 1.0000\n",
      "Epoch 45/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1735 - val_acc: 1.0000\n",
      "Epoch 46/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1743 - val_acc: 0.9615\n",
      "Epoch 47/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1666 - acc: 0.9986 - val_loss: 2.1703 - val_acc: 1.0000\n",
      "Epoch 48/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1678 - acc: 0.9957 - val_loss: 2.1685 - val_acc: 1.0000\n",
      "Epoch 49/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1698 - val_acc: 1.0000\n",
      "Epoch 50/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1699 - val_acc: 0.9744\n",
      "Epoch 51/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1699 - val_acc: 0.9872\n",
      "Epoch 52/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1719 - val_acc: 1.0000\n",
      "Epoch 53/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1698 - val_acc: 1.0000\n",
      "Epoch 54/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1684 - val_acc: 1.0000\n",
      "Epoch 55/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1690 - val_acc: 1.0000\n",
      "Epoch 56/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1710 - val_acc: 1.0000\n",
      "Epoch 57/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1703 - val_acc: 1.0000\n",
      "Epoch 58/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1687 - val_acc: 1.0000\n",
      "Epoch 59/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1692 - val_acc: 1.0000\n",
      "Epoch 60/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1687 - val_acc: 1.0000\n",
      "Epoch 61/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1695 - val_acc: 1.0000\n",
      "Epoch 62/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1711 - val_acc: 0.9872\n",
      "Epoch 63/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1695 - val_acc: 1.0000\n",
      "Epoch 64/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1688 - val_acc: 1.0000\n",
      "Epoch 65/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1698 - val_acc: 0.9872\n",
      "Epoch 66/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1703 - val_acc: 1.0000\n",
      "Epoch 67/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1694 - val_acc: 1.0000\n",
      "Epoch 68/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1689 - val_acc: 1.0000\n",
      "Epoch 69/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1693 - val_acc: 1.0000\n",
      "Epoch 70/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1692 - val_acc: 0.9872\n",
      "Epoch 71/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1692 - val_acc: 1.0000\n",
      "Epoch 72/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1691 - val_acc: 0.9872\n",
      "Epoch 73/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1690 - val_acc: 1.0000\n",
      "Epoch 74/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1688 - val_acc: 1.0000\n",
      "Epoch 75/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1691 - val_acc: 1.0000\n",
      "Epoch 76/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1696 - val_acc: 1.0000\n",
      "Epoch 77/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1688 - val_acc: 0.9872\n",
      "Epoch 78/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1691 - val_acc: 1.0000\n",
      "Epoch 79/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1690 - val_acc: 1.0000\n",
      "Epoch 80/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1692 - val_acc: 1.0000\n",
      "Epoch 81/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1692 - val_acc: 0.9872\n",
      "Epoch 82/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1698 - val_acc: 0.9872\n",
      "Epoch 83/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1693 - val_acc: 0.9872\n",
      "Epoch 84/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1693 - val_acc: 0.9872\n",
      "Epoch 85/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1692 - val_acc: 0.9872\n",
      "Epoch 86/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1697 - val_acc: 0.9872\n",
      "Epoch 87/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1697 - val_acc: 0.9872\n",
      "Epoch 88/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1695 - val_acc: 0.9872\n",
      "Epoch 89/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1696 - val_acc: 0.9872\n",
      "Epoch 90/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1695 - val_acc: 0.9872\n",
      "Epoch 91/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1693 - val_acc: 0.9872\n",
      "Epoch 92/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1693 - val_acc: 0.9872\n",
      "Epoch 93/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1696 - val_acc: 0.9872\n",
      "Epoch 94/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1694 - val_acc: 0.9872\n",
      "Epoch 95/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1693 - val_acc: 0.9872\n",
      "Epoch 96/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1696 - val_acc: 0.9872\n",
      "Epoch 97/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1694 - val_acc: 0.9872\n",
      "Epoch 98/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1695 - val_acc: 0.9872\n",
      "Epoch 99/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1693 - val_acc: 0.9872\n",
      "Epoch 100/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1694 - val_acc: 0.9872\n",
      "78/78 [==============================] - 0s 386us/step\n",
      "Score for fold 3: loss of 2.1692805840418887; acc of 98.71794871794873%\n",
      "Train on 702 samples, validate on 78 samples\n",
      "Epoch 1/100\n",
      "702/702 [==============================] - 18s 26ms/step - loss: 2.4765 - acc: 0.2934 - val_loss: 2.3513 - val_acc: 0.4231\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.3058 - acc: 0.5798 - val_loss: 2.3168 - val_acc: 0.5256\n",
      "Epoch 3/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2592 - acc: 0.6966 - val_loss: 2.2214 - val_acc: 0.7436\n",
      "Epoch 4/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2178 - acc: 0.8419 - val_loss: 2.2061 - val_acc: 0.8718\n",
      "Epoch 5/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2004 - acc: 0.8903 - val_loss: 2.2118 - val_acc: 0.8846\n",
      "Epoch 6/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1965 - acc: 0.9131 - val_loss: 2.1900 - val_acc: 0.8974\n",
      "Epoch 7/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1864 - acc: 0.9359 - val_loss: 2.1879 - val_acc: 0.9103\n",
      "Epoch 8/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1844 - acc: 0.9516 - val_loss: 2.2014 - val_acc: 0.8077\n",
      "Epoch 9/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1861 - acc: 0.9416 - val_loss: 2.1810 - val_acc: 0.9744\n",
      "Epoch 10/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1755 - acc: 0.9729 - val_loss: 2.1765 - val_acc: 0.9487\n",
      "Epoch 11/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1800 - acc: 0.9530 - val_loss: 2.1759 - val_acc: 0.9359\n",
      "Epoch 12/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1755 - acc: 0.9786 - val_loss: 2.1849 - val_acc: 0.9359\n",
      "Epoch 13/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1731 - acc: 0.9858 - val_loss: 2.1852 - val_acc: 0.9615\n",
      "Epoch 14/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1735 - acc: 0.9843 - val_loss: 2.1721 - val_acc: 1.0000\n",
      "Epoch 15/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1736 - acc: 0.9786 - val_loss: 2.1803 - val_acc: 0.9359\n",
      "Epoch 16/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1732 - acc: 0.9872 - val_loss: 2.1716 - val_acc: 1.0000\n",
      "Epoch 17/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1699 - acc: 0.9929 - val_loss: 2.1789 - val_acc: 0.9487\n",
      "Epoch 18/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1712 - acc: 0.9915 - val_loss: 2.1709 - val_acc: 0.9872\n",
      "Epoch 19/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1754 - acc: 0.9772 - val_loss: 2.1708 - val_acc: 0.9872\n",
      "Epoch 20/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1726 - acc: 0.9915 - val_loss: 2.1710 - val_acc: 1.0000\n",
      "Epoch 21/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1708 - acc: 0.9943 - val_loss: 2.1707 - val_acc: 0.9872\n",
      "Epoch 22/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1727 - acc: 0.9801 - val_loss: 2.1697 - val_acc: 1.0000\n",
      "Epoch 23/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1724 - acc: 0.9900 - val_loss: 2.1716 - val_acc: 0.9872\n",
      "Epoch 24/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1683 - acc: 0.9972 - val_loss: 2.1704 - val_acc: 0.9872\n",
      "Epoch 25/100\n",
      "702/702 [==============================] - 6s 9ms/step - loss: 2.1703 - acc: 0.9957 - val_loss: 2.1703 - val_acc: 0.9744\n",
      "Epoch 26/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1677 - acc: 0.9986 - val_loss: 2.1698 - val_acc: 0.9872\n",
      "Epoch 27/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1675 - acc: 0.9986 - val_loss: 2.1689 - val_acc: 1.0000\n",
      "Epoch 28/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1695 - acc: 0.9915 - val_loss: 2.1697 - val_acc: 1.0000\n",
      "Epoch 29/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1673 - acc: 0.9986 - val_loss: 2.1699 - val_acc: 0.9744\n",
      "Epoch 30/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1672 - acc: 0.9972 - val_loss: 2.1693 - val_acc: 0.9872\n",
      "Epoch 31/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1683 - acc: 0.9972 - val_loss: 2.1782 - val_acc: 0.9744\n",
      "Epoch 32/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1671 - acc: 1.0000 - val_loss: 2.1698 - val_acc: 0.9872\n",
      "Epoch 33/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1670 - acc: 1.0000 - val_loss: 2.1691 - val_acc: 1.0000\n",
      "Epoch 34/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1680 - acc: 0.9972 - val_loss: 2.1686 - val_acc: 1.0000\n",
      "Epoch 35/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1671 - acc: 0.9986 - val_loss: 2.1699 - val_acc: 1.0000\n",
      "Epoch 36/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1667 - acc: 1.0000 - val_loss: 2.1686 - val_acc: 1.0000\n",
      "Epoch 37/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1666 - acc: 1.0000 - val_loss: 2.1705 - val_acc: 1.0000\n",
      "Epoch 38/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1666 - acc: 1.0000 - val_loss: 2.1685 - val_acc: 1.0000\n",
      "Epoch 39/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1667 - acc: 1.0000 - val_loss: 2.1710 - val_acc: 0.9872\n",
      "Epoch 40/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1666 - acc: 1.0000 - val_loss: 2.1687 - val_acc: 0.9872\n",
      "Epoch 41/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1667 - acc: 0.9986 - val_loss: 2.1680 - val_acc: 1.0000\n",
      "Epoch 42/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1679 - val_acc: 1.0000\n",
      "Epoch 43/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1683 - val_acc: 1.0000\n",
      "Epoch 44/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1666 - acc: 1.0000 - val_loss: 2.1684 - val_acc: 1.0000\n",
      "Epoch 45/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1677 - acc: 0.9972 - val_loss: 2.1685 - val_acc: 1.0000\n",
      "Epoch 46/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1683 - val_acc: 1.0000\n",
      "Epoch 47/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1682 - val_acc: 1.0000\n",
      "Epoch 48/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1683 - val_acc: 1.0000\n",
      "Epoch 49/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1681 - val_acc: 1.0000\n",
      "Epoch 50/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1678 - val_acc: 1.0000\n",
      "Epoch 51/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1682 - val_acc: 1.0000\n",
      "Epoch 52/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1682 - val_acc: 1.0000\n",
      "Epoch 53/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1684 - val_acc: 1.0000\n",
      "Epoch 54/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1680 - val_acc: 1.0000\n",
      "Epoch 55/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1712 - val_acc: 0.9872\n",
      "Epoch 56/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1679 - val_acc: 1.0000\n",
      "Epoch 57/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1678 - val_acc: 1.0000\n",
      "Epoch 58/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1681 - val_acc: 1.0000\n",
      "Epoch 59/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1675 - val_acc: 1.0000\n",
      "Epoch 60/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1681 - val_acc: 1.0000\n",
      "Epoch 61/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1678 - val_acc: 1.0000\n",
      "Epoch 62/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1680 - val_acc: 1.0000\n",
      "Epoch 63/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1680 - val_acc: 1.0000\n",
      "Epoch 64/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1678 - val_acc: 1.0000\n",
      "Epoch 65/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1679 - val_acc: 1.0000\n",
      "Epoch 66/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1677 - val_acc: 1.0000\n",
      "Epoch 67/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1677 - val_acc: 1.0000\n",
      "Epoch 68/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1676 - val_acc: 1.0000\n",
      "Epoch 69/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1677 - val_acc: 1.0000\n",
      "Epoch 70/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1678 - val_acc: 1.0000\n",
      "Epoch 71/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1676 - val_acc: 1.0000\n",
      "Epoch 72/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1677 - val_acc: 1.0000\n",
      "Epoch 73/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1680 - val_acc: 1.0000\n",
      "Epoch 74/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1679 - val_acc: 1.0000\n",
      "Epoch 75/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1678 - val_acc: 1.0000\n",
      "Epoch 76/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1678 - val_acc: 1.0000\n",
      "Epoch 77/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1678 - val_acc: 1.0000\n",
      "Epoch 78/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1676 - val_acc: 1.0000\n",
      "Epoch 79/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1679 - val_acc: 1.0000\n",
      "Epoch 80/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1677 - val_acc: 1.0000\n",
      "Epoch 81/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1679 - val_acc: 1.0000\n",
      "Epoch 82/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1677 - val_acc: 1.0000\n",
      "Epoch 83/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1678 - val_acc: 1.0000\n",
      "Epoch 84/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1679 - val_acc: 1.0000\n",
      "Epoch 85/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1676 - val_acc: 1.0000\n",
      "Epoch 86/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1677 - val_acc: 1.0000\n",
      "Epoch 87/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1676 - val_acc: 1.0000\n",
      "Epoch 88/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1677 - val_acc: 1.0000\n",
      "Epoch 89/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1677 - val_acc: 1.0000\n",
      "Epoch 90/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1677 - val_acc: 1.0000\n",
      "Epoch 91/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1677 - val_acc: 1.0000\n",
      "Epoch 92/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1677 - val_acc: 1.0000\n",
      "Epoch 93/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1677 - val_acc: 1.0000\n",
      "Epoch 94/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1678 - val_acc: 1.0000\n",
      "Epoch 95/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1677 - val_acc: 1.0000\n",
      "Epoch 96/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1678 - val_acc: 1.0000\n",
      "Epoch 97/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1678 - val_acc: 1.0000\n",
      "Epoch 98/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1677 - val_acc: 1.0000\n",
      "Epoch 99/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1676 - val_acc: 1.0000\n",
      "Epoch 100/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1677 - val_acc: 1.0000\n",
      "78/78 [==============================] - 0s 380us/step\n",
      "Score for fold 4: loss of 2.1678229050758557; acc of 100.0%\n",
      "Train on 702 samples, validate on 78 samples\n",
      "Epoch 1/100\n",
      "702/702 [==============================] - 18s 26ms/step - loss: 2.4996 - acc: 0.2692 - val_loss: 2.3655 - val_acc: 0.4231\n",
      "Epoch 2/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.3041 - acc: 0.5741 - val_loss: 2.3490 - val_acc: 0.3205\n",
      "Epoch 3/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2539 - acc: 0.7379 - val_loss: 2.2646 - val_acc: 0.7179\n",
      "Epoch 4/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2373 - acc: 0.8048 - val_loss: 2.2338 - val_acc: 0.8333\n",
      "Epoch 5/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2018 - acc: 0.8789 - val_loss: 2.2301 - val_acc: 0.6538\n",
      "Epoch 6/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2008 - acc: 0.8889 - val_loss: 2.2064 - val_acc: 0.8462\n",
      "Epoch 7/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1892 - acc: 0.9373 - val_loss: 2.1994 - val_acc: 0.8846\n",
      "Epoch 8/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1840 - acc: 0.9387 - val_loss: 2.2202 - val_acc: 0.8205\n",
      "Epoch 9/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1806 - acc: 0.9473 - val_loss: 2.2071 - val_acc: 0.9359\n",
      "Epoch 10/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1795 - acc: 0.9630 - val_loss: 2.1808 - val_acc: 0.9872\n",
      "Epoch 11/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1805 - acc: 0.9615 - val_loss: 2.1838 - val_acc: 0.9487\n",
      "Epoch 12/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1754 - acc: 0.9758 - val_loss: 2.2251 - val_acc: 0.8205\n",
      "Epoch 13/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1742 - acc: 0.9786 - val_loss: 2.1814 - val_acc: 0.9744\n",
      "Epoch 14/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1751 - acc: 0.9701 - val_loss: 2.1938 - val_acc: 0.9103\n",
      "Epoch 15/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1772 - acc: 0.9729 - val_loss: 2.1887 - val_acc: 0.9487\n",
      "Epoch 16/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1720 - acc: 0.9858 - val_loss: 2.1793 - val_acc: 0.9872\n",
      "Epoch 17/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1740 - acc: 0.9815 - val_loss: 2.1840 - val_acc: 0.9487\n",
      "Epoch 18/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1712 - acc: 0.9872 - val_loss: 2.1781 - val_acc: 0.9487\n",
      "Epoch 19/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1711 - acc: 0.9801 - val_loss: 2.1762 - val_acc: 0.9872\n",
      "Epoch 20/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1733 - acc: 0.9843 - val_loss: 2.1788 - val_acc: 0.9872\n",
      "Epoch 21/100\n",
      "702/702 [==============================] - 6s 9ms/step - loss: 2.1709 - acc: 0.9886 - val_loss: 2.1731 - val_acc: 0.9744\n",
      "Epoch 22/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1684 - acc: 0.9943 - val_loss: 2.1737 - val_acc: 0.9872\n",
      "Epoch 23/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1686 - acc: 0.9915 - val_loss: 2.1765 - val_acc: 0.9744\n",
      "Epoch 24/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1697 - acc: 0.9929 - val_loss: 2.1725 - val_acc: 0.9872\n",
      "Epoch 25/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1714 - acc: 0.9900 - val_loss: 2.1738 - val_acc: 0.9872\n",
      "Epoch 26/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1673 - acc: 0.9986 - val_loss: 2.1838 - val_acc: 0.9744\n",
      "Epoch 27/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1682 - acc: 0.9929 - val_loss: 2.1736 - val_acc: 0.9872\n",
      "Epoch 28/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1675 - acc: 0.9972 - val_loss: 2.1734 - val_acc: 0.9872\n",
      "Epoch 29/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1674 - acc: 1.0000 - val_loss: 2.1734 - val_acc: 0.9872\n",
      "Epoch 30/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1689 - acc: 0.9972 - val_loss: 2.1733 - val_acc: 0.9872\n",
      "Epoch 31/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1671 - acc: 1.0000 - val_loss: 2.1743 - val_acc: 0.9872\n",
      "Epoch 32/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1674 - acc: 0.9972 - val_loss: 2.1742 - val_acc: 0.9872\n",
      "Epoch 33/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1668 - acc: 0.9972 - val_loss: 2.1735 - val_acc: 0.9872\n",
      "Epoch 34/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1667 - acc: 1.0000 - val_loss: 2.1730 - val_acc: 0.9872\n",
      "Epoch 35/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1667 - acc: 1.0000 - val_loss: 2.1726 - val_acc: 0.9872\n",
      "Epoch 36/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1669 - acc: 0.9972 - val_loss: 2.1717 - val_acc: 0.9872\n",
      "Epoch 37/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1666 - acc: 1.0000 - val_loss: 2.1716 - val_acc: 0.9872\n",
      "Epoch 38/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1671 - acc: 0.9986 - val_loss: 2.1711 - val_acc: 0.9872\n",
      "Epoch 39/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1666 - acc: 1.0000 - val_loss: 2.1710 - val_acc: 0.9872\n",
      "Epoch 40/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1671 - acc: 0.9986 - val_loss: 2.1720 - val_acc: 0.9872\n",
      "Epoch 41/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1709 - val_acc: 0.9872\n",
      "Epoch 42/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1707 - val_acc: 0.9872\n",
      "Epoch 43/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1708 - val_acc: 0.9872\n",
      "Epoch 44/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1714 - val_acc: 0.9872\n",
      "Epoch 45/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1712 - val_acc: 0.9872\n",
      "Epoch 46/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1711 - val_acc: 0.9872\n",
      "Epoch 47/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1712 - val_acc: 0.9872\n",
      "Epoch 48/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1708 - val_acc: 0.9872\n",
      "Epoch 49/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1717 - val_acc: 0.9872\n",
      "Epoch 50/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1706 - val_acc: 0.9872\n",
      "Epoch 51/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 0.9986 - val_loss: 2.1712 - val_acc: 0.9872\n",
      "Epoch 52/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1708 - val_acc: 0.9872\n",
      "Epoch 53/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1705 - val_acc: 0.9872\n",
      "Epoch 54/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1706 - val_acc: 0.9872\n",
      "Epoch 55/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1714 - val_acc: 0.9872\n",
      "Epoch 56/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1703 - val_acc: 0.9872\n",
      "Epoch 57/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1708 - val_acc: 0.9872\n",
      "Epoch 58/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1705 - val_acc: 0.9872\n",
      "Epoch 59/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1707 - val_acc: 0.9872\n",
      "Epoch 60/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1701 - val_acc: 0.9872\n",
      "Epoch 61/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1703 - val_acc: 0.9872\n",
      "Epoch 62/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1702 - val_acc: 0.9872\n",
      "Epoch 63/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1706 - val_acc: 0.9872\n",
      "Epoch 64/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1707 - val_acc: 0.9872\n",
      "Epoch 65/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1702 - val_acc: 0.9872\n",
      "Epoch 66/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1711 - val_acc: 0.9872\n",
      "Epoch 67/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1699 - val_acc: 0.9872\n",
      "Epoch 68/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1706 - val_acc: 0.9872\n",
      "Epoch 69/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1706 - val_acc: 0.9872\n",
      "Epoch 70/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1701 - val_acc: 0.9872\n",
      "Epoch 71/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1709 - val_acc: 0.9872\n",
      "Epoch 72/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1701 - val_acc: 0.9872\n",
      "Epoch 73/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1705 - val_acc: 0.9872\n",
      "Epoch 74/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1696 - val_acc: 0.9872\n",
      "Epoch 75/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1708 - val_acc: 0.9872\n",
      "Epoch 76/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1703 - val_acc: 0.9872\n",
      "Epoch 77/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1701 - val_acc: 0.9872\n",
      "Epoch 78/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1702 - val_acc: 0.9872\n",
      "Epoch 79/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1701 - val_acc: 0.9872\n",
      "Epoch 80/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1700 - val_acc: 0.9872\n",
      "Epoch 81/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1702 - val_acc: 0.9872\n",
      "Epoch 82/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1700 - val_acc: 0.9872\n",
      "Epoch 83/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1699 - val_acc: 0.9872\n",
      "Epoch 84/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1703 - val_acc: 0.9872\n",
      "Epoch 85/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1703 - val_acc: 0.9872\n",
      "Epoch 86/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1704 - val_acc: 0.9872\n",
      "Epoch 87/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1705 - val_acc: 0.9872\n",
      "Epoch 88/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1703 - val_acc: 0.9872\n",
      "Epoch 89/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1703 - val_acc: 0.9872\n",
      "Epoch 90/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1701 - val_acc: 0.9872\n",
      "Epoch 91/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1699 - val_acc: 0.9872\n",
      "Epoch 92/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1701 - val_acc: 0.9872\n",
      "Epoch 93/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1699 - val_acc: 0.9872\n",
      "Epoch 94/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1699 - val_acc: 0.9872\n",
      "Epoch 95/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1701 - val_acc: 0.9872\n",
      "Epoch 96/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1701 - val_acc: 0.9872\n",
      "Epoch 97/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1700 - val_acc: 0.9872\n",
      "Epoch 98/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1702 - val_acc: 0.9872\n",
      "Epoch 99/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1703 - val_acc: 0.9872\n",
      "Epoch 100/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1702 - val_acc: 0.9872\n",
      "78/78 [==============================] - 0s 335us/step\n",
      "Score for fold 5: loss of 2.1701415135310245; acc of 98.71794871794873%\n",
      "Train on 702 samples, validate on 78 samples\n",
      "Epoch 1/100\n",
      "702/702 [==============================] - 19s 28ms/step - loss: 2.4945 - acc: 0.2749 - val_loss: 2.3893 - val_acc: 0.3846\n",
      "Epoch 2/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.3042 - acc: 0.5783 - val_loss: 2.2814 - val_acc: 0.6026\n",
      "Epoch 3/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2555 - acc: 0.7379 - val_loss: 2.2772 - val_acc: 0.6282\n",
      "Epoch 4/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2214 - acc: 0.8333 - val_loss: 2.2568 - val_acc: 0.7949\n",
      "Epoch 5/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2026 - acc: 0.8704 - val_loss: 2.2418 - val_acc: 0.7949\n",
      "Epoch 6/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1928 - acc: 0.9217 - val_loss: 2.2818 - val_acc: 0.6795\n",
      "Epoch 7/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1909 - acc: 0.9359 - val_loss: 2.2028 - val_acc: 0.8974\n",
      "Epoch 8/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1864 - acc: 0.9473 - val_loss: 2.2300 - val_acc: 0.8077\n",
      "Epoch 9/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1876 - acc: 0.9473 - val_loss: 2.1816 - val_acc: 0.9359\n",
      "Epoch 10/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1752 - acc: 0.9786 - val_loss: 2.1856 - val_acc: 0.9359\n",
      "Epoch 11/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1779 - acc: 0.9672 - val_loss: 2.1815 - val_acc: 0.9487\n",
      "Epoch 12/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1766 - acc: 0.9744 - val_loss: 2.1803 - val_acc: 0.9615\n",
      "Epoch 13/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1731 - acc: 0.9858 - val_loss: 2.1799 - val_acc: 0.9744\n",
      "Epoch 14/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1724 - acc: 0.9815 - val_loss: 2.1861 - val_acc: 0.9615\n",
      "Epoch 15/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1755 - acc: 0.9744 - val_loss: 2.1825 - val_acc: 0.9744\n",
      "Epoch 16/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1712 - acc: 0.9886 - val_loss: 2.2001 - val_acc: 0.9103\n",
      "Epoch 17/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1710 - acc: 0.9843 - val_loss: 2.2235 - val_acc: 0.8846\n",
      "Epoch 18/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1703 - acc: 0.9943 - val_loss: 2.1767 - val_acc: 0.9615\n",
      "Epoch 19/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1720 - acc: 0.9843 - val_loss: 2.1729 - val_acc: 0.9744\n",
      "Epoch 20/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1706 - acc: 0.9915 - val_loss: 2.1759 - val_acc: 0.9487\n",
      "Epoch 21/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1709 - acc: 0.9915 - val_loss: 2.1803 - val_acc: 0.9615\n",
      "Epoch 22/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1692 - acc: 0.9929 - val_loss: 2.2073 - val_acc: 0.9359\n",
      "Epoch 23/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.1705 - acc: 0.9915 - val_loss: 2.1818 - val_acc: 0.9487\n",
      "Epoch 24/100\n",
      "702/702 [==============================] - 7s 10ms/step - loss: 2.1697 - acc: 0.9900 - val_loss: 2.1752 - val_acc: 0.9231\n",
      "Epoch 25/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1676 - acc: 0.9957 - val_loss: 2.1723 - val_acc: 0.9872\n",
      "Epoch 26/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1687 - acc: 0.9943 - val_loss: 2.1715 - val_acc: 0.9872\n",
      "Epoch 27/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1674 - acc: 0.9986 - val_loss: 2.1747 - val_acc: 0.9744\n",
      "Epoch 28/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1677 - acc: 0.9929 - val_loss: 2.1722 - val_acc: 0.9744\n",
      "Epoch 29/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1708 - acc: 0.9929 - val_loss: 2.1713 - val_acc: 0.9872\n",
      "Epoch 30/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1670 - acc: 0.9986 - val_loss: 2.1743 - val_acc: 0.9872\n",
      "Epoch 31/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1672 - acc: 0.9957 - val_loss: 2.1724 - val_acc: 0.9872\n",
      "Epoch 32/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1669 - acc: 1.0000 - val_loss: 2.1716 - val_acc: 1.0000\n",
      "Epoch 33/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1676 - acc: 1.0000 - val_loss: 2.1706 - val_acc: 1.0000\n",
      "Epoch 34/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1673 - acc: 0.9986 - val_loss: 2.1717 - val_acc: 0.9615\n",
      "Epoch 35/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1667 - acc: 1.0000 - val_loss: 2.1757 - val_acc: 0.9744\n",
      "Epoch 36/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1669 - acc: 1.0000 - val_loss: 2.1705 - val_acc: 0.9872\n",
      "Epoch 37/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1685 - acc: 0.9972 - val_loss: 2.1712 - val_acc: 0.9872\n",
      "Epoch 38/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1668 - acc: 1.0000 - val_loss: 2.1760 - val_acc: 0.9872\n",
      "Epoch 39/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1666 - acc: 1.0000 - val_loss: 2.1714 - val_acc: 0.9744\n",
      "Epoch 40/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1670 - acc: 0.9972 - val_loss: 2.1729 - val_acc: 0.9744\n",
      "Epoch 41/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1666 - acc: 1.0000 - val_loss: 2.1711 - val_acc: 0.9872\n",
      "Epoch 42/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1727 - val_acc: 0.9744\n",
      "Epoch 43/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1666 - acc: 0.9986 - val_loss: 2.1848 - val_acc: 0.9744\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1675 - acc: 0.9986 - val_loss: 2.1710 - val_acc: 0.9872\n",
      "Epoch 45/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1667 - acc: 0.9986 - val_loss: 2.1699 - val_acc: 1.0000\n",
      "Epoch 46/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1704 - val_acc: 0.9872\n",
      "Epoch 47/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 0.9986 - val_loss: 2.1702 - val_acc: 0.9872\n",
      "Epoch 48/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1698 - val_acc: 0.9872\n",
      "Epoch 49/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1694 - val_acc: 0.9872\n",
      "Epoch 50/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1704 - val_acc: 1.0000\n",
      "Epoch 51/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1695 - val_acc: 1.0000\n",
      "Epoch 52/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1706 - val_acc: 0.9872\n",
      "Epoch 53/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1699 - val_acc: 1.0000\n",
      "Epoch 54/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1697 - val_acc: 0.9872\n",
      "Epoch 55/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1701 - val_acc: 0.9872\n",
      "Epoch 56/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1695 - val_acc: 0.9872\n",
      "Epoch 57/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1701 - val_acc: 1.0000\n",
      "Epoch 58/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1701 - val_acc: 0.9872\n",
      "Epoch 59/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1697 - val_acc: 1.0000\n",
      "Epoch 60/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1699 - val_acc: 0.9872\n",
      "Epoch 61/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1695 - val_acc: 1.0000\n",
      "Epoch 62/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1694 - val_acc: 1.0000\n",
      "Epoch 63/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1694 - val_acc: 1.0000\n",
      "Epoch 64/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 0.9986 - val_loss: 2.1695 - val_acc: 0.9872\n",
      "Epoch 65/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1694 - val_acc: 1.0000\n",
      "Epoch 66/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1696 - val_acc: 0.9872\n",
      "Epoch 67/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1690 - val_acc: 1.0000\n",
      "Epoch 68/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1695 - val_acc: 1.0000\n",
      "Epoch 69/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1692 - val_acc: 0.9872\n",
      "Epoch 70/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1690 - val_acc: 1.0000\n",
      "Epoch 71/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1691 - val_acc: 1.0000\n",
      "Epoch 72/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1695 - val_acc: 0.9872\n",
      "Epoch 73/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1693 - val_acc: 0.9872\n",
      "Epoch 74/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1694 - val_acc: 1.0000\n",
      "Epoch 75/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1693 - val_acc: 0.9872\n",
      "Epoch 76/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1692 - val_acc: 0.9872\n",
      "Epoch 77/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1689 - val_acc: 0.9872\n",
      "Epoch 78/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1692 - val_acc: 0.9872\n",
      "Epoch 79/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1692 - val_acc: 0.9872\n",
      "Epoch 80/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1699 - val_acc: 0.9872\n",
      "Epoch 81/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1691 - val_acc: 0.9872\n",
      "Epoch 82/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1700 - val_acc: 0.9872\n",
      "Epoch 83/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1691 - val_acc: 1.0000\n",
      "Epoch 84/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1690 - val_acc: 0.9872\n",
      "Epoch 85/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1693 - val_acc: 0.9872\n",
      "Epoch 86/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1693 - val_acc: 0.9872\n",
      "Epoch 87/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1690 - val_acc: 1.0000\n",
      "Epoch 88/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1690 - val_acc: 1.0000\n",
      "Epoch 89/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1693 - val_acc: 1.0000\n",
      "Epoch 90/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1698 - val_acc: 0.9872\n",
      "Epoch 91/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1694 - val_acc: 0.9872\n",
      "Epoch 92/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1694 - val_acc: 0.9872\n",
      "Epoch 93/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1700 - val_acc: 0.9872\n",
      "Epoch 94/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1698 - val_acc: 0.9872\n",
      "Epoch 95/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1695 - val_acc: 0.9872\n",
      "Epoch 96/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1691 - val_acc: 0.9872\n",
      "Epoch 97/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1692 - val_acc: 0.9872\n",
      "Epoch 98/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1694 - val_acc: 1.0000\n",
      "Epoch 99/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1692 - val_acc: 0.9872\n",
      "Epoch 100/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1693 - val_acc: 0.9872\n",
      "78/78 [==============================] - 0s 339us/step\n",
      "Score for fold 6: loss of 2.169172005775647; acc of 98.71794825945145%\n",
      "Train on 702 samples, validate on 78 samples\n",
      "Epoch 1/100\n",
      "702/702 [==============================] - 20s 28ms/step - loss: 2.4770 - acc: 0.2835 - val_loss: 2.4264 - val_acc: 0.3718\n",
      "Epoch 2/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 2.3019 - acc: 0.5926 - val_loss: 2.2793 - val_acc: 0.6410\n",
      "Epoch 3/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2506 - acc: 0.7350 - val_loss: 2.2530 - val_acc: 0.7436\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2144 - acc: 0.8632 - val_loss: 2.2059 - val_acc: 0.8590\n",
      "Epoch 5/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2052 - acc: 0.8533 - val_loss: 2.1988 - val_acc: 0.8718\n",
      "Epoch 6/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1926 - acc: 0.9088 - val_loss: 2.2081 - val_acc: 0.8590\n",
      "Epoch 7/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1895 - acc: 0.9402 - val_loss: 2.2067 - val_acc: 0.8974\n",
      "Epoch 8/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1859 - acc: 0.9387 - val_loss: 2.2062 - val_acc: 0.8590\n",
      "Epoch 9/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1793 - acc: 0.9573 - val_loss: 2.2040 - val_acc: 0.8718\n",
      "Epoch 10/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1795 - acc: 0.9558 - val_loss: 2.1897 - val_acc: 0.9359\n",
      "Epoch 11/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1810 - acc: 0.9687 - val_loss: 2.1839 - val_acc: 0.9359\n",
      "Epoch 12/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1735 - acc: 0.9858 - val_loss: 2.2019 - val_acc: 0.9359\n",
      "Epoch 13/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1775 - acc: 0.9701 - val_loss: 2.1849 - val_acc: 0.9487\n",
      "Epoch 14/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1718 - acc: 0.9943 - val_loss: 2.1846 - val_acc: 0.9103\n",
      "Epoch 15/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1713 - acc: 0.9929 - val_loss: 2.2255 - val_acc: 0.8462\n",
      "Epoch 16/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1732 - acc: 0.9872 - val_loss: 2.1816 - val_acc: 0.9487\n",
      "Epoch 17/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1709 - acc: 0.9801 - val_loss: 2.1868 - val_acc: 0.9487\n",
      "Epoch 18/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1708 - acc: 0.9886 - val_loss: 2.1841 - val_acc: 0.9359\n",
      "Epoch 19/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1704 - acc: 0.9915 - val_loss: 2.1842 - val_acc: 0.9615\n",
      "Epoch 20/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1695 - acc: 0.9929 - val_loss: 2.1820 - val_acc: 0.9744\n",
      "Epoch 21/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1691 - acc: 0.9957 - val_loss: 2.1814 - val_acc: 0.9744\n",
      "Epoch 22/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1740 - acc: 0.9872 - val_loss: 2.1784 - val_acc: 0.9615\n",
      "Epoch 23/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1687 - acc: 0.9986 - val_loss: 2.1845 - val_acc: 0.9744\n",
      "Epoch 24/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1686 - acc: 0.9943 - val_loss: 2.1846 - val_acc: 0.9744\n",
      "Epoch 25/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1691 - acc: 0.9929 - val_loss: 2.1853 - val_acc: 0.9487\n",
      "Epoch 26/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1688 - acc: 0.9915 - val_loss: 2.1814 - val_acc: 0.9744\n",
      "Epoch 27/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1681 - acc: 0.9957 - val_loss: 2.1823 - val_acc: 0.9615\n",
      "Epoch 28/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1681 - acc: 0.9986 - val_loss: 2.1794 - val_acc: 0.9744\n",
      "Epoch 29/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1704 - acc: 0.9900 - val_loss: 2.1814 - val_acc: 0.9615\n",
      "Epoch 30/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1673 - acc: 1.0000 - val_loss: 2.1772 - val_acc: 0.9744\n",
      "Epoch 31/100\n",
      "702/702 [==============================] - 7s 9ms/step - loss: 2.1676 - acc: 0.9986 - val_loss: 2.1816 - val_acc: 0.9744\n",
      "Epoch 32/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1674 - acc: 0.9986 - val_loss: 2.1799 - val_acc: 0.9744\n",
      "Epoch 33/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1674 - acc: 1.0000 - val_loss: 2.1818 - val_acc: 0.9615\n",
      "Epoch 34/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1669 - acc: 1.0000 - val_loss: 2.1815 - val_acc: 0.9744\n",
      "Epoch 35/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1668 - acc: 1.0000 - val_loss: 2.1832 - val_acc: 0.9744\n",
      "Epoch 36/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1670 - acc: 0.9972 - val_loss: 2.1790 - val_acc: 0.9744\n",
      "Epoch 37/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1668 - acc: 1.0000 - val_loss: 2.1783 - val_acc: 0.9744\n",
      "Epoch 38/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1690 - acc: 0.9972 - val_loss: 2.1830 - val_acc: 0.9744\n",
      "Epoch 39/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1666 - acc: 1.0000 - val_loss: 2.1811 - val_acc: 0.9744\n",
      "Epoch 40/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1666 - acc: 1.0000 - val_loss: 2.1826 - val_acc: 0.9744\n",
      "Epoch 41/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1668 - acc: 0.9986 - val_loss: 2.1791 - val_acc: 0.9615\n",
      "Epoch 42/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1667 - acc: 1.0000 - val_loss: 2.1806 - val_acc: 0.9744\n",
      "Epoch 43/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1776 - val_acc: 0.9615\n",
      "Epoch 44/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1810 - val_acc: 0.9744\n",
      "Epoch 45/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1806 - val_acc: 0.9744\n",
      "Epoch 46/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1810 - val_acc: 0.9744\n",
      "Epoch 47/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1811 - val_acc: 0.9744\n",
      "Epoch 48/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1777 - val_acc: 0.9744\n",
      "Epoch 49/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1756 - val_acc: 0.9872\n",
      "Epoch 50/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1780 - val_acc: 0.9744\n",
      "Epoch 51/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1784 - val_acc: 0.9744\n",
      "Epoch 52/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1786 - val_acc: 0.9744\n",
      "Epoch 53/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1797 - val_acc: 0.9744\n",
      "Epoch 54/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1802 - val_acc: 0.9744\n",
      "Epoch 55/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1778 - val_acc: 0.9744\n",
      "Epoch 56/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1827 - val_acc: 0.9744\n",
      "Epoch 57/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1817 - val_acc: 0.9744\n",
      "Epoch 58/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1772 - val_acc: 0.9744\n",
      "Epoch 59/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1802 - val_acc: 0.9744\n",
      "Epoch 60/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1801 - val_acc: 0.9744\n",
      "Epoch 61/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1797 - val_acc: 0.9744\n",
      "Epoch 62/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1802 - val_acc: 0.9744\n",
      "Epoch 63/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1813 - val_acc: 0.9744\n",
      "Epoch 64/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1816 - val_acc: 0.9744\n",
      "Epoch 65/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1801 - val_acc: 0.9744\n",
      "Epoch 66/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1809 - val_acc: 0.9744\n",
      "Epoch 67/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1805 - val_acc: 0.9744\n",
      "Epoch 68/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1814 - val_acc: 0.9744\n",
      "Epoch 69/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1794 - val_acc: 0.9744\n",
      "Epoch 70/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1773 - val_acc: 0.9744\n",
      "Epoch 71/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1803 - val_acc: 0.9744\n",
      "Epoch 72/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1804 - val_acc: 0.9744\n",
      "Epoch 73/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1810 - val_acc: 0.9744\n",
      "Epoch 74/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1803 - val_acc: 0.9744\n",
      "Epoch 75/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1806 - val_acc: 0.9744\n",
      "Epoch 76/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1807 - val_acc: 0.9744\n",
      "Epoch 77/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1796 - val_acc: 0.9744\n",
      "Epoch 78/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1801 - val_acc: 0.9744\n",
      "Epoch 79/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1791 - val_acc: 0.9744\n",
      "Epoch 80/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1806 - val_acc: 0.9744\n",
      "Epoch 81/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1796 - val_acc: 0.9744\n",
      "Epoch 82/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1806 - val_acc: 0.9744\n",
      "Epoch 83/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1803 - val_acc: 0.9744\n",
      "Epoch 84/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1778 - val_acc: 0.9744\n",
      "Epoch 85/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1773 - val_acc: 0.9744\n",
      "Epoch 86/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1792 - val_acc: 0.9744\n",
      "Epoch 87/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1793 - val_acc: 0.9744\n",
      "Epoch 88/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1786 - val_acc: 0.9744\n",
      "Epoch 89/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1806 - val_acc: 0.9744\n",
      "Epoch 90/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1796 - val_acc: 0.9744\n",
      "Epoch 91/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1800 - val_acc: 0.9744\n",
      "Epoch 92/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1791 - val_acc: 0.9744\n",
      "Epoch 93/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1783 - val_acc: 0.9744\n",
      "Epoch 94/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1792 - val_acc: 0.9744\n",
      "Epoch 95/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1797 - val_acc: 0.9744\n",
      "Epoch 96/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1783 - val_acc: 0.9744\n",
      "Epoch 97/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1794 - val_acc: 0.9744\n",
      "Epoch 98/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1787 - val_acc: 0.9744\n",
      "Epoch 99/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1792 - val_acc: 0.9744\n",
      "Epoch 100/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1785 - val_acc: 0.9744\n",
      "78/78 [==============================] - 0s 347us/step\n",
      "Score for fold 7: loss of 2.1779189659998965; acc of 97.43589697740018%\n",
      "Train on 702 samples, validate on 78 samples\n",
      "Epoch 1/100\n",
      "702/702 [==============================] - 19s 28ms/step - loss: 2.4961 - acc: 0.2507 - val_loss: 2.4080 - val_acc: 0.3590\n",
      "Epoch 2/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.3152 - acc: 0.5484 - val_loss: 2.3267 - val_acc: 0.5128\n",
      "Epoch 3/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2620 - acc: 0.7222 - val_loss: 2.2414 - val_acc: 0.7436\n",
      "Epoch 4/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2246 - acc: 0.7963 - val_loss: 2.2144 - val_acc: 0.8718\n",
      "Epoch 5/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2064 - acc: 0.8803 - val_loss: 2.2535 - val_acc: 0.7308\n",
      "Epoch 6/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2034 - acc: 0.9031 - val_loss: 2.1962 - val_acc: 0.8718\n",
      "Epoch 7/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1880 - acc: 0.9288 - val_loss: 2.1907 - val_acc: 0.9231\n",
      "Epoch 8/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1807 - acc: 0.9544 - val_loss: 2.2016 - val_acc: 0.8846\n",
      "Epoch 9/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1874 - acc: 0.9402 - val_loss: 2.2166 - val_acc: 0.8846\n",
      "Epoch 10/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1779 - acc: 0.9744 - val_loss: 2.1946 - val_acc: 0.8974\n",
      "Epoch 11/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1813 - acc: 0.9544 - val_loss: 2.1869 - val_acc: 0.9359\n",
      "Epoch 12/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1758 - acc: 0.9815 - val_loss: 2.1868 - val_acc: 0.8590\n",
      "Epoch 13/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1737 - acc: 0.9843 - val_loss: 2.1812 - val_acc: 0.9231\n",
      "Epoch 14/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1778 - acc: 0.9644 - val_loss: 2.1788 - val_acc: 0.9359\n",
      "Epoch 15/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1722 - acc: 0.9900 - val_loss: 2.1820 - val_acc: 0.9103\n",
      "Epoch 16/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1722 - acc: 0.9886 - val_loss: 2.1808 - val_acc: 0.9487\n",
      "Epoch 17/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1711 - acc: 0.9858 - val_loss: 2.1817 - val_acc: 0.9359\n",
      "Epoch 18/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1734 - acc: 0.9915 - val_loss: 2.1793 - val_acc: 0.9231\n",
      "Epoch 19/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1718 - acc: 0.9900 - val_loss: 2.1770 - val_acc: 0.9744\n",
      "Epoch 20/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1692 - acc: 0.9957 - val_loss: 2.1896 - val_acc: 0.9487\n",
      "Epoch 21/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1723 - acc: 0.9915 - val_loss: 2.1773 - val_acc: 0.9615\n",
      "Epoch 22/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1689 - acc: 0.9943 - val_loss: 2.1741 - val_acc: 0.9872\n",
      "Epoch 23/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1711 - acc: 0.9915 - val_loss: 2.1814 - val_acc: 0.9615\n",
      "Epoch 24/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1691 - acc: 0.9929 - val_loss: 2.2037 - val_acc: 0.9231\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1683 - acc: 0.9972 - val_loss: 2.1741 - val_acc: 0.9744\n",
      "Epoch 26/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1689 - acc: 0.9972 - val_loss: 2.1822 - val_acc: 0.9231\n",
      "Epoch 27/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1707 - acc: 0.9929 - val_loss: 2.1776 - val_acc: 0.9744\n",
      "Epoch 28/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1679 - acc: 0.9957 - val_loss: 2.1751 - val_acc: 0.9615\n",
      "Epoch 29/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1684 - acc: 0.9972 - val_loss: 2.1819 - val_acc: 0.9872\n",
      "Epoch 30/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1698 - acc: 0.9943 - val_loss: 2.1767 - val_acc: 0.9744\n",
      "Epoch 31/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1675 - acc: 0.9986 - val_loss: 2.1816 - val_acc: 0.9615\n",
      "Epoch 32/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1715 - acc: 0.9929 - val_loss: 2.1758 - val_acc: 0.9615\n",
      "Epoch 33/100\n",
      "702/702 [==============================] - 7s 10ms/step - loss: 2.1678 - acc: 0.9972 - val_loss: 2.1799 - val_acc: 0.9615\n",
      "Epoch 34/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1672 - acc: 0.9986 - val_loss: 2.1816 - val_acc: 0.9744\n",
      "Epoch 35/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1676 - acc: 1.0000 - val_loss: 2.1849 - val_acc: 0.9359\n",
      "Epoch 36/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1677 - acc: 0.9957 - val_loss: 2.1790 - val_acc: 0.9487\n",
      "Epoch 37/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1670 - acc: 1.0000 - val_loss: 2.1738 - val_acc: 0.9615\n",
      "Epoch 38/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1668 - acc: 1.0000 - val_loss: 2.1778 - val_acc: 0.9615\n",
      "Epoch 39/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1670 - acc: 0.9986 - val_loss: 2.1746 - val_acc: 0.9744\n",
      "Epoch 40/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1668 - acc: 1.0000 - val_loss: 2.1784 - val_acc: 0.9615\n",
      "Epoch 41/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1668 - acc: 0.9972 - val_loss: 2.1727 - val_acc: 0.9872\n",
      "Epoch 42/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1669 - acc: 0.9986 - val_loss: 2.1757 - val_acc: 0.9615\n",
      "Epoch 43/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1758 - val_acc: 0.9744\n",
      "Epoch 44/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1666 - acc: 1.0000 - val_loss: 2.1733 - val_acc: 0.9744\n",
      "Epoch 45/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1795 - val_acc: 0.9615\n",
      "Epoch 46/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1731 - val_acc: 0.9744\n",
      "Epoch 47/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1726 - val_acc: 0.9744\n",
      "Epoch 48/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 0.9986 - val_loss: 2.1718 - val_acc: 0.9872\n",
      "Epoch 49/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1721 - val_acc: 0.9872\n",
      "Epoch 50/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1732 - val_acc: 0.9744\n",
      "Epoch 51/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1760 - val_acc: 0.9744\n",
      "Epoch 52/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1735 - val_acc: 0.9744\n",
      "Epoch 53/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1732 - val_acc: 0.9872\n",
      "Epoch 54/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1747 - val_acc: 0.9615\n",
      "Epoch 55/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1753 - val_acc: 0.9744\n",
      "Epoch 56/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1733 - val_acc: 0.9744\n",
      "Epoch 57/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1749 - val_acc: 0.9744\n",
      "Epoch 58/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1737 - val_acc: 0.9615\n",
      "Epoch 59/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1729 - val_acc: 0.9872\n",
      "Epoch 60/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1726 - val_acc: 0.9744\n",
      "Epoch 61/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1735 - val_acc: 0.9615\n",
      "Epoch 62/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1729 - val_acc: 0.9744\n",
      "Epoch 63/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1728 - val_acc: 0.9744\n",
      "Epoch 64/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1724 - val_acc: 0.9744\n",
      "Epoch 65/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1739 - val_acc: 0.9615\n",
      "Epoch 66/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1733 - val_acc: 0.9744\n",
      "Epoch 67/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1741 - val_acc: 0.9615\n",
      "Epoch 68/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1724 - val_acc: 0.9744\n",
      "Epoch 69/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1723 - val_acc: 0.9872\n",
      "Epoch 70/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1736 - val_acc: 0.9744\n",
      "Epoch 71/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1724 - val_acc: 0.9744\n",
      "Epoch 72/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1732 - val_acc: 0.9744\n",
      "Epoch 73/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1718 - val_acc: 0.9744\n",
      "Epoch 74/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1728 - val_acc: 0.9744\n",
      "Epoch 75/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1731 - val_acc: 0.9615\n",
      "Epoch 76/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1728 - val_acc: 0.9744\n",
      "Epoch 77/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1722 - val_acc: 0.9872\n",
      "Epoch 78/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1725 - val_acc: 0.9872\n",
      "Epoch 79/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1721 - val_acc: 0.9872\n",
      "Epoch 80/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1732 - val_acc: 0.9615\n",
      "Epoch 81/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1720 - val_acc: 0.9872\n",
      "Epoch 82/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1722 - val_acc: 0.9872\n",
      "Epoch 83/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1726 - val_acc: 0.9872\n",
      "Epoch 84/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1722 - val_acc: 0.9872\n",
      "Epoch 85/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1719 - val_acc: 0.9872\n",
      "Epoch 86/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1729 - val_acc: 0.9615\n",
      "Epoch 87/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1720 - val_acc: 0.9872\n",
      "Epoch 88/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1723 - val_acc: 0.9872\n",
      "Epoch 89/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1724 - val_acc: 0.9744\n",
      "Epoch 90/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1721 - val_acc: 0.9744\n",
      "Epoch 91/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1722 - val_acc: 0.9872\n",
      "Epoch 92/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1721 - val_acc: 0.9872\n",
      "Epoch 93/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1720 - val_acc: 0.9872\n",
      "Epoch 94/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1722 - val_acc: 0.9744\n",
      "Epoch 95/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1721 - val_acc: 0.9744\n",
      "Epoch 96/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1725 - val_acc: 0.9744\n",
      "Epoch 97/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1720 - val_acc: 0.9872\n",
      "Epoch 98/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1719 - val_acc: 0.9872\n",
      "Epoch 99/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1721 - val_acc: 0.9872\n",
      "Epoch 100/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1718 - val_acc: 0.9872\n",
      "78/78 [==============================] - 0s 350us/step\n",
      "Score for fold 8: loss of 2.1719303008837576; acc of 98.71794871794873%\n",
      "Train on 702 samples, validate on 78 samples\n",
      "Epoch 1/100\n",
      "702/702 [==============================] - 19s 28ms/step - loss: 2.4854 - acc: 0.2906 - val_loss: 2.3866 - val_acc: 0.4487\n",
      "Epoch 2/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2838 - acc: 0.6567 - val_loss: 2.2895 - val_acc: 0.6026\n",
      "Epoch 3/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2568 - acc: 0.6994 - val_loss: 2.2217 - val_acc: 0.8205\n",
      "Epoch 4/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2183 - acc: 0.8447 - val_loss: 2.2124 - val_acc: 0.8846\n",
      "Epoch 5/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.2024 - acc: 0.8761 - val_loss: 2.2070 - val_acc: 0.8077\n",
      "Epoch 6/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1996 - acc: 0.9060 - val_loss: 2.2548 - val_acc: 0.7179\n",
      "Epoch 7/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1870 - acc: 0.9444 - val_loss: 2.2151 - val_acc: 0.8590\n",
      "Epoch 8/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1838 - acc: 0.9444 - val_loss: 2.1878 - val_acc: 0.9359\n",
      "Epoch 9/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1827 - acc: 0.9544 - val_loss: 2.1918 - val_acc: 0.9487\n",
      "Epoch 10/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1797 - acc: 0.9644 - val_loss: 2.2131 - val_acc: 0.9231\n",
      "Epoch 11/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1773 - acc: 0.9687 - val_loss: 2.1844 - val_acc: 0.9487\n",
      "Epoch 12/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1735 - acc: 0.9758 - val_loss: 2.1841 - val_acc: 0.9615\n",
      "Epoch 13/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1760 - acc: 0.9815 - val_loss: 2.1807 - val_acc: 0.9744\n",
      "Epoch 14/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1729 - acc: 0.9858 - val_loss: 2.1826 - val_acc: 0.9615\n",
      "Epoch 15/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1720 - acc: 0.9829 - val_loss: 2.1881 - val_acc: 0.9487\n",
      "Epoch 16/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1700 - acc: 0.9915 - val_loss: 2.1808 - val_acc: 0.9744\n",
      "Epoch 17/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1725 - acc: 0.9858 - val_loss: 2.1783 - val_acc: 0.9744\n",
      "Epoch 18/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1685 - acc: 0.9929 - val_loss: 2.1800 - val_acc: 0.9744\n",
      "Epoch 19/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1719 - acc: 0.9843 - val_loss: 2.1783 - val_acc: 0.9744\n",
      "Epoch 20/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1698 - acc: 0.9957 - val_loss: 2.1797 - val_acc: 0.9744\n",
      "Epoch 21/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1687 - acc: 0.9915 - val_loss: 2.1794 - val_acc: 0.9744\n",
      "Epoch 22/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1688 - acc: 0.9943 - val_loss: 2.1968 - val_acc: 0.8718\n",
      "Epoch 23/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1693 - acc: 0.9957 - val_loss: 2.1811 - val_acc: 0.9615\n",
      "Epoch 24/100\n",
      "702/702 [==============================] - 7s 10ms/step - loss: 2.1696 - acc: 0.9957 - val_loss: 2.1800 - val_acc: 0.9615\n",
      "Epoch 25/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1674 - acc: 0.9972 - val_loss: 2.1800 - val_acc: 0.9487\n",
      "Epoch 26/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1672 - acc: 0.9943 - val_loss: 2.1807 - val_acc: 0.9615\n",
      "Epoch 27/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1679 - acc: 0.9972 - val_loss: 2.1777 - val_acc: 0.9744\n",
      "Epoch 28/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1668 - acc: 1.0000 - val_loss: 2.1779 - val_acc: 0.9744\n",
      "Epoch 29/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1678 - acc: 0.9972 - val_loss: 2.1785 - val_acc: 0.9744\n",
      "Epoch 30/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1669 - acc: 0.9986 - val_loss: 2.1781 - val_acc: 0.9744\n",
      "Epoch 31/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1672 - acc: 0.9972 - val_loss: 2.1780 - val_acc: 0.9744\n",
      "Epoch 32/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1670 - acc: 1.0000 - val_loss: 2.1809 - val_acc: 0.9615\n",
      "Epoch 33/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1668 - acc: 0.9972 - val_loss: 2.1778 - val_acc: 0.9744\n",
      "Epoch 34/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1668 - acc: 1.0000 - val_loss: 2.1778 - val_acc: 0.9744\n",
      "Epoch 35/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1666 - acc: 1.0000 - val_loss: 2.1778 - val_acc: 0.9744\n",
      "Epoch 36/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1777 - val_acc: 0.9744\n",
      "Epoch 37/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 0.9986 - val_loss: 2.1776 - val_acc: 0.9744\n",
      "Epoch 38/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1666 - acc: 0.9986 - val_loss: 2.1786 - val_acc: 0.9615\n",
      "Epoch 39/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 0.9986 - val_loss: 2.1777 - val_acc: 0.9744\n",
      "Epoch 40/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1784 - val_acc: 0.9744\n",
      "Epoch 41/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1780 - val_acc: 0.9744\n",
      "Epoch 42/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1802 - val_acc: 0.9615\n",
      "Epoch 43/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1666 - acc: 1.0000 - val_loss: 2.1778 - val_acc: 0.9744\n",
      "Epoch 44/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1780 - val_acc: 0.9744\n",
      "Epoch 45/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 1.0000 - val_loss: 2.1779 - val_acc: 0.9744\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1776 - val_acc: 0.9744\n",
      "Epoch 47/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1774 - val_acc: 0.9744\n",
      "Epoch 48/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1775 - val_acc: 0.9744\n",
      "Epoch 49/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 0.9986 - val_loss: 2.1777 - val_acc: 0.9744\n",
      "Epoch 50/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1774 - val_acc: 0.9744\n",
      "Epoch 51/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1788 - val_acc: 0.9744\n",
      "Epoch 52/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1780 - val_acc: 0.9744\n",
      "Epoch 53/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1779 - val_acc: 0.9744\n",
      "Epoch 54/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1665 - acc: 1.0000 - val_loss: 2.1774 - val_acc: 0.9744\n",
      "Epoch 55/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1664 - acc: 0.9986 - val_loss: 2.1774 - val_acc: 0.9744\n",
      "Epoch 56/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1775 - val_acc: 0.9744\n",
      "Epoch 57/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1773 - val_acc: 0.9744\n",
      "Epoch 58/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1771 - val_acc: 0.9744\n",
      "Epoch 59/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1774 - val_acc: 0.9744\n",
      "Epoch 60/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1773 - val_acc: 0.9744\n",
      "Epoch 61/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1774 - val_acc: 0.9744\n",
      "Epoch 62/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1774 - val_acc: 0.9744\n",
      "Epoch 63/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1663 - acc: 1.0000 - val_loss: 2.1773 - val_acc: 0.9744\n",
      "Epoch 64/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1772 - val_acc: 0.9744\n",
      "Epoch 65/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1775 - val_acc: 0.9744\n",
      "Epoch 66/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1774 - val_acc: 0.9744\n",
      "Epoch 67/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1773 - val_acc: 0.9744\n",
      "Epoch 68/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1774 - val_acc: 0.9744\n",
      "Epoch 69/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1774 - val_acc: 0.9744\n",
      "Epoch 70/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1776 - val_acc: 0.9744\n",
      "Epoch 71/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1773 - val_acc: 0.9744\n",
      "Epoch 72/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1775 - val_acc: 0.9744\n",
      "Epoch 73/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1773 - val_acc: 0.9744\n",
      "Epoch 74/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1774 - val_acc: 0.9744\n",
      "Epoch 75/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1773 - val_acc: 0.9744\n",
      "Epoch 76/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1773 - val_acc: 0.9744\n",
      "Epoch 77/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 2.1662 - acc: 1.0000 - val_loss: 2.1774 - val_acc: 0.9744\n",
      "Epoch 78/100\n",
      "630/702 [=========================>....] - ETA: 0s - loss: 2.1688 - acc: 1.0000"
     ]
    }
   ],
   "source": [
    "verbose = 1\n",
    "\n",
    "\n",
    "# Bounded region of parameter space\n",
    "pbounds = {'dr': (0.9, 0.999), 'es': (5, 12)}\n",
    "\n",
    "optimizer = BayesianOptimization(\n",
    "    f=create_model,\n",
    "    pbounds=pbounds,\n",
    "    verbose=2,  # verbose = 1 prints only when a maximum is observed, verbose = 0 is silent\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "optimizer.maximize(init_points=4, n_iter=6,)\n",
    "\n",
    "\n",
    "for i, res in enumerate(optimizer.res):\n",
    "    print(\"Iteration {}: \\n\\t{}\".format(i, res))\n",
    "\n",
    "print(optimizer.max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.7 10"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
