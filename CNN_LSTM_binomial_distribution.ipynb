{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import keras\n",
    "from confusion_matrix import CM\n",
    "from functools import reduce\n",
    "from numpy.lib.scimath import log\n",
    "from scipy.special import factorial\n",
    "from functools import reduce\n",
    "import operator as op\n",
    "import numpy as np\n",
    "import math \n",
    "from scipy.special import factorial\n",
    "from scipy.stats import nbinom\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras import models, optimizers, layers\n",
    "from keras import backend as K\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Layer, Dense, Embedding,TimeDistributed,Conv2D,Input, Flatten,Reshape ,MaxPooling2D, Dropout,Activation, LSTM, Bidirectional, Lambda\n",
    "from keras.callbacks import ModelCheckpoint,LearningRateScheduler\n",
    "from keras import callbacks\n",
    "from keras import initializers, layers\n",
    "\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from functools import partial\n",
    "from bayes_opt import BayesianOptimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = np.load('/home/yi/forest/hardness/np/pressure_1.npy')\n",
    "liju = np.load('/home/yi/forest/hardness/np/torque_1.npy')\n",
    "finger1 = images[:,::6,:24]\n",
    "finger2 = images[:,::6,24:48]\n",
    "finger3 = images[:,::6,48:]\n",
    "time_steps = 12\n",
    "fg1 = finger1[:,:].reshape((780, time_steps, 8, 3))\n",
    "fg2 = finger2[:,:].reshape((780, time_steps, 8, 3))\n",
    "fg3 = finger3[:,:].reshape((780, time_steps, 8, 3))\n",
    "fgall = np.concatenate((fg1, fg3, fg2), axis=-1)\n",
    "\n",
    "liju_bet = liju[:,::6,:]\n",
    "\n",
    "liju_arr = np.concatenate((liju_bet, liju_bet, liju_bet), axis=-1)\n",
    "liju_exp = np.expand_dims(liju_arr, axis=-2)\n",
    "X = np.concatenate((fgall, liju_exp), axis=-2)\n",
    "X = np.expand_dims(X, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cats_nums = 13\n",
    "y_ohot = np.repeat(np.arange(cats_nums), 60)\n",
    "np.random.seed(42)\n",
    "shuffle_indices = np.random.permutation(np.arange(len(y_ohot)))\n",
    "\n",
    "X = X[shuffle_indices]\n",
    "y = y_ohot[shuffle_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "generalize one-hot encoding (ground truth) using binomial distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Bi_true(y):\n",
    "    \n",
    "    def softmax(x):\n",
    "        return (np.exp(x, dtype=float) / np.sum(np.exp(x, dtype=float), axis=0, dtype=float)).astype(float)\n",
    "    \n",
    "    def const(n, r):\n",
    "        \n",
    "        r = min(r, n-r)\n",
    "        numer = reduce(op.mul, range(n, n-r, -1), 1)\n",
    "        denom = reduce(op.mul, range(1, r+1), 1)\n",
    "        return numer / denom\n",
    "    \n",
    "    def binomial(n, i):\n",
    "        \n",
    "        p =  np.sqrt(((i)*(i+1))/((14-i)*(15-i)))/(np.sqrt(((i)*(i+1))/((14-i)*(15-i)))+1)\n",
    "        q = 1 - p\n",
    "        bio = [const(n, k) * (p ** k) * (q ** (n-k)) for k in range(n)]\n",
    "\n",
    "        return bio  \n",
    "             \n",
    "    y_bi = np.zeros((len(y),13)) \n",
    "             \n",
    "    for i in range(len(y)):\n",
    "        \n",
    "        bio = binomial(14, y[i]+1)[1:]\n",
    "        \n",
    "        bio = log(bio)\n",
    "        bio = softmax(bio/0.2)\n",
    "        \n",
    "        y_bi[i] = bio\n",
    "                            \n",
    "    return y_bi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    \n",
    "    K.set_learning_phase(1)\n",
    "    acc_per_fold = []\n",
    "    loss_per_fold = []\n",
    "    train_accur_per_fold = []\n",
    "    test_accur_per_fold = []\n",
    "    train_loss_per_fold = []\n",
    "    test_loss_per_fold = []\n",
    "    cm_data_per_fold = []\n",
    "    pred_per_fold = []\n",
    "    y_test_per_fold = []\n",
    "\n",
    "    fold_no = 1\n",
    "   \n",
    "    \n",
    "    class MyLoss(Layer):\n",
    "        def __init__(self, var1):\n",
    "            super(MyLoss, self).__init__()\n",
    "            self.var1 = K.variable(var1) \n",
    "\n",
    "\n",
    "        def get_vars(self):\n",
    "            return self.var1\n",
    "\n",
    "        def custom_loss(self, y_true, y_pred):\n",
    "            return self.var1 * K.mean(K.square(y_true-y_pred))\n",
    "\n",
    "        def call(self, y_true, y_pred):\n",
    "            self.add_loss(self.custom_loss(y_true, y_pred))\n",
    "            return y_pred\n",
    "\n",
    "    for train, test in kfold.split(X, y):\n",
    "        model1 = Sequential()\n",
    "\n",
    "        model1.add(layers.Conv2D(filters=32, kernel_size=6,strides=1, padding='same', activation='relu', name='cl_conv1', \n",
    "                  input_shape=[9, 9, 1]))\n",
    "\n",
    "        model1.add(layers.BatchNormalization())\n",
    "        model1.add(layers.Activation('relu'))\n",
    "\n",
    "        model1.add(MaxPooling2D(pool_size=2,strides=2, padding='same', name='cl_maxpool1'))\n",
    "\n",
    "        model1.add(layers.Conv2D(filters=64, kernel_size=6,strides=1, padding='same', activation='relu', name='cl_conv2'))\n",
    "        model1.add(layers.BatchNormalization())\n",
    "        model1.add(layers.Activation('relu'))\n",
    "\n",
    "        model1.add(layers.MaxPooling2D(pool_size=2,strides=2, padding='same', name='cl_maxpool2'))\n",
    "        model1.add(Flatten())\n",
    "   \n",
    "\n",
    "        model2 = Sequential()\n",
    "        model2.add(TimeDistributed(model1,input_shape = (12,9,9,1)))\n",
    "\n",
    "        model2.add(LSTM(units=100, name=\"lstm_out\"))\n",
    "       \n",
    "        model2.add(Dense(units=13,  name=\"Dense\", activation='softmax'))\n",
    "        \n",
    "       \n",
    "  \n",
    "        \n",
    "        early_stop = callbacks.EarlyStopping(monitor='val_accuracy', mode='max',patience=6)\n",
    "        reduce_lr = ReduceLROnPlateau(monitor='val_acc', factor=0.7578553374517447, patience=6, mode='max')\n",
    "        model2.compile(loss=\"categorical_crossentropy\",optimizer=optimizers.RMSprop(lr=0.001), metrics = [\"accuracy\"])#QWK loss\n",
    "        tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "        y_true = to_categorical(y[train], cats_nums)\n",
    "        \n",
    "               \n",
    "        y_tr = Bi_true(y[train])\n",
    "        y_ts = Bi_true(y[test])\n",
    "\n",
    "        history = model2.fit(X[train], y_tr, batch_size=30, nb_epoch=100,verbose=1,validation_data=(X[test], y_ts) , callbacks=[reduce_lr, early_stop])\n",
    "\n",
    "        train_accur_per_fold.append(history.history[\"acc\"])\n",
    "        test_accur_per_fold.append(history.history[\"val_acc\"])\n",
    "        train_loss_per_fold.append(history.history[\"loss\"])\n",
    "        test_loss_per_fold.append(history.history[\"val_loss\"])\n",
    "\n",
    "        scores = model2.evaluate(X[test], y_ts, verbose=1)\n",
    "        print(f'Score for fold {fold_no}: {model2.metrics_names[0]} of {scores[0]}; {model2.metrics_names[1]} of {scores[1]*100}%')\n",
    "        acc_per_fold.append(scores[1] * 100)\n",
    "        loss_per_fold.append(scores[0])\n",
    "\n",
    "        pred = model2.predict(X[test])\n",
    "        pred_per_fold.append(pred)\n",
    "        y_test_per_fold.append(y[test])\n",
    "        cm_data = confusion_matrix(y_ts.argmax(axis=1), pred.argmax(axis=1))\n",
    "        cm_data_per_fold.append(cm_data)\n",
    "        fold_no = fold_no + 1\n",
    "\n",
    "    train_acc = np.mean(train_accur_per_fold, axis = 0)\n",
    "    test_acc = np.mean(test_accur_per_fold, axis = 0)\n",
    "    train_loss = np.mean(train_loss_per_fold, axis = 0)\n",
    "    test_loss = np.mean(test_loss_per_fold, axis = 0)\n",
    "    cm_data = np.sum(cm_data_per_fold, axis=0)\n",
    "\n",
    "    acc = np.mean(np.array(acc_per_fold))\n",
    "    \n",
    "\n",
    "    name = \"shape1_B\" + \"pa\" + str(0.75785)+ \"fa\" + str(6) + \"es\" + str(6)\n",
    "    train_test_acc_loss = np.concatenate((np.expand_dims(train_acc, axis=-1), \n",
    "                    np.expand_dims(test_acc, axis=-1), \n",
    "                    np.expand_dims(train_loss, axis=-1),\n",
    "                    np.expand_dims(train_loss, axis=-1)), axis=-1)\n",
    "\n",
    "    path = \"/home/yi/forest/hardness/paper_code/Acc/result1/data_shape1/\"\n",
    "    np.save(path + 'pred/'+name + '.npy', np.array(pred_per_fold))\n",
    "    np.save(path + 'y_test/'+name + '.npy', np.array(y_test_per_fold))\n",
    "    np.save(path + 'train_test_acc_loss/'+name + '.npy', train_test_acc_loss) \n",
    "    np.save(path + 'cm_data/'+name + '.npy', cm_data)\n",
    "    np.save(path + 'final_acc/'+name + '.npy', np.array(acc_per_fold))\n",
    "    CM(cm_data, name)\n",
    "    \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/yi/anaconda3/envs/tf/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:138: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/yi/anaconda3/envs/tf/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:492: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/yi/anaconda3/envs/tf/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3630: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/yi/anaconda3/envs/tf/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3458: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/yi/anaconda3/envs/tf/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1259: calling reduce_prod_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From /home/yi/anaconda3/envs/tf/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1242: calling reduce_sum_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From /home/yi/anaconda3/envs/tf/lib/python3.6/site-packages/keras/optimizers.py:711: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/yi/anaconda3/envs/tf/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:2884: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "Train on 702 samples, validate on 78 samples\n",
      "Epoch 1/100\n",
      "702/702 [==============================] - 2s 3ms/step - loss: 2.1890 - acc: 0.3775 - val_loss: 1.9362 - val_acc: 0.4231\n",
      "Epoch 2/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 1.6504 - acc: 0.6709 - val_loss: 1.4893 - val_acc: 0.7308\n",
      "Epoch 3/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 1.3756 - acc: 0.8020 - val_loss: 1.3382 - val_acc: 0.7821\n",
      "Epoch 4/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 1.2623 - acc: 0.8490 - val_loss: 1.4084 - val_acc: 0.7821\n",
      "Epoch 5/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 1.2179 - acc: 0.8803 - val_loss: 1.2500 - val_acc: 0.8333\n",
      "Epoch 6/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 1.1644 - acc: 0.9060 - val_loss: 1.2421 - val_acc: 0.8462\n",
      "Epoch 7/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 1.1317 - acc: 0.9145 - val_loss: 1.1242 - val_acc: 0.8974\n",
      "Epoch 8/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 1.0959 - acc: 0.9644 - val_loss: 1.1734 - val_acc: 0.7949\n",
      "Epoch 9/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 1.1115 - acc: 0.9288 - val_loss: 1.1199 - val_acc: 0.9231\n",
      "Epoch 10/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 1.0941 - acc: 0.9587 - val_loss: 1.1029 - val_acc: 0.9231\n",
      "Epoch 11/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 1.0719 - acc: 0.9701 - val_loss: 1.0873 - val_acc: 0.9359\n",
      "Epoch 12/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 1.1233 - acc: 0.9387 - val_loss: 1.1041 - val_acc: 0.8846\n",
      "Epoch 13/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 1.0624 - acc: 0.9815 - val_loss: 1.0860 - val_acc: 0.9744\n",
      "Epoch 14/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 1.0750 - acc: 0.9601 - val_loss: 1.0972 - val_acc: 0.9487\n",
      "Epoch 15/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 1.0561 - acc: 0.9900 - val_loss: 1.0972 - val_acc: 0.9103\n",
      "Epoch 16/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 1.0623 - acc: 0.9744 - val_loss: 1.0755 - val_acc: 0.9487\n",
      "Epoch 17/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 1.1034 - acc: 0.9530 - val_loss: 1.0890 - val_acc: 0.9359\n",
      "Epoch 18/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 1.0505 - acc: 0.9972 - val_loss: 1.0910 - val_acc: 0.9359\n",
      "Epoch 19/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 1.0634 - acc: 0.9858 - val_loss: 1.0864 - val_acc: 0.9487\n",
      "Epoch 20/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 1.0516 - acc: 0.9886 - val_loss: 1.0861 - val_acc: 0.9615\n",
      "Epoch 21/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0520 - acc: 0.9858 - val_loss: 1.0854 - val_acc: 0.9359\n",
      "Epoch 22/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0468 - acc: 0.9972 - val_loss: 1.0766 - val_acc: 0.9487\n",
      "Epoch 23/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0505 - acc: 0.9900 - val_loss: 1.1002 - val_acc: 0.8974\n",
      "Epoch 24/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0474 - acc: 0.9957 - val_loss: 1.0801 - val_acc: 0.9487\n",
      "Epoch 25/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0467 - acc: 0.9972 - val_loss: 1.0729 - val_acc: 0.9359\n",
      "Epoch 26/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0482 - acc: 0.9915 - val_loss: 1.0754 - val_acc: 0.9359\n",
      "Epoch 27/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0454 - acc: 0.9957 - val_loss: 1.0803 - val_acc: 0.9615\n",
      "Epoch 28/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0441 - acc: 0.9957 - val_loss: 1.0799 - val_acc: 0.9615\n",
      "Epoch 29/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0436 - acc: 0.9986 - val_loss: 1.0782 - val_acc: 0.9359\n",
      "Epoch 30/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0434 - acc: 0.9986 - val_loss: 1.0779 - val_acc: 0.9487\n",
      "Epoch 31/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0532 - acc: 0.9915 - val_loss: 1.0812 - val_acc: 0.9359\n",
      "Epoch 32/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0428 - acc: 0.9986 - val_loss: 1.0765 - val_acc: 0.9615\n",
      "Epoch 33/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0426 - acc: 0.9972 - val_loss: 1.0708 - val_acc: 0.9487\n",
      "Epoch 34/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0421 - acc: 1.0000 - val_loss: 1.0745 - val_acc: 0.9359\n",
      "Epoch 35/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0427 - acc: 0.9972 - val_loss: 1.0767 - val_acc: 0.9615\n",
      "Epoch 36/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0418 - acc: 1.0000 - val_loss: 1.0693 - val_acc: 0.9487\n",
      "Epoch 37/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0420 - acc: 1.0000 - val_loss: 1.0738 - val_acc: 0.9487\n",
      "Epoch 38/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0420 - acc: 0.9986 - val_loss: 1.0759 - val_acc: 0.9359\n",
      "Epoch 39/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0413 - acc: 1.0000 - val_loss: 1.0763 - val_acc: 0.9359\n",
      "Epoch 40/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0411 - acc: 1.0000 - val_loss: 1.0669 - val_acc: 0.9615\n",
      "Epoch 41/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0413 - acc: 1.0000 - val_loss: 1.0757 - val_acc: 0.9487\n",
      "Epoch 42/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0409 - acc: 1.0000 - val_loss: 1.0720 - val_acc: 0.9359\n",
      "Epoch 43/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0412 - acc: 1.0000 - val_loss: 1.0719 - val_acc: 0.9487\n",
      "Epoch 44/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0409 - acc: 1.0000 - val_loss: 1.0721 - val_acc: 0.9487\n",
      "Epoch 45/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0407 - acc: 1.0000 - val_loss: 1.0711 - val_acc: 0.9487\n",
      "Epoch 46/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0407 - acc: 1.0000 - val_loss: 1.0684 - val_acc: 0.9487\n",
      "Epoch 47/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0407 - acc: 1.0000 - val_loss: 1.0721 - val_acc: 0.9487\n",
      "Epoch 48/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0728 - val_acc: 0.9487\n",
      "Epoch 49/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0409 - acc: 1.0000 - val_loss: 1.0701 - val_acc: 0.9359\n",
      "Epoch 50/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0411 - acc: 0.9986 - val_loss: 1.0689 - val_acc: 0.9359\n",
      "Epoch 51/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0408 - acc: 0.9986 - val_loss: 1.0657 - val_acc: 0.9359\n",
      "Epoch 52/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0661 - val_acc: 0.9487\n",
      "Epoch 53/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0697 - val_acc: 0.9487\n",
      "Epoch 54/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0715 - val_acc: 0.9359\n",
      "Epoch 55/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0712 - val_acc: 0.9359\n",
      "Epoch 56/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0676 - val_acc: 0.9359\n",
      "Epoch 57/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0695 - val_acc: 0.9359\n",
      "Epoch 58/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0709 - val_acc: 0.9359\n",
      "Epoch 59/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0654 - val_acc: 0.9615\n",
      "Epoch 60/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0691 - val_acc: 0.9359\n",
      "Epoch 61/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0700 - val_acc: 0.9359\n",
      "Epoch 62/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0718 - val_acc: 0.9359\n",
      "Epoch 63/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0669 - val_acc: 0.9487\n",
      "Epoch 64/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0675 - val_acc: 0.9359\n",
      "Epoch 65/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0679 - val_acc: 0.9487\n",
      "Epoch 66/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0713 - val_acc: 0.9359\n",
      "Epoch 67/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0683 - val_acc: 0.9359\n",
      "Epoch 68/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0710 - val_acc: 0.9359\n",
      "Epoch 69/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0716 - val_acc: 0.9359\n",
      "Epoch 70/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0703 - val_acc: 0.9359\n",
      "Epoch 71/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0710 - val_acc: 0.9359\n",
      "Epoch 72/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0700 - val_acc: 0.9359\n",
      "Epoch 73/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0715 - val_acc: 0.9359\n",
      "Epoch 74/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0687 - val_acc: 0.9359\n",
      "Epoch 75/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0703 - val_acc: 0.9359\n",
      "Epoch 76/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0696 - val_acc: 0.9487\n",
      "Epoch 77/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0694 - val_acc: 0.9359\n",
      "Epoch 78/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0686 - val_acc: 0.9359\n",
      "Epoch 79/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0695 - val_acc: 0.9359\n",
      "Epoch 80/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0703 - val_acc: 0.9359\n",
      "Epoch 81/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0690 - val_acc: 0.9359\n",
      "Epoch 82/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0700 - val_acc: 0.9359\n",
      "Epoch 83/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0682 - val_acc: 0.9359\n",
      "Epoch 84/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0696 - val_acc: 0.9359\n",
      "Epoch 85/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0694 - val_acc: 0.9359\n",
      "Epoch 86/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0700 - val_acc: 0.9359\n",
      "Epoch 87/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0692 - val_acc: 0.9359\n",
      "Epoch 88/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0697 - val_acc: 0.9359\n",
      "Epoch 89/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0693 - val_acc: 0.9359\n",
      "Epoch 90/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0698 - val_acc: 0.9359\n",
      "Epoch 91/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0695 - val_acc: 0.9359\n",
      "Epoch 92/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0692 - val_acc: 0.9359\n",
      "Epoch 93/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0699 - val_acc: 0.9359\n",
      "Epoch 94/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0687 - val_acc: 0.9359\n",
      "Epoch 95/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0686 - val_acc: 0.9359\n",
      "Epoch 96/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0692 - val_acc: 0.9359\n",
      "Epoch 97/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0691 - val_acc: 0.9359\n",
      "Epoch 98/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0702 - val_acc: 0.9359\n",
      "Epoch 99/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0694 - val_acc: 0.9359\n",
      "Epoch 100/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0696 - val_acc: 0.9359\n",
      "78/78 [==============================] - 0s 335us/step\n",
      "Score for fold 1: loss of 1.0701459585092006; acc of 93.58974358974359%\n",
      "Train on 702 samples, validate on 78 samples\n",
      "Epoch 1/100\n",
      "702/702 [==============================] - 2s 3ms/step - loss: 2.3280 - acc: 0.2977 - val_loss: 2.0224 - val_acc: 0.3974\n",
      "Epoch 2/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.7459 - acc: 0.6453 - val_loss: 1.5610 - val_acc: 0.7179\n",
      "Epoch 3/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.4227 - acc: 0.7920 - val_loss: 1.4700 - val_acc: 0.7949\n",
      "Epoch 4/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.3053 - acc: 0.8148 - val_loss: 1.2663 - val_acc: 0.8974\n",
      "Epoch 5/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1898 - acc: 0.9017 - val_loss: 1.1864 - val_acc: 0.9103\n",
      "Epoch 6/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1436 - acc: 0.9174 - val_loss: 1.2612 - val_acc: 0.8333\n",
      "Epoch 7/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1342 - acc: 0.9345 - val_loss: 1.1299 - val_acc: 0.9615\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1091 - acc: 0.9402 - val_loss: 1.1226 - val_acc: 0.9231\n",
      "Epoch 9/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0822 - acc: 0.9601 - val_loss: 1.1665 - val_acc: 0.8846\n",
      "Epoch 10/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1012 - acc: 0.9416 - val_loss: 1.1348 - val_acc: 0.8718\n",
      "Epoch 11/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0775 - acc: 0.9587 - val_loss: 1.0964 - val_acc: 0.8718\n",
      "Epoch 12/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0816 - acc: 0.9644 - val_loss: 1.0879 - val_acc: 0.9487\n",
      "Epoch 13/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0594 - acc: 0.9886 - val_loss: 1.0880 - val_acc: 0.9103\n",
      "Epoch 14/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0855 - acc: 0.9729 - val_loss: 1.0665 - val_acc: 0.9744\n",
      "Epoch 15/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0572 - acc: 0.9886 - val_loss: 1.0736 - val_acc: 0.9615\n",
      "Epoch 16/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0794 - acc: 0.9644 - val_loss: 1.0654 - val_acc: 0.9744\n",
      "Epoch 17/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0530 - acc: 0.9900 - val_loss: 1.0727 - val_acc: 0.9487\n",
      "Epoch 18/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0548 - acc: 0.9915 - val_loss: 1.0667 - val_acc: 0.9744\n",
      "Epoch 19/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0684 - acc: 0.9758 - val_loss: 1.1096 - val_acc: 0.8333\n",
      "Epoch 20/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0524 - acc: 0.9858 - val_loss: 1.0645 - val_acc: 0.9744\n",
      "Epoch 21/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 1.0491 - acc: 0.9957 - val_loss: 1.0595 - val_acc: 0.9744\n",
      "Epoch 22/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0501 - acc: 0.9886 - val_loss: 1.0590 - val_acc: 0.9872\n",
      "Epoch 23/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0470 - acc: 0.9957 - val_loss: 1.0938 - val_acc: 0.9231\n",
      "Epoch 24/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0486 - acc: 0.9957 - val_loss: 1.0513 - val_acc: 1.0000\n",
      "Epoch 25/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0448 - acc: 0.9986 - val_loss: 1.0578 - val_acc: 0.9744\n",
      "Epoch 26/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0474 - acc: 0.9915 - val_loss: 1.0518 - val_acc: 0.9744\n",
      "Epoch 27/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0453 - acc: 0.9943 - val_loss: 1.0580 - val_acc: 0.9615\n",
      "Epoch 28/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0482 - acc: 0.9915 - val_loss: 1.0559 - val_acc: 0.9872\n",
      "Epoch 29/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0446 - acc: 0.9972 - val_loss: 1.0523 - val_acc: 0.9872\n",
      "Epoch 30/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0442 - acc: 0.9972 - val_loss: 1.0478 - val_acc: 0.9744\n",
      "Epoch 31/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0459 - acc: 0.9972 - val_loss: 1.0499 - val_acc: 0.9744\n",
      "Epoch 32/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0432 - acc: 0.9986 - val_loss: 1.0510 - val_acc: 0.9744\n",
      "Epoch 33/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0430 - acc: 0.9986 - val_loss: 1.0480 - val_acc: 0.9744\n",
      "Epoch 34/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0442 - acc: 0.9943 - val_loss: 1.0632 - val_acc: 0.9359\n",
      "Epoch 35/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0428 - acc: 0.9972 - val_loss: 1.0466 - val_acc: 0.9744\n",
      "Epoch 36/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0437 - acc: 0.9957 - val_loss: 1.0523 - val_acc: 0.9872\n",
      "Epoch 37/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0426 - acc: 0.9957 - val_loss: 1.0525 - val_acc: 0.9744\n",
      "Epoch 38/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0424 - acc: 0.9972 - val_loss: 1.0493 - val_acc: 0.9744\n",
      "Epoch 39/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0415 - acc: 1.0000 - val_loss: 1.0520 - val_acc: 0.9872\n",
      "Epoch 40/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0416 - acc: 1.0000 - val_loss: 1.0486 - val_acc: 0.9744\n",
      "Epoch 41/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0417 - acc: 1.0000 - val_loss: 1.0489 - val_acc: 0.9872\n",
      "Epoch 42/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0414 - acc: 1.0000 - val_loss: 1.0567 - val_acc: 0.9872\n",
      "Epoch 43/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0413 - acc: 1.0000 - val_loss: 1.0468 - val_acc: 0.9872\n",
      "Epoch 44/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0410 - acc: 1.0000 - val_loss: 1.0448 - val_acc: 1.0000\n",
      "Epoch 45/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0410 - acc: 1.0000 - val_loss: 1.0522 - val_acc: 0.9615\n",
      "Epoch 46/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0409 - acc: 1.0000 - val_loss: 1.0450 - val_acc: 0.9872\n",
      "Epoch 47/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0408 - acc: 1.0000 - val_loss: 1.0466 - val_acc: 0.9744\n",
      "Epoch 48/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0407 - acc: 1.0000 - val_loss: 1.0481 - val_acc: 0.9744\n",
      "Epoch 49/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0407 - acc: 1.0000 - val_loss: 1.0470 - val_acc: 0.9744\n",
      "Epoch 50/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0450 - val_acc: 0.9872\n",
      "Epoch 51/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0458 - val_acc: 0.9872\n",
      "Epoch 52/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0443 - val_acc: 0.9872\n",
      "Epoch 53/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0470 - val_acc: 0.9872\n",
      "Epoch 54/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0442 - val_acc: 0.9872\n",
      "Epoch 55/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0437 - val_acc: 0.9872\n",
      "Epoch 56/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0438 - val_acc: 0.9872\n",
      "Epoch 57/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0456 - val_acc: 0.9872\n",
      "Epoch 58/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0462 - val_acc: 0.9872\n",
      "Epoch 59/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0461 - val_acc: 0.9872\n",
      "Epoch 60/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0446 - val_acc: 0.9872\n",
      "Epoch 61/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0462 - val_acc: 0.9872\n",
      "Epoch 62/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0454 - val_acc: 0.9872\n",
      "Epoch 63/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0440 - val_acc: 0.9872\n",
      "Epoch 64/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0460 - val_acc: 0.9744\n",
      "Epoch 65/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0445 - val_acc: 0.9744\n",
      "Epoch 66/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0437 - val_acc: 1.0000\n",
      "Epoch 67/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0437 - val_acc: 1.0000\n",
      "Epoch 68/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0439 - val_acc: 0.9872\n",
      "Epoch 69/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0443 - val_acc: 0.9744\n",
      "Epoch 70/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0437 - val_acc: 1.0000\n",
      "Epoch 71/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0438 - val_acc: 0.9872\n",
      "Epoch 72/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0443 - val_acc: 1.0000\n",
      "Epoch 73/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0439 - val_acc: 0.9744\n",
      "Epoch 74/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0438 - val_acc: 0.9872\n",
      "Epoch 75/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0447 - val_acc: 0.9872\n",
      "Epoch 76/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0452 - val_acc: 0.9872\n",
      "Epoch 77/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0447 - val_acc: 0.9744\n",
      "Epoch 78/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0449 - val_acc: 0.9872\n",
      "Epoch 79/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0442 - val_acc: 0.9872\n",
      "Epoch 80/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0448 - val_acc: 0.9744\n",
      "Epoch 81/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0442 - val_acc: 0.9744\n",
      "Epoch 82/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0442 - val_acc: 0.9872\n",
      "Epoch 83/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0449 - val_acc: 0.9872\n",
      "Epoch 84/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0440 - val_acc: 0.9744\n",
      "Epoch 85/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0444 - val_acc: 0.9744\n",
      "Epoch 86/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0442 - val_acc: 0.9744\n",
      "Epoch 87/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0443 - val_acc: 0.9744\n",
      "Epoch 88/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0443 - val_acc: 0.9744\n",
      "Epoch 89/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0445 - val_acc: 0.9744\n",
      "Epoch 90/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0442 - val_acc: 0.9744\n",
      "Epoch 91/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0447 - val_acc: 0.9744\n",
      "Epoch 92/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0444 - val_acc: 0.9744\n",
      "Epoch 93/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0442 - val_acc: 0.9744\n",
      "Epoch 94/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0445 - val_acc: 0.9744\n",
      "Epoch 95/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0445 - val_acc: 0.9744\n",
      "Epoch 96/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0444 - val_acc: 0.9744\n",
      "Epoch 97/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0444 - val_acc: 0.9744\n",
      "Epoch 98/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0441 - val_acc: 0.9744\n",
      "Epoch 99/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0445 - val_acc: 0.9744\n",
      "Epoch 100/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0444 - val_acc: 0.9744\n",
      "78/78 [==============================] - 0s 358us/step\n",
      "Score for fold 2: loss of 1.044326744018457; acc of 97.43589743589743%\n",
      "Train on 702 samples, validate on 78 samples\n",
      "Epoch 1/100\n",
      "702/702 [==============================] - 2s 3ms/step - loss: 2.3111 - acc: 0.3006 - val_loss: 1.9852 - val_acc: 0.4359\n",
      "Epoch 2/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.7314 - acc: 0.6296 - val_loss: 1.8181 - val_acc: 0.4744\n",
      "Epoch 3/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.4303 - acc: 0.7949 - val_loss: 1.3449 - val_acc: 0.7949\n",
      "Epoch 4/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.2684 - acc: 0.8490 - val_loss: 1.2422 - val_acc: 0.8462\n",
      "Epoch 5/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.2233 - acc: 0.8789 - val_loss: 1.1636 - val_acc: 0.8333\n",
      "Epoch 6/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1483 - acc: 0.9060 - val_loss: 1.1588 - val_acc: 0.9231\n",
      "Epoch 7/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1343 - acc: 0.9330 - val_loss: 1.1743 - val_acc: 0.8462\n",
      "Epoch 8/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1118 - acc: 0.9174 - val_loss: 1.1815 - val_acc: 0.7949\n",
      "Epoch 9/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1012 - acc: 0.9501 - val_loss: 1.1066 - val_acc: 0.9487\n",
      "Epoch 10/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0824 - acc: 0.9715 - val_loss: 1.4854 - val_acc: 0.6538\n",
      "Epoch 11/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0977 - acc: 0.9601 - val_loss: 1.0873 - val_acc: 0.9103\n",
      "Epoch 12/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0807 - acc: 0.9573 - val_loss: 1.1139 - val_acc: 0.8718\n",
      "Epoch 13/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0755 - acc: 0.9715 - val_loss: 1.0943 - val_acc: 0.9359\n",
      "Epoch 14/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0626 - acc: 0.9886 - val_loss: 1.0817 - val_acc: 0.9615\n",
      "Epoch 15/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0641 - acc: 0.9786 - val_loss: 1.1147 - val_acc: 0.8974\n",
      "Epoch 16/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0618 - acc: 0.9815 - val_loss: 1.0849 - val_acc: 0.9487\n",
      "Epoch 17/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0604 - acc: 0.9815 - val_loss: 1.0750 - val_acc: 0.9872\n",
      "Epoch 18/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0723 - acc: 0.9815 - val_loss: 1.0729 - val_acc: 0.9615\n",
      "Epoch 19/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0542 - acc: 0.9900 - val_loss: 1.0739 - val_acc: 0.9615\n",
      "Epoch 20/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0530 - acc: 0.9886 - val_loss: 1.0717 - val_acc: 0.9872\n",
      "Epoch 21/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0555 - acc: 0.9858 - val_loss: 1.0647 - val_acc: 0.9744\n",
      "Epoch 22/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0520 - acc: 0.9872 - val_loss: 1.0674 - val_acc: 0.9487\n",
      "Epoch 23/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0556 - acc: 0.9886 - val_loss: 1.1273 - val_acc: 0.9744\n",
      "Epoch 24/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 1.0479 - acc: 0.9957 - val_loss: 1.0600 - val_acc: 0.9615\n",
      "Epoch 25/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0467 - acc: 0.9929 - val_loss: 1.0586 - val_acc: 0.9744\n",
      "Epoch 26/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0494 - acc: 0.9915 - val_loss: 1.0974 - val_acc: 0.9615\n",
      "Epoch 27/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0460 - acc: 0.9957 - val_loss: 1.0669 - val_acc: 0.9615\n",
      "Epoch 28/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0482 - acc: 0.9900 - val_loss: 1.1343 - val_acc: 0.9615\n",
      "Epoch 29/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0452 - acc: 0.9957 - val_loss: 1.0931 - val_acc: 0.9872\n",
      "Epoch 30/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0467 - acc: 0.9929 - val_loss: 1.0555 - val_acc: 1.0000\n",
      "Epoch 31/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0439 - acc: 0.9986 - val_loss: 1.0533 - val_acc: 1.0000\n",
      "Epoch 32/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0434 - acc: 0.9986 - val_loss: 1.0767 - val_acc: 0.9744\n",
      "Epoch 33/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0520 - acc: 0.9957 - val_loss: 1.0525 - val_acc: 0.9872\n",
      "Epoch 34/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0448 - acc: 0.9972 - val_loss: 1.0748 - val_acc: 0.9744\n",
      "Epoch 35/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0424 - acc: 1.0000 - val_loss: 1.0511 - val_acc: 0.9872\n",
      "Epoch 36/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0448 - acc: 0.9943 - val_loss: 1.0506 - val_acc: 1.0000\n",
      "Epoch 37/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0423 - acc: 1.0000 - val_loss: 1.1476 - val_acc: 0.9872\n",
      "Epoch 38/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0421 - acc: 1.0000 - val_loss: 1.0485 - val_acc: 1.0000\n",
      "Epoch 39/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0436 - acc: 0.9957 - val_loss: 1.0532 - val_acc: 0.9744\n",
      "Epoch 40/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0420 - acc: 0.9986 - val_loss: 1.0517 - val_acc: 1.0000\n",
      "Epoch 41/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0422 - acc: 0.9986 - val_loss: 1.0492 - val_acc: 1.0000\n",
      "Epoch 42/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0412 - acc: 1.0000 - val_loss: 1.0788 - val_acc: 0.9872\n",
      "Epoch 43/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0412 - acc: 1.0000 - val_loss: 1.0452 - val_acc: 1.0000\n",
      "Epoch 44/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0416 - acc: 0.9986 - val_loss: 1.0608 - val_acc: 0.9744\n",
      "Epoch 45/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0408 - acc: 1.0000 - val_loss: 1.0501 - val_acc: 0.9872\n",
      "Epoch 46/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0414 - acc: 0.9986 - val_loss: 1.0590 - val_acc: 0.9872\n",
      "Epoch 47/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0411 - acc: 1.0000 - val_loss: 1.1095 - val_acc: 0.9744\n",
      "Epoch 48/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0409 - acc: 1.0000 - val_loss: 1.0728 - val_acc: 0.9872\n",
      "Epoch 49/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0407 - acc: 1.0000 - val_loss: 1.0519 - val_acc: 1.0000\n",
      "Epoch 50/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0407 - acc: 1.0000 - val_loss: 1.0633 - val_acc: 0.9872\n",
      "Epoch 51/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0990 - val_acc: 0.9744\n",
      "Epoch 52/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0652 - val_acc: 0.9744\n",
      "Epoch 53/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0407 - acc: 1.0000 - val_loss: 1.0679 - val_acc: 0.9872\n",
      "Epoch 54/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0409 - acc: 0.9986 - val_loss: 1.0999 - val_acc: 0.9744\n",
      "Epoch 55/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0965 - val_acc: 0.9872\n",
      "Epoch 56/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0455 - val_acc: 1.0000\n",
      "Epoch 57/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0478 - val_acc: 1.0000\n",
      "Epoch 58/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0479 - val_acc: 1.0000\n",
      "Epoch 59/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0454 - val_acc: 1.0000\n",
      "Epoch 60/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0462 - val_acc: 1.0000\n",
      "Epoch 61/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0811 - val_acc: 0.9744\n",
      "Epoch 62/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0464 - val_acc: 1.0000\n",
      "Epoch 63/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0479 - val_acc: 1.0000\n",
      "Epoch 64/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0685 - val_acc: 0.9744\n",
      "Epoch 65/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0479 - val_acc: 1.0000\n",
      "Epoch 66/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0567 - val_acc: 1.0000\n",
      "Epoch 67/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0594 - val_acc: 0.9872\n",
      "Epoch 68/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0747 - val_acc: 0.9872\n",
      "Epoch 69/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0533 - val_acc: 0.9872\n",
      "Epoch 70/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0918 - val_acc: 0.9872\n",
      "Epoch 71/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0563 - val_acc: 1.0000\n",
      "Epoch 72/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0739 - val_acc: 0.9872\n",
      "Epoch 73/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0643 - val_acc: 0.9872\n",
      "Epoch 74/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0590 - val_acc: 0.9872\n",
      "Epoch 75/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0472 - val_acc: 1.0000\n",
      "Epoch 76/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0450 - val_acc: 1.0000\n",
      "Epoch 77/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0550 - val_acc: 1.0000\n",
      "Epoch 78/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0522 - val_acc: 1.0000\n",
      "Epoch 79/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0444 - val_acc: 1.0000\n",
      "Epoch 80/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0446 - val_acc: 1.0000\n",
      "Epoch 81/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0478 - val_acc: 1.0000\n",
      "Epoch 82/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0442 - val_acc: 1.0000\n",
      "Epoch 83/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0451 - val_acc: 1.0000\n",
      "Epoch 84/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0440 - val_acc: 1.0000\n",
      "Epoch 85/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0456 - val_acc: 1.0000\n",
      "Epoch 86/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0444 - val_acc: 1.0000\n",
      "Epoch 87/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0497 - val_acc: 1.0000\n",
      "Epoch 88/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0451 - val_acc: 1.0000\n",
      "Epoch 89/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0443 - val_acc: 1.0000\n",
      "Epoch 90/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0451 - val_acc: 1.0000\n",
      "Epoch 91/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0447 - val_acc: 1.0000\n",
      "Epoch 92/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0445 - val_acc: 1.0000\n",
      "Epoch 93/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0449 - val_acc: 1.0000\n",
      "Epoch 94/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0452 - val_acc: 1.0000\n",
      "Epoch 95/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0453 - val_acc: 1.0000\n",
      "Epoch 96/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0453 - val_acc: 1.0000\n",
      "Epoch 97/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0460 - val_acc: 1.0000\n",
      "Epoch 98/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0463 - val_acc: 1.0000\n",
      "Epoch 99/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0456 - val_acc: 1.0000\n",
      "Epoch 100/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0453 - val_acc: 1.0000\n",
      "78/78 [==============================] - 0s 352us/step\n",
      "Score for fold 3: loss of 1.044636118106353; acc of 100.0%\n",
      "Train on 702 samples, validate on 78 samples\n",
      "Epoch 1/100\n",
      "702/702 [==============================] - 2s 3ms/step - loss: 2.2862 - acc: 0.3134 - val_loss: 1.9579 - val_acc: 0.4487\n",
      "Epoch 2/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.6392 - acc: 0.6795 - val_loss: 1.6075 - val_acc: 0.5513\n",
      "Epoch 3/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.3763 - acc: 0.8205 - val_loss: 1.2517 - val_acc: 0.8333\n",
      "Epoch 4/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.2380 - acc: 0.8875 - val_loss: 1.1490 - val_acc: 0.9359\n",
      "Epoch 5/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1967 - acc: 0.8917 - val_loss: 1.1771 - val_acc: 0.8846\n",
      "Epoch 6/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1342 - acc: 0.9217 - val_loss: 1.3639 - val_acc: 0.7436\n",
      "Epoch 7/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1246 - acc: 0.9402 - val_loss: 1.4247 - val_acc: 0.7179\n",
      "Epoch 8/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1238 - acc: 0.9359 - val_loss: 1.1715 - val_acc: 0.8718\n",
      "Epoch 9/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0961 - acc: 0.9558 - val_loss: 1.0962 - val_acc: 0.9231\n",
      "Epoch 10/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0686 - acc: 0.9772 - val_loss: 1.0917 - val_acc: 0.9615\n",
      "Epoch 11/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0854 - acc: 0.9644 - val_loss: 1.0706 - val_acc: 0.9615\n",
      "Epoch 12/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0663 - acc: 0.9729 - val_loss: 1.0889 - val_acc: 0.9487\n",
      "Epoch 13/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0670 - acc: 0.9786 - val_loss: 1.0942 - val_acc: 0.9615\n",
      "Epoch 14/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0652 - acc: 0.9786 - val_loss: 1.0689 - val_acc: 0.9615\n",
      "Epoch 15/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0719 - acc: 0.9729 - val_loss: 1.0575 - val_acc: 0.9744\n",
      "Epoch 16/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0556 - acc: 0.9900 - val_loss: 1.0919 - val_acc: 0.8974\n",
      "Epoch 17/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0555 - acc: 0.9915 - val_loss: 1.0544 - val_acc: 0.9872\n",
      "Epoch 18/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0565 - acc: 0.9915 - val_loss: 1.0736 - val_acc: 0.9615\n",
      "Epoch 19/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0669 - acc: 0.9672 - val_loss: 1.0750 - val_acc: 0.9359\n",
      "Epoch 20/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0565 - acc: 0.9886 - val_loss: 1.0557 - val_acc: 0.9744\n",
      "Epoch 21/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0474 - acc: 0.9986 - val_loss: 1.0617 - val_acc: 0.9744\n",
      "Epoch 22/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0544 - acc: 0.9900 - val_loss: 1.0544 - val_acc: 0.9872\n",
      "Epoch 23/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0860 - acc: 0.9786 - val_loss: 1.0549 - val_acc: 0.9872\n",
      "Epoch 24/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 1.0493 - acc: 0.9929 - val_loss: 1.0556 - val_acc: 0.9872\n",
      "Epoch 25/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0455 - acc: 0.9972 - val_loss: 1.0638 - val_acc: 0.9744\n",
      "Epoch 26/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0463 - acc: 0.9943 - val_loss: 1.0514 - val_acc: 0.9872\n",
      "Epoch 27/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0456 - acc: 0.9957 - val_loss: 1.0526 - val_acc: 0.9872\n",
      "Epoch 28/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0489 - acc: 0.9900 - val_loss: 1.0485 - val_acc: 1.0000\n",
      "Epoch 29/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0435 - acc: 0.9986 - val_loss: 1.0527 - val_acc: 0.9872\n",
      "Epoch 30/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0456 - acc: 0.9943 - val_loss: 1.0657 - val_acc: 0.9744\n",
      "Epoch 31/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0484 - acc: 0.9957 - val_loss: 1.0735 - val_acc: 0.9744\n",
      "Epoch 32/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0436 - acc: 0.9986 - val_loss: 1.0556 - val_acc: 0.9872\n",
      "Epoch 33/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0443 - acc: 0.9929 - val_loss: 1.0558 - val_acc: 0.9744\n",
      "Epoch 34/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0439 - acc: 0.9986 - val_loss: 1.0509 - val_acc: 0.9872\n",
      "Epoch 35/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0426 - acc: 1.0000 - val_loss: 1.0551 - val_acc: 0.9615\n",
      "Epoch 36/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0422 - acc: 1.0000 - val_loss: 1.0604 - val_acc: 0.9615\n",
      "Epoch 37/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0447 - acc: 0.9943 - val_loss: 1.0475 - val_acc: 1.0000\n",
      "Epoch 38/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0414 - acc: 1.0000 - val_loss: 1.0490 - val_acc: 0.9872\n",
      "Epoch 39/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0418 - acc: 1.0000 - val_loss: 1.0490 - val_acc: 0.9872\n",
      "Epoch 40/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0417 - acc: 0.9986 - val_loss: 1.0516 - val_acc: 0.9744\n",
      "Epoch 41/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0426 - acc: 0.9972 - val_loss: 1.0502 - val_acc: 0.9872\n",
      "Epoch 42/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0410 - acc: 1.0000 - val_loss: 1.0478 - val_acc: 0.9872\n",
      "Epoch 43/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0409 - acc: 1.0000 - val_loss: 1.0482 - val_acc: 0.9872\n",
      "Epoch 44/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0408 - acc: 1.0000 - val_loss: 1.0472 - val_acc: 0.9872\n",
      "Epoch 45/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0410 - acc: 1.0000 - val_loss: 1.0493 - val_acc: 0.9872\n",
      "Epoch 46/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0412 - acc: 1.0000 - val_loss: 1.0502 - val_acc: 0.9872\n",
      "Epoch 47/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0407 - acc: 1.0000 - val_loss: 1.0488 - val_acc: 0.9872\n",
      "Epoch 48/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0487 - val_acc: 0.9872\n",
      "Epoch 49/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0439 - val_acc: 1.0000\n",
      "Epoch 50/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0407 - acc: 1.0000 - val_loss: 1.0489 - val_acc: 0.9872\n",
      "Epoch 51/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0491 - val_acc: 0.9872\n",
      "Epoch 52/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0529 - val_acc: 0.9872\n",
      "Epoch 53/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0492 - val_acc: 0.9872\n",
      "Epoch 54/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0529 - val_acc: 0.9872\n",
      "Epoch 55/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0408 - acc: 0.9986 - val_loss: 1.0439 - val_acc: 0.9872\n",
      "Epoch 56/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0444 - val_acc: 0.9872\n",
      "Epoch 57/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0458 - val_acc: 0.9872\n",
      "Epoch 58/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0442 - val_acc: 0.9872\n",
      "Epoch 59/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0431 - val_acc: 0.9872\n",
      "Epoch 60/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0500 - val_acc: 0.9872\n",
      "Epoch 61/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0492 - val_acc: 0.9872\n",
      "Epoch 62/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0486 - val_acc: 0.9872\n",
      "Epoch 63/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0484 - val_acc: 0.9872\n",
      "Epoch 64/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0485 - val_acc: 0.9872\n",
      "Epoch 65/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0453 - val_acc: 0.9872\n",
      "Epoch 66/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0458 - val_acc: 0.9872\n",
      "Epoch 67/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0466 - val_acc: 0.9872\n",
      "Epoch 68/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0437 - val_acc: 0.9872\n",
      "Epoch 69/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0480 - val_acc: 0.9872\n",
      "Epoch 70/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0488 - val_acc: 0.9872\n",
      "Epoch 71/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0481 - val_acc: 0.9872\n",
      "Epoch 72/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0480 - val_acc: 0.9872\n",
      "Epoch 73/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0494 - val_acc: 0.9872\n",
      "Epoch 74/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0492 - val_acc: 0.9872\n",
      "Epoch 75/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0487 - val_acc: 0.9872\n",
      "Epoch 76/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0482 - val_acc: 0.9872\n",
      "Epoch 77/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0465 - val_acc: 0.9872\n",
      "Epoch 78/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0471 - val_acc: 0.9872\n",
      "Epoch 79/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0475 - val_acc: 0.9872\n",
      "Epoch 80/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0465 - val_acc: 0.9872\n",
      "Epoch 81/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0484 - val_acc: 0.9872\n",
      "Epoch 82/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0474 - val_acc: 0.9872\n",
      "Epoch 83/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0480 - val_acc: 0.9872\n",
      "Epoch 84/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0480 - val_acc: 0.9872\n",
      "Epoch 85/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0473 - val_acc: 0.9872\n",
      "Epoch 86/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0471 - val_acc: 0.9872\n",
      "Epoch 87/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0468 - val_acc: 0.9872\n",
      "Epoch 88/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0455 - val_acc: 0.9872\n",
      "Epoch 89/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0475 - val_acc: 0.9872\n",
      "Epoch 90/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0475 - val_acc: 0.9872\n",
      "Epoch 91/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0463 - val_acc: 0.9872\n",
      "Epoch 92/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0473 - val_acc: 0.9872\n",
      "Epoch 93/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0469 - val_acc: 0.9872\n",
      "Epoch 94/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0466 - val_acc: 0.9872\n",
      "Epoch 95/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0465 - val_acc: 0.9872\n",
      "Epoch 96/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0460 - val_acc: 0.9872\n",
      "Epoch 97/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0461 - val_acc: 0.9872\n",
      "Epoch 98/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0462 - val_acc: 0.9872\n",
      "Epoch 99/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0458 - val_acc: 0.9872\n",
      "Epoch 100/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0468 - val_acc: 0.9872\n",
      "78/78 [==============================] - 0s 311us/step\n",
      "Score for fold 4: loss of 1.0480719529665434; acc of 98.71794825945145%\n",
      "Train on 702 samples, validate on 78 samples\n",
      "Epoch 1/100\n",
      "702/702 [==============================] - 2s 3ms/step - loss: 2.2877 - acc: 0.3105 - val_loss: 1.9655 - val_acc: 0.4359\n",
      "Epoch 2/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.6574 - acc: 0.6425 - val_loss: 1.5336 - val_acc: 0.6410\n",
      "Epoch 3/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.3735 - acc: 0.8177 - val_loss: 1.3790 - val_acc: 0.6923\n",
      "Epoch 4/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.2516 - acc: 0.8148 - val_loss: 1.3350 - val_acc: 0.7949\n",
      "Epoch 5/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1757 - acc: 0.8903 - val_loss: 1.2742 - val_acc: 0.8846\n",
      "Epoch 6/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1655 - acc: 0.9131 - val_loss: 1.1333 - val_acc: 0.9231\n",
      "Epoch 7/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1050 - acc: 0.9516 - val_loss: 1.1213 - val_acc: 0.9103\n",
      "Epoch 8/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1216 - acc: 0.9145 - val_loss: 1.1131 - val_acc: 0.9103\n",
      "Epoch 9/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0974 - acc: 0.9630 - val_loss: 1.1470 - val_acc: 0.8718\n",
      "Epoch 10/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0763 - acc: 0.9701 - val_loss: 1.0809 - val_acc: 0.9872\n",
      "Epoch 11/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1065 - acc: 0.9558 - val_loss: 1.1426 - val_acc: 0.9103\n",
      "Epoch 12/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0665 - acc: 0.9786 - val_loss: 1.0849 - val_acc: 0.9615\n",
      "Epoch 13/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0797 - acc: 0.9630 - val_loss: 1.1051 - val_acc: 0.9231\n",
      "Epoch 14/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0617 - acc: 0.9815 - val_loss: 1.0790 - val_acc: 0.9744\n",
      "Epoch 15/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0612 - acc: 0.9772 - val_loss: 1.1077 - val_acc: 0.9103\n",
      "Epoch 16/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0644 - acc: 0.9786 - val_loss: 1.0665 - val_acc: 0.9872\n",
      "Epoch 17/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 1.0763 - acc: 0.9801 - val_loss: 1.0589 - val_acc: 0.9872\n",
      "Epoch 18/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0490 - acc: 0.9957 - val_loss: 1.0886 - val_acc: 0.9744\n",
      "Epoch 19/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0501 - acc: 0.9929 - val_loss: 1.0566 - val_acc: 0.9872\n",
      "Epoch 20/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0493 - acc: 0.9943 - val_loss: 1.0549 - val_acc: 0.9872\n",
      "Epoch 21/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0478 - acc: 0.9943 - val_loss: 1.0536 - val_acc: 1.0000\n",
      "Epoch 22/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0486 - acc: 0.9929 - val_loss: 1.0545 - val_acc: 0.9872\n",
      "Epoch 23/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0552 - acc: 0.9929 - val_loss: 1.0530 - val_acc: 0.9872\n",
      "Epoch 24/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0444 - acc: 0.9986 - val_loss: 1.0513 - val_acc: 1.0000\n",
      "Epoch 25/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0455 - acc: 0.9972 - val_loss: 1.0473 - val_acc: 1.0000\n",
      "Epoch 26/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0560 - acc: 0.9900 - val_loss: 1.0499 - val_acc: 0.9872\n",
      "Epoch 27/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0450 - acc: 0.9986 - val_loss: 1.0552 - val_acc: 0.9872\n",
      "Epoch 28/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0445 - acc: 0.9972 - val_loss: 1.0506 - val_acc: 1.0000\n",
      "Epoch 29/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0439 - acc: 0.9972 - val_loss: 1.0480 - val_acc: 1.0000\n",
      "Epoch 30/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0435 - acc: 0.9986 - val_loss: 1.0529 - val_acc: 0.9872\n",
      "Epoch 31/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0424 - acc: 0.9986 - val_loss: 1.0612 - val_acc: 0.9359\n",
      "Epoch 32/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0427 - acc: 1.0000 - val_loss: 1.0524 - val_acc: 0.9872\n",
      "Epoch 33/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0427 - acc: 0.9986 - val_loss: 1.0529 - val_acc: 1.0000\n",
      "Epoch 34/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0416 - acc: 1.0000 - val_loss: 1.0484 - val_acc: 1.0000\n",
      "Epoch 35/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0413 - acc: 1.0000 - val_loss: 1.0480 - val_acc: 1.0000\n",
      "Epoch 36/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0430 - acc: 0.9986 - val_loss: 1.0468 - val_acc: 1.0000\n",
      "Epoch 37/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0410 - acc: 1.0000 - val_loss: 1.0452 - val_acc: 1.0000\n",
      "Epoch 38/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0417 - acc: 0.9986 - val_loss: 1.0451 - val_acc: 1.0000\n",
      "Epoch 39/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0411 - acc: 1.0000 - val_loss: 1.0515 - val_acc: 0.9872\n",
      "Epoch 40/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0416 - acc: 0.9986 - val_loss: 1.0465 - val_acc: 1.0000\n",
      "Epoch 41/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0408 - acc: 1.0000 - val_loss: 1.0519 - val_acc: 0.9744\n",
      "Epoch 42/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0408 - acc: 1.0000 - val_loss: 1.0501 - val_acc: 0.9872\n",
      "Epoch 43/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0408 - acc: 1.0000 - val_loss: 1.0458 - val_acc: 1.0000\n",
      "Epoch 44/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0463 - val_acc: 0.9872\n",
      "Epoch 45/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0411 - acc: 1.0000 - val_loss: 1.0440 - val_acc: 1.0000\n",
      "Epoch 46/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0441 - val_acc: 1.0000\n",
      "Epoch 47/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0444 - val_acc: 1.0000\n",
      "Epoch 48/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0445 - val_acc: 1.0000\n",
      "Epoch 49/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0440 - val_acc: 1.0000\n",
      "Epoch 50/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0439 - val_acc: 1.0000\n",
      "Epoch 51/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0440 - val_acc: 1.0000\n",
      "Epoch 52/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0446 - val_acc: 1.0000\n",
      "Epoch 53/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0435 - val_acc: 1.0000\n",
      "Epoch 54/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0431 - val_acc: 1.0000\n",
      "Epoch 55/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0434 - val_acc: 1.0000\n",
      "Epoch 56/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0433 - val_acc: 1.0000\n",
      "Epoch 57/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0439 - val_acc: 1.0000\n",
      "Epoch 58/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0433 - val_acc: 1.0000\n",
      "Epoch 59/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0428 - val_acc: 1.0000\n",
      "Epoch 60/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0432 - val_acc: 1.0000\n",
      "Epoch 61/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0435 - val_acc: 1.0000\n",
      "Epoch 62/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0433 - val_acc: 1.0000\n",
      "Epoch 63/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0434 - val_acc: 1.0000\n",
      "Epoch 64/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0435 - val_acc: 1.0000\n",
      "Epoch 65/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0429 - val_acc: 1.0000\n",
      "Epoch 66/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0429 - val_acc: 1.0000\n",
      "Epoch 67/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0434 - val_acc: 1.0000\n",
      "Epoch 68/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0435 - val_acc: 1.0000\n",
      "Epoch 69/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0433 - val_acc: 1.0000\n",
      "Epoch 70/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0438 - val_acc: 1.0000\n",
      "Epoch 71/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0433 - val_acc: 1.0000\n",
      "Epoch 72/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0433 - val_acc: 1.0000\n",
      "Epoch 73/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0429 - val_acc: 1.0000\n",
      "Epoch 74/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0429 - val_acc: 1.0000\n",
      "Epoch 75/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0431 - val_acc: 1.0000\n",
      "Epoch 76/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0430 - val_acc: 1.0000\n",
      "Epoch 77/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0433 - val_acc: 1.0000\n",
      "Epoch 78/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0434 - val_acc: 1.0000\n",
      "Epoch 79/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0431 - val_acc: 1.0000\n",
      "Epoch 80/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0424 - val_acc: 1.0000\n",
      "Epoch 81/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0430 - val_acc: 1.0000\n",
      "Epoch 82/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0436 - val_acc: 1.0000\n",
      "Epoch 83/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0430 - val_acc: 1.0000\n",
      "Epoch 84/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0429 - val_acc: 1.0000\n",
      "Epoch 85/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0431 - val_acc: 1.0000\n",
      "Epoch 86/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0430 - val_acc: 1.0000\n",
      "Epoch 87/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0428 - val_acc: 1.0000\n",
      "Epoch 88/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0429 - val_acc: 1.0000\n",
      "Epoch 89/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0430 - val_acc: 1.0000\n",
      "Epoch 90/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0428 - val_acc: 1.0000\n",
      "Epoch 91/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0428 - val_acc: 1.0000\n",
      "Epoch 92/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0428 - val_acc: 1.0000\n",
      "Epoch 93/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0430 - val_acc: 1.0000\n",
      "Epoch 94/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0428 - val_acc: 1.0000\n",
      "Epoch 95/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0430 - val_acc: 1.0000\n",
      "Epoch 96/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0427 - val_acc: 1.0000\n",
      "Epoch 97/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0427 - val_acc: 1.0000\n",
      "Epoch 98/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0429 - val_acc: 1.0000\n",
      "Epoch 99/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0427 - val_acc: 1.0000\n",
      "Epoch 100/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0429 - val_acc: 1.0000\n",
      "78/78 [==============================] - 0s 301us/step\n",
      "Score for fold 5: loss of 1.0424160254307282; acc of 100.0%\n",
      "Train on 702 samples, validate on 78 samples\n",
      "Epoch 1/100\n",
      "702/702 [==============================] - 2s 3ms/step - loss: 2.2320 - acc: 0.3476 - val_loss: 2.0153 - val_acc: 0.4231\n",
      "Epoch 2/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.6442 - acc: 0.6652 - val_loss: 1.5235 - val_acc: 0.6795\n",
      "Epoch 3/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.3636 - acc: 0.8020 - val_loss: 1.3000 - val_acc: 0.8205\n",
      "Epoch 4/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.2376 - acc: 0.8761 - val_loss: 1.2205 - val_acc: 0.8974\n",
      "Epoch 5/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.2074 - acc: 0.8746 - val_loss: 1.1722 - val_acc: 0.9359\n",
      "Epoch 6/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1278 - acc: 0.9245 - val_loss: 1.1257 - val_acc: 0.8974\n",
      "Epoch 7/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1300 - acc: 0.9017 - val_loss: 1.2073 - val_acc: 0.8718\n",
      "Epoch 8/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1196 - acc: 0.9288 - val_loss: 1.1243 - val_acc: 0.9231\n",
      "Epoch 9/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0872 - acc: 0.9658 - val_loss: 1.0958 - val_acc: 0.8974\n",
      "Epoch 10/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0850 - acc: 0.9630 - val_loss: 1.0950 - val_acc: 0.9231\n",
      "Epoch 11/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0909 - acc: 0.9501 - val_loss: 1.0835 - val_acc: 0.9615\n",
      "Epoch 12/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0653 - acc: 0.9843 - val_loss: 1.0818 - val_acc: 0.9615\n",
      "Epoch 13/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0680 - acc: 0.9772 - val_loss: 1.0612 - val_acc: 0.9872\n",
      "Epoch 14/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0753 - acc: 0.9744 - val_loss: 1.0758 - val_acc: 0.9487\n",
      "Epoch 15/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0604 - acc: 0.9801 - val_loss: 1.0594 - val_acc: 0.9744\n",
      "Epoch 16/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0928 - acc: 0.9715 - val_loss: 1.0596 - val_acc: 0.9744\n",
      "Epoch 17/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0520 - acc: 0.9900 - val_loss: 1.0538 - val_acc: 0.9872\n",
      "Epoch 18/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0608 - acc: 0.9744 - val_loss: 1.0522 - val_acc: 0.9872\n",
      "Epoch 19/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0526 - acc: 0.9915 - val_loss: 1.0539 - val_acc: 0.9872\n",
      "Epoch 20/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0663 - acc: 0.9829 - val_loss: 1.0521 - val_acc: 1.0000\n",
      "Epoch 21/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0494 - acc: 0.9943 - val_loss: 1.1024 - val_acc: 0.9231\n",
      "Epoch 22/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0564 - acc: 0.9815 - val_loss: 1.0546 - val_acc: 0.9744\n",
      "Epoch 23/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0465 - acc: 0.9972 - val_loss: 1.0494 - val_acc: 0.9872\n",
      "Epoch 24/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0695 - acc: 0.9658 - val_loss: 1.0559 - val_acc: 0.9872\n",
      "Epoch 25/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0490 - acc: 0.9929 - val_loss: 1.0996 - val_acc: 0.9359\n",
      "Epoch 26/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0610 - acc: 0.9900 - val_loss: 1.1600 - val_acc: 0.8846\n",
      "Epoch 27/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 1.0531 - acc: 0.9872 - val_loss: 1.0507 - val_acc: 0.9872\n",
      "Epoch 28/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0464 - acc: 0.9943 - val_loss: 1.0539 - val_acc: 0.9744\n",
      "Epoch 29/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0463 - acc: 0.9972 - val_loss: 1.0559 - val_acc: 1.0000\n",
      "Epoch 30/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0447 - acc: 0.9986 - val_loss: 1.0510 - val_acc: 0.9872\n",
      "Epoch 31/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0452 - acc: 0.9972 - val_loss: 1.0572 - val_acc: 0.9615\n",
      "Epoch 32/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0448 - acc: 0.9957 - val_loss: 1.0490 - val_acc: 0.9872\n",
      "Epoch 33/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0436 - acc: 0.9986 - val_loss: 1.0497 - val_acc: 0.9872\n",
      "Epoch 34/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0429 - acc: 0.9986 - val_loss: 1.0467 - val_acc: 1.0000\n",
      "Epoch 35/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0424 - acc: 0.9986 - val_loss: 1.0536 - val_acc: 0.9615\n",
      "Epoch 36/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0448 - acc: 0.9943 - val_loss: 1.0494 - val_acc: 0.9872\n",
      "Epoch 37/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0418 - acc: 0.9986 - val_loss: 1.0479 - val_acc: 0.9872\n",
      "Epoch 38/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0417 - acc: 1.0000 - val_loss: 1.0480 - val_acc: 0.9744\n",
      "Epoch 39/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0416 - acc: 1.0000 - val_loss: 1.0486 - val_acc: 0.9872\n",
      "Epoch 40/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0414 - acc: 1.0000 - val_loss: 1.0490 - val_acc: 0.9872\n",
      "Epoch 41/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0414 - acc: 1.0000 - val_loss: 1.0488 - val_acc: 0.9872\n",
      "Epoch 42/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0414 - acc: 1.0000 - val_loss: 1.0479 - val_acc: 0.9872\n",
      "Epoch 43/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0410 - acc: 1.0000 - val_loss: 1.0479 - val_acc: 0.9872\n",
      "Epoch 44/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0416 - acc: 1.0000 - val_loss: 1.0477 - val_acc: 0.9872\n",
      "Epoch 45/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0409 - acc: 1.0000 - val_loss: 1.0481 - val_acc: 0.9872\n",
      "Epoch 46/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0408 - acc: 1.0000 - val_loss: 1.0494 - val_acc: 0.9872\n",
      "Epoch 47/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0407 - acc: 1.0000 - val_loss: 1.0500 - val_acc: 0.9872\n",
      "Epoch 48/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0408 - acc: 1.0000 - val_loss: 1.0502 - val_acc: 0.9872\n",
      "Epoch 49/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0408 - acc: 1.0000 - val_loss: 1.0489 - val_acc: 0.9744\n",
      "Epoch 50/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0409 - acc: 1.0000 - val_loss: 1.0491 - val_acc: 0.9872\n",
      "Epoch 51/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0485 - val_acc: 0.9872\n",
      "Epoch 52/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0485 - val_acc: 0.9872\n",
      "Epoch 53/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0467 - val_acc: 0.9872\n",
      "Epoch 54/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0477 - val_acc: 0.9872\n",
      "Epoch 55/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0471 - val_acc: 0.9872\n",
      "Epoch 56/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0473 - val_acc: 0.9872\n",
      "Epoch 57/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0466 - val_acc: 0.9872\n",
      "Epoch 58/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0472 - val_acc: 0.9872\n",
      "Epoch 59/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0477 - val_acc: 0.9872\n",
      "Epoch 60/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0476 - val_acc: 0.9872\n",
      "Epoch 61/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0470 - val_acc: 0.9872\n",
      "Epoch 62/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0472 - val_acc: 0.9872\n",
      "Epoch 63/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0468 - val_acc: 0.9872\n",
      "Epoch 64/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0473 - val_acc: 0.9872\n",
      "Epoch 65/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0473 - val_acc: 0.9872\n",
      "Epoch 66/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0471 - val_acc: 0.9872\n",
      "Epoch 67/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0476 - val_acc: 0.9872\n",
      "Epoch 68/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0468 - val_acc: 0.9872\n",
      "Epoch 69/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0468 - val_acc: 0.9872\n",
      "Epoch 70/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0471 - val_acc: 0.9872\n",
      "Epoch 71/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0474 - val_acc: 0.9872\n",
      "Epoch 72/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0483 - val_acc: 0.9872\n",
      "Epoch 73/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0478 - val_acc: 0.9872\n",
      "Epoch 74/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0484 - val_acc: 0.9872\n",
      "Epoch 75/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0473 - val_acc: 0.9872\n",
      "Epoch 76/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0478 - val_acc: 0.9872\n",
      "Epoch 77/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0474 - val_acc: 0.9872\n",
      "Epoch 78/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0474 - val_acc: 0.9872\n",
      "Epoch 79/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0478 - val_acc: 0.9872\n",
      "Epoch 80/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0480 - val_acc: 0.9872\n",
      "Epoch 81/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0481 - val_acc: 0.9872\n",
      "Epoch 82/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0479 - val_acc: 0.9872\n",
      "Epoch 83/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0477 - val_acc: 0.9872\n",
      "Epoch 84/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0480 - val_acc: 0.9872\n",
      "Epoch 85/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0475 - val_acc: 0.9872\n",
      "Epoch 86/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0477 - val_acc: 0.9872\n",
      "Epoch 87/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0478 - val_acc: 0.9872\n",
      "Epoch 88/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0483 - val_acc: 0.9872\n",
      "Epoch 89/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0479 - val_acc: 0.9872\n",
      "Epoch 90/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0482 - val_acc: 0.9872\n",
      "Epoch 91/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0478 - val_acc: 0.9872\n",
      "Epoch 92/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0476 - val_acc: 0.9872\n",
      "Epoch 93/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0478 - val_acc: 0.9872\n",
      "Epoch 94/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0478 - val_acc: 0.9872\n",
      "Epoch 95/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0479 - val_acc: 0.9872\n",
      "Epoch 96/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0481 - val_acc: 0.9872\n",
      "Epoch 97/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0478 - val_acc: 0.9872\n",
      "Epoch 98/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0479 - val_acc: 0.9872\n",
      "Epoch 99/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0478 - val_acc: 0.9872\n",
      "Epoch 100/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0479 - val_acc: 0.9872\n",
      "78/78 [==============================] - 0s 301us/step\n",
      "Score for fold 6: loss of 1.0484067415579772; acc of 98.71794825945145%\n",
      "Train on 702 samples, validate on 78 samples\n",
      "Epoch 1/100\n",
      "702/702 [==============================] - 3s 4ms/step - loss: 2.3132 - acc: 0.3348 - val_loss: 1.8467 - val_acc: 0.5769\n",
      "Epoch 2/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.6496 - acc: 0.6880 - val_loss: 1.4532 - val_acc: 0.7308\n",
      "Epoch 3/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.3544 - acc: 0.8077 - val_loss: 1.5620 - val_acc: 0.7051\n",
      "Epoch 4/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.2553 - acc: 0.8704 - val_loss: 1.2903 - val_acc: 0.7692\n",
      "Epoch 5/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1770 - acc: 0.8917 - val_loss: 1.1831 - val_acc: 0.8590\n",
      "Epoch 6/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1508 - acc: 0.9103 - val_loss: 1.1524 - val_acc: 0.8846\n",
      "Epoch 7/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1006 - acc: 0.9444 - val_loss: 1.1448 - val_acc: 0.8718\n",
      "Epoch 8/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1334 - acc: 0.9245 - val_loss: 1.1156 - val_acc: 0.9231\n",
      "Epoch 9/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0785 - acc: 0.9744 - val_loss: 1.1145 - val_acc: 0.9359\n",
      "Epoch 10/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0752 - acc: 0.9729 - val_loss: 1.0961 - val_acc: 0.9487\n",
      "Epoch 11/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1028 - acc: 0.9501 - val_loss: 1.1366 - val_acc: 0.9359\n",
      "Epoch 12/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0651 - acc: 0.9843 - val_loss: 1.0877 - val_acc: 0.9615\n",
      "Epoch 13/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0711 - acc: 0.9701 - val_loss: 1.3448 - val_acc: 0.7821\n",
      "Epoch 14/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0679 - acc: 0.9786 - val_loss: 1.0948 - val_acc: 0.9615\n",
      "Epoch 15/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0582 - acc: 0.9886 - val_loss: 1.1019 - val_acc: 0.9487\n",
      "Epoch 16/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0733 - acc: 0.9758 - val_loss: 1.1055 - val_acc: 0.9744\n",
      "Epoch 17/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0527 - acc: 0.9972 - val_loss: 1.1121 - val_acc: 0.9744\n",
      "Epoch 18/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0662 - acc: 0.9729 - val_loss: 1.1214 - val_acc: 0.9359\n",
      "Epoch 19/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0592 - acc: 0.9900 - val_loss: 1.0883 - val_acc: 0.9744\n",
      "Epoch 20/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0699 - acc: 0.9758 - val_loss: 1.0855 - val_acc: 0.9615\n",
      "Epoch 21/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0484 - acc: 0.9986 - val_loss: 1.0927 - val_acc: 0.9744\n",
      "Epoch 22/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0523 - acc: 0.9915 - val_loss: 1.1092 - val_acc: 0.9359\n",
      "Epoch 23/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 1.0758 - acc: 0.9786 - val_loss: 1.1007 - val_acc: 0.9615\n",
      "Epoch 24/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0460 - acc: 0.9986 - val_loss: 1.0893 - val_acc: 0.9615\n",
      "Epoch 25/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0513 - acc: 0.9900 - val_loss: 1.1029 - val_acc: 0.9872\n",
      "Epoch 26/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0451 - acc: 0.9986 - val_loss: 1.0899 - val_acc: 0.9744\n",
      "Epoch 27/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0526 - acc: 0.9929 - val_loss: 1.0876 - val_acc: 0.9615\n",
      "Epoch 28/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0444 - acc: 0.9972 - val_loss: 1.0838 - val_acc: 0.9744\n",
      "Epoch 29/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0456 - acc: 0.9972 - val_loss: 1.0590 - val_acc: 0.9872\n",
      "Epoch 30/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0445 - acc: 0.9986 - val_loss: 1.1538 - val_acc: 0.8846\n",
      "Epoch 31/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0474 - acc: 0.9957 - val_loss: 1.0871 - val_acc: 0.9615\n",
      "Epoch 32/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0440 - acc: 0.9986 - val_loss: 1.1025 - val_acc: 0.9872\n",
      "Epoch 33/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0449 - acc: 0.9943 - val_loss: 1.0844 - val_acc: 0.9615\n",
      "Epoch 34/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0425 - acc: 0.9986 - val_loss: 1.0953 - val_acc: 0.9744\n",
      "Epoch 35/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0429 - acc: 0.9986 - val_loss: 1.0981 - val_acc: 0.9744\n",
      "Epoch 36/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0425 - acc: 0.9986 - val_loss: 1.1093 - val_acc: 0.9487\n",
      "Epoch 37/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0419 - acc: 1.0000 - val_loss: 1.2473 - val_acc: 0.8974\n",
      "Epoch 38/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0438 - acc: 0.9986 - val_loss: 1.1095 - val_acc: 0.9487\n",
      "Epoch 39/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0414 - acc: 1.0000 - val_loss: 1.0981 - val_acc: 0.9744\n",
      "Epoch 40/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0412 - acc: 1.0000 - val_loss: 1.1166 - val_acc: 0.9487\n",
      "Epoch 41/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0414 - acc: 1.0000 - val_loss: 1.0844 - val_acc: 0.9872\n",
      "Epoch 42/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0413 - acc: 1.0000 - val_loss: 1.0922 - val_acc: 0.9615\n",
      "Epoch 43/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0417 - acc: 0.9986 - val_loss: 1.1010 - val_acc: 0.9744\n",
      "Epoch 44/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0410 - acc: 1.0000 - val_loss: 1.1086 - val_acc: 0.9615\n",
      "Epoch 45/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0409 - acc: 1.0000 - val_loss: 1.1076 - val_acc: 0.9744\n",
      "Epoch 46/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0407 - acc: 1.0000 - val_loss: 1.1053 - val_acc: 0.9615\n",
      "Epoch 47/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0411 - acc: 1.0000 - val_loss: 1.1170 - val_acc: 0.9744\n",
      "Epoch 48/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0407 - acc: 1.0000 - val_loss: 1.0960 - val_acc: 0.9744\n",
      "Epoch 49/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0984 - val_acc: 0.9872\n",
      "Epoch 50/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0407 - acc: 1.0000 - val_loss: 1.1055 - val_acc: 0.9744\n",
      "Epoch 51/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.1085 - val_acc: 0.9744\n",
      "Epoch 52/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0407 - acc: 1.0000 - val_loss: 1.1062 - val_acc: 0.9872\n",
      "Epoch 53/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.1042 - val_acc: 0.9872\n",
      "Epoch 54/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0965 - val_acc: 0.9872\n",
      "Epoch 55/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.1072 - val_acc: 0.9744\n",
      "Epoch 56/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.1101 - val_acc: 0.9744\n",
      "Epoch 57/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.1087 - val_acc: 0.9872\n",
      "Epoch 58/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.1069 - val_acc: 0.9615\n",
      "Epoch 59/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.1054 - val_acc: 0.9615\n",
      "Epoch 60/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.1081 - val_acc: 0.9615\n",
      "Epoch 61/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.1076 - val_acc: 0.9744\n",
      "Epoch 62/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.1016 - val_acc: 0.9744\n",
      "Epoch 63/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.1048 - val_acc: 0.9744\n",
      "Epoch 64/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.1049 - val_acc: 0.9744\n",
      "Epoch 65/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.1064 - val_acc: 0.9744\n",
      "Epoch 66/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.1062 - val_acc: 0.9744\n",
      "Epoch 67/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.1066 - val_acc: 0.9744\n",
      "Epoch 68/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.1068 - val_acc: 0.9615\n",
      "Epoch 69/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.1074 - val_acc: 0.9744\n",
      "Epoch 70/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.1099 - val_acc: 0.9615\n",
      "Epoch 71/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.1022 - val_acc: 0.9872\n",
      "Epoch 72/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.1117 - val_acc: 0.9615\n",
      "Epoch 73/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.1060 - val_acc: 0.9615\n",
      "Epoch 74/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.1085 - val_acc: 0.9615\n",
      "Epoch 75/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.1080 - val_acc: 0.9615\n",
      "Epoch 76/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.1051 - val_acc: 0.9744\n",
      "Epoch 77/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.1078 - val_acc: 0.9615\n",
      "Epoch 78/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.1059 - val_acc: 0.9744\n",
      "Epoch 79/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.1044 - val_acc: 0.9615\n",
      "Epoch 80/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.1053 - val_acc: 0.9744\n",
      "Epoch 81/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.1062 - val_acc: 0.9744\n",
      "Epoch 82/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.1068 - val_acc: 0.9744\n",
      "Epoch 83/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.1086 - val_acc: 0.9744\n",
      "Epoch 84/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.1084 - val_acc: 0.9744\n",
      "Epoch 85/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.1069 - val_acc: 0.9744\n",
      "Epoch 86/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.1069 - val_acc: 0.9615\n",
      "Epoch 87/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.1045 - val_acc: 0.9744\n",
      "Epoch 88/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.1061 - val_acc: 0.9744\n",
      "Epoch 89/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.1064 - val_acc: 0.9615\n",
      "Epoch 90/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.1056 - val_acc: 0.9744\n",
      "Epoch 91/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.1043 - val_acc: 0.9744\n",
      "Epoch 92/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.1060 - val_acc: 0.9744\n",
      "Epoch 93/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.1042 - val_acc: 0.9744\n",
      "Epoch 94/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.1032 - val_acc: 0.9744\n",
      "Epoch 95/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.1041 - val_acc: 0.9744\n",
      "Epoch 96/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.1040 - val_acc: 0.9744\n",
      "Epoch 97/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.1037 - val_acc: 0.9744\n",
      "Epoch 98/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.1054 - val_acc: 0.9744\n",
      "Epoch 99/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.1042 - val_acc: 0.9744\n",
      "Epoch 100/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.1046 - val_acc: 0.9744\n",
      "78/78 [==============================] - 0s 296us/step\n",
      "Score for fold 7: loss of 1.098954738714756; acc of 97.43589697740018%\n",
      "Train on 702 samples, validate on 78 samples\n",
      "Epoch 1/100\n",
      "702/702 [==============================] - 3s 4ms/step - loss: 2.2291 - acc: 0.3319 - val_loss: 1.8715 - val_acc: 0.4615\n",
      "Epoch 2/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.5860 - acc: 0.6966 - val_loss: 1.5950 - val_acc: 0.7179\n",
      "Epoch 3/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.3918 - acc: 0.7835 - val_loss: 1.2841 - val_acc: 0.8590\n",
      "Epoch 4/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.2575 - acc: 0.8490 - val_loss: 1.2046 - val_acc: 0.8974\n",
      "Epoch 5/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1635 - acc: 0.8989 - val_loss: 1.2442 - val_acc: 0.7564\n",
      "Epoch 6/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1446 - acc: 0.9131 - val_loss: 1.1363 - val_acc: 0.9231\n",
      "Epoch 7/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1143 - acc: 0.9402 - val_loss: 1.1320 - val_acc: 0.8846\n",
      "Epoch 8/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0983 - acc: 0.9473 - val_loss: 1.1723 - val_acc: 0.7564\n",
      "Epoch 9/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0815 - acc: 0.9601 - val_loss: 1.1191 - val_acc: 0.9231\n",
      "Epoch 10/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0750 - acc: 0.9687 - val_loss: 1.1241 - val_acc: 0.8590\n",
      "Epoch 11/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0789 - acc: 0.9587 - val_loss: 1.0856 - val_acc: 0.9744\n",
      "Epoch 12/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0898 - acc: 0.9558 - val_loss: 1.1257 - val_acc: 0.9103\n",
      "Epoch 13/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0726 - acc: 0.9801 - val_loss: 1.1225 - val_acc: 0.8974\n",
      "Epoch 14/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0579 - acc: 0.9872 - val_loss: 1.1267 - val_acc: 0.9103\n",
      "Epoch 15/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0584 - acc: 0.9900 - val_loss: 1.1704 - val_acc: 0.8974\n",
      "Epoch 16/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0611 - acc: 0.9801 - val_loss: 1.0928 - val_acc: 0.9615\n",
      "Epoch 17/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0551 - acc: 0.9929 - val_loss: 1.1147 - val_acc: 0.9231\n",
      "Epoch 18/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 1.0543 - acc: 0.9872 - val_loss: 1.0821 - val_acc: 0.9487\n",
      "Epoch 19/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0507 - acc: 0.9943 - val_loss: 1.0777 - val_acc: 0.9615\n",
      "Epoch 20/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0532 - acc: 0.9915 - val_loss: 1.0762 - val_acc: 0.9744\n",
      "Epoch 21/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0490 - acc: 0.9915 - val_loss: 1.0709 - val_acc: 0.9744\n",
      "Epoch 22/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0500 - acc: 0.9872 - val_loss: 1.0775 - val_acc: 0.9487\n",
      "Epoch 23/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0464 - acc: 0.9972 - val_loss: 1.0778 - val_acc: 0.9615\n",
      "Epoch 24/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0508 - acc: 0.9929 - val_loss: 1.0768 - val_acc: 0.9615\n",
      "Epoch 25/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0441 - acc: 0.9986 - val_loss: 1.0601 - val_acc: 0.9872\n",
      "Epoch 26/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0444 - acc: 0.9972 - val_loss: 1.0680 - val_acc: 0.9744\n",
      "Epoch 27/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0440 - acc: 0.9986 - val_loss: 1.0605 - val_acc: 0.9744\n",
      "Epoch 28/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0430 - acc: 0.9986 - val_loss: 1.0694 - val_acc: 0.9615\n",
      "Epoch 29/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0444 - acc: 0.9957 - val_loss: 1.0703 - val_acc: 0.9615\n",
      "Epoch 30/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0435 - acc: 0.9972 - val_loss: 1.0663 - val_acc: 0.9744\n",
      "Epoch 31/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0424 - acc: 0.9986 - val_loss: 1.0673 - val_acc: 0.9744\n",
      "Epoch 32/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0434 - acc: 0.9986 - val_loss: 1.0644 - val_acc: 0.9744\n",
      "Epoch 33/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0418 - acc: 1.0000 - val_loss: 1.0607 - val_acc: 0.9744\n",
      "Epoch 34/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0421 - acc: 1.0000 - val_loss: 1.0639 - val_acc: 0.9744\n",
      "Epoch 35/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0424 - acc: 1.0000 - val_loss: 1.0610 - val_acc: 0.9744\n",
      "Epoch 36/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0417 - acc: 0.9986 - val_loss: 1.0615 - val_acc: 0.9615\n",
      "Epoch 37/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0411 - acc: 1.0000 - val_loss: 1.0658 - val_acc: 0.9744\n",
      "Epoch 38/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0413 - acc: 1.0000 - val_loss: 1.0634 - val_acc: 0.9744\n",
      "Epoch 39/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0411 - acc: 1.0000 - val_loss: 1.0593 - val_acc: 0.9744\n",
      "Epoch 40/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0412 - acc: 1.0000 - val_loss: 1.0617 - val_acc: 0.9744\n",
      "Epoch 41/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0409 - acc: 1.0000 - val_loss: 1.0616 - val_acc: 0.9615\n",
      "Epoch 42/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0408 - acc: 1.0000 - val_loss: 1.0636 - val_acc: 0.9615\n",
      "Epoch 43/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0409 - acc: 1.0000 - val_loss: 1.0594 - val_acc: 0.9872\n",
      "Epoch 44/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0409 - acc: 1.0000 - val_loss: 1.0621 - val_acc: 0.9744\n",
      "Epoch 45/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0407 - acc: 1.0000 - val_loss: 1.0705 - val_acc: 0.9744\n",
      "Epoch 46/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0605 - val_acc: 0.9744\n",
      "Epoch 47/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0408 - acc: 1.0000 - val_loss: 1.0615 - val_acc: 0.9744\n",
      "Epoch 48/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0633 - val_acc: 0.9744\n",
      "Epoch 49/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0622 - val_acc: 0.9744\n",
      "Epoch 50/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0596 - val_acc: 0.9744\n",
      "Epoch 51/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0635 - val_acc: 0.9744\n",
      "Epoch 52/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0611 - val_acc: 0.9744\n",
      "Epoch 53/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0619 - val_acc: 0.9744\n",
      "Epoch 54/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0604 - val_acc: 0.9744\n",
      "Epoch 55/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0607 - val_acc: 0.9744\n",
      "Epoch 56/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0569 - val_acc: 0.9872\n",
      "Epoch 57/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0600 - val_acc: 0.9744\n",
      "Epoch 58/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0580 - val_acc: 0.9744\n",
      "Epoch 59/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0580 - val_acc: 0.9744\n",
      "Epoch 60/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0619 - val_acc: 0.9744\n",
      "Epoch 61/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0608 - val_acc: 0.9744\n",
      "Epoch 62/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0581 - val_acc: 0.9744\n",
      "Epoch 63/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0609 - val_acc: 0.9744\n",
      "Epoch 64/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0604 - val_acc: 0.9744\n",
      "Epoch 65/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0586 - val_acc: 0.9744\n",
      "Epoch 66/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0583 - val_acc: 0.9744\n",
      "Epoch 67/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0591 - val_acc: 0.9744\n",
      "Epoch 68/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0575 - val_acc: 0.9744\n",
      "Epoch 69/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0566 - val_acc: 0.9872\n",
      "Epoch 70/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0590 - val_acc: 0.9744\n",
      "Epoch 71/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0587 - val_acc: 0.9744\n",
      "Epoch 72/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0568 - val_acc: 0.9744\n",
      "Epoch 73/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0571 - val_acc: 0.9744\n",
      "Epoch 74/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0574 - val_acc: 0.9744\n",
      "Epoch 75/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0577 - val_acc: 0.9744\n",
      "Epoch 76/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0585 - val_acc: 0.9744\n",
      "Epoch 77/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0585 - val_acc: 0.9744\n",
      "Epoch 78/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0588 - val_acc: 0.9744\n",
      "Epoch 79/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0586 - val_acc: 0.9744\n",
      "Epoch 80/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0594 - val_acc: 0.9744\n",
      "Epoch 81/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0569 - val_acc: 0.9744\n",
      "Epoch 82/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0594 - val_acc: 0.9744\n",
      "Epoch 83/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0583 - val_acc: 0.9744\n",
      "Epoch 84/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0594 - val_acc: 0.9744\n",
      "Epoch 85/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0592 - val_acc: 0.9744\n",
      "Epoch 86/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0573 - val_acc: 0.9744\n",
      "Epoch 87/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0581 - val_acc: 0.9744\n",
      "Epoch 88/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0584 - val_acc: 0.9744\n",
      "Epoch 89/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0576 - val_acc: 0.9744\n",
      "Epoch 90/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0577 - val_acc: 0.9744\n",
      "Epoch 91/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0585 - val_acc: 0.9744\n",
      "Epoch 92/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0617 - val_acc: 0.9744\n",
      "Epoch 93/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0605 - val_acc: 0.9744\n",
      "Epoch 94/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0583 - val_acc: 0.9744\n",
      "Epoch 95/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0580 - val_acc: 0.9744\n",
      "Epoch 96/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0601 - val_acc: 0.9744\n",
      "Epoch 97/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0586 - val_acc: 0.9744\n",
      "Epoch 98/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0587 - val_acc: 0.9744\n",
      "Epoch 99/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0583 - val_acc: 0.9744\n",
      "Epoch 100/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0584 - val_acc: 0.9744\n",
      "78/78 [==============================] - 0s 307us/step\n",
      "Score for fold 8: loss of 1.0583945940702388; acc of 97.43589743589743%\n",
      "Train on 702 samples, validate on 78 samples\n",
      "Epoch 1/100\n",
      "702/702 [==============================] - 3s 4ms/step - loss: 2.2614 - acc: 0.3462 - val_loss: 1.9428 - val_acc: 0.4744\n",
      "Epoch 2/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.6683 - acc: 0.6624 - val_loss: 1.5598 - val_acc: 0.6026\n",
      "Epoch 3/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.3781 - acc: 0.8120 - val_loss: 1.2173 - val_acc: 0.9103\n",
      "Epoch 4/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.2599 - acc: 0.8590 - val_loss: 1.1966 - val_acc: 0.9103\n",
      "Epoch 5/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1720 - acc: 0.8932 - val_loss: 1.1880 - val_acc: 0.9231\n",
      "Epoch 6/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1536 - acc: 0.8832 - val_loss: 1.1044 - val_acc: 0.9487\n",
      "Epoch 7/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1220 - acc: 0.9316 - val_loss: 1.1566 - val_acc: 0.8590\n",
      "Epoch 8/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1029 - acc: 0.9487 - val_loss: 1.0916 - val_acc: 0.9487\n",
      "Epoch 9/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0971 - acc: 0.9516 - val_loss: 1.0852 - val_acc: 0.9487\n",
      "Epoch 10/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1131 - acc: 0.9302 - val_loss: 1.1675 - val_acc: 0.9359\n",
      "Epoch 11/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0706 - acc: 0.9829 - val_loss: 1.0834 - val_acc: 0.9744\n",
      "Epoch 12/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0683 - acc: 0.9715 - val_loss: 1.0801 - val_acc: 0.9744\n",
      "Epoch 13/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0744 - acc: 0.9587 - val_loss: 1.2483 - val_acc: 0.7692\n",
      "Epoch 14/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0694 - acc: 0.9801 - val_loss: 1.0898 - val_acc: 0.9487\n",
      "Epoch 15/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0758 - acc: 0.9772 - val_loss: 1.0802 - val_acc: 0.9615\n",
      "Epoch 16/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0518 - acc: 0.9986 - val_loss: 1.0989 - val_acc: 0.9359\n",
      "Epoch 17/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0575 - acc: 0.9872 - val_loss: 1.0758 - val_acc: 0.9615\n",
      "Epoch 18/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 1.0824 - acc: 0.9729 - val_loss: 1.0719 - val_acc: 0.9744\n",
      "Epoch 19/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0506 - acc: 0.9957 - val_loss: 1.0780 - val_acc: 0.9615\n",
      "Epoch 20/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0503 - acc: 0.9943 - val_loss: 1.0791 - val_acc: 0.9487\n",
      "Epoch 21/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0517 - acc: 0.9886 - val_loss: 1.0737 - val_acc: 0.9744\n",
      "Epoch 22/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0477 - acc: 0.9929 - val_loss: 1.0762 - val_acc: 0.9487\n",
      "Epoch 23/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0483 - acc: 0.9943 - val_loss: 1.0755 - val_acc: 0.9615\n",
      "Epoch 24/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0487 - acc: 0.9915 - val_loss: 1.0692 - val_acc: 0.9744\n",
      "Epoch 25/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0433 - acc: 1.0000 - val_loss: 1.0655 - val_acc: 0.9744\n",
      "Epoch 26/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0430 - acc: 1.0000 - val_loss: 1.0680 - val_acc: 0.9744\n",
      "Epoch 27/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0441 - acc: 0.9957 - val_loss: 1.0689 - val_acc: 0.9615\n",
      "Epoch 28/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0435 - acc: 0.9972 - val_loss: 1.0653 - val_acc: 0.9744\n",
      "Epoch 29/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0440 - acc: 0.9972 - val_loss: 1.0666 - val_acc: 0.9744\n",
      "Epoch 30/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0422 - acc: 0.9986 - val_loss: 1.0652 - val_acc: 0.9744\n",
      "Epoch 31/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0436 - acc: 0.9972 - val_loss: 1.0647 - val_acc: 0.9744\n",
      "Epoch 32/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0417 - acc: 1.0000 - val_loss: 1.0657 - val_acc: 0.9615\n",
      "Epoch 33/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0414 - acc: 1.0000 - val_loss: 1.0657 - val_acc: 0.9744\n",
      "Epoch 34/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0435 - acc: 0.9972 - val_loss: 1.0644 - val_acc: 0.9744\n",
      "Epoch 35/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0414 - acc: 1.0000 - val_loss: 1.0653 - val_acc: 0.9744\n",
      "Epoch 36/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0412 - acc: 1.0000 - val_loss: 1.0669 - val_acc: 0.9744\n",
      "Epoch 37/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0413 - acc: 1.0000 - val_loss: 1.0656 - val_acc: 0.9744\n",
      "Epoch 38/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0410 - acc: 1.0000 - val_loss: 1.0638 - val_acc: 0.9744\n",
      "Epoch 39/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0409 - acc: 1.0000 - val_loss: 1.0656 - val_acc: 0.9744\n",
      "Epoch 40/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0415 - acc: 1.0000 - val_loss: 1.0637 - val_acc: 0.9744\n",
      "Epoch 41/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0409 - acc: 1.0000 - val_loss: 1.0657 - val_acc: 0.9744\n",
      "Epoch 42/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0408 - acc: 1.0000 - val_loss: 1.0663 - val_acc: 0.9744\n",
      "Epoch 43/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0407 - acc: 1.0000 - val_loss: 1.0648 - val_acc: 0.9744\n",
      "Epoch 44/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0408 - acc: 1.0000 - val_loss: 1.0638 - val_acc: 0.9744\n",
      "Epoch 45/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0410 - acc: 0.9986 - val_loss: 1.0640 - val_acc: 0.9744\n",
      "Epoch 46/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0642 - val_acc: 0.9744\n",
      "Epoch 47/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0653 - val_acc: 0.9744\n",
      "Epoch 48/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0643 - val_acc: 0.9744\n",
      "Epoch 49/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0666 - val_acc: 0.9744\n",
      "Epoch 50/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0641 - val_acc: 0.9744\n",
      "Epoch 51/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0641 - val_acc: 0.9744\n",
      "Epoch 52/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0639 - val_acc: 0.9744\n",
      "Epoch 53/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0641 - val_acc: 0.9744\n",
      "Epoch 54/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0644 - val_acc: 0.9744\n",
      "Epoch 55/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0643 - val_acc: 0.9744\n",
      "Epoch 56/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0644 - val_acc: 0.9744\n",
      "Epoch 57/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0646 - val_acc: 0.9744\n",
      "Epoch 58/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0649 - val_acc: 0.9744\n",
      "Epoch 59/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0647 - val_acc: 0.9744\n",
      "Epoch 60/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0656 - val_acc: 0.9744\n",
      "Epoch 61/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0645 - val_acc: 0.9744\n",
      "Epoch 62/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0649 - val_acc: 0.9744\n",
      "Epoch 63/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0650 - val_acc: 0.9744\n",
      "Epoch 64/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0645 - val_acc: 0.9744\n",
      "Epoch 65/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0647 - val_acc: 0.9744\n",
      "Epoch 66/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0650 - val_acc: 0.9744\n",
      "Epoch 67/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0650 - val_acc: 0.9744\n",
      "Epoch 68/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0647 - val_acc: 0.9744\n",
      "Epoch 69/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0644 - val_acc: 0.9744\n",
      "Epoch 70/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0642 - val_acc: 0.9744\n",
      "Epoch 71/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0643 - val_acc: 0.9744\n",
      "Epoch 72/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0642 - val_acc: 0.9744\n",
      "Epoch 73/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0640 - val_acc: 0.9744\n",
      "Epoch 74/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0639 - val_acc: 0.9744\n",
      "Epoch 75/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0642 - val_acc: 0.9744\n",
      "Epoch 76/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0644 - val_acc: 0.9744\n",
      "Epoch 77/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0644 - val_acc: 0.9744\n",
      "Epoch 78/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0643 - val_acc: 0.9744\n",
      "Epoch 79/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0647 - val_acc: 0.9744\n",
      "Epoch 80/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0650 - val_acc: 0.9744\n",
      "Epoch 81/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0643 - val_acc: 0.9744\n",
      "Epoch 82/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0639 - val_acc: 0.9744\n",
      "Epoch 83/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0642 - val_acc: 0.9744\n",
      "Epoch 84/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0644 - val_acc: 0.9744\n",
      "Epoch 85/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0644 - val_acc: 0.9744\n",
      "Epoch 86/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0640 - val_acc: 0.9744\n",
      "Epoch 87/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0646 - val_acc: 0.9744\n",
      "Epoch 88/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0643 - val_acc: 0.9744\n",
      "Epoch 89/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0648 - val_acc: 0.9744\n",
      "Epoch 90/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0645 - val_acc: 0.9744\n",
      "Epoch 91/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0646 - val_acc: 0.9744\n",
      "Epoch 92/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0644 - val_acc: 0.9744\n",
      "Epoch 93/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0642 - val_acc: 0.9744\n",
      "Epoch 94/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0641 - val_acc: 0.9744\n",
      "Epoch 95/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0643 - val_acc: 0.9744\n",
      "Epoch 96/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0644 - val_acc: 0.9744\n",
      "Epoch 97/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0642 - val_acc: 0.9744\n",
      "Epoch 98/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0644 - val_acc: 0.9744\n",
      "Epoch 99/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0643 - val_acc: 0.9744\n",
      "Epoch 100/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0643 - val_acc: 0.9744\n",
      "78/78 [==============================] - 0s 305us/step\n",
      "Score for fold 9: loss of 1.0642197651740832; acc of 97.43589697740018%\n",
      "Train on 702 samples, validate on 78 samples\n",
      "Epoch 1/100\n",
      "702/702 [==============================] - 3s 5ms/step - loss: 2.2016 - acc: 0.3604 - val_loss: 1.8272 - val_acc: 0.6154\n",
      "Epoch 2/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.6303 - acc: 0.6610 - val_loss: 1.3664 - val_acc: 0.8590\n",
      "Epoch 3/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.3408 - acc: 0.8148 - val_loss: 1.4403 - val_acc: 0.7179\n",
      "Epoch 4/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.2701 - acc: 0.8504 - val_loss: 1.2493 - val_acc: 0.8590\n",
      "Epoch 5/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1482 - acc: 0.9359 - val_loss: 1.8148 - val_acc: 0.5385\n",
      "Epoch 6/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1776 - acc: 0.9074 - val_loss: 1.1276 - val_acc: 0.9231\n",
      "Epoch 7/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1433 - acc: 0.9202 - val_loss: 1.1070 - val_acc: 0.9615\n",
      "Epoch 8/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0949 - acc: 0.9516 - val_loss: 1.1506 - val_acc: 0.8333\n",
      "Epoch 9/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0982 - acc: 0.9444 - val_loss: 1.1009 - val_acc: 0.9487\n",
      "Epoch 10/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1036 - acc: 0.9459 - val_loss: 1.0987 - val_acc: 0.9615\n",
      "Epoch 11/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0724 - acc: 0.9801 - val_loss: 1.1342 - val_acc: 0.8846\n",
      "Epoch 12/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0732 - acc: 0.9658 - val_loss: 1.1646 - val_acc: 0.8718\n",
      "Epoch 13/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0932 - acc: 0.9701 - val_loss: 1.0870 - val_acc: 0.9487\n",
      "Epoch 14/100\n",
      "702/702 [==============================] - 1s 2ms/step - loss: 1.0621 - acc: 0.9829 - val_loss: 1.0968 - val_acc: 0.9231\n",
      "Epoch 15/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0562 - acc: 0.9886 - val_loss: 1.0612 - val_acc: 0.9872\n",
      "Epoch 16/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0536 - acc: 0.9929 - val_loss: 1.0604 - val_acc: 0.9872\n",
      "Epoch 17/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0532 - acc: 0.9943 - val_loss: 1.0582 - val_acc: 0.9872\n",
      "Epoch 18/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0511 - acc: 0.9943 - val_loss: 1.0973 - val_acc: 0.9231\n",
      "Epoch 19/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0582 - acc: 0.9801 - val_loss: 1.0515 - val_acc: 0.9872\n",
      "Epoch 20/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0501 - acc: 0.9929 - val_loss: 1.0687 - val_acc: 0.9615\n",
      "Epoch 21/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0488 - acc: 0.9972 - val_loss: 1.0809 - val_acc: 0.9487\n",
      "Epoch 22/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0525 - acc: 0.9915 - val_loss: 1.0576 - val_acc: 0.9487\n",
      "Epoch 23/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0464 - acc: 0.9972 - val_loss: 1.0514 - val_acc: 0.9872\n",
      "Epoch 24/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0465 - acc: 0.9957 - val_loss: 1.0500 - val_acc: 0.9872\n",
      "Epoch 25/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0459 - acc: 0.9972 - val_loss: 1.0505 - val_acc: 0.9744\n",
      "Epoch 26/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0449 - acc: 0.9972 - val_loss: 1.0507 - val_acc: 0.9744\n",
      "Epoch 27/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0458 - acc: 0.9943 - val_loss: 1.0488 - val_acc: 1.0000\n",
      "Epoch 28/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0454 - acc: 0.9957 - val_loss: 1.0506 - val_acc: 0.9872\n",
      "Epoch 29/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0436 - acc: 0.9986 - val_loss: 1.0604 - val_acc: 0.9872\n",
      "Epoch 30/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0466 - acc: 0.9915 - val_loss: 1.0586 - val_acc: 0.9744\n",
      "Epoch 31/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0450 - acc: 0.9957 - val_loss: 1.0491 - val_acc: 0.9872\n",
      "Epoch 32/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0436 - acc: 0.9986 - val_loss: 1.0485 - val_acc: 1.0000\n",
      "Epoch 33/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0435 - acc: 0.9986 - val_loss: 1.0523 - val_acc: 0.9744\n",
      "Epoch 34/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0434 - acc: 0.9986 - val_loss: 1.0537 - val_acc: 0.9872\n",
      "Epoch 35/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0425 - acc: 1.0000 - val_loss: 1.0521 - val_acc: 0.9487\n",
      "Epoch 36/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0423 - acc: 0.9986 - val_loss: 1.0483 - val_acc: 0.9744\n",
      "Epoch 37/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0416 - acc: 1.0000 - val_loss: 1.0491 - val_acc: 1.0000\n",
      "Epoch 38/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0415 - acc: 1.0000 - val_loss: 1.0446 - val_acc: 1.0000\n",
      "Epoch 39/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0419 - acc: 1.0000 - val_loss: 1.0477 - val_acc: 1.0000\n",
      "Epoch 40/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0415 - acc: 1.0000 - val_loss: 1.0445 - val_acc: 1.0000\n",
      "Epoch 41/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0411 - acc: 1.0000 - val_loss: 1.0431 - val_acc: 1.0000\n",
      "Epoch 42/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0410 - acc: 1.0000 - val_loss: 1.0457 - val_acc: 1.0000\n",
      "Epoch 43/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0412 - acc: 1.0000 - val_loss: 1.0435 - val_acc: 1.0000\n",
      "Epoch 44/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0409 - acc: 1.0000 - val_loss: 1.0447 - val_acc: 1.0000\n",
      "Epoch 45/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0409 - acc: 1.0000 - val_loss: 1.0453 - val_acc: 0.9872\n",
      "Epoch 46/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0413 - acc: 1.0000 - val_loss: 1.0464 - val_acc: 1.0000\n",
      "Epoch 47/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0408 - acc: 1.0000 - val_loss: 1.0434 - val_acc: 1.0000\n",
      "Epoch 48/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0441 - val_acc: 1.0000\n",
      "Epoch 49/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0407 - acc: 1.0000 - val_loss: 1.0453 - val_acc: 1.0000\n",
      "Epoch 50/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0407 - acc: 1.0000 - val_loss: 1.0432 - val_acc: 1.0000\n",
      "Epoch 51/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0408 - acc: 1.0000 - val_loss: 1.0439 - val_acc: 1.0000\n",
      "Epoch 52/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0422 - val_acc: 1.0000\n",
      "Epoch 53/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0431 - val_acc: 1.0000\n",
      "Epoch 54/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0435 - val_acc: 1.0000\n",
      "Epoch 55/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0426 - val_acc: 1.0000\n",
      "Epoch 56/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0428 - val_acc: 1.0000\n",
      "Epoch 57/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0431 - val_acc: 1.0000\n",
      "Epoch 58/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0424 - val_acc: 1.0000\n",
      "Epoch 59/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0430 - val_acc: 1.0000\n",
      "Epoch 60/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0423 - val_acc: 1.0000\n",
      "Epoch 61/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0430 - val_acc: 1.0000\n",
      "Epoch 62/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0433 - val_acc: 1.0000\n",
      "Epoch 63/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0427 - val_acc: 1.0000\n",
      "Epoch 64/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0436 - val_acc: 1.0000\n",
      "Epoch 65/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0432 - val_acc: 1.0000\n",
      "Epoch 66/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0434 - val_acc: 1.0000\n",
      "Epoch 67/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0432 - val_acc: 1.0000\n",
      "Epoch 68/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0442 - val_acc: 1.0000\n",
      "Epoch 69/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0430 - val_acc: 1.0000\n",
      "Epoch 70/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0428 - val_acc: 1.0000\n",
      "Epoch 71/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0441 - val_acc: 1.0000\n",
      "Epoch 72/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0430 - val_acc: 1.0000\n",
      "Epoch 73/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0442 - val_acc: 1.0000\n",
      "Epoch 74/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0432 - val_acc: 1.0000\n",
      "Epoch 75/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0434 - val_acc: 1.0000\n",
      "Epoch 76/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0431 - val_acc: 1.0000\n",
      "Epoch 77/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0430 - val_acc: 1.0000\n",
      "Epoch 78/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0439 - val_acc: 1.0000\n",
      "Epoch 79/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0430 - val_acc: 1.0000\n",
      "Epoch 80/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0441 - val_acc: 1.0000\n",
      "Epoch 81/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0443 - val_acc: 1.0000\n",
      "Epoch 82/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0445 - val_acc: 1.0000\n",
      "Epoch 83/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0437 - val_acc: 1.0000\n",
      "Epoch 84/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0434 - val_acc: 1.0000\n",
      "Epoch 85/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0440 - val_acc: 1.0000\n",
      "Epoch 86/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0438 - val_acc: 1.0000\n",
      "Epoch 87/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0440 - val_acc: 1.0000\n",
      "Epoch 88/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0440 - val_acc: 1.0000\n",
      "Epoch 89/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0440 - val_acc: 1.0000\n",
      "Epoch 90/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0436 - val_acc: 1.0000\n",
      "Epoch 91/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0435 - val_acc: 1.0000\n",
      "Epoch 92/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0433 - val_acc: 1.0000\n",
      "Epoch 93/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0437 - val_acc: 1.0000\n",
      "Epoch 94/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0433 - val_acc: 1.0000\n",
      "Epoch 95/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0435 - val_acc: 1.0000\n",
      "Epoch 96/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0432 - val_acc: 1.0000\n",
      "Epoch 97/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0432 - val_acc: 1.0000\n",
      "Epoch 98/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0434 - val_acc: 1.0000\n",
      "Epoch 99/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0435 - val_acc: 1.0000\n",
      "Epoch 100/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0438 - val_acc: 1.0000\n",
      "78/78 [==============================] - 0s 296us/step\n",
      "Score for fold 10: loss of 1.043397232508048; acc of 100.0%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1296x1296 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98.07692289352416\n",
      "Train on 702 samples, validate on 78 samples\n",
      "Epoch 1/100\n",
      "702/702 [==============================] - 3s 5ms/step - loss: 2.1751 - acc: 0.3846 - val_loss: 1.8977 - val_acc: 0.4615\n",
      "Epoch 2/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.5360 - acc: 0.7365 - val_loss: 1.3860 - val_acc: 0.7051\n",
      "Epoch 3/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.3194 - acc: 0.8305 - val_loss: 1.5411 - val_acc: 0.6795\n",
      "Epoch 4/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.2476 - acc: 0.8832 - val_loss: 1.2481 - val_acc: 0.7949\n",
      "Epoch 5/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1933 - acc: 0.9017 - val_loss: 1.1493 - val_acc: 0.8974\n",
      "Epoch 6/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1297 - acc: 0.9288 - val_loss: 1.1807 - val_acc: 0.8590\n",
      "Epoch 7/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1238 - acc: 0.9288 - val_loss: 1.1224 - val_acc: 0.8718\n",
      "Epoch 8/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0959 - acc: 0.9501 - val_loss: 1.1191 - val_acc: 0.8974\n",
      "Epoch 9/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0840 - acc: 0.9672 - val_loss: 1.0973 - val_acc: 0.9359\n",
      "Epoch 10/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0958 - acc: 0.9373 - val_loss: 1.0891 - val_acc: 0.9359\n",
      "Epoch 11/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0634 - acc: 0.9929 - val_loss: 1.1110 - val_acc: 0.9487\n",
      "Epoch 12/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1018 - acc: 0.9473 - val_loss: 1.0940 - val_acc: 0.9487\n",
      "Epoch 13/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0583 - acc: 0.9929 - val_loss: 1.0861 - val_acc: 0.9231\n",
      "Epoch 14/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0641 - acc: 0.9858 - val_loss: 1.1184 - val_acc: 0.9231\n",
      "Epoch 15/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0679 - acc: 0.9772 - val_loss: 1.0884 - val_acc: 0.9487\n",
      "Epoch 16/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0525 - acc: 0.9943 - val_loss: 1.0817 - val_acc: 0.9359\n",
      "Epoch 17/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0569 - acc: 0.9858 - val_loss: 1.0992 - val_acc: 0.9103\n",
      "Epoch 18/100\n",
      "702/702 [==============================] - 2s 2ms/step - loss: 1.0591 - acc: 0.9872 - val_loss: 1.1369 - val_acc: 0.8590\n",
      "Epoch 19/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0517 - acc: 0.9929 - val_loss: 1.0777 - val_acc: 0.9359\n",
      "Epoch 20/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0503 - acc: 0.9929 - val_loss: 1.0861 - val_acc: 0.9231\n",
      "Epoch 21/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0464 - acc: 0.9986 - val_loss: 1.0758 - val_acc: 0.9359\n",
      "Epoch 22/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0538 - acc: 0.9858 - val_loss: 1.0733 - val_acc: 0.9487\n",
      "Epoch 23/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0457 - acc: 0.9986 - val_loss: 1.0711 - val_acc: 0.9615\n",
      "Epoch 24/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0449 - acc: 0.9986 - val_loss: 1.0747 - val_acc: 0.9487\n",
      "Epoch 25/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0498 - acc: 0.9915 - val_loss: 1.0685 - val_acc: 0.9487\n",
      "Epoch 26/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0442 - acc: 0.9986 - val_loss: 1.0730 - val_acc: 0.9359\n",
      "Epoch 27/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0439 - acc: 0.9986 - val_loss: 1.0728 - val_acc: 0.9615\n",
      "Epoch 28/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0473 - acc: 0.9943 - val_loss: 1.0687 - val_acc: 0.9487\n",
      "Epoch 29/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0456 - acc: 0.9957 - val_loss: 1.0739 - val_acc: 0.9615\n",
      "Epoch 30/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0450 - acc: 0.9972 - val_loss: 1.0950 - val_acc: 0.9359\n",
      "Epoch 31/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0443 - acc: 0.9972 - val_loss: 1.0779 - val_acc: 0.9487\n",
      "Epoch 32/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0432 - acc: 0.9986 - val_loss: 1.0717 - val_acc: 0.9615\n",
      "Epoch 33/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0422 - acc: 0.9986 - val_loss: 1.0726 - val_acc: 0.9615\n",
      "Epoch 34/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0447 - acc: 0.9957 - val_loss: 1.0699 - val_acc: 0.9615\n",
      "Epoch 35/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0424 - acc: 1.0000 - val_loss: 1.0673 - val_acc: 0.9359\n",
      "Epoch 36/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0421 - acc: 1.0000 - val_loss: 1.0680 - val_acc: 0.9487\n",
      "Epoch 37/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0416 - acc: 1.0000 - val_loss: 1.0727 - val_acc: 0.9487\n",
      "Epoch 38/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0417 - acc: 1.0000 - val_loss: 1.0718 - val_acc: 0.9487\n",
      "Epoch 39/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0416 - acc: 1.0000 - val_loss: 1.0663 - val_acc: 0.9487\n",
      "Epoch 40/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0418 - acc: 0.9986 - val_loss: 1.0838 - val_acc: 0.9359\n",
      "Epoch 41/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0418 - acc: 1.0000 - val_loss: 1.0649 - val_acc: 0.9487\n",
      "Epoch 42/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0411 - acc: 1.0000 - val_loss: 1.0718 - val_acc: 0.9487\n",
      "Epoch 43/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0410 - acc: 1.0000 - val_loss: 1.0724 - val_acc: 0.9487\n",
      "Epoch 44/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0410 - acc: 1.0000 - val_loss: 1.0719 - val_acc: 0.9487\n",
      "Epoch 45/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0410 - acc: 1.0000 - val_loss: 1.0717 - val_acc: 0.9615\n",
      "Epoch 46/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0408 - acc: 1.0000 - val_loss: 1.0702 - val_acc: 0.9487\n",
      "Epoch 47/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0408 - acc: 1.0000 - val_loss: 1.0643 - val_acc: 0.9615\n",
      "Epoch 48/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0408 - acc: 1.0000 - val_loss: 1.0702 - val_acc: 0.9487\n",
      "Epoch 49/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0407 - acc: 1.0000 - val_loss: 1.0714 - val_acc: 0.9487\n",
      "Epoch 50/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0704 - val_acc: 0.9487\n",
      "Epoch 51/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0697 - val_acc: 0.9487\n",
      "Epoch 52/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0705 - val_acc: 0.9487\n",
      "Epoch 53/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0719 - val_acc: 0.9487\n",
      "Epoch 54/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0717 - val_acc: 0.9487\n",
      "Epoch 55/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0708 - val_acc: 0.9487\n",
      "Epoch 56/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0695 - val_acc: 0.9487\n",
      "Epoch 57/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0700 - val_acc: 0.9487\n",
      "Epoch 58/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0703 - val_acc: 0.9487\n",
      "Epoch 59/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0696 - val_acc: 0.9487\n",
      "Epoch 60/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0701 - val_acc: 0.9487\n",
      "Epoch 61/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0703 - val_acc: 0.9487\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 62/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0692 - val_acc: 0.9487\n",
      "Epoch 63/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0704 - val_acc: 0.9487\n",
      "Epoch 64/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0709 - val_acc: 0.9487\n",
      "Epoch 65/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0686 - val_acc: 0.9487\n",
      "Epoch 66/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0695 - val_acc: 0.9487\n",
      "Epoch 67/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0691 - val_acc: 0.9487\n",
      "Epoch 68/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0693 - val_acc: 0.9487\n",
      "Epoch 69/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0685 - val_acc: 0.9487\n",
      "Epoch 70/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0695 - val_acc: 0.9487\n",
      "Epoch 71/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0691 - val_acc: 0.9487\n",
      "Epoch 72/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0687 - val_acc: 0.9487\n",
      "Epoch 73/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0705 - val_acc: 0.9487\n",
      "Epoch 74/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0701 - val_acc: 0.9487\n",
      "Epoch 75/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0677 - val_acc: 0.9487\n",
      "Epoch 76/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0690 - val_acc: 0.9487\n",
      "Epoch 77/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0695 - val_acc: 0.9487\n",
      "Epoch 78/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0687 - val_acc: 0.9487\n",
      "Epoch 79/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0692 - val_acc: 0.9487\n",
      "Epoch 80/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0690 - val_acc: 0.9487\n",
      "Epoch 81/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0689 - val_acc: 0.9487\n",
      "Epoch 82/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0699 - val_acc: 0.9487\n",
      "Epoch 83/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0691 - val_acc: 0.9487\n",
      "Epoch 84/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0692 - val_acc: 0.9487\n",
      "Epoch 85/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0682 - val_acc: 0.9487\n",
      "Epoch 86/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0692 - val_acc: 0.9487\n",
      "Epoch 87/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0687 - val_acc: 0.9487\n",
      "Epoch 88/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0686 - val_acc: 0.9487\n",
      "Epoch 89/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0690 - val_acc: 0.9487\n",
      "Epoch 90/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0685 - val_acc: 0.9487\n",
      "Epoch 91/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0687 - val_acc: 0.9487\n",
      "Epoch 92/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0684 - val_acc: 0.9487\n",
      "Epoch 93/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0685 - val_acc: 0.9487\n",
      "Epoch 94/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0689 - val_acc: 0.9487\n",
      "Epoch 95/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0691 - val_acc: 0.9487\n",
      "Epoch 96/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0690 - val_acc: 0.9487\n",
      "Epoch 97/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0690 - val_acc: 0.9487\n",
      "Epoch 98/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0683 - val_acc: 0.9487\n",
      "Epoch 99/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0685 - val_acc: 0.9487\n",
      "Epoch 100/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0678 - val_acc: 0.9487\n",
      "78/78 [==============================] - 0s 304us/step\n",
      "Score for fold 1: loss of 1.0692931444216998; acc of 94.87179487179486%\n",
      "Train on 702 samples, validate on 78 samples\n",
      "Epoch 1/100\n",
      "702/702 [==============================] - 4s 5ms/step - loss: 2.3143 - acc: 0.2749 - val_loss: 1.9493 - val_acc: 0.4744\n",
      "Epoch 2/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.6769 - acc: 0.7080 - val_loss: 1.6167 - val_acc: 0.6795\n",
      "Epoch 3/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.3688 - acc: 0.8148 - val_loss: 1.3217 - val_acc: 0.7949\n",
      "Epoch 4/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.2339 - acc: 0.8789 - val_loss: 1.2743 - val_acc: 0.8205\n",
      "Epoch 5/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1872 - acc: 0.8846 - val_loss: 1.2698 - val_acc: 0.8846\n",
      "Epoch 6/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1679 - acc: 0.8903 - val_loss: 1.1695 - val_acc: 0.8718\n",
      "Epoch 7/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1269 - acc: 0.9416 - val_loss: 1.1272 - val_acc: 0.9231\n",
      "Epoch 8/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0927 - acc: 0.9601 - val_loss: 1.1557 - val_acc: 0.8718\n",
      "Epoch 9/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0824 - acc: 0.9701 - val_loss: 1.0948 - val_acc: 0.9615\n",
      "Epoch 10/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0954 - acc: 0.9530 - val_loss: 1.1198 - val_acc: 0.8974\n",
      "Epoch 11/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0745 - acc: 0.9630 - val_loss: 1.0850 - val_acc: 0.9359\n",
      "Epoch 12/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1034 - acc: 0.9530 - val_loss: 1.0880 - val_acc: 0.9359\n",
      "Epoch 13/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0638 - acc: 0.9786 - val_loss: 1.0876 - val_acc: 0.9615\n",
      "Epoch 14/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0704 - acc: 0.9701 - val_loss: 1.1203 - val_acc: 0.8846\n",
      "Epoch 15/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0672 - acc: 0.9786 - val_loss: 1.0704 - val_acc: 0.9615\n",
      "Epoch 16/100\n",
      "702/702 [==============================] - 2s 2ms/step - loss: 1.0550 - acc: 0.9886 - val_loss: 1.0746 - val_acc: 0.9615\n",
      "Epoch 17/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0572 - acc: 0.9872 - val_loss: 1.0577 - val_acc: 0.9872\n",
      "Epoch 18/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0480 - acc: 0.9986 - val_loss: 1.0619 - val_acc: 0.9872\n",
      "Epoch 19/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0626 - acc: 0.9858 - val_loss: 1.0565 - val_acc: 0.9872\n",
      "Epoch 20/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0482 - acc: 0.9929 - val_loss: 1.0615 - val_acc: 0.9744\n",
      "Epoch 21/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0506 - acc: 0.9929 - val_loss: 1.0666 - val_acc: 0.9487\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0511 - acc: 0.9915 - val_loss: 1.0533 - val_acc: 0.9872\n",
      "Epoch 23/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0496 - acc: 0.9915 - val_loss: 1.0541 - val_acc: 0.9872\n",
      "Epoch 24/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0486 - acc: 0.9915 - val_loss: 1.0543 - val_acc: 1.0000\n",
      "Epoch 25/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0454 - acc: 0.9986 - val_loss: 1.0534 - val_acc: 0.9872\n",
      "Epoch 26/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0476 - acc: 0.9915 - val_loss: 1.0534 - val_acc: 0.9744\n",
      "Epoch 27/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0490 - acc: 0.9929 - val_loss: 1.0511 - val_acc: 0.9872\n",
      "Epoch 28/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0471 - acc: 0.9943 - val_loss: 1.0612 - val_acc: 0.9615\n",
      "Epoch 29/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0455 - acc: 0.9957 - val_loss: 1.0565 - val_acc: 0.9744\n",
      "Epoch 30/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0474 - acc: 0.9929 - val_loss: 1.0842 - val_acc: 0.9487\n",
      "Epoch 31/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0463 - acc: 0.9957 - val_loss: 1.0497 - val_acc: 0.9872\n",
      "Epoch 32/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0432 - acc: 1.0000 - val_loss: 1.0649 - val_acc: 0.9744\n",
      "Epoch 33/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0452 - acc: 0.9972 - val_loss: 1.0549 - val_acc: 0.9615\n",
      "Epoch 34/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0427 - acc: 0.9986 - val_loss: 1.0484 - val_acc: 0.9872\n",
      "Epoch 35/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0419 - acc: 1.0000 - val_loss: 1.0477 - val_acc: 1.0000\n",
      "Epoch 36/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0424 - acc: 1.0000 - val_loss: 1.0504 - val_acc: 0.9872\n",
      "Epoch 37/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0424 - acc: 1.0000 - val_loss: 1.0523 - val_acc: 0.9872\n",
      "Epoch 38/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0414 - acc: 1.0000 - val_loss: 1.0498 - val_acc: 0.9872\n",
      "Epoch 39/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0422 - acc: 1.0000 - val_loss: 1.0546 - val_acc: 1.0000\n",
      "Epoch 40/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0416 - acc: 1.0000 - val_loss: 1.0451 - val_acc: 1.0000\n",
      "Epoch 41/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0413 - acc: 1.0000 - val_loss: 1.0473 - val_acc: 0.9872\n",
      "Epoch 42/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0413 - acc: 1.0000 - val_loss: 1.0475 - val_acc: 0.9872\n",
      "Epoch 43/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0416 - acc: 1.0000 - val_loss: 1.0458 - val_acc: 0.9872\n",
      "Epoch 44/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0410 - acc: 1.0000 - val_loss: 1.0452 - val_acc: 1.0000\n",
      "Epoch 45/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0409 - acc: 1.0000 - val_loss: 1.0465 - val_acc: 0.9872\n",
      "Epoch 46/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0409 - acc: 1.0000 - val_loss: 1.0475 - val_acc: 0.9872\n",
      "Epoch 47/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0408 - acc: 1.0000 - val_loss: 1.0452 - val_acc: 1.0000\n",
      "Epoch 48/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0410 - acc: 1.0000 - val_loss: 1.0457 - val_acc: 1.0000\n",
      "Epoch 49/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0407 - acc: 1.0000 - val_loss: 1.0528 - val_acc: 0.9744\n",
      "Epoch 50/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0407 - acc: 1.0000 - val_loss: 1.0466 - val_acc: 0.9872\n",
      "Epoch 51/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0407 - acc: 1.0000 - val_loss: 1.0449 - val_acc: 1.0000\n",
      "Epoch 52/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0407 - acc: 1.0000 - val_loss: 1.0448 - val_acc: 1.0000\n",
      "Epoch 53/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0438 - val_acc: 1.0000\n",
      "Epoch 54/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0450 - val_acc: 1.0000\n",
      "Epoch 55/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0449 - val_acc: 0.9872\n",
      "Epoch 56/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0447 - val_acc: 1.0000\n",
      "Epoch 57/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0446 - val_acc: 1.0000\n",
      "Epoch 58/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0447 - val_acc: 1.0000\n",
      "Epoch 59/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0438 - val_acc: 1.0000\n",
      "Epoch 60/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0438 - val_acc: 1.0000\n",
      "Epoch 61/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0439 - val_acc: 1.0000\n",
      "Epoch 62/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0458 - val_acc: 0.9744\n",
      "Epoch 63/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0442 - val_acc: 1.0000\n",
      "Epoch 64/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0442 - val_acc: 1.0000\n",
      "Epoch 65/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0445 - val_acc: 1.0000\n",
      "Epoch 66/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0450 - val_acc: 1.0000\n",
      "Epoch 67/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0440 - val_acc: 1.0000\n",
      "Epoch 68/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0445 - val_acc: 1.0000\n",
      "Epoch 69/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0438 - val_acc: 1.0000\n",
      "Epoch 70/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0439 - val_acc: 1.0000\n",
      "Epoch 71/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0439 - val_acc: 1.0000\n",
      "Epoch 72/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0440 - val_acc: 1.0000\n",
      "Epoch 73/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0444 - val_acc: 1.0000\n",
      "Epoch 74/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0441 - val_acc: 1.0000\n",
      "Epoch 75/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0440 - val_acc: 1.0000\n",
      "Epoch 76/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0439 - val_acc: 1.0000\n",
      "Epoch 77/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0441 - val_acc: 1.0000\n",
      "Epoch 78/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0439 - val_acc: 1.0000\n",
      "Epoch 79/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0442 - val_acc: 1.0000\n",
      "Epoch 80/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0444 - val_acc: 1.0000\n",
      "Epoch 81/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0441 - val_acc: 1.0000\n",
      "Epoch 82/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0440 - val_acc: 1.0000\n",
      "Epoch 83/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0442 - val_acc: 1.0000\n",
      "Epoch 84/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0443 - val_acc: 1.0000\n",
      "Epoch 85/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0441 - val_acc: 1.0000\n",
      "Epoch 86/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0443 - val_acc: 1.0000\n",
      "Epoch 87/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0443 - val_acc: 1.0000\n",
      "Epoch 88/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0442 - val_acc: 1.0000\n",
      "Epoch 89/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0442 - val_acc: 1.0000\n",
      "Epoch 90/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0441 - val_acc: 1.0000\n",
      "Epoch 91/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0442 - val_acc: 1.0000\n",
      "Epoch 92/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0441 - val_acc: 1.0000\n",
      "Epoch 93/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0442 - val_acc: 1.0000\n",
      "Epoch 94/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0442 - val_acc: 1.0000\n",
      "Epoch 95/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0440 - val_acc: 1.0000\n",
      "Epoch 96/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0440 - val_acc: 1.0000\n",
      "Epoch 97/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0442 - val_acc: 1.0000\n",
      "Epoch 98/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0442 - val_acc: 1.0000\n",
      "Epoch 99/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0442 - val_acc: 1.0000\n",
      "Epoch 100/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0441 - val_acc: 1.0000\n",
      "78/78 [==============================] - 0s 310us/step\n",
      "Score for fold 2: loss of 1.0443613834870167; acc of 100.0%\n",
      "Train on 702 samples, validate on 78 samples\n",
      "Epoch 1/100\n",
      "702/702 [==============================] - 4s 6ms/step - loss: 2.2288 - acc: 0.3604 - val_loss: 1.8298 - val_acc: 0.4231\n",
      "Epoch 2/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.6475 - acc: 0.6809 - val_loss: 1.7096 - val_acc: 0.6154\n",
      "Epoch 3/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.3894 - acc: 0.8134 - val_loss: 1.3183 - val_acc: 0.7564\n",
      "Epoch 4/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.2595 - acc: 0.8718 - val_loss: 1.2277 - val_acc: 0.8590\n",
      "Epoch 5/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.2059 - acc: 0.8932 - val_loss: 1.1364 - val_acc: 0.9487\n",
      "Epoch 6/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1757 - acc: 0.8974 - val_loss: 1.1760 - val_acc: 0.8718\n",
      "Epoch 7/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1172 - acc: 0.9601 - val_loss: 1.1247 - val_acc: 0.8590\n",
      "Epoch 8/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1287 - acc: 0.9231 - val_loss: 1.1325 - val_acc: 0.9231\n",
      "Epoch 9/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0866 - acc: 0.9573 - val_loss: 1.0862 - val_acc: 0.9744\n",
      "Epoch 10/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0900 - acc: 0.9558 - val_loss: 1.2230 - val_acc: 0.8718\n",
      "Epoch 11/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0761 - acc: 0.9744 - val_loss: 1.1489 - val_acc: 0.9487\n",
      "Epoch 12/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0692 - acc: 0.9744 - val_loss: 1.0950 - val_acc: 0.9231\n",
      "Epoch 13/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0754 - acc: 0.9701 - val_loss: 1.1014 - val_acc: 0.9359\n",
      "Epoch 14/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0655 - acc: 0.9858 - val_loss: 1.1092 - val_acc: 0.9487\n",
      "Epoch 15/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0667 - acc: 0.9772 - val_loss: 1.0756 - val_acc: 0.9231\n",
      "Epoch 16/100\n",
      "702/702 [==============================] - 2s 2ms/step - loss: 1.0544 - acc: 0.9886 - val_loss: 1.1030 - val_acc: 0.9615\n",
      "Epoch 17/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0555 - acc: 0.9900 - val_loss: 1.0641 - val_acc: 0.9359\n",
      "Epoch 18/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0612 - acc: 0.9815 - val_loss: 1.0661 - val_acc: 0.9744\n",
      "Epoch 19/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0481 - acc: 0.9957 - val_loss: 1.0604 - val_acc: 0.9615\n",
      "Epoch 20/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0521 - acc: 0.9943 - val_loss: 1.0688 - val_acc: 0.9744\n",
      "Epoch 21/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0522 - acc: 0.9915 - val_loss: 1.0729 - val_acc: 0.9744\n",
      "Epoch 22/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0468 - acc: 0.9957 - val_loss: 1.0657 - val_acc: 0.9872\n",
      "Epoch 23/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0459 - acc: 0.9972 - val_loss: 1.0670 - val_acc: 0.9872\n",
      "Epoch 24/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0515 - acc: 0.9900 - val_loss: 1.0525 - val_acc: 0.9872\n",
      "Epoch 25/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0477 - acc: 0.9943 - val_loss: 1.0597 - val_acc: 0.9872\n",
      "Epoch 26/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0470 - acc: 0.9957 - val_loss: 1.0696 - val_acc: 0.9359\n",
      "Epoch 27/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0469 - acc: 0.9915 - val_loss: 1.0601 - val_acc: 0.9615\n",
      "Epoch 28/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0451 - acc: 0.9943 - val_loss: 1.0531 - val_acc: 1.0000\n",
      "Epoch 29/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0469 - acc: 0.9929 - val_loss: 1.0700 - val_acc: 0.9744\n",
      "Epoch 30/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0531 - acc: 0.9929 - val_loss: 1.0579 - val_acc: 0.9744\n",
      "Epoch 31/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0441 - acc: 0.9986 - val_loss: 1.0549 - val_acc: 0.9744\n",
      "Epoch 32/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0434 - acc: 0.9986 - val_loss: 1.0771 - val_acc: 0.9615\n",
      "Epoch 33/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0440 - acc: 0.9972 - val_loss: 1.0696 - val_acc: 0.9744\n",
      "Epoch 34/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0448 - acc: 0.9972 - val_loss: 1.0695 - val_acc: 0.9872\n",
      "Epoch 35/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0436 - acc: 0.9986 - val_loss: 1.0554 - val_acc: 0.9744\n",
      "Epoch 36/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0438 - acc: 0.9972 - val_loss: 1.0645 - val_acc: 0.9744\n",
      "Epoch 37/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0419 - acc: 1.0000 - val_loss: 1.0539 - val_acc: 0.9872\n",
      "Epoch 38/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0423 - acc: 0.9986 - val_loss: 1.0499 - val_acc: 0.9744\n",
      "Epoch 39/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0437 - acc: 0.9972 - val_loss: 1.0527 - val_acc: 0.9872\n",
      "Epoch 40/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0413 - acc: 1.0000 - val_loss: 1.0511 - val_acc: 0.9872\n",
      "Epoch 41/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0442 - acc: 0.9957 - val_loss: 1.0499 - val_acc: 0.9872\n",
      "Epoch 42/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0413 - acc: 1.0000 - val_loss: 1.0475 - val_acc: 0.9872\n",
      "Epoch 43/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0409 - acc: 1.0000 - val_loss: 1.0471 - val_acc: 0.9872\n",
      "Epoch 44/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0410 - acc: 1.0000 - val_loss: 1.0461 - val_acc: 0.9872\n",
      "Epoch 45/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0412 - acc: 1.0000 - val_loss: 1.0468 - val_acc: 0.9872\n",
      "Epoch 46/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0409 - acc: 1.0000 - val_loss: 1.0499 - val_acc: 0.9872\n",
      "Epoch 47/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0412 - acc: 1.0000 - val_loss: 1.0545 - val_acc: 0.9872\n",
      "Epoch 48/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0408 - acc: 0.9986 - val_loss: 1.0499 - val_acc: 0.9872\n",
      "Epoch 49/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0527 - val_acc: 0.9872\n",
      "Epoch 50/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0500 - val_acc: 0.9872\n",
      "Epoch 51/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0412 - acc: 0.9986 - val_loss: 1.0583 - val_acc: 0.9872\n",
      "Epoch 52/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0569 - val_acc: 0.9872\n",
      "Epoch 53/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0528 - val_acc: 0.9872\n",
      "Epoch 54/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0577 - val_acc: 0.9872\n",
      "Epoch 55/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0523 - val_acc: 0.9872\n",
      "Epoch 56/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0574 - val_acc: 0.9872\n",
      "Epoch 57/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0508 - val_acc: 0.9872\n",
      "Epoch 58/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0541 - val_acc: 0.9872\n",
      "Epoch 59/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0524 - val_acc: 0.9872\n",
      "Epoch 60/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0560 - val_acc: 0.9872\n",
      "Epoch 61/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0569 - val_acc: 0.9872\n",
      "Epoch 62/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0540 - val_acc: 0.9872\n",
      "Epoch 63/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0499 - val_acc: 0.9872\n",
      "Epoch 64/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0536 - val_acc: 0.9872\n",
      "Epoch 65/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0542 - val_acc: 0.9872\n",
      "Epoch 66/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0555 - val_acc: 0.9872\n",
      "Epoch 67/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0534 - val_acc: 0.9872\n",
      "Epoch 68/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0587 - val_acc: 0.9872\n",
      "Epoch 69/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0533 - val_acc: 0.9872\n",
      "Epoch 70/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0583 - val_acc: 0.9872\n",
      "Epoch 71/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0556 - val_acc: 0.9872\n",
      "Epoch 72/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0586 - val_acc: 0.9872\n",
      "Epoch 73/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0573 - val_acc: 0.9872\n",
      "Epoch 74/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0552 - val_acc: 0.9872\n",
      "Epoch 75/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0549 - val_acc: 0.9872\n",
      "Epoch 76/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0573 - val_acc: 0.9872\n",
      "Epoch 77/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0582 - val_acc: 0.9872\n",
      "Epoch 78/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0567 - val_acc: 0.9872\n",
      "Epoch 79/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0569 - val_acc: 0.9872\n",
      "Epoch 80/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0556 - val_acc: 0.9872\n",
      "Epoch 81/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0567 - val_acc: 0.9872\n",
      "Epoch 82/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0539 - val_acc: 0.9872\n",
      "Epoch 83/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0560 - val_acc: 0.9872\n",
      "Epoch 84/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0549 - val_acc: 0.9872\n",
      "Epoch 85/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0557 - val_acc: 0.9872\n",
      "Epoch 86/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0562 - val_acc: 0.9872\n",
      "Epoch 87/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0576 - val_acc: 0.9872\n",
      "Epoch 88/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0571 - val_acc: 0.9872\n",
      "Epoch 89/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0563 - val_acc: 0.9872\n",
      "Epoch 90/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0573 - val_acc: 0.9872\n",
      "Epoch 91/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0577 - val_acc: 0.9872\n",
      "Epoch 92/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0565 - val_acc: 0.9872\n",
      "Epoch 93/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0553 - val_acc: 0.9872\n",
      "Epoch 94/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0566 - val_acc: 0.9872\n",
      "Epoch 95/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0575 - val_acc: 0.9872\n",
      "Epoch 96/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0579 - val_acc: 0.9872\n",
      "Epoch 97/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0576 - val_acc: 0.9872\n",
      "Epoch 98/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0582 - val_acc: 0.9872\n",
      "Epoch 99/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0577 - val_acc: 0.9872\n",
      "Epoch 100/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0577 - val_acc: 0.9872\n",
      "78/78 [==============================] - 0s 317us/step\n",
      "Score for fold 3: loss of 1.058140638547066; acc of 98.71794871794873%\n",
      "Train on 702 samples, validate on 78 samples\n",
      "Epoch 1/100\n",
      "702/702 [==============================] - 4s 6ms/step - loss: 2.2506 - acc: 0.3533 - val_loss: 2.0135 - val_acc: 0.4487\n",
      "Epoch 2/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.6303 - acc: 0.7009 - val_loss: 1.5156 - val_acc: 0.6923\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.3481 - acc: 0.8362 - val_loss: 1.2856 - val_acc: 0.8462\n",
      "Epoch 4/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.2422 - acc: 0.8675 - val_loss: 1.3301 - val_acc: 0.7692\n",
      "Epoch 5/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1846 - acc: 0.8803 - val_loss: 1.2319 - val_acc: 0.8205\n",
      "Epoch 6/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1286 - acc: 0.9117 - val_loss: 1.1694 - val_acc: 0.8077\n",
      "Epoch 7/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1035 - acc: 0.9430 - val_loss: 1.1036 - val_acc: 0.9359\n",
      "Epoch 8/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1290 - acc: 0.9330 - val_loss: 1.2022 - val_acc: 0.8462\n",
      "Epoch 9/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0808 - acc: 0.9644 - val_loss: 1.0806 - val_acc: 0.9744\n",
      "Epoch 10/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0773 - acc: 0.9701 - val_loss: 1.5059 - val_acc: 0.7436\n",
      "Epoch 11/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1109 - acc: 0.9558 - val_loss: 1.0883 - val_acc: 0.9487\n",
      "Epoch 12/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0683 - acc: 0.9815 - val_loss: 1.0601 - val_acc: 1.0000\n",
      "Epoch 13/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0926 - acc: 0.9729 - val_loss: 1.0714 - val_acc: 0.9872\n",
      "Epoch 14/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0539 - acc: 0.9929 - val_loss: 1.0722 - val_acc: 0.9615\n",
      "Epoch 15/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0654 - acc: 0.9786 - val_loss: 1.0599 - val_acc: 0.9872\n",
      "Epoch 16/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0584 - acc: 0.9843 - val_loss: 1.0647 - val_acc: 0.9615\n",
      "Epoch 17/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0534 - acc: 0.9886 - val_loss: 1.0716 - val_acc: 0.9231\n",
      "Epoch 18/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0574 - acc: 0.9829 - val_loss: 1.1109 - val_acc: 0.9615\n",
      "Epoch 19/100\n",
      "702/702 [==============================] - 2s 2ms/step - loss: 1.0540 - acc: 0.9886 - val_loss: 1.0765 - val_acc: 0.9615\n",
      "Epoch 20/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0472 - acc: 0.9957 - val_loss: 1.0473 - val_acc: 0.9872\n",
      "Epoch 21/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0455 - acc: 0.9972 - val_loss: 1.0485 - val_acc: 0.9872\n",
      "Epoch 22/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0492 - acc: 0.9929 - val_loss: 1.0469 - val_acc: 0.9872\n",
      "Epoch 23/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0443 - acc: 0.9986 - val_loss: 1.0464 - val_acc: 1.0000\n",
      "Epoch 24/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0476 - acc: 0.9943 - val_loss: 1.0590 - val_acc: 0.9615\n",
      "Epoch 25/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0505 - acc: 0.9943 - val_loss: 1.0482 - val_acc: 1.0000\n",
      "Epoch 26/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0429 - acc: 1.0000 - val_loss: 1.0448 - val_acc: 1.0000\n",
      "Epoch 27/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0447 - acc: 0.9972 - val_loss: 1.0542 - val_acc: 0.9744\n",
      "Epoch 28/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0428 - acc: 1.0000 - val_loss: 1.0442 - val_acc: 0.9872\n",
      "Epoch 29/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0426 - acc: 1.0000 - val_loss: 1.0708 - val_acc: 0.9744\n",
      "Epoch 30/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0425 - acc: 1.0000 - val_loss: 1.0522 - val_acc: 0.9872\n",
      "Epoch 31/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0427 - acc: 0.9986 - val_loss: 1.0438 - val_acc: 1.0000\n",
      "Epoch 32/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0415 - acc: 1.0000 - val_loss: 1.0431 - val_acc: 1.0000\n",
      "Epoch 33/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0412 - acc: 1.0000 - val_loss: 1.0434 - val_acc: 1.0000\n",
      "Epoch 34/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0416 - acc: 1.0000 - val_loss: 1.0577 - val_acc: 0.9744\n",
      "Epoch 35/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0414 - acc: 1.0000 - val_loss: 1.0435 - val_acc: 1.0000\n",
      "Epoch 36/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0417 - acc: 0.9972 - val_loss: 1.0455 - val_acc: 0.9872\n",
      "Epoch 37/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0410 - acc: 1.0000 - val_loss: 1.0434 - val_acc: 1.0000\n",
      "Epoch 38/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0408 - acc: 1.0000 - val_loss: 1.0426 - val_acc: 1.0000\n",
      "Epoch 39/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0408 - acc: 1.0000 - val_loss: 1.0439 - val_acc: 0.9872\n",
      "Epoch 40/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0407 - acc: 1.0000 - val_loss: 1.0425 - val_acc: 1.0000\n",
      "Epoch 41/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0409 - acc: 1.0000 - val_loss: 1.0542 - val_acc: 0.9872\n",
      "Epoch 42/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0418 - acc: 0.9986 - val_loss: 1.0444 - val_acc: 0.9872\n",
      "Epoch 43/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0408 - acc: 1.0000 - val_loss: 1.0488 - val_acc: 0.9872\n",
      "Epoch 44/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0455 - val_acc: 1.0000\n",
      "Epoch 45/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0460 - val_acc: 1.0000\n",
      "Epoch 46/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0445 - val_acc: 0.9872\n",
      "Epoch 47/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0467 - val_acc: 1.0000\n",
      "Epoch 48/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0437 - val_acc: 1.0000\n",
      "Epoch 49/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0428 - val_acc: 1.0000\n",
      "Epoch 50/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0429 - val_acc: 1.0000\n",
      "Epoch 51/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0436 - val_acc: 1.0000\n",
      "Epoch 52/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0424 - val_acc: 1.0000\n",
      "Epoch 53/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0432 - val_acc: 1.0000\n",
      "Epoch 54/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0429 - val_acc: 1.0000\n",
      "Epoch 55/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0432 - val_acc: 1.0000\n",
      "Epoch 56/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0431 - val_acc: 1.0000\n",
      "Epoch 57/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0447 - val_acc: 1.0000\n",
      "Epoch 58/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0430 - val_acc: 0.9872\n",
      "Epoch 59/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0424 - val_acc: 1.0000\n",
      "Epoch 60/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0434 - val_acc: 1.0000\n",
      "Epoch 61/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0430 - val_acc: 1.0000\n",
      "Epoch 62/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0433 - val_acc: 1.0000\n",
      "Epoch 63/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0434 - val_acc: 1.0000\n",
      "Epoch 64/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0432 - val_acc: 1.0000\n",
      "Epoch 65/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0444 - val_acc: 0.9872\n",
      "Epoch 66/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0424 - val_acc: 1.0000\n",
      "Epoch 67/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0430 - val_acc: 1.0000\n",
      "Epoch 68/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0425 - val_acc: 1.0000\n",
      "Epoch 69/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0432 - val_acc: 1.0000\n",
      "Epoch 70/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0422 - val_acc: 1.0000\n",
      "Epoch 71/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0431 - val_acc: 1.0000\n",
      "Epoch 72/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0422 - val_acc: 1.0000\n",
      "Epoch 73/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0430 - val_acc: 1.0000\n",
      "Epoch 74/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0427 - val_acc: 1.0000\n",
      "Epoch 75/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0429 - val_acc: 1.0000\n",
      "Epoch 76/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0434 - val_acc: 1.0000\n",
      "Epoch 77/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0430 - val_acc: 1.0000\n",
      "Epoch 78/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0432 - val_acc: 1.0000\n",
      "Epoch 79/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0430 - val_acc: 1.0000\n",
      "Epoch 80/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0428 - val_acc: 1.0000\n",
      "Epoch 81/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0428 - val_acc: 1.0000\n",
      "Epoch 82/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0428 - val_acc: 1.0000\n",
      "Epoch 83/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0427 - val_acc: 1.0000\n",
      "Epoch 84/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0427 - val_acc: 1.0000\n",
      "Epoch 85/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0427 - val_acc: 1.0000\n",
      "Epoch 86/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0431 - val_acc: 1.0000\n",
      "Epoch 87/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0429 - val_acc: 1.0000\n",
      "Epoch 88/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0429 - val_acc: 1.0000\n",
      "Epoch 89/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0428 - val_acc: 1.0000\n",
      "Epoch 90/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0427 - val_acc: 1.0000\n",
      "Epoch 91/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0426 - val_acc: 1.0000\n",
      "Epoch 92/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0426 - val_acc: 1.0000\n",
      "Epoch 93/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0426 - val_acc: 1.0000\n",
      "Epoch 94/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0426 - val_acc: 1.0000\n",
      "Epoch 95/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0425 - val_acc: 1.0000\n",
      "Epoch 96/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0425 - val_acc: 1.0000\n",
      "Epoch 97/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0426 - val_acc: 1.0000\n",
      "Epoch 98/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0425 - val_acc: 1.0000\n",
      "Epoch 99/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0426 - val_acc: 1.0000\n",
      "Epoch 100/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0426 - val_acc: 1.0000\n",
      "78/78 [==============================] - 0s 302us/step\n",
      "Score for fold 4: loss of 1.0439049708537567; acc of 100.0%\n",
      "Train on 702 samples, validate on 78 samples\n",
      "Epoch 1/100\n",
      "702/702 [==============================] - 4s 6ms/step - loss: 2.2383 - acc: 0.3319 - val_loss: 1.9139 - val_acc: 0.4744\n",
      "Epoch 2/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.6122 - acc: 0.6994 - val_loss: 1.5089 - val_acc: 0.7949\n",
      "Epoch 3/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.3632 - acc: 0.8177 - val_loss: 1.3906 - val_acc: 0.7308\n",
      "Epoch 4/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.2626 - acc: 0.8533 - val_loss: 1.3251 - val_acc: 0.8077\n",
      "Epoch 5/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1818 - acc: 0.9074 - val_loss: 1.3055 - val_acc: 0.7692\n",
      "Epoch 6/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1504 - acc: 0.9117 - val_loss: 1.1675 - val_acc: 0.8718\n",
      "Epoch 7/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1477 - acc: 0.9060 - val_loss: 1.2021 - val_acc: 0.8846\n",
      "Epoch 8/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1062 - acc: 0.9345 - val_loss: 1.1310 - val_acc: 0.9744\n",
      "Epoch 9/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0920 - acc: 0.9658 - val_loss: 1.2681 - val_acc: 0.7821\n",
      "Epoch 10/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1004 - acc: 0.9573 - val_loss: 1.1394 - val_acc: 0.9231\n",
      "Epoch 11/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0981 - acc: 0.9530 - val_loss: 1.0925 - val_acc: 0.9615\n",
      "Epoch 12/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0689 - acc: 0.9801 - val_loss: 1.0858 - val_acc: 0.9615\n",
      "Epoch 13/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0953 - acc: 0.9516 - val_loss: 1.0814 - val_acc: 0.9487\n",
      "Epoch 14/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0596 - acc: 0.9843 - val_loss: 1.0831 - val_acc: 0.9487\n",
      "Epoch 15/100\n",
      "702/702 [==============================] - 2s 3ms/step - loss: 1.0629 - acc: 0.9758 - val_loss: 1.3372 - val_acc: 0.8077\n",
      "Epoch 16/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0668 - acc: 0.9801 - val_loss: 1.0660 - val_acc: 0.9744\n",
      "Epoch 17/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0727 - acc: 0.9801 - val_loss: 1.0618 - val_acc: 0.9872\n",
      "Epoch 18/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0522 - acc: 0.9915 - val_loss: 1.0629 - val_acc: 0.9615\n",
      "Epoch 19/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0501 - acc: 0.9929 - val_loss: 1.0561 - val_acc: 0.9872\n",
      "Epoch 20/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0490 - acc: 0.9943 - val_loss: 1.1510 - val_acc: 0.8846\n",
      "Epoch 21/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0548 - acc: 0.9843 - val_loss: 1.0589 - val_acc: 0.9744\n",
      "Epoch 22/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0500 - acc: 0.9900 - val_loss: 1.0643 - val_acc: 0.9615\n",
      "Epoch 23/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0455 - acc: 0.9986 - val_loss: 1.0518 - val_acc: 0.9872\n",
      "Epoch 24/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0501 - acc: 0.9900 - val_loss: 1.0511 - val_acc: 0.9872\n",
      "Epoch 25/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0442 - acc: 0.9986 - val_loss: 1.0526 - val_acc: 0.9744\n",
      "Epoch 26/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0437 - acc: 1.0000 - val_loss: 1.0517 - val_acc: 0.9872\n",
      "Epoch 27/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0440 - acc: 0.9986 - val_loss: 1.0550 - val_acc: 0.9872\n",
      "Epoch 28/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0452 - acc: 0.9972 - val_loss: 1.0516 - val_acc: 0.9615\n",
      "Epoch 29/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0428 - acc: 0.9986 - val_loss: 1.0513 - val_acc: 0.9744\n",
      "Epoch 30/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0433 - acc: 1.0000 - val_loss: 1.0592 - val_acc: 0.9615\n",
      "Epoch 31/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0431 - acc: 0.9986 - val_loss: 1.0589 - val_acc: 0.9744\n",
      "Epoch 32/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0419 - acc: 1.0000 - val_loss: 1.0493 - val_acc: 0.9872\n",
      "Epoch 33/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0419 - acc: 1.0000 - val_loss: 1.0511 - val_acc: 0.9744\n",
      "Epoch 34/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0416 - acc: 1.0000 - val_loss: 1.0497 - val_acc: 0.9872\n",
      "Epoch 35/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0428 - acc: 0.9972 - val_loss: 1.0494 - val_acc: 0.9744\n",
      "Epoch 36/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0414 - acc: 1.0000 - val_loss: 1.0529 - val_acc: 0.9615\n",
      "Epoch 37/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0413 - acc: 1.0000 - val_loss: 1.0478 - val_acc: 0.9744\n",
      "Epoch 38/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0416 - acc: 1.0000 - val_loss: 1.0525 - val_acc: 0.9615\n",
      "Epoch 39/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0411 - acc: 1.0000 - val_loss: 1.0506 - val_acc: 0.9872\n",
      "Epoch 40/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0418 - acc: 0.9986 - val_loss: 1.0491 - val_acc: 0.9744\n",
      "Epoch 41/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0410 - acc: 1.0000 - val_loss: 1.0479 - val_acc: 0.9872\n",
      "Epoch 42/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0409 - acc: 1.0000 - val_loss: 1.0476 - val_acc: 0.9744\n",
      "Epoch 43/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0409 - acc: 1.0000 - val_loss: 1.0500 - val_acc: 0.9615\n",
      "Epoch 44/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0408 - acc: 1.0000 - val_loss: 1.0480 - val_acc: 0.9744\n",
      "Epoch 45/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0415 - acc: 0.9986 - val_loss: 1.0467 - val_acc: 0.9872\n",
      "Epoch 46/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0408 - acc: 1.0000 - val_loss: 1.0491 - val_acc: 0.9744\n",
      "Epoch 47/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0407 - acc: 1.0000 - val_loss: 1.0484 - val_acc: 0.9744\n",
      "Epoch 48/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0486 - val_acc: 0.9744\n",
      "Epoch 49/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0494 - val_acc: 0.9744\n",
      "Epoch 50/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0516 - val_acc: 0.9744\n",
      "Epoch 51/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0471 - val_acc: 0.9872\n",
      "Epoch 52/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0467 - val_acc: 0.9872\n",
      "Epoch 53/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0478 - val_acc: 0.9872\n",
      "Epoch 54/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0486 - val_acc: 0.9744\n",
      "Epoch 55/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0476 - val_acc: 0.9744\n",
      "Epoch 56/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0467 - val_acc: 0.9872\n",
      "Epoch 57/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0488 - val_acc: 0.9744\n",
      "Epoch 58/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0483 - val_acc: 0.9744\n",
      "Epoch 59/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0478 - val_acc: 0.9872\n",
      "Epoch 60/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0481 - val_acc: 0.9872\n",
      "Epoch 61/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0476 - val_acc: 0.9872\n",
      "Epoch 62/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0470 - val_acc: 0.9872\n",
      "Epoch 63/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0474 - val_acc: 0.9872\n",
      "Epoch 64/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0478 - val_acc: 0.9872\n",
      "Epoch 65/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0470 - val_acc: 0.9872\n",
      "Epoch 66/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0470 - val_acc: 0.9872\n",
      "Epoch 67/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0473 - val_acc: 0.9872\n",
      "Epoch 68/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0479 - val_acc: 0.9872\n",
      "Epoch 69/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0469 - val_acc: 0.9872\n",
      "Epoch 70/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0475 - val_acc: 0.9872\n",
      "Epoch 71/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0469 - val_acc: 0.9872\n",
      "Epoch 72/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0470 - val_acc: 0.9872\n",
      "Epoch 73/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0469 - val_acc: 0.9872\n",
      "Epoch 74/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0471 - val_acc: 0.9872\n",
      "Epoch 75/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0471 - val_acc: 0.9872\n",
      "Epoch 76/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0467 - val_acc: 0.9872\n",
      "Epoch 77/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0476 - val_acc: 0.9872\n",
      "Epoch 78/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0475 - val_acc: 0.9744\n",
      "Epoch 79/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0475 - val_acc: 0.9872\n",
      "Epoch 80/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0467 - val_acc: 0.9872\n",
      "Epoch 81/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0469 - val_acc: 0.9872\n",
      "Epoch 82/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0474 - val_acc: 0.9872\n",
      "Epoch 83/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0473 - val_acc: 0.9872\n",
      "Epoch 84/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0472 - val_acc: 0.9872\n",
      "Epoch 85/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0465 - val_acc: 0.9872\n",
      "Epoch 86/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0467 - val_acc: 0.9872\n",
      "Epoch 87/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0472 - val_acc: 0.9872\n",
      "Epoch 88/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0470 - val_acc: 0.9872\n",
      "Epoch 89/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0474 - val_acc: 0.9872\n",
      "Epoch 90/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0467 - val_acc: 0.9872\n",
      "Epoch 91/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0468 - val_acc: 0.9872\n",
      "Epoch 92/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0473 - val_acc: 0.9872\n",
      "Epoch 93/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0473 - val_acc: 0.9872\n",
      "Epoch 94/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0471 - val_acc: 0.9872\n",
      "Epoch 95/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0469 - val_acc: 0.9872\n",
      "Epoch 96/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0469 - val_acc: 0.9872\n",
      "Epoch 97/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0471 - val_acc: 0.9872\n",
      "Epoch 98/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0470 - val_acc: 0.9872\n",
      "Epoch 99/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0473 - val_acc: 0.9872\n",
      "Epoch 100/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0475 - val_acc: 0.9872\n",
      "78/78 [==============================] - 0s 301us/step\n",
      "Score for fold 5: loss of 1.0476399446145082; acc of 98.71794871794873%\n",
      "Train on 702 samples, validate on 78 samples\n",
      "Epoch 1/100\n",
      "702/702 [==============================] - 5s 6ms/step - loss: 2.2670 - acc: 0.3077 - val_loss: 1.9783 - val_acc: 0.4103\n",
      "Epoch 2/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.6152 - acc: 0.6966 - val_loss: 1.4741 - val_acc: 0.7308\n",
      "Epoch 3/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.3528 - acc: 0.8063 - val_loss: 1.3574 - val_acc: 0.7949\n",
      "Epoch 4/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.2690 - acc: 0.8405 - val_loss: 1.2978 - val_acc: 0.8462\n",
      "Epoch 5/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1763 - acc: 0.8875 - val_loss: 1.1927 - val_acc: 0.9103\n",
      "Epoch 6/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1754 - acc: 0.9046 - val_loss: 1.1704 - val_acc: 0.8718\n",
      "Epoch 7/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1102 - acc: 0.9402 - val_loss: 1.1919 - val_acc: 0.7564\n",
      "Epoch 8/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1223 - acc: 0.9074 - val_loss: 1.2263 - val_acc: 0.7949\n",
      "Epoch 9/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1134 - acc: 0.9530 - val_loss: 1.0854 - val_acc: 0.9487\n",
      "Epoch 10/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0781 - acc: 0.9658 - val_loss: 1.1283 - val_acc: 0.9487\n",
      "Epoch 11/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0787 - acc: 0.9630 - val_loss: 1.1254 - val_acc: 0.8974\n",
      "Epoch 12/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0902 - acc: 0.9516 - val_loss: 1.0844 - val_acc: 0.9231\n",
      "Epoch 13/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0635 - acc: 0.9843 - val_loss: 1.1793 - val_acc: 0.9231\n",
      "Epoch 14/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0671 - acc: 0.9786 - val_loss: 1.1203 - val_acc: 0.8974\n",
      "Epoch 15/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0620 - acc: 0.9772 - val_loss: 1.0688 - val_acc: 0.9872\n",
      "Epoch 16/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0617 - acc: 0.9843 - val_loss: 1.0545 - val_acc: 0.9744\n",
      "Epoch 17/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0604 - acc: 0.9815 - val_loss: 1.0792 - val_acc: 0.9359\n",
      "Epoch 18/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0775 - acc: 0.9615 - val_loss: 1.0623 - val_acc: 0.9744\n",
      "Epoch 19/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0547 - acc: 0.9886 - val_loss: 1.0589 - val_acc: 0.9615\n",
      "Epoch 20/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0523 - acc: 0.9929 - val_loss: 1.0733 - val_acc: 0.9487\n",
      "Epoch 21/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0537 - acc: 0.9886 - val_loss: 1.0630 - val_acc: 0.9744\n",
      "Epoch 22/100\n",
      "702/702 [==============================] - 2s 3ms/step - loss: 1.0552 - acc: 0.9858 - val_loss: 1.0853 - val_acc: 0.8718\n",
      "Epoch 23/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0500 - acc: 0.9872 - val_loss: 1.0507 - val_acc: 0.9872\n",
      "Epoch 24/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0465 - acc: 0.9943 - val_loss: 1.0853 - val_acc: 0.9359\n",
      "Epoch 25/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0457 - acc: 0.9957 - val_loss: 1.0536 - val_acc: 0.9744\n",
      "Epoch 26/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0443 - acc: 0.9972 - val_loss: 1.0570 - val_acc: 0.9872\n",
      "Epoch 27/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0487 - acc: 0.9900 - val_loss: 1.0484 - val_acc: 0.9872\n",
      "Epoch 28/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0544 - acc: 0.9943 - val_loss: 1.0498 - val_acc: 0.9872\n",
      "Epoch 29/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0424 - acc: 1.0000 - val_loss: 1.0485 - val_acc: 0.9744\n",
      "Epoch 30/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0418 - acc: 1.0000 - val_loss: 1.0486 - val_acc: 0.9872\n",
      "Epoch 31/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0421 - acc: 1.0000 - val_loss: 1.0529 - val_acc: 1.0000\n",
      "Epoch 32/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0428 - acc: 1.0000 - val_loss: 1.0503 - val_acc: 0.9872\n",
      "Epoch 33/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0436 - acc: 0.9943 - val_loss: 1.0469 - val_acc: 0.9872\n",
      "Epoch 34/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0421 - acc: 0.9986 - val_loss: 1.0483 - val_acc: 0.9872\n",
      "Epoch 35/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0425 - acc: 0.9986 - val_loss: 1.0483 - val_acc: 0.9872\n",
      "Epoch 36/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0434 - acc: 0.9972 - val_loss: 1.0463 - val_acc: 1.0000\n",
      "Epoch 37/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0415 - acc: 1.0000 - val_loss: 1.0490 - val_acc: 0.9744\n",
      "Epoch 38/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0425 - acc: 0.9986 - val_loss: 1.0481 - val_acc: 0.9744\n",
      "Epoch 39/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0411 - acc: 1.0000 - val_loss: 1.0463 - val_acc: 1.0000\n",
      "Epoch 40/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0425 - acc: 0.9972 - val_loss: 1.0454 - val_acc: 1.0000\n",
      "Epoch 41/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0408 - acc: 1.0000 - val_loss: 1.0469 - val_acc: 0.9872\n",
      "Epoch 42/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0409 - acc: 1.0000 - val_loss: 1.0447 - val_acc: 1.0000\n",
      "Epoch 43/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0411 - acc: 1.0000 - val_loss: 1.0448 - val_acc: 1.0000\n",
      "Epoch 44/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0408 - acc: 1.0000 - val_loss: 1.0477 - val_acc: 1.0000\n",
      "Epoch 45/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0410 - acc: 1.0000 - val_loss: 1.0474 - val_acc: 1.0000\n",
      "Epoch 46/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0407 - acc: 1.0000 - val_loss: 1.0459 - val_acc: 0.9872\n",
      "Epoch 47/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0452 - val_acc: 1.0000\n",
      "Epoch 48/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0407 - acc: 1.0000 - val_loss: 1.0448 - val_acc: 1.0000\n",
      "Epoch 49/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0407 - acc: 1.0000 - val_loss: 1.0460 - val_acc: 0.9872\n",
      "Epoch 50/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0456 - val_acc: 0.9872\n",
      "Epoch 51/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0450 - val_acc: 1.0000\n",
      "Epoch 52/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0465 - val_acc: 0.9872\n",
      "Epoch 53/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0456 - val_acc: 1.0000\n",
      "Epoch 54/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0457 - val_acc: 1.0000\n",
      "Epoch 55/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0461 - val_acc: 1.0000\n",
      "Epoch 56/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0454 - val_acc: 0.9872\n",
      "Epoch 57/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0452 - val_acc: 1.0000\n",
      "Epoch 58/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0453 - val_acc: 0.9872\n",
      "Epoch 59/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0461 - val_acc: 1.0000\n",
      "Epoch 60/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0452 - val_acc: 1.0000\n",
      "Epoch 61/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0472 - val_acc: 0.9872\n",
      "Epoch 62/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0447 - val_acc: 1.0000\n",
      "Epoch 63/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0452 - val_acc: 1.0000\n",
      "Epoch 64/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0446 - val_acc: 1.0000\n",
      "Epoch 65/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0449 - val_acc: 1.0000\n",
      "Epoch 66/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0448 - val_acc: 1.0000\n",
      "Epoch 67/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0458 - val_acc: 1.0000\n",
      "Epoch 68/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0447 - val_acc: 1.0000\n",
      "Epoch 69/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0457 - val_acc: 1.0000\n",
      "Epoch 70/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0450 - val_acc: 1.0000\n",
      "Epoch 71/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0445 - val_acc: 1.0000\n",
      "Epoch 72/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0446 - val_acc: 1.0000\n",
      "Epoch 73/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0443 - val_acc: 1.0000\n",
      "Epoch 74/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0449 - val_acc: 1.0000\n",
      "Epoch 75/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0447 - val_acc: 1.0000\n",
      "Epoch 76/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0449 - val_acc: 1.0000\n",
      "Epoch 77/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0447 - val_acc: 1.0000\n",
      "Epoch 78/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0448 - val_acc: 1.0000\n",
      "Epoch 79/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0449 - val_acc: 1.0000\n",
      "Epoch 80/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0453 - val_acc: 1.0000\n",
      "Epoch 81/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0448 - val_acc: 1.0000\n",
      "Epoch 82/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0451 - val_acc: 1.0000\n",
      "Epoch 83/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0449 - val_acc: 1.0000\n",
      "Epoch 84/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0449 - val_acc: 1.0000\n",
      "Epoch 85/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0448 - val_acc: 1.0000\n",
      "Epoch 86/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0449 - val_acc: 1.0000\n",
      "Epoch 87/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0450 - val_acc: 1.0000\n",
      "Epoch 88/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0448 - val_acc: 1.0000\n",
      "Epoch 89/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0459 - val_acc: 1.0000\n",
      "Epoch 90/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0449 - val_acc: 1.0000\n",
      "Epoch 91/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0447 - val_acc: 1.0000\n",
      "Epoch 92/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0449 - val_acc: 1.0000\n",
      "Epoch 93/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0449 - val_acc: 1.0000\n",
      "Epoch 94/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0451 - val_acc: 1.0000\n",
      "Epoch 95/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0451 - val_acc: 1.0000\n",
      "Epoch 96/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0449 - val_acc: 1.0000\n",
      "Epoch 97/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0448 - val_acc: 1.0000\n",
      "Epoch 98/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0450 - val_acc: 1.0000\n",
      "Epoch 99/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0450 - val_acc: 1.0000\n",
      "Epoch 100/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0450 - val_acc: 1.0000\n",
      "78/78 [==============================] - 0s 305us/step\n",
      "Score for fold 6: loss of 1.045926103225121; acc of 100.0%\n",
      "Train on 702 samples, validate on 78 samples\n",
      "Epoch 1/100\n",
      "702/702 [==============================] - 5s 7ms/step - loss: 2.2770 - acc: 0.3205 - val_loss: 1.9064 - val_acc: 0.6410\n",
      "Epoch 2/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.6874 - acc: 0.6538 - val_loss: 1.5696 - val_acc: 0.7179\n",
      "Epoch 3/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.3763 - acc: 0.8319 - val_loss: 1.3513 - val_acc: 0.7949\n",
      "Epoch 4/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.2487 - acc: 0.8789 - val_loss: 1.1808 - val_acc: 0.9615\n",
      "Epoch 5/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1713 - acc: 0.8946 - val_loss: 1.1982 - val_acc: 0.7821\n",
      "Epoch 6/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1246 - acc: 0.9274 - val_loss: 1.2387 - val_acc: 0.8077\n",
      "Epoch 7/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1199 - acc: 0.9231 - val_loss: 1.1396 - val_acc: 0.9103\n",
      "Epoch 8/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1271 - acc: 0.9330 - val_loss: 1.1275 - val_acc: 0.9103\n",
      "Epoch 9/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0860 - acc: 0.9587 - val_loss: 1.0865 - val_acc: 0.9615\n",
      "Epoch 10/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0699 - acc: 0.9786 - val_loss: 1.1775 - val_acc: 0.7821\n",
      "Epoch 11/100\n",
      "702/702 [==============================] - 2s 3ms/step - loss: 1.0949 - acc: 0.9658 - val_loss: 1.1236 - val_acc: 0.8846\n",
      "Epoch 12/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0602 - acc: 0.9829 - val_loss: 1.0910 - val_acc: 0.9487\n",
      "Epoch 13/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0618 - acc: 0.9843 - val_loss: 1.0818 - val_acc: 0.9872\n",
      "Epoch 14/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0548 - acc: 0.9915 - val_loss: 1.1211 - val_acc: 0.9231\n",
      "Epoch 15/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0603 - acc: 0.9815 - val_loss: 1.0731 - val_acc: 0.9872\n",
      "Epoch 16/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0655 - acc: 0.9772 - val_loss: 1.0748 - val_acc: 0.9872\n",
      "Epoch 17/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0559 - acc: 0.9886 - val_loss: 1.0881 - val_acc: 0.9359\n",
      "Epoch 18/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0493 - acc: 0.9943 - val_loss: 1.1027 - val_acc: 0.9744\n",
      "Epoch 19/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0500 - acc: 0.9972 - val_loss: 1.0745 - val_acc: 0.9872\n",
      "Epoch 20/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0563 - acc: 0.9858 - val_loss: 1.0844 - val_acc: 0.9615\n",
      "Epoch 21/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0471 - acc: 0.9957 - val_loss: 1.0828 - val_acc: 0.9872\n",
      "Epoch 22/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0464 - acc: 0.9957 - val_loss: 1.1030 - val_acc: 0.9744\n",
      "Epoch 23/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0480 - acc: 0.9943 - val_loss: 1.1338 - val_acc: 0.9615\n",
      "Epoch 24/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0462 - acc: 0.9943 - val_loss: 1.1020 - val_acc: 0.9615\n",
      "Epoch 25/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0457 - acc: 0.9957 - val_loss: 1.0889 - val_acc: 0.9872\n",
      "Epoch 26/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0447 - acc: 0.9972 - val_loss: 1.0876 - val_acc: 0.9872\n",
      "Epoch 27/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0435 - acc: 0.9986 - val_loss: 1.0820 - val_acc: 0.9744\n",
      "Epoch 28/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0437 - acc: 0.9986 - val_loss: 1.0821 - val_acc: 0.9872\n",
      "Epoch 29/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0429 - acc: 0.9986 - val_loss: 1.1006 - val_acc: 0.9615\n",
      "Epoch 30/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0464 - acc: 0.9943 - val_loss: 1.0894 - val_acc: 0.9744\n",
      "Epoch 31/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0427 - acc: 0.9986 - val_loss: 1.0807 - val_acc: 0.9872\n",
      "Epoch 32/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0420 - acc: 1.0000 - val_loss: 1.0793 - val_acc: 0.9872\n",
      "Epoch 33/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0417 - acc: 1.0000 - val_loss: 1.0771 - val_acc: 0.9872\n",
      "Epoch 34/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0419 - acc: 1.0000 - val_loss: 1.0746 - val_acc: 0.9872\n",
      "Epoch 35/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0418 - acc: 1.0000 - val_loss: 1.0792 - val_acc: 0.9744\n",
      "Epoch 36/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0415 - acc: 1.0000 - val_loss: 1.0724 - val_acc: 0.9872\n",
      "Epoch 37/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0422 - acc: 0.9986 - val_loss: 1.0792 - val_acc: 0.9872\n",
      "Epoch 38/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0412 - acc: 1.0000 - val_loss: 1.0773 - val_acc: 0.9872\n",
      "Epoch 39/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0410 - acc: 1.0000 - val_loss: 1.0769 - val_acc: 0.9872\n",
      "Epoch 40/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0413 - acc: 1.0000 - val_loss: 1.0761 - val_acc: 0.9744\n",
      "Epoch 41/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0410 - acc: 1.0000 - val_loss: 1.0763 - val_acc: 0.9872\n",
      "Epoch 42/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0410 - acc: 1.0000 - val_loss: 1.0783 - val_acc: 0.9872\n",
      "Epoch 43/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0409 - acc: 1.0000 - val_loss: 1.0844 - val_acc: 0.9744\n",
      "Epoch 44/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0410 - acc: 1.0000 - val_loss: 1.0786 - val_acc: 0.9872\n",
      "Epoch 45/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0408 - acc: 1.0000 - val_loss: 1.0753 - val_acc: 0.9744\n",
      "Epoch 46/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0408 - acc: 1.0000 - val_loss: 1.0780 - val_acc: 0.9872\n",
      "Epoch 47/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0408 - acc: 1.0000 - val_loss: 1.0784 - val_acc: 0.9744\n",
      "Epoch 48/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0407 - acc: 1.0000 - val_loss: 1.0757 - val_acc: 0.9872\n",
      "Epoch 49/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0806 - val_acc: 0.9744\n",
      "Epoch 50/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0408 - acc: 1.0000 - val_loss: 1.0763 - val_acc: 0.9872\n",
      "Epoch 51/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0770 - val_acc: 0.9744\n",
      "Epoch 52/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0765 - val_acc: 0.9872\n",
      "Epoch 53/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0755 - val_acc: 0.9872\n",
      "Epoch 54/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0768 - val_acc: 0.9872\n",
      "Epoch 55/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0774 - val_acc: 0.9872\n",
      "Epoch 56/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0793 - val_acc: 0.9872\n",
      "Epoch 57/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0772 - val_acc: 0.9872\n",
      "Epoch 58/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0769 - val_acc: 0.9872\n",
      "Epoch 59/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0769 - val_acc: 0.9872\n",
      "Epoch 60/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0766 - val_acc: 0.9872\n",
      "Epoch 61/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0781 - val_acc: 0.9872\n",
      "Epoch 62/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0759 - val_acc: 0.9872\n",
      "Epoch 63/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0769 - val_acc: 0.9872\n",
      "Epoch 64/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0765 - val_acc: 0.9872\n",
      "Epoch 65/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0770 - val_acc: 0.9872\n",
      "Epoch 66/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0779 - val_acc: 0.9744\n",
      "Epoch 67/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0783 - val_acc: 0.9872\n",
      "Epoch 68/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0750 - val_acc: 0.9872\n",
      "Epoch 69/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0764 - val_acc: 0.9872\n",
      "Epoch 70/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0763 - val_acc: 0.9872\n",
      "Epoch 71/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0764 - val_acc: 0.9872\n",
      "Epoch 72/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0768 - val_acc: 0.9872\n",
      "Epoch 73/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0767 - val_acc: 0.9872\n",
      "Epoch 74/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0764 - val_acc: 0.9872\n",
      "Epoch 75/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0757 - val_acc: 0.9872\n",
      "Epoch 76/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0765 - val_acc: 0.9872\n",
      "Epoch 77/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0764 - val_acc: 0.9872\n",
      "Epoch 78/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0757 - val_acc: 0.9872\n",
      "Epoch 79/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0751 - val_acc: 0.9872\n",
      "Epoch 80/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0754 - val_acc: 0.9872\n",
      "Epoch 81/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0746 - val_acc: 0.9872\n",
      "Epoch 82/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0758 - val_acc: 0.9872\n",
      "Epoch 83/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0749 - val_acc: 0.9872\n",
      "Epoch 84/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0754 - val_acc: 0.9872\n",
      "Epoch 85/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0742 - val_acc: 0.9872\n",
      "Epoch 86/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0757 - val_acc: 0.9872\n",
      "Epoch 87/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0753 - val_acc: 0.9872\n",
      "Epoch 88/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0756 - val_acc: 0.9872\n",
      "Epoch 89/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0748 - val_acc: 0.9872\n",
      "Epoch 90/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0765 - val_acc: 0.9872\n",
      "Epoch 91/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0754 - val_acc: 0.9872\n",
      "Epoch 92/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0750 - val_acc: 0.9872\n",
      "Epoch 93/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0751 - val_acc: 0.9872\n",
      "Epoch 94/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0748 - val_acc: 0.9872\n",
      "Epoch 95/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0747 - val_acc: 0.9872\n",
      "Epoch 96/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0734 - val_acc: 0.9872\n",
      "Epoch 97/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0742 - val_acc: 0.9872\n",
      "Epoch 98/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0744 - val_acc: 0.9872\n",
      "Epoch 99/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0743 - val_acc: 0.9872\n",
      "Epoch 100/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0746 - val_acc: 0.9872\n",
      "78/78 [==============================] - 0s 317us/step\n",
      "Score for fold 7: loss of 1.0755767058103511; acc of 98.71794871794873%\n",
      "Train on 702 samples, validate on 78 samples\n",
      "Epoch 1/100\n",
      "702/702 [==============================] - 5s 7ms/step - loss: 2.2911 - acc: 0.3476 - val_loss: 1.8932 - val_acc: 0.6154\n",
      "Epoch 2/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.6540 - acc: 0.6681 - val_loss: 1.5072 - val_acc: 0.7179\n",
      "Epoch 3/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.3954 - acc: 0.7849 - val_loss: 1.3241 - val_acc: 0.8077\n",
      "Epoch 4/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.2583 - acc: 0.8675 - val_loss: 1.3050 - val_acc: 0.7179\n",
      "Epoch 5/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.2189 - acc: 0.8689 - val_loss: 1.1714 - val_acc: 0.8718\n",
      "Epoch 6/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1544 - acc: 0.9088 - val_loss: 1.2096 - val_acc: 0.8590\n",
      "Epoch 7/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1203 - acc: 0.9245 - val_loss: 1.1311 - val_acc: 0.9103\n",
      "Epoch 8/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1193 - acc: 0.9188 - val_loss: 1.1232 - val_acc: 0.9231\n",
      "Epoch 9/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0943 - acc: 0.9501 - val_loss: 1.1105 - val_acc: 0.9103\n",
      "Epoch 10/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0828 - acc: 0.9658 - val_loss: 1.1671 - val_acc: 0.9615\n",
      "Epoch 11/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0930 - acc: 0.9658 - val_loss: 1.7065 - val_acc: 0.8077\n",
      "Epoch 12/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0867 - acc: 0.9772 - val_loss: 1.1229 - val_acc: 0.8846\n",
      "Epoch 13/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0788 - acc: 0.9772 - val_loss: 1.1082 - val_acc: 0.8974\n",
      "Epoch 14/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0586 - acc: 0.9829 - val_loss: 2.0444 - val_acc: 0.6795\n",
      "Epoch 15/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0770 - acc: 0.9786 - val_loss: 1.0817 - val_acc: 0.9744\n",
      "Epoch 16/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0548 - acc: 0.9900 - val_loss: 1.0901 - val_acc: 0.9487\n",
      "Epoch 17/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0757 - acc: 0.9786 - val_loss: 1.1013 - val_acc: 0.9615\n",
      "Epoch 18/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0541 - acc: 0.9872 - val_loss: 1.0805 - val_acc: 0.9359\n",
      "Epoch 19/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0619 - acc: 0.9815 - val_loss: 1.0928 - val_acc: 0.9359\n",
      "Epoch 20/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0540 - acc: 0.9900 - val_loss: 1.0770 - val_acc: 0.9615\n",
      "Epoch 21/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0504 - acc: 0.9900 - val_loss: 1.1352 - val_acc: 0.9231\n",
      "Epoch 22/100\n",
      "702/702 [==============================] - 2s 3ms/step - loss: 1.0557 - acc: 0.9900 - val_loss: 1.0852 - val_acc: 0.9231\n",
      "Epoch 23/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0480 - acc: 0.9929 - val_loss: 1.0740 - val_acc: 0.9487\n",
      "Epoch 24/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0481 - acc: 0.9943 - val_loss: 1.0825 - val_acc: 0.9615\n",
      "Epoch 25/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0453 - acc: 0.9986 - val_loss: 1.0755 - val_acc: 0.9615\n",
      "Epoch 26/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0472 - acc: 0.9943 - val_loss: 1.0689 - val_acc: 0.9615\n",
      "Epoch 27/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0498 - acc: 0.9886 - val_loss: 1.0705 - val_acc: 0.9744\n",
      "Epoch 28/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0440 - acc: 0.9986 - val_loss: 1.0745 - val_acc: 0.9615\n",
      "Epoch 29/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0443 - acc: 0.9957 - val_loss: 1.0751 - val_acc: 0.9487\n",
      "Epoch 30/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0459 - acc: 0.9943 - val_loss: 1.0608 - val_acc: 0.9615\n",
      "Epoch 31/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0432 - acc: 0.9986 - val_loss: 1.0729 - val_acc: 0.9487\n",
      "Epoch 32/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0436 - acc: 0.9986 - val_loss: 1.1064 - val_acc: 0.9359\n",
      "Epoch 33/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0431 - acc: 0.9972 - val_loss: 1.0574 - val_acc: 0.9744\n",
      "Epoch 34/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0425 - acc: 0.9986 - val_loss: 1.0633 - val_acc: 0.9744\n",
      "Epoch 35/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0426 - acc: 0.9986 - val_loss: 1.0661 - val_acc: 0.9744\n",
      "Epoch 36/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0421 - acc: 0.9986 - val_loss: 1.0577 - val_acc: 0.9744\n",
      "Epoch 37/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0416 - acc: 1.0000 - val_loss: 1.0601 - val_acc: 0.9872\n",
      "Epoch 38/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0420 - acc: 0.9986 - val_loss: 1.0662 - val_acc: 0.9615\n",
      "Epoch 39/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0417 - acc: 0.9986 - val_loss: 1.0634 - val_acc: 0.9744\n",
      "Epoch 40/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0415 - acc: 1.0000 - val_loss: 1.0632 - val_acc: 0.9744\n",
      "Epoch 41/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0416 - acc: 1.0000 - val_loss: 1.0608 - val_acc: 0.9615\n",
      "Epoch 42/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0418 - acc: 1.0000 - val_loss: 1.0671 - val_acc: 0.9744\n",
      "Epoch 43/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0412 - acc: 1.0000 - val_loss: 1.0676 - val_acc: 0.9744\n",
      "Epoch 44/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0412 - acc: 1.0000 - val_loss: 1.0591 - val_acc: 0.9872\n",
      "Epoch 45/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0411 - acc: 1.0000 - val_loss: 1.0597 - val_acc: 0.9744\n",
      "Epoch 46/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0411 - acc: 1.0000 - val_loss: 1.0593 - val_acc: 0.9744\n",
      "Epoch 47/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0408 - acc: 1.0000 - val_loss: 1.0589 - val_acc: 0.9744\n",
      "Epoch 48/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0408 - acc: 1.0000 - val_loss: 1.0656 - val_acc: 0.9615\n",
      "Epoch 49/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0409 - acc: 1.0000 - val_loss: 1.0724 - val_acc: 0.9487\n",
      "Epoch 50/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0410 - acc: 1.0000 - val_loss: 1.0664 - val_acc: 0.9744\n",
      "Epoch 51/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0668 - val_acc: 0.9615\n",
      "Epoch 52/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0670 - val_acc: 0.9744\n",
      "Epoch 53/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0597 - val_acc: 0.9744\n",
      "Epoch 54/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0661 - val_acc: 0.9744\n",
      "Epoch 55/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0631 - val_acc: 0.9744\n",
      "Epoch 56/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0655 - val_acc: 0.9744\n",
      "Epoch 57/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0665 - val_acc: 0.9744\n",
      "Epoch 58/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0642 - val_acc: 0.9744\n",
      "Epoch 59/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0589 - val_acc: 0.9744\n",
      "Epoch 60/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0685 - val_acc: 0.9744\n",
      "Epoch 61/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0673 - val_acc: 0.9744\n",
      "Epoch 62/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0691 - val_acc: 0.9744\n",
      "Epoch 63/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0664 - val_acc: 0.9744\n",
      "Epoch 64/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0690 - val_acc: 0.9744\n",
      "Epoch 65/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0627 - val_acc: 0.9744\n",
      "Epoch 66/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0602 - val_acc: 0.9744\n",
      "Epoch 67/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0584 - val_acc: 0.9744\n",
      "Epoch 68/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0663 - val_acc: 0.9615\n",
      "Epoch 69/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0592 - val_acc: 0.9744\n",
      "Epoch 70/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0637 - val_acc: 0.9744\n",
      "Epoch 71/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0568 - val_acc: 0.9872\n",
      "Epoch 72/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0591 - val_acc: 0.9744\n",
      "Epoch 73/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0625 - val_acc: 0.9744\n",
      "Epoch 74/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0592 - val_acc: 0.9744\n",
      "Epoch 75/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0616 - val_acc: 0.9744\n",
      "Epoch 76/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0593 - val_acc: 0.9744\n",
      "Epoch 77/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0591 - val_acc: 0.9744\n",
      "Epoch 78/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0638 - val_acc: 0.9744\n",
      "Epoch 79/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0593 - val_acc: 0.9744\n",
      "Epoch 80/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0633 - val_acc: 0.9744\n",
      "Epoch 81/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0574 - val_acc: 0.9872\n",
      "Epoch 82/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0578 - val_acc: 0.9744\n",
      "Epoch 83/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0592 - val_acc: 0.9744\n",
      "Epoch 84/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0585 - val_acc: 0.9744\n",
      "Epoch 85/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0587 - val_acc: 0.9744\n",
      "Epoch 86/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0581 - val_acc: 0.9744\n",
      "Epoch 87/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0581 - val_acc: 0.9744\n",
      "Epoch 88/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0585 - val_acc: 0.9744\n",
      "Epoch 89/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0588 - val_acc: 0.9744\n",
      "Epoch 90/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0585 - val_acc: 0.9744\n",
      "Epoch 91/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0582 - val_acc: 0.9744\n",
      "Epoch 92/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0596 - val_acc: 0.9744\n",
      "Epoch 93/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0585 - val_acc: 0.9744\n",
      "Epoch 94/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0593 - val_acc: 0.9744\n",
      "Epoch 95/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0589 - val_acc: 0.9744\n",
      "Epoch 96/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0584 - val_acc: 0.9744\n",
      "Epoch 97/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0602 - val_acc: 0.9744\n",
      "Epoch 98/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0610 - val_acc: 0.9744\n",
      "Epoch 99/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0600 - val_acc: 0.9744\n",
      "Epoch 100/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0594 - val_acc: 0.9744\n",
      "78/78 [==============================] - 0s 294us/step\n",
      "Score for fold 8: loss of 1.0588542559208014; acc of 97.43589743589743%\n",
      "Train on 702 samples, validate on 78 samples\n",
      "Epoch 1/100\n",
      "702/702 [==============================] - 5s 7ms/step - loss: 2.2051 - acc: 0.3419 - val_loss: 1.7334 - val_acc: 0.6410\n",
      "Epoch 2/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.5487 - acc: 0.7222 - val_loss: 1.3530 - val_acc: 0.8205\n",
      "Epoch 3/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.3240 - acc: 0.8205 - val_loss: 1.2973 - val_acc: 0.8462\n",
      "Epoch 4/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1741 - acc: 0.9131 - val_loss: 1.2720 - val_acc: 0.7949\n",
      "Epoch 5/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1748 - acc: 0.9060 - val_loss: 1.1261 - val_acc: 0.9359\n",
      "Epoch 6/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1441 - acc: 0.9345 - val_loss: 1.1087 - val_acc: 0.9487\n",
      "Epoch 7/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1009 - acc: 0.9558 - val_loss: 1.0997 - val_acc: 0.9103\n",
      "Epoch 8/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1067 - acc: 0.9587 - val_loss: 1.1160 - val_acc: 0.8846\n",
      "Epoch 9/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0864 - acc: 0.9558 - val_loss: 1.1042 - val_acc: 0.9231\n",
      "Epoch 10/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1039 - acc: 0.9487 - val_loss: 1.1008 - val_acc: 0.9359\n",
      "Epoch 11/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0813 - acc: 0.9672 - val_loss: 1.1333 - val_acc: 0.8718\n",
      "Epoch 12/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0692 - acc: 0.9715 - val_loss: 1.0784 - val_acc: 0.9744\n",
      "Epoch 13/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0722 - acc: 0.9772 - val_loss: 1.0744 - val_acc: 0.9487\n",
      "Epoch 14/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0552 - acc: 0.9829 - val_loss: 1.0773 - val_acc: 0.9615\n",
      "Epoch 15/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0728 - acc: 0.9658 - val_loss: 1.0703 - val_acc: 0.9744\n",
      "Epoch 16/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0519 - acc: 0.9929 - val_loss: 1.0779 - val_acc: 0.9615\n",
      "Epoch 17/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0664 - acc: 0.9900 - val_loss: 1.0674 - val_acc: 0.9744\n",
      "Epoch 18/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0542 - acc: 0.9915 - val_loss: 1.2422 - val_acc: 0.8205\n",
      "Epoch 19/100\n",
      "702/702 [==============================] - 2s 3ms/step - loss: 1.0565 - acc: 0.9843 - val_loss: 1.0700 - val_acc: 0.9744\n",
      "Epoch 20/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0479 - acc: 0.9957 - val_loss: 1.1286 - val_acc: 0.9487\n",
      "Epoch 21/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0475 - acc: 0.9943 - val_loss: 1.0669 - val_acc: 0.9744\n",
      "Epoch 22/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0491 - acc: 0.9943 - val_loss: 1.0667 - val_acc: 0.9744\n",
      "Epoch 23/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0456 - acc: 0.9972 - val_loss: 1.0702 - val_acc: 0.9744\n",
      "Epoch 24/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0522 - acc: 0.9957 - val_loss: 1.0658 - val_acc: 0.9744\n",
      "Epoch 25/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0515 - acc: 0.9957 - val_loss: 1.0684 - val_acc: 0.9744\n",
      "Epoch 26/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0427 - acc: 1.0000 - val_loss: 1.0659 - val_acc: 0.9744\n",
      "Epoch 27/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0436 - acc: 0.9972 - val_loss: 1.0668 - val_acc: 0.9744\n",
      "Epoch 28/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0430 - acc: 0.9972 - val_loss: 1.0679 - val_acc: 0.9744\n",
      "Epoch 29/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0418 - acc: 1.0000 - val_loss: 1.0670 - val_acc: 0.9744\n",
      "Epoch 30/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0422 - acc: 1.0000 - val_loss: 1.0656 - val_acc: 0.9744\n",
      "Epoch 31/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0424 - acc: 0.9986 - val_loss: 1.0659 - val_acc: 0.9744\n",
      "Epoch 32/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0419 - acc: 1.0000 - val_loss: 1.0623 - val_acc: 0.9744\n",
      "Epoch 33/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0414 - acc: 1.0000 - val_loss: 1.0667 - val_acc: 0.9744\n",
      "Epoch 34/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0412 - acc: 1.0000 - val_loss: 1.0662 - val_acc: 0.9744\n",
      "Epoch 35/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0412 - acc: 1.0000 - val_loss: 1.0665 - val_acc: 0.9744\n",
      "Epoch 36/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0413 - acc: 1.0000 - val_loss: 1.0646 - val_acc: 0.9744\n",
      "Epoch 37/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0412 - acc: 1.0000 - val_loss: 1.0700 - val_acc: 0.9615\n",
      "Epoch 38/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0421 - acc: 0.9972 - val_loss: 1.0648 - val_acc: 0.9744\n",
      "Epoch 39/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0408 - acc: 1.0000 - val_loss: 1.0650 - val_acc: 0.9744\n",
      "Epoch 40/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0407 - acc: 1.0000 - val_loss: 1.0646 - val_acc: 0.9744\n",
      "Epoch 41/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0407 - acc: 1.0000 - val_loss: 1.0702 - val_acc: 0.9615\n",
      "Epoch 42/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0408 - acc: 1.0000 - val_loss: 1.0646 - val_acc: 0.9744\n",
      "Epoch 43/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0407 - acc: 1.0000 - val_loss: 1.0649 - val_acc: 0.9744\n",
      "Epoch 44/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0640 - val_acc: 0.9744\n",
      "Epoch 45/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0653 - val_acc: 0.9744\n",
      "Epoch 46/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0649 - val_acc: 0.9744\n",
      "Epoch 47/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0647 - val_acc: 0.9744\n",
      "Epoch 48/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0657 - val_acc: 0.9744\n",
      "Epoch 49/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0650 - val_acc: 0.9744\n",
      "Epoch 50/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0655 - val_acc: 0.9744\n",
      "Epoch 51/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0651 - val_acc: 0.9744\n",
      "Epoch 52/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0646 - val_acc: 0.9744\n",
      "Epoch 53/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0648 - val_acc: 0.9744\n",
      "Epoch 54/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0643 - val_acc: 0.9744\n",
      "Epoch 55/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0658 - val_acc: 0.9744\n",
      "Epoch 56/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0646 - val_acc: 0.9744\n",
      "Epoch 57/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0650 - val_acc: 0.9744\n",
      "Epoch 58/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0649 - val_acc: 0.9744\n",
      "Epoch 59/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0649 - val_acc: 0.9744\n",
      "Epoch 60/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0647 - val_acc: 0.9744\n",
      "Epoch 61/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0648 - val_acc: 0.9744\n",
      "Epoch 62/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0647 - val_acc: 0.9744\n",
      "Epoch 63/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0645 - val_acc: 0.9744\n",
      "Epoch 64/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0659 - val_acc: 0.9744\n",
      "Epoch 65/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0645 - val_acc: 0.9744\n",
      "Epoch 66/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0645 - val_acc: 0.9744\n",
      "Epoch 67/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0644 - val_acc: 0.9744\n",
      "Epoch 68/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0645 - val_acc: 0.9744\n",
      "Epoch 69/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0649 - val_acc: 0.9744\n",
      "Epoch 70/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0646 - val_acc: 0.9744\n",
      "Epoch 71/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0650 - val_acc: 0.9744\n",
      "Epoch 72/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0646 - val_acc: 0.9744\n",
      "Epoch 73/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0647 - val_acc: 0.9744\n",
      "Epoch 74/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0646 - val_acc: 0.9744\n",
      "Epoch 75/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0646 - val_acc: 0.9744\n",
      "Epoch 76/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0647 - val_acc: 0.9744\n",
      "Epoch 77/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0647 - val_acc: 0.9744\n",
      "Epoch 78/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0646 - val_acc: 0.9744\n",
      "Epoch 79/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0647 - val_acc: 0.9744\n",
      "Epoch 80/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0646 - val_acc: 0.9744\n",
      "Epoch 81/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0646 - val_acc: 0.9744\n",
      "Epoch 82/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0650 - val_acc: 0.9744\n",
      "Epoch 83/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0648 - val_acc: 0.9744\n",
      "Epoch 84/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0647 - val_acc: 0.9744\n",
      "Epoch 85/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0642 - val_acc: 0.9744\n",
      "Epoch 86/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0644 - val_acc: 0.9744\n",
      "Epoch 87/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0645 - val_acc: 0.9744\n",
      "Epoch 88/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0647 - val_acc: 0.9744\n",
      "Epoch 89/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0647 - val_acc: 0.9744\n",
      "Epoch 90/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0646 - val_acc: 0.9744\n",
      "Epoch 91/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0647 - val_acc: 0.9744\n",
      "Epoch 92/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0646 - val_acc: 0.9744\n",
      "Epoch 93/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0645 - val_acc: 0.9744\n",
      "Epoch 94/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0648 - val_acc: 0.9744\n",
      "Epoch 95/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0645 - val_acc: 0.9744\n",
      "Epoch 96/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0647 - val_acc: 0.9744\n",
      "Epoch 97/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0647 - val_acc: 0.9744\n",
      "Epoch 98/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0647 - val_acc: 0.9744\n",
      "Epoch 99/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0647 - val_acc: 0.9744\n",
      "Epoch 100/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0646 - val_acc: 0.9744\n",
      "78/78 [==============================] - 0s 410us/step\n",
      "Score for fold 9: loss of 1.0649176836013794; acc of 97.43589697740018%\n",
      "Train on 702 samples, validate on 78 samples\n",
      "Epoch 1/100\n",
      "702/702 [==============================] - 5s 8ms/step - loss: 2.2282 - acc: 0.3647 - val_loss: 1.9463 - val_acc: 0.4615\n",
      "Epoch 2/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.6546 - acc: 0.6553 - val_loss: 1.5267 - val_acc: 0.6410\n",
      "Epoch 3/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.3422 - acc: 0.8034 - val_loss: 1.3849 - val_acc: 0.7051\n",
      "Epoch 4/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.2297 - acc: 0.8732 - val_loss: 1.2468 - val_acc: 0.8462\n",
      "Epoch 5/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1827 - acc: 0.8932 - val_loss: 1.1984 - val_acc: 0.8974\n",
      "Epoch 6/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1433 - acc: 0.9088 - val_loss: 1.1078 - val_acc: 0.9615\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1218 - acc: 0.9345 - val_loss: 1.1161 - val_acc: 0.8846\n",
      "Epoch 8/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0976 - acc: 0.9516 - val_loss: 1.0870 - val_acc: 0.9872\n",
      "Epoch 9/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0875 - acc: 0.9601 - val_loss: 1.1054 - val_acc: 0.9487\n",
      "Epoch 10/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0787 - acc: 0.9644 - val_loss: 1.0763 - val_acc: 0.9615\n",
      "Epoch 11/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0920 - acc: 0.9487 - val_loss: 1.3247 - val_acc: 0.8462\n",
      "Epoch 12/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0702 - acc: 0.9815 - val_loss: 1.0708 - val_acc: 0.9744\n",
      "Epoch 13/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0670 - acc: 0.9858 - val_loss: 1.0671 - val_acc: 0.9487\n",
      "Epoch 14/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0587 - acc: 0.9843 - val_loss: 1.0866 - val_acc: 0.9744\n",
      "Epoch 15/100\n",
      "702/702 [==============================] - 2s 3ms/step - loss: 1.0611 - acc: 0.9858 - val_loss: 1.2232 - val_acc: 0.8462\n",
      "Epoch 16/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0741 - acc: 0.9786 - val_loss: 1.0532 - val_acc: 1.0000\n",
      "Epoch 17/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0505 - acc: 0.9929 - val_loss: 1.0842 - val_acc: 0.9359\n",
      "Epoch 18/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0541 - acc: 0.9829 - val_loss: 1.0597 - val_acc: 0.9872\n",
      "Epoch 19/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0622 - acc: 0.9900 - val_loss: 1.0863 - val_acc: 0.9615\n",
      "Epoch 20/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0473 - acc: 0.9986 - val_loss: 1.0548 - val_acc: 0.9872\n",
      "Epoch 21/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0454 - acc: 0.9972 - val_loss: 1.0761 - val_acc: 0.9615\n",
      "Epoch 22/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0491 - acc: 0.9900 - val_loss: 1.0617 - val_acc: 0.9744\n",
      "Epoch 23/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0462 - acc: 0.9972 - val_loss: 1.0505 - val_acc: 0.9872\n",
      "Epoch 24/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0460 - acc: 0.9972 - val_loss: 1.0489 - val_acc: 0.9872\n",
      "Epoch 25/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0442 - acc: 0.9986 - val_loss: 1.0526 - val_acc: 0.9872\n",
      "Epoch 26/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0445 - acc: 0.9957 - val_loss: 1.0503 - val_acc: 0.9872\n",
      "Epoch 27/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0444 - acc: 0.9986 - val_loss: 1.0707 - val_acc: 0.9744\n",
      "Epoch 28/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0457 - acc: 0.9943 - val_loss: 1.0484 - val_acc: 0.9872\n",
      "Epoch 29/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0429 - acc: 0.9986 - val_loss: 1.0475 - val_acc: 1.0000\n",
      "Epoch 30/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0425 - acc: 0.9986 - val_loss: 1.0476 - val_acc: 0.9744\n",
      "Epoch 31/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0441 - acc: 0.9957 - val_loss: 1.0450 - val_acc: 0.9872\n",
      "Epoch 32/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0423 - acc: 0.9986 - val_loss: 1.0526 - val_acc: 0.9744\n",
      "Epoch 33/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0424 - acc: 0.9986 - val_loss: 1.0517 - val_acc: 1.0000\n",
      "Epoch 34/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0422 - acc: 0.9972 - val_loss: 1.0820 - val_acc: 0.9872\n",
      "Epoch 35/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0420 - acc: 0.9986 - val_loss: 1.0609 - val_acc: 0.9744\n",
      "Epoch 36/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0420 - acc: 0.9986 - val_loss: 1.0495 - val_acc: 1.0000\n",
      "Epoch 37/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0414 - acc: 1.0000 - val_loss: 1.0487 - val_acc: 0.9872\n",
      "Epoch 38/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0415 - acc: 1.0000 - val_loss: 1.0457 - val_acc: 1.0000\n",
      "Epoch 39/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0414 - acc: 1.0000 - val_loss: 1.0508 - val_acc: 1.0000\n",
      "Epoch 40/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0413 - acc: 1.0000 - val_loss: 1.0556 - val_acc: 0.9872\n",
      "Epoch 41/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0410 - acc: 1.0000 - val_loss: 1.0620 - val_acc: 0.9872\n",
      "Epoch 42/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0411 - acc: 1.0000 - val_loss: 1.0529 - val_acc: 1.0000\n",
      "Epoch 43/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0409 - acc: 1.0000 - val_loss: 1.0534 - val_acc: 1.0000\n",
      "Epoch 44/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0408 - acc: 1.0000 - val_loss: 1.0595 - val_acc: 0.9872\n",
      "Epoch 45/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0408 - acc: 1.0000 - val_loss: 1.0519 - val_acc: 1.0000\n",
      "Epoch 46/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0407 - acc: 1.0000 - val_loss: 1.0621 - val_acc: 0.9744\n",
      "Epoch 47/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0407 - acc: 1.0000 - val_loss: 1.0599 - val_acc: 0.9744\n",
      "Epoch 48/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0682 - val_acc: 0.9744\n",
      "Epoch 49/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0529 - val_acc: 1.0000\n",
      "Epoch 50/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0524 - val_acc: 0.9744\n",
      "Epoch 51/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0569 - val_acc: 0.9872\n",
      "Epoch 52/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0576 - val_acc: 0.9872\n",
      "Epoch 53/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0527 - val_acc: 0.9872\n",
      "Epoch 54/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0511 - val_acc: 1.0000\n",
      "Epoch 55/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0487 - val_acc: 1.0000\n",
      "Epoch 56/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0505 - val_acc: 1.0000\n",
      "Epoch 57/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0498 - val_acc: 1.0000\n",
      "Epoch 58/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0547 - val_acc: 0.9744\n",
      "Epoch 59/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0517 - val_acc: 0.9872\n",
      "Epoch 60/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0498 - val_acc: 0.9872\n",
      "Epoch 61/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0517 - val_acc: 0.9872\n",
      "Epoch 62/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0526 - val_acc: 0.9872\n",
      "Epoch 63/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0516 - val_acc: 0.9872\n",
      "Epoch 64/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0547 - val_acc: 0.9872\n",
      "Epoch 65/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0515 - val_acc: 0.9872\n",
      "Epoch 66/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0541 - val_acc: 0.9872\n",
      "Epoch 67/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0516 - val_acc: 0.9872\n",
      "Epoch 68/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0541 - val_acc: 0.9872\n",
      "Epoch 69/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0533 - val_acc: 0.9872\n",
      "Epoch 70/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0562 - val_acc: 0.9872\n",
      "Epoch 71/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0554 - val_acc: 0.9872\n",
      "Epoch 72/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0552 - val_acc: 0.9872\n",
      "Epoch 73/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0542 - val_acc: 0.9872\n",
      "Epoch 74/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0538 - val_acc: 0.9872\n",
      "Epoch 75/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0514 - val_acc: 0.9872\n",
      "Epoch 76/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0544 - val_acc: 0.9872\n",
      "Epoch 77/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0548 - val_acc: 0.9872\n",
      "Epoch 78/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0534 - val_acc: 0.9872\n",
      "Epoch 79/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0582 - val_acc: 0.9872\n",
      "Epoch 80/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0544 - val_acc: 0.9872\n",
      "Epoch 81/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0530 - val_acc: 0.9872\n",
      "Epoch 82/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0545 - val_acc: 0.9872\n",
      "Epoch 83/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0550 - val_acc: 0.9872\n",
      "Epoch 84/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0547 - val_acc: 0.9872\n",
      "Epoch 85/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0549 - val_acc: 0.9872\n",
      "Epoch 86/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0531 - val_acc: 0.9872\n",
      "Epoch 87/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0524 - val_acc: 0.9872\n",
      "Epoch 88/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0557 - val_acc: 0.9872\n",
      "Epoch 89/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0569 - val_acc: 0.9872\n",
      "Epoch 90/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0558 - val_acc: 0.9872\n",
      "Epoch 91/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0562 - val_acc: 0.9872\n",
      "Epoch 92/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0555 - val_acc: 0.9872\n",
      "Epoch 93/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0546 - val_acc: 0.9872\n",
      "Epoch 94/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0550 - val_acc: 0.9872\n",
      "Epoch 95/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0557 - val_acc: 0.9872\n",
      "Epoch 96/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0563 - val_acc: 0.9872\n",
      "Epoch 97/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0562 - val_acc: 0.9872\n",
      "Epoch 98/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0568 - val_acc: 0.9872\n",
      "Epoch 99/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0548 - val_acc: 0.9872\n",
      "Epoch 100/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0558 - val_acc: 0.9872\n",
      "78/78 [==============================] - 0s 312us/step\n",
      "Score for fold 10: loss of 1.0528231033912072; acc of 98.71794825945145%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1296x1296 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98.46153836983902\n",
      "Train on 702 samples, validate on 78 samples\n",
      "Epoch 1/100\n",
      "702/702 [==============================] - 6s 8ms/step - loss: 2.2437 - acc: 0.3362 - val_loss: 1.8430 - val_acc: 0.6026\n",
      "Epoch 2/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.6782 - acc: 0.6809 - val_loss: 1.5145 - val_acc: 0.7436\n",
      "Epoch 3/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.3852 - acc: 0.8162 - val_loss: 1.2805 - val_acc: 0.7692\n",
      "Epoch 4/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.2794 - acc: 0.8504 - val_loss: 1.3084 - val_acc: 0.7949\n",
      "Epoch 5/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1666 - acc: 0.9060 - val_loss: 1.1474 - val_acc: 0.8974\n",
      "Epoch 6/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1684 - acc: 0.8818 - val_loss: 1.1411 - val_acc: 0.9231\n",
      "Epoch 7/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1382 - acc: 0.9316 - val_loss: 1.1224 - val_acc: 0.9231\n",
      "Epoch 8/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0855 - acc: 0.9729 - val_loss: 1.1088 - val_acc: 0.8974\n",
      "Epoch 9/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1160 - acc: 0.9359 - val_loss: 1.1304 - val_acc: 0.8974\n",
      "Epoch 10/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1035 - acc: 0.9530 - val_loss: 1.2127 - val_acc: 0.8590\n",
      "Epoch 11/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0689 - acc: 0.9829 - val_loss: 1.1038 - val_acc: 0.9103\n",
      "Epoch 12/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0729 - acc: 0.9815 - val_loss: 1.1096 - val_acc: 0.8590\n",
      "Epoch 13/100\n",
      "702/702 [==============================] - 2s 3ms/step - loss: 1.0698 - acc: 0.9729 - val_loss: 1.1386 - val_acc: 0.8590\n",
      "Epoch 14/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0584 - acc: 0.9843 - val_loss: 1.0897 - val_acc: 0.9231\n",
      "Epoch 15/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0614 - acc: 0.9815 - val_loss: 1.0764 - val_acc: 0.9487\n",
      "Epoch 16/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0600 - acc: 0.9829 - val_loss: 1.0839 - val_acc: 0.9487\n",
      "Epoch 17/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0494 - acc: 0.9957 - val_loss: 1.0939 - val_acc: 0.9231\n",
      "Epoch 18/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0529 - acc: 0.9886 - val_loss: 1.1112 - val_acc: 0.8846\n",
      "Epoch 19/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0576 - acc: 0.9886 - val_loss: 1.1785 - val_acc: 0.8718\n",
      "Epoch 20/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0553 - acc: 0.9915 - val_loss: 1.0821 - val_acc: 0.9487\n",
      "Epoch 21/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0491 - acc: 0.9929 - val_loss: 1.0761 - val_acc: 0.9615\n",
      "Epoch 22/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0479 - acc: 0.9957 - val_loss: 1.0915 - val_acc: 0.9487\n",
      "Epoch 23/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0461 - acc: 0.9986 - val_loss: 1.1238 - val_acc: 0.9103\n",
      "Epoch 24/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0513 - acc: 0.9900 - val_loss: 1.0879 - val_acc: 0.9359\n",
      "Epoch 25/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0465 - acc: 0.9972 - val_loss: 1.0787 - val_acc: 0.9487\n",
      "Epoch 26/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0557 - acc: 0.9957 - val_loss: 1.0773 - val_acc: 0.9615\n",
      "Epoch 27/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0451 - acc: 0.9986 - val_loss: 1.0744 - val_acc: 0.9487\n",
      "Epoch 28/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0468 - acc: 0.9957 - val_loss: 1.0753 - val_acc: 0.9487\n",
      "Epoch 29/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0441 - acc: 0.9986 - val_loss: 1.0732 - val_acc: 0.9615\n",
      "Epoch 30/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0440 - acc: 0.9972 - val_loss: 1.0831 - val_acc: 0.9487\n",
      "Epoch 31/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0451 - acc: 0.9972 - val_loss: 1.0784 - val_acc: 0.9487\n",
      "Epoch 32/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0431 - acc: 0.9986 - val_loss: 1.0777 - val_acc: 0.9359\n",
      "Epoch 33/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0446 - acc: 0.9957 - val_loss: 1.0796 - val_acc: 0.9359\n",
      "Epoch 34/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0432 - acc: 0.9986 - val_loss: 1.0786 - val_acc: 0.9487\n",
      "Epoch 35/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0427 - acc: 0.9986 - val_loss: 1.0780 - val_acc: 0.9487\n",
      "Epoch 36/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0432 - acc: 0.9972 - val_loss: 1.0776 - val_acc: 0.9487\n",
      "Epoch 37/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0425 - acc: 0.9986 - val_loss: 1.0794 - val_acc: 0.9359\n",
      "Epoch 38/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0425 - acc: 0.9986 - val_loss: 1.0778 - val_acc: 0.9487\n",
      "Epoch 39/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0423 - acc: 0.9986 - val_loss: 1.0766 - val_acc: 0.9487\n",
      "Epoch 40/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0419 - acc: 1.0000 - val_loss: 1.0809 - val_acc: 0.9487\n",
      "Epoch 41/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0417 - acc: 1.0000 - val_loss: 1.0762 - val_acc: 0.9487\n",
      "Epoch 42/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0415 - acc: 1.0000 - val_loss: 1.0732 - val_acc: 0.9487\n",
      "Epoch 43/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0414 - acc: 1.0000 - val_loss: 1.0743 - val_acc: 0.9487\n",
      "Epoch 44/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0413 - acc: 1.0000 - val_loss: 1.0748 - val_acc: 0.9487\n",
      "Epoch 45/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0416 - acc: 1.0000 - val_loss: 1.0800 - val_acc: 0.9487\n",
      "Epoch 46/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0413 - acc: 1.0000 - val_loss: 1.0886 - val_acc: 0.9359\n",
      "Epoch 47/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0414 - acc: 0.9986 - val_loss: 1.0785 - val_acc: 0.9487\n",
      "Epoch 48/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0410 - acc: 1.0000 - val_loss: 1.0789 - val_acc: 0.9487\n",
      "Epoch 49/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0408 - acc: 1.0000 - val_loss: 1.0787 - val_acc: 0.9487\n",
      "Epoch 50/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0409 - acc: 1.0000 - val_loss: 1.0752 - val_acc: 0.9487\n",
      "Epoch 51/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0409 - acc: 1.0000 - val_loss: 1.0789 - val_acc: 0.9487\n",
      "Epoch 52/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0408 - acc: 1.0000 - val_loss: 1.0806 - val_acc: 0.9487\n",
      "Epoch 53/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0407 - acc: 1.0000 - val_loss: 1.0808 - val_acc: 0.9487\n",
      "Epoch 54/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0407 - acc: 1.0000 - val_loss: 1.0785 - val_acc: 0.9487\n",
      "Epoch 55/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0785 - val_acc: 0.9487\n",
      "Epoch 56/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0793 - val_acc: 0.9487\n",
      "Epoch 57/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0407 - acc: 1.0000 - val_loss: 1.0788 - val_acc: 0.9487\n",
      "Epoch 58/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0783 - val_acc: 0.9487\n",
      "Epoch 59/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0789 - val_acc: 0.9487\n",
      "Epoch 60/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0774 - val_acc: 0.9487\n",
      "Epoch 61/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0797 - val_acc: 0.9487\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 62/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0784 - val_acc: 0.9487\n",
      "Epoch 63/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0784 - val_acc: 0.9487\n",
      "Epoch 64/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0785 - val_acc: 0.9487\n",
      "Epoch 65/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0791 - val_acc: 0.9487\n",
      "Epoch 66/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0794 - val_acc: 0.9487\n",
      "Epoch 67/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0784 - val_acc: 0.9487\n",
      "Epoch 68/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0776 - val_acc: 0.9487\n",
      "Epoch 69/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0789 - val_acc: 0.9487\n",
      "Epoch 70/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0786 - val_acc: 0.9487\n",
      "Epoch 71/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0792 - val_acc: 0.9487\n",
      "Epoch 72/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0789 - val_acc: 0.9487\n",
      "Epoch 73/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0785 - val_acc: 0.9487\n",
      "Epoch 74/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0785 - val_acc: 0.9487\n",
      "Epoch 75/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0788 - val_acc: 0.9487\n",
      "Epoch 76/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0791 - val_acc: 0.9487\n",
      "Epoch 77/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0786 - val_acc: 0.9487\n",
      "Epoch 78/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0787 - val_acc: 0.9487\n",
      "Epoch 79/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0789 - val_acc: 0.9487\n",
      "Epoch 80/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0781 - val_acc: 0.9487\n",
      "Epoch 81/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0782 - val_acc: 0.9487\n",
      "Epoch 82/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0794 - val_acc: 0.9487\n",
      "Epoch 83/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0792 - val_acc: 0.9487\n",
      "Epoch 84/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0787 - val_acc: 0.9487\n",
      "Epoch 85/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0791 - val_acc: 0.9487\n",
      "Epoch 86/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0793 - val_acc: 0.9487\n",
      "Epoch 87/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0792 - val_acc: 0.9487\n",
      "Epoch 88/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0796 - val_acc: 0.9487\n",
      "Epoch 89/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0783 - val_acc: 0.9487\n",
      "Epoch 90/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0789 - val_acc: 0.9487\n",
      "Epoch 91/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0781 - val_acc: 0.9487\n",
      "Epoch 92/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0790 - val_acc: 0.9487\n",
      "Epoch 93/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0789 - val_acc: 0.9487\n",
      "Epoch 94/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0789 - val_acc: 0.9487\n",
      "Epoch 95/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0790 - val_acc: 0.9487\n",
      "Epoch 96/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0793 - val_acc: 0.9487\n",
      "Epoch 97/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0790 - val_acc: 0.9487\n",
      "Epoch 98/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0787 - val_acc: 0.9487\n",
      "Epoch 99/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0790 - val_acc: 0.9487\n",
      "Epoch 100/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0788 - val_acc: 0.9487\n",
      "78/78 [==============================] - 0s 300us/step\n",
      "Score for fold 1: loss of 1.0790800956579356; acc of 94.87179487179486%\n",
      "Train on 702 samples, validate on 78 samples\n",
      "Epoch 1/100\n",
      "702/702 [==============================] - 6s 8ms/step - loss: 2.2232 - acc: 0.3348 - val_loss: 1.9688 - val_acc: 0.4359\n",
      "Epoch 2/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.6000 - acc: 0.6866 - val_loss: 1.5136 - val_acc: 0.7179\n",
      "Epoch 3/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.3327 - acc: 0.8291 - val_loss: 1.4416 - val_acc: 0.6923\n",
      "Epoch 4/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.2426 - acc: 0.8661 - val_loss: 1.2866 - val_acc: 0.8333\n",
      "Epoch 5/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1533 - acc: 0.9046 - val_loss: 1.2466 - val_acc: 0.7949\n",
      "Epoch 6/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1497 - acc: 0.9330 - val_loss: 1.1801 - val_acc: 0.8590\n",
      "Epoch 7/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1070 - acc: 0.9387 - val_loss: 1.1922 - val_acc: 0.9231\n",
      "Epoch 8/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1085 - acc: 0.9530 - val_loss: 1.1182 - val_acc: 0.9231\n",
      "Epoch 9/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1142 - acc: 0.9373 - val_loss: 1.1193 - val_acc: 0.9359\n",
      "Epoch 10/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0747 - acc: 0.9687 - val_loss: 1.1270 - val_acc: 0.8462\n",
      "Epoch 11/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1045 - acc: 0.9558 - val_loss: 1.0821 - val_acc: 0.9744\n",
      "Epoch 12/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0673 - acc: 0.9744 - val_loss: 1.0723 - val_acc: 0.9744\n",
      "Epoch 13/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0611 - acc: 0.9843 - val_loss: 1.0921 - val_acc: 0.9359\n",
      "Epoch 14/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0804 - acc: 0.9701 - val_loss: 1.0785 - val_acc: 0.9872\n",
      "Epoch 15/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0656 - acc: 0.9801 - val_loss: 1.0913 - val_acc: 0.9231\n",
      "Epoch 16/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0568 - acc: 0.9858 - val_loss: 1.1226 - val_acc: 0.8718\n",
      "Epoch 17/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0603 - acc: 0.9843 - val_loss: 1.0732 - val_acc: 0.9615\n",
      "Epoch 18/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0527 - acc: 0.9929 - val_loss: 1.1258 - val_acc: 0.9103\n",
      "Epoch 19/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0579 - acc: 0.9815 - val_loss: 1.0739 - val_acc: 0.9487\n",
      "Epoch 20/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0507 - acc: 0.9957 - val_loss: 1.0780 - val_acc: 0.9103\n",
      "Epoch 21/100\n",
      "702/702 [==============================] - 2s 3ms/step - loss: 1.0575 - acc: 0.9900 - val_loss: 1.1199 - val_acc: 0.9231\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0523 - acc: 0.9900 - val_loss: 1.0599 - val_acc: 0.9615\n",
      "Epoch 23/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0468 - acc: 0.9972 - val_loss: 1.0554 - val_acc: 0.9872\n",
      "Epoch 24/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0536 - acc: 0.9915 - val_loss: 1.0733 - val_acc: 0.9359\n",
      "Epoch 25/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0452 - acc: 0.9972 - val_loss: 1.0485 - val_acc: 0.9872\n",
      "Epoch 26/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0460 - acc: 0.9943 - val_loss: 1.0538 - val_acc: 0.9744\n",
      "Epoch 27/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0629 - acc: 0.9829 - val_loss: 1.0600 - val_acc: 0.9872\n",
      "Epoch 28/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0449 - acc: 0.9972 - val_loss: 1.0478 - val_acc: 1.0000\n",
      "Epoch 29/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0434 - acc: 0.9986 - val_loss: 1.0486 - val_acc: 1.0000\n",
      "Epoch 30/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0434 - acc: 0.9986 - val_loss: 1.0468 - val_acc: 1.0000\n",
      "Epoch 31/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0433 - acc: 0.9986 - val_loss: 1.0723 - val_acc: 0.9744\n",
      "Epoch 32/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0492 - acc: 0.9972 - val_loss: 1.0517 - val_acc: 0.9872\n",
      "Epoch 33/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0433 - acc: 0.9972 - val_loss: 1.1000 - val_acc: 0.9744\n",
      "Epoch 34/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0434 - acc: 0.9972 - val_loss: 1.0608 - val_acc: 0.9744\n",
      "Epoch 35/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0443 - acc: 0.9972 - val_loss: 1.0651 - val_acc: 0.9744\n",
      "Epoch 36/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0434 - acc: 0.9957 - val_loss: 1.0504 - val_acc: 0.9872\n",
      "Epoch 37/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0418 - acc: 0.9986 - val_loss: 1.0500 - val_acc: 0.9872\n",
      "Epoch 38/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0418 - acc: 0.9986 - val_loss: 1.0506 - val_acc: 0.9872\n",
      "Epoch 39/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0417 - acc: 0.9986 - val_loss: 1.0560 - val_acc: 0.9744\n",
      "Epoch 40/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0416 - acc: 0.9986 - val_loss: 1.0466 - val_acc: 1.0000\n",
      "Epoch 41/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0415 - acc: 1.0000 - val_loss: 1.0469 - val_acc: 1.0000\n",
      "Epoch 42/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0412 - acc: 1.0000 - val_loss: 1.0462 - val_acc: 0.9872\n",
      "Epoch 43/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0410 - acc: 1.0000 - val_loss: 1.0488 - val_acc: 0.9872\n",
      "Epoch 44/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0411 - acc: 1.0000 - val_loss: 1.0456 - val_acc: 1.0000\n",
      "Epoch 45/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0411 - acc: 1.0000 - val_loss: 1.0453 - val_acc: 1.0000\n",
      "Epoch 46/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0409 - acc: 1.0000 - val_loss: 1.0462 - val_acc: 1.0000\n",
      "Epoch 47/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0407 - acc: 1.0000 - val_loss: 1.0485 - val_acc: 0.9872\n",
      "Epoch 48/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0409 - acc: 1.0000 - val_loss: 1.0463 - val_acc: 0.9872\n",
      "Epoch 49/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0490 - val_acc: 0.9872\n",
      "Epoch 50/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0486 - val_acc: 0.9872\n",
      "Epoch 51/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0487 - val_acc: 0.9872\n",
      "Epoch 52/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0407 - acc: 1.0000 - val_loss: 1.0486 - val_acc: 1.0000\n",
      "Epoch 53/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0476 - val_acc: 0.9872\n",
      "Epoch 54/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0455 - val_acc: 1.0000\n",
      "Epoch 55/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0455 - val_acc: 1.0000\n",
      "Epoch 56/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0563 - val_acc: 0.9872\n",
      "Epoch 57/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0483 - val_acc: 0.9872\n",
      "Epoch 58/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0507 - val_acc: 0.9872\n",
      "Epoch 59/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0527 - val_acc: 0.9744\n",
      "Epoch 60/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0470 - val_acc: 0.9872\n",
      "Epoch 61/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0493 - val_acc: 0.9872\n",
      "Epoch 62/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0498 - val_acc: 0.9744\n",
      "Epoch 63/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0460 - val_acc: 0.9872\n",
      "Epoch 64/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0464 - val_acc: 1.0000\n",
      "Epoch 65/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0456 - val_acc: 1.0000\n",
      "Epoch 66/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0458 - val_acc: 0.9872\n",
      "Epoch 67/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0462 - val_acc: 0.9872\n",
      "Epoch 68/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0500 - val_acc: 0.9872\n",
      "Epoch 69/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0472 - val_acc: 0.9872\n",
      "Epoch 70/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0497 - val_acc: 0.9872\n",
      "Epoch 71/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0487 - val_acc: 0.9872\n",
      "Epoch 72/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0476 - val_acc: 0.9872\n",
      "Epoch 73/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0488 - val_acc: 0.9872\n",
      "Epoch 74/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0460 - val_acc: 0.9872\n",
      "Epoch 75/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0462 - val_acc: 0.9872\n",
      "Epoch 76/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0482 - val_acc: 0.9872\n",
      "Epoch 77/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0457 - val_acc: 1.0000\n",
      "Epoch 78/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0473 - val_acc: 0.9872\n",
      "Epoch 79/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0475 - val_acc: 0.9872\n",
      "Epoch 80/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0468 - val_acc: 0.9872\n",
      "Epoch 81/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0459 - val_acc: 0.9872\n",
      "Epoch 82/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0460 - val_acc: 0.9872\n",
      "Epoch 83/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0458 - val_acc: 1.0000\n",
      "Epoch 84/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0462 - val_acc: 0.9872\n",
      "Epoch 85/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0458 - val_acc: 0.9872\n",
      "Epoch 86/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0470 - val_acc: 0.9872\n",
      "Epoch 87/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0455 - val_acc: 1.0000\n",
      "Epoch 88/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0456 - val_acc: 1.0000\n",
      "Epoch 89/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0460 - val_acc: 1.0000\n",
      "Epoch 90/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0462 - val_acc: 1.0000\n",
      "Epoch 91/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0468 - val_acc: 1.0000\n",
      "Epoch 92/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0476 - val_acc: 0.9872\n",
      "Epoch 93/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0463 - val_acc: 0.9872\n",
      "Epoch 94/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0470 - val_acc: 0.9872\n",
      "Epoch 95/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0461 - val_acc: 1.0000\n",
      "Epoch 96/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0463 - val_acc: 1.0000\n",
      "Epoch 97/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0461 - val_acc: 1.0000\n",
      "Epoch 98/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0464 - val_acc: 1.0000\n",
      "Epoch 99/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0460 - val_acc: 1.0000\n",
      "Epoch 100/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0460 - val_acc: 0.9872\n",
      "78/78 [==============================] - 0s 309us/step\n",
      "Score for fold 2: loss of 1.0469578458712652; acc of 98.71794871794873%\n",
      "Train on 702 samples, validate on 78 samples\n",
      "Epoch 1/100\n",
      "702/702 [==============================] - 6s 8ms/step - loss: 2.2138 - acc: 0.3561 - val_loss: 1.7712 - val_acc: 0.6026\n",
      "Epoch 2/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.5976 - acc: 0.7279 - val_loss: 1.4185 - val_acc: 0.7564\n",
      "Epoch 3/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.3494 - acc: 0.8219 - val_loss: 1.3039 - val_acc: 0.7821\n",
      "Epoch 4/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.2449 - acc: 0.8561 - val_loss: 1.3518 - val_acc: 0.7821\n",
      "Epoch 5/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1712 - acc: 0.9145 - val_loss: 1.1163 - val_acc: 0.9487\n",
      "Epoch 6/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1251 - acc: 0.9274 - val_loss: 1.3186 - val_acc: 0.8205\n",
      "Epoch 7/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1540 - acc: 0.9131 - val_loss: 1.1601 - val_acc: 0.8590\n",
      "Epoch 8/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0985 - acc: 0.9345 - val_loss: 1.0980 - val_acc: 0.9103\n",
      "Epoch 9/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0868 - acc: 0.9444 - val_loss: 1.2549 - val_acc: 0.8974\n",
      "Epoch 10/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1051 - acc: 0.9558 - val_loss: 1.1099 - val_acc: 0.9615\n",
      "Epoch 11/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0683 - acc: 0.9801 - val_loss: 1.0962 - val_acc: 0.9231\n",
      "Epoch 12/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0708 - acc: 0.9729 - val_loss: 1.0848 - val_acc: 0.9487\n",
      "Epoch 13/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0636 - acc: 0.9701 - val_loss: 1.0684 - val_acc: 0.9615\n",
      "Epoch 14/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0945 - acc: 0.9416 - val_loss: 1.0883 - val_acc: 0.9487\n",
      "Epoch 15/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0564 - acc: 0.9929 - val_loss: 1.0755 - val_acc: 0.9487\n",
      "Epoch 16/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0537 - acc: 0.9915 - val_loss: 1.0846 - val_acc: 0.9103\n",
      "Epoch 17/100\n",
      "702/702 [==============================] - 2s 3ms/step - loss: 1.0622 - acc: 0.9744 - val_loss: 1.2118 - val_acc: 0.7821\n",
      "Epoch 18/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0649 - acc: 0.9772 - val_loss: 1.0938 - val_acc: 0.8974\n",
      "Epoch 19/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0509 - acc: 0.9972 - val_loss: 1.0542 - val_acc: 0.9744\n",
      "Epoch 20/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0498 - acc: 0.9943 - val_loss: 1.0633 - val_acc: 0.9744\n",
      "Epoch 21/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0491 - acc: 0.9957 - val_loss: 1.0537 - val_acc: 1.0000\n",
      "Epoch 22/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0466 - acc: 0.9972 - val_loss: 1.0677 - val_acc: 0.9103\n",
      "Epoch 23/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0481 - acc: 0.9929 - val_loss: 1.0509 - val_acc: 1.0000\n",
      "Epoch 24/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0460 - acc: 0.9943 - val_loss: 1.0893 - val_acc: 0.9487\n",
      "Epoch 25/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0489 - acc: 0.9915 - val_loss: 1.0725 - val_acc: 0.9744\n",
      "Epoch 26/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0487 - acc: 0.9943 - val_loss: 1.0543 - val_acc: 0.9872\n",
      "Epoch 27/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0449 - acc: 0.9972 - val_loss: 1.0494 - val_acc: 0.9872\n",
      "Epoch 28/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0453 - acc: 0.9972 - val_loss: 1.0543 - val_acc: 0.9872\n",
      "Epoch 29/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0443 - acc: 0.9972 - val_loss: 1.0585 - val_acc: 0.9615\n",
      "Epoch 30/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0432 - acc: 0.9986 - val_loss: 1.0455 - val_acc: 1.0000\n",
      "Epoch 31/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0428 - acc: 0.9986 - val_loss: 1.0464 - val_acc: 1.0000\n",
      "Epoch 32/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0429 - acc: 1.0000 - val_loss: 1.0794 - val_acc: 0.9615\n",
      "Epoch 33/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0437 - acc: 0.9972 - val_loss: 1.0487 - val_acc: 0.9744\n",
      "Epoch 34/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0435 - acc: 0.9986 - val_loss: 1.0468 - val_acc: 0.9872\n",
      "Epoch 35/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0413 - acc: 1.0000 - val_loss: 1.0469 - val_acc: 0.9872\n",
      "Epoch 36/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0414 - acc: 1.0000 - val_loss: 1.0458 - val_acc: 1.0000\n",
      "Epoch 37/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0414 - acc: 1.0000 - val_loss: 1.0438 - val_acc: 1.0000\n",
      "Epoch 38/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0413 - acc: 1.0000 - val_loss: 1.0442 - val_acc: 1.0000\n",
      "Epoch 39/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0418 - acc: 1.0000 - val_loss: 1.0495 - val_acc: 0.9872\n",
      "Epoch 40/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0416 - acc: 1.0000 - val_loss: 1.0445 - val_acc: 1.0000\n",
      "Epoch 41/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0408 - acc: 1.0000 - val_loss: 1.0440 - val_acc: 1.0000\n",
      "Epoch 42/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0408 - acc: 1.0000 - val_loss: 1.0440 - val_acc: 1.0000\n",
      "Epoch 43/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0408 - acc: 1.0000 - val_loss: 1.0443 - val_acc: 1.0000\n",
      "Epoch 44/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0408 - acc: 1.0000 - val_loss: 1.0440 - val_acc: 1.0000\n",
      "Epoch 45/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0408 - acc: 1.0000 - val_loss: 1.0500 - val_acc: 0.9872\n",
      "Epoch 46/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0408 - acc: 1.0000 - val_loss: 1.0433 - val_acc: 1.0000\n",
      "Epoch 47/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0440 - val_acc: 1.0000\n",
      "Epoch 48/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0442 - val_acc: 1.0000\n",
      "Epoch 49/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0434 - val_acc: 1.0000\n",
      "Epoch 50/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0433 - val_acc: 1.0000\n",
      "Epoch 51/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0434 - val_acc: 1.0000\n",
      "Epoch 52/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0429 - val_acc: 1.0000\n",
      "Epoch 53/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0433 - val_acc: 1.0000\n",
      "Epoch 54/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0437 - val_acc: 1.0000\n",
      "Epoch 55/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0436 - val_acc: 1.0000\n",
      "Epoch 56/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0430 - val_acc: 1.0000\n",
      "Epoch 57/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0429 - val_acc: 1.0000\n",
      "Epoch 58/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0430 - val_acc: 1.0000\n",
      "Epoch 59/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0433 - val_acc: 1.0000\n",
      "Epoch 60/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0436 - val_acc: 1.0000\n",
      "Epoch 61/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0434 - val_acc: 1.0000\n",
      "Epoch 62/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0431 - val_acc: 1.0000\n",
      "Epoch 63/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0432 - val_acc: 1.0000\n",
      "Epoch 64/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0429 - val_acc: 1.0000\n",
      "Epoch 65/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0432 - val_acc: 1.0000\n",
      "Epoch 66/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0428 - val_acc: 1.0000\n",
      "Epoch 67/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0431 - val_acc: 1.0000\n",
      "Epoch 68/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0429 - val_acc: 1.0000\n",
      "Epoch 69/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0436 - val_acc: 1.0000\n",
      "Epoch 70/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0436 - val_acc: 0.9872\n",
      "Epoch 71/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0440 - val_acc: 0.9872\n",
      "Epoch 72/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0443 - val_acc: 0.9872\n",
      "Epoch 73/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0441 - val_acc: 0.9872\n",
      "Epoch 74/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0441 - val_acc: 0.9872\n",
      "Epoch 75/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0441 - val_acc: 0.9872\n",
      "Epoch 76/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0442 - val_acc: 0.9872\n",
      "Epoch 77/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0439 - val_acc: 0.9872\n",
      "Epoch 78/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0444 - val_acc: 0.9872\n",
      "Epoch 79/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0444 - val_acc: 0.9872\n",
      "Epoch 80/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0442 - val_acc: 0.9872\n",
      "Epoch 81/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0446 - val_acc: 0.9872\n",
      "Epoch 82/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0443 - val_acc: 0.9872\n",
      "Epoch 83/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0447 - val_acc: 0.9872\n",
      "Epoch 84/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0446 - val_acc: 0.9872\n",
      "Epoch 85/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0446 - val_acc: 0.9872\n",
      "Epoch 86/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0445 - val_acc: 0.9872\n",
      "Epoch 87/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0442 - val_acc: 0.9872\n",
      "Epoch 88/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0445 - val_acc: 0.9872\n",
      "Epoch 89/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0445 - val_acc: 0.9872\n",
      "Epoch 90/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0446 - val_acc: 0.9872\n",
      "Epoch 91/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0444 - val_acc: 0.9872\n",
      "Epoch 92/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0446 - val_acc: 0.9872\n",
      "Epoch 93/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0447 - val_acc: 0.9872\n",
      "Epoch 94/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0445 - val_acc: 0.9872\n",
      "Epoch 95/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0446 - val_acc: 0.9872\n",
      "Epoch 96/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0447 - val_acc: 0.9872\n",
      "Epoch 97/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0446 - val_acc: 0.9872\n",
      "Epoch 98/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0445 - val_acc: 0.9872\n",
      "Epoch 99/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0447 - val_acc: 0.9872\n",
      "Epoch 100/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0447 - val_acc: 0.9872\n",
      "78/78 [==============================] - 0s 307us/step\n",
      "Score for fold 3: loss of 1.0447745384314122; acc of 98.71794825945145%\n",
      "Train on 702 samples, validate on 78 samples\n",
      "Epoch 1/100\n",
      "702/702 [==============================] - 6s 9ms/step - loss: 2.3028 - acc: 0.3191 - val_loss: 1.9924 - val_acc: 0.5513\n",
      "Epoch 2/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.6852 - acc: 0.6994 - val_loss: 1.5947 - val_acc: 0.6538\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.4059 - acc: 0.8148 - val_loss: 1.3210 - val_acc: 0.8590\n",
      "Epoch 4/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.2294 - acc: 0.8860 - val_loss: 1.3745 - val_acc: 0.6923\n",
      "Epoch 5/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1987 - acc: 0.8832 - val_loss: 1.1248 - val_acc: 0.9487\n",
      "Epoch 6/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1470 - acc: 0.8917 - val_loss: 1.3197 - val_acc: 0.8590\n",
      "Epoch 7/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1292 - acc: 0.9359 - val_loss: 1.0943 - val_acc: 0.9487\n",
      "Epoch 8/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1315 - acc: 0.9231 - val_loss: 1.0903 - val_acc: 0.9359\n",
      "Epoch 9/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0805 - acc: 0.9758 - val_loss: 1.2329 - val_acc: 0.8846\n",
      "Epoch 10/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0876 - acc: 0.9573 - val_loss: 1.1041 - val_acc: 0.9359\n",
      "Epoch 11/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0843 - acc: 0.9615 - val_loss: 1.0830 - val_acc: 0.9744\n",
      "Epoch 12/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0781 - acc: 0.9772 - val_loss: 1.0627 - val_acc: 0.9872\n",
      "Epoch 13/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0636 - acc: 0.9786 - val_loss: 1.0954 - val_acc: 0.9744\n",
      "Epoch 14/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0665 - acc: 0.9829 - val_loss: 1.0913 - val_acc: 0.9487\n",
      "Epoch 15/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0611 - acc: 0.9786 - val_loss: 1.0652 - val_acc: 0.9872\n",
      "Epoch 16/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0619 - acc: 0.9801 - val_loss: 1.0659 - val_acc: 1.0000\n",
      "Epoch 17/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0637 - acc: 0.9758 - val_loss: 1.0546 - val_acc: 0.9744\n",
      "Epoch 18/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0541 - acc: 0.9872 - val_loss: 1.0706 - val_acc: 0.9359\n",
      "Epoch 19/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0531 - acc: 0.9900 - val_loss: 1.0520 - val_acc: 0.9744\n",
      "Epoch 20/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0630 - acc: 0.9772 - val_loss: 1.0561 - val_acc: 0.9744\n",
      "Epoch 21/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0479 - acc: 0.9986 - val_loss: 1.0630 - val_acc: 0.9744\n",
      "Epoch 22/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0531 - acc: 0.9900 - val_loss: 1.0476 - val_acc: 1.0000\n",
      "Epoch 23/100\n",
      "702/702 [==============================] - 2s 3ms/step - loss: 1.0606 - acc: 0.9843 - val_loss: 1.0598 - val_acc: 0.9615\n",
      "Epoch 24/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0472 - acc: 0.9957 - val_loss: 1.0485 - val_acc: 0.9872\n",
      "Epoch 25/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0455 - acc: 0.9972 - val_loss: 1.0566 - val_acc: 1.0000\n",
      "Epoch 26/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0464 - acc: 0.9972 - val_loss: 1.0456 - val_acc: 1.0000\n",
      "Epoch 27/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0506 - acc: 0.9900 - val_loss: 1.0472 - val_acc: 1.0000\n",
      "Epoch 28/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0471 - acc: 0.9943 - val_loss: 1.0470 - val_acc: 1.0000\n",
      "Epoch 29/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0448 - acc: 0.9972 - val_loss: 1.0501 - val_acc: 0.9744\n",
      "Epoch 30/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0452 - acc: 0.9972 - val_loss: 1.0508 - val_acc: 0.9872\n",
      "Epoch 31/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0437 - acc: 0.9972 - val_loss: 1.0493 - val_acc: 0.9872\n",
      "Epoch 32/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0430 - acc: 0.9986 - val_loss: 1.0497 - val_acc: 0.9872\n",
      "Epoch 33/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0445 - acc: 0.9972 - val_loss: 1.0473 - val_acc: 0.9872\n",
      "Epoch 34/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0428 - acc: 0.9986 - val_loss: 1.0588 - val_acc: 0.9744\n",
      "Epoch 35/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0445 - acc: 0.9957 - val_loss: 1.0537 - val_acc: 0.9872\n",
      "Epoch 36/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0422 - acc: 0.9986 - val_loss: 1.0520 - val_acc: 0.9872\n",
      "Epoch 37/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0419 - acc: 1.0000 - val_loss: 1.0487 - val_acc: 0.9872\n",
      "Epoch 38/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0419 - acc: 1.0000 - val_loss: 1.0485 - val_acc: 0.9872\n",
      "Epoch 39/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0417 - acc: 1.0000 - val_loss: 1.0455 - val_acc: 1.0000\n",
      "Epoch 40/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0417 - acc: 1.0000 - val_loss: 1.0436 - val_acc: 1.0000\n",
      "Epoch 41/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0417 - acc: 1.0000 - val_loss: 1.0503 - val_acc: 0.9872\n",
      "Epoch 42/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0413 - acc: 1.0000 - val_loss: 1.0486 - val_acc: 0.9872\n",
      "Epoch 43/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0411 - acc: 1.0000 - val_loss: 1.0475 - val_acc: 0.9872\n",
      "Epoch 44/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0411 - acc: 1.0000 - val_loss: 1.0470 - val_acc: 0.9872\n",
      "Epoch 45/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0412 - acc: 1.0000 - val_loss: 1.0504 - val_acc: 0.9872\n",
      "Epoch 46/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0409 - acc: 1.0000 - val_loss: 1.0474 - val_acc: 0.9872\n",
      "Epoch 47/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0417 - acc: 0.9986 - val_loss: 1.0490 - val_acc: 0.9872\n",
      "Epoch 48/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0408 - acc: 1.0000 - val_loss: 1.0471 - val_acc: 0.9872\n",
      "Epoch 49/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0408 - acc: 1.0000 - val_loss: 1.0483 - val_acc: 0.9872\n",
      "Epoch 50/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0408 - acc: 1.0000 - val_loss: 1.0457 - val_acc: 0.9872\n",
      "Epoch 51/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0407 - acc: 1.0000 - val_loss: 1.0496 - val_acc: 0.9872\n",
      "Epoch 52/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0407 - acc: 1.0000 - val_loss: 1.0466 - val_acc: 0.9872\n",
      "Epoch 53/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0407 - acc: 1.0000 - val_loss: 1.0466 - val_acc: 0.9872\n",
      "Epoch 54/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0461 - val_acc: 0.9872\n",
      "Epoch 55/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0490 - val_acc: 0.9872\n",
      "Epoch 56/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0487 - val_acc: 0.9872\n",
      "Epoch 57/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0489 - val_acc: 0.9872\n",
      "Epoch 58/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0478 - val_acc: 0.9872\n",
      "Epoch 59/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0473 - val_acc: 0.9872\n",
      "Epoch 60/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0456 - val_acc: 0.9872\n",
      "Epoch 61/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0472 - val_acc: 0.9872\n",
      "Epoch 62/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0482 - val_acc: 0.9872\n",
      "Epoch 63/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0466 - val_acc: 0.9872\n",
      "Epoch 64/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0449 - val_acc: 0.9872\n",
      "Epoch 65/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0476 - val_acc: 0.9872\n",
      "Epoch 66/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0482 - val_acc: 0.9872\n",
      "Epoch 67/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0476 - val_acc: 0.9872\n",
      "Epoch 68/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0481 - val_acc: 0.9872\n",
      "Epoch 69/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0479 - val_acc: 0.9872\n",
      "Epoch 70/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0477 - val_acc: 0.9872\n",
      "Epoch 71/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0445 - val_acc: 0.9872\n",
      "Epoch 72/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0444 - val_acc: 0.9872\n",
      "Epoch 73/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0462 - val_acc: 0.9872\n",
      "Epoch 74/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0461 - val_acc: 0.9872\n",
      "Epoch 75/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0479 - val_acc: 0.9872\n",
      "Epoch 76/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0465 - val_acc: 0.9872\n",
      "Epoch 77/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0471 - val_acc: 0.9872\n",
      "Epoch 78/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0476 - val_acc: 0.9872\n",
      "Epoch 79/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0477 - val_acc: 0.9872\n",
      "Epoch 80/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0470 - val_acc: 0.9872\n",
      "Epoch 81/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0477 - val_acc: 0.9872\n",
      "Epoch 82/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0475 - val_acc: 0.9872\n",
      "Epoch 83/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0475 - val_acc: 0.9872\n",
      "Epoch 84/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0474 - val_acc: 0.9872\n",
      "Epoch 85/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0473 - val_acc: 0.9872\n",
      "Epoch 86/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0472 - val_acc: 0.9872\n",
      "Epoch 87/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0479 - val_acc: 0.9872\n",
      "Epoch 88/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0474 - val_acc: 0.9872\n",
      "Epoch 89/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0473 - val_acc: 0.9872\n",
      "Epoch 90/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0480 - val_acc: 0.9872\n",
      "Epoch 91/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0476 - val_acc: 0.9872\n",
      "Epoch 92/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0475 - val_acc: 0.9872\n",
      "Epoch 93/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0476 - val_acc: 0.9872\n",
      "Epoch 94/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0481 - val_acc: 0.9872\n",
      "Epoch 95/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0477 - val_acc: 0.9872\n",
      "Epoch 96/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0475 - val_acc: 0.9872\n",
      "Epoch 97/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0479 - val_acc: 0.9872\n",
      "Epoch 98/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0476 - val_acc: 0.9872\n",
      "Epoch 99/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0477 - val_acc: 0.9872\n",
      "Epoch 100/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0479 - val_acc: 0.9872\n",
      "78/78 [==============================] - 0s 315us/step\n",
      "Score for fold 4: loss of 1.0483613381019006; acc of 98.71794825945145%\n",
      "Train on 702 samples, validate on 78 samples\n",
      "Epoch 1/100\n",
      "702/702 [==============================] - 6s 9ms/step - loss: 2.2555 - acc: 0.3262 - val_loss: 2.0601 - val_acc: 0.3590\n",
      "Epoch 2/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.6074 - acc: 0.6994 - val_loss: 1.7679 - val_acc: 0.5897\n",
      "Epoch 3/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.3918 - acc: 0.7806 - val_loss: 1.4172 - val_acc: 0.7821\n",
      "Epoch 4/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.3038 - acc: 0.8134 - val_loss: 1.4521 - val_acc: 0.7436\n",
      "Epoch 5/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1964 - acc: 0.8818 - val_loss: 1.2438 - val_acc: 0.7821\n",
      "Epoch 6/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1636 - acc: 0.8974 - val_loss: 1.1855 - val_acc: 0.8333\n",
      "Epoch 7/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1142 - acc: 0.9259 - val_loss: 1.3282 - val_acc: 0.7692\n",
      "Epoch 8/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1548 - acc: 0.9074 - val_loss: 1.2196 - val_acc: 0.8333\n",
      "Epoch 9/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0953 - acc: 0.9402 - val_loss: 1.1238 - val_acc: 0.9359\n",
      "Epoch 10/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0968 - acc: 0.9573 - val_loss: 1.1534 - val_acc: 0.8974\n",
      "Epoch 11/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1026 - acc: 0.9530 - val_loss: 1.1102 - val_acc: 0.9487\n",
      "Epoch 12/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0693 - acc: 0.9729 - val_loss: 1.0949 - val_acc: 0.9359\n",
      "Epoch 13/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0746 - acc: 0.9729 - val_loss: 1.1004 - val_acc: 0.9487\n",
      "Epoch 14/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0636 - acc: 0.9758 - val_loss: 1.1034 - val_acc: 0.9359\n",
      "Epoch 15/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0738 - acc: 0.9715 - val_loss: 1.1071 - val_acc: 0.8974\n",
      "Epoch 16/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0613 - acc: 0.9843 - val_loss: 1.0662 - val_acc: 0.9744\n",
      "Epoch 17/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1165 - acc: 0.9630 - val_loss: 1.0951 - val_acc: 0.9487\n",
      "Epoch 18/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0562 - acc: 0.9872 - val_loss: 1.0690 - val_acc: 0.9744\n",
      "Epoch 19/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0498 - acc: 0.9972 - val_loss: 1.2470 - val_acc: 0.8718\n",
      "Epoch 20/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0950 - acc: 0.9630 - val_loss: 1.1216 - val_acc: 0.9744\n",
      "Epoch 21/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0542 - acc: 0.9886 - val_loss: 1.1377 - val_acc: 0.9487\n",
      "Epoch 22/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0510 - acc: 0.9886 - val_loss: 1.1658 - val_acc: 0.9359\n",
      "Epoch 23/100\n",
      "702/702 [==============================] - 2s 3ms/step - loss: 1.0567 - acc: 0.9829 - val_loss: 1.0747 - val_acc: 0.9231\n",
      "Epoch 24/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0470 - acc: 0.9972 - val_loss: 1.0710 - val_acc: 0.9615\n",
      "Epoch 25/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0492 - acc: 0.9943 - val_loss: 1.1114 - val_acc: 0.9744\n",
      "Epoch 26/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0497 - acc: 0.9915 - val_loss: 1.0596 - val_acc: 0.9744\n",
      "Epoch 27/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0471 - acc: 0.9986 - val_loss: 1.0822 - val_acc: 0.9744\n",
      "Epoch 28/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0502 - acc: 0.9900 - val_loss: 1.1059 - val_acc: 0.9487\n",
      "Epoch 29/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0459 - acc: 0.9972 - val_loss: 1.1143 - val_acc: 0.9487\n",
      "Epoch 30/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0456 - acc: 0.9929 - val_loss: 1.0754 - val_acc: 0.9744\n",
      "Epoch 31/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0434 - acc: 0.9986 - val_loss: 1.0740 - val_acc: 0.9744\n",
      "Epoch 32/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0441 - acc: 0.9972 - val_loss: 1.0586 - val_acc: 0.9615\n",
      "Epoch 33/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0425 - acc: 1.0000 - val_loss: 1.0851 - val_acc: 0.9744\n",
      "Epoch 34/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0455 - acc: 0.9943 - val_loss: 1.0661 - val_acc: 0.9487\n",
      "Epoch 35/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0421 - acc: 1.0000 - val_loss: 1.0776 - val_acc: 0.9744\n",
      "Epoch 36/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0417 - acc: 1.0000 - val_loss: 1.0603 - val_acc: 0.9744\n",
      "Epoch 37/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0419 - acc: 1.0000 - val_loss: 1.0554 - val_acc: 0.9615\n",
      "Epoch 38/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0421 - acc: 0.9986 - val_loss: 1.0676 - val_acc: 0.9744\n",
      "Epoch 39/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0423 - acc: 0.9986 - val_loss: 1.0559 - val_acc: 0.9744\n",
      "Epoch 40/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0413 - acc: 1.0000 - val_loss: 1.0534 - val_acc: 0.9744\n",
      "Epoch 41/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0413 - acc: 1.0000 - val_loss: 1.0586 - val_acc: 0.9744\n",
      "Epoch 42/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0413 - acc: 1.0000 - val_loss: 1.0515 - val_acc: 0.9744\n",
      "Epoch 43/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0413 - acc: 1.0000 - val_loss: 1.0562 - val_acc: 0.9744\n",
      "Epoch 44/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0409 - acc: 1.0000 - val_loss: 1.0591 - val_acc: 0.9744\n",
      "Epoch 45/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0410 - acc: 1.0000 - val_loss: 1.0537 - val_acc: 0.9744\n",
      "Epoch 46/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0409 - acc: 1.0000 - val_loss: 1.0558 - val_acc: 0.9615\n",
      "Epoch 47/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0411 - acc: 1.0000 - val_loss: 1.0506 - val_acc: 0.9744\n",
      "Epoch 48/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0407 - acc: 1.0000 - val_loss: 1.0531 - val_acc: 0.9615\n",
      "Epoch 49/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0407 - acc: 1.0000 - val_loss: 1.0537 - val_acc: 0.9744\n",
      "Epoch 50/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0538 - val_acc: 0.9744\n",
      "Epoch 51/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0407 - acc: 1.0000 - val_loss: 1.0516 - val_acc: 0.9744\n",
      "Epoch 52/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0407 - acc: 1.0000 - val_loss: 1.0517 - val_acc: 0.9615\n",
      "Epoch 53/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0498 - val_acc: 0.9744\n",
      "Epoch 54/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0482 - val_acc: 0.9872\n",
      "Epoch 55/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0490 - val_acc: 0.9872\n",
      "Epoch 56/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0482 - val_acc: 0.9872\n",
      "Epoch 57/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0487 - val_acc: 0.9744\n",
      "Epoch 58/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0518 - val_acc: 0.9744\n",
      "Epoch 59/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0501 - val_acc: 0.9744\n",
      "Epoch 60/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0512 - val_acc: 0.9744\n",
      "Epoch 61/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0491 - val_acc: 0.9744\n",
      "Epoch 62/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0488 - val_acc: 0.9744\n",
      "Epoch 63/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0483 - val_acc: 0.9744\n",
      "Epoch 64/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0486 - val_acc: 0.9744\n",
      "Epoch 65/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0488 - val_acc: 0.9872\n",
      "Epoch 66/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0472 - val_acc: 0.9872\n",
      "Epoch 67/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0478 - val_acc: 0.9872\n",
      "Epoch 68/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0481 - val_acc: 0.9872\n",
      "Epoch 69/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0476 - val_acc: 0.9872\n",
      "Epoch 70/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0480 - val_acc: 0.9872\n",
      "Epoch 71/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0484 - val_acc: 0.9872\n",
      "Epoch 72/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0491 - val_acc: 0.9872\n",
      "Epoch 73/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0475 - val_acc: 0.9872\n",
      "Epoch 74/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0475 - val_acc: 0.9872\n",
      "Epoch 75/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0469 - val_acc: 0.9872\n",
      "Epoch 76/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0482 - val_acc: 0.9872\n",
      "Epoch 77/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0490 - val_acc: 0.9744\n",
      "Epoch 78/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0479 - val_acc: 0.9872\n",
      "Epoch 79/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0486 - val_acc: 0.9872\n",
      "Epoch 80/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0491 - val_acc: 0.9744\n",
      "Epoch 81/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0476 - val_acc: 0.9872\n",
      "Epoch 82/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0479 - val_acc: 0.9872\n",
      "Epoch 83/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0475 - val_acc: 0.9872\n",
      "Epoch 84/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0474 - val_acc: 0.9872\n",
      "Epoch 85/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0477 - val_acc: 0.9872\n",
      "Epoch 86/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0483 - val_acc: 0.9872\n",
      "Epoch 87/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0487 - val_acc: 0.9872\n",
      "Epoch 88/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0482 - val_acc: 0.9872\n",
      "Epoch 89/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0473 - val_acc: 0.9872\n",
      "Epoch 90/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0481 - val_acc: 0.9872\n",
      "Epoch 91/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0478 - val_acc: 0.9872\n",
      "Epoch 92/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0473 - val_acc: 0.9872\n",
      "Epoch 93/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0476 - val_acc: 0.9872\n",
      "Epoch 94/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0470 - val_acc: 0.9872\n",
      "Epoch 95/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0475 - val_acc: 0.9872\n",
      "Epoch 96/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0471 - val_acc: 0.9872\n",
      "Epoch 97/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0472 - val_acc: 0.9872\n",
      "Epoch 98/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0471 - val_acc: 0.9872\n",
      "Epoch 99/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0467 - val_acc: 0.9872\n",
      "Epoch 100/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0466 - val_acc: 0.9872\n",
      "78/78 [==============================] - 0s 322us/step\n",
      "Score for fold 5: loss of 1.0457877318064372; acc of 98.71794871794873%\n",
      "Train on 702 samples, validate on 78 samples\n",
      "Epoch 1/100\n",
      "702/702 [==============================] - 7s 9ms/step - loss: 2.1956 - acc: 0.3590 - val_loss: 1.8070 - val_acc: 0.5256\n",
      "Epoch 2/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.5697 - acc: 0.7365 - val_loss: 1.4227 - val_acc: 0.8077\n",
      "Epoch 3/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.3276 - acc: 0.8191 - val_loss: 1.2928 - val_acc: 0.8205\n",
      "Epoch 4/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.2218 - acc: 0.8647 - val_loss: 1.2648 - val_acc: 0.8462\n",
      "Epoch 5/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1663 - acc: 0.9074 - val_loss: 1.1369 - val_acc: 0.9487\n",
      "Epoch 6/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1370 - acc: 0.9302 - val_loss: 1.2482 - val_acc: 0.8333\n",
      "Epoch 7/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1297 - acc: 0.9274 - val_loss: 1.1597 - val_acc: 0.8974\n",
      "Epoch 8/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1076 - acc: 0.9501 - val_loss: 1.1993 - val_acc: 0.8590\n",
      "Epoch 9/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0769 - acc: 0.9630 - val_loss: 1.0960 - val_acc: 0.9487\n",
      "Epoch 10/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0823 - acc: 0.9615 - val_loss: 1.1199 - val_acc: 0.8846\n",
      "Epoch 11/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0742 - acc: 0.9644 - val_loss: 1.0819 - val_acc: 0.9359\n",
      "Epoch 12/100\n",
      "702/702 [==============================] - 2s 4ms/step - loss: 1.0779 - acc: 0.9744 - val_loss: 1.0830 - val_acc: 0.9359\n",
      "Epoch 13/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0587 - acc: 0.9815 - val_loss: 1.0711 - val_acc: 0.9359\n",
      "Epoch 14/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0541 - acc: 0.9915 - val_loss: 1.0715 - val_acc: 0.9487\n",
      "Epoch 15/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0535 - acc: 0.9900 - val_loss: 1.2497 - val_acc: 0.8846\n",
      "Epoch 16/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0659 - acc: 0.9758 - val_loss: 1.0632 - val_acc: 0.9615\n",
      "Epoch 17/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0507 - acc: 0.9900 - val_loss: 1.0586 - val_acc: 0.9615\n",
      "Epoch 18/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0518 - acc: 0.9929 - val_loss: 1.0638 - val_acc: 0.9487\n",
      "Epoch 19/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0526 - acc: 0.9915 - val_loss: 1.0553 - val_acc: 0.9744\n",
      "Epoch 20/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0478 - acc: 0.9943 - val_loss: 1.0670 - val_acc: 0.9615\n",
      "Epoch 21/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0467 - acc: 0.9972 - val_loss: 1.0569 - val_acc: 0.9744\n",
      "Epoch 22/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0532 - acc: 0.9957 - val_loss: 1.0559 - val_acc: 0.9872\n",
      "Epoch 23/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0544 - acc: 0.9858 - val_loss: 1.0587 - val_acc: 0.9615\n",
      "Epoch 24/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0449 - acc: 0.9986 - val_loss: 1.0735 - val_acc: 0.9615\n",
      "Epoch 25/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0466 - acc: 0.9986 - val_loss: 1.0660 - val_acc: 0.9744\n",
      "Epoch 26/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0461 - acc: 0.9957 - val_loss: 1.0620 - val_acc: 0.9615\n",
      "Epoch 27/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0528 - acc: 0.9943 - val_loss: 1.2720 - val_acc: 0.8846\n",
      "Epoch 28/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0487 - acc: 0.9872 - val_loss: 1.0575 - val_acc: 0.9744\n",
      "Epoch 29/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0425 - acc: 1.0000 - val_loss: 1.0601 - val_acc: 0.9615\n",
      "Epoch 30/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0428 - acc: 1.0000 - val_loss: 1.0577 - val_acc: 0.9615\n",
      "Epoch 31/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0422 - acc: 1.0000 - val_loss: 1.0553 - val_acc: 0.9744\n",
      "Epoch 32/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0442 - acc: 0.9957 - val_loss: 1.0518 - val_acc: 0.9744\n",
      "Epoch 33/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0416 - acc: 1.0000 - val_loss: 1.0530 - val_acc: 0.9744\n",
      "Epoch 34/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0419 - acc: 1.0000 - val_loss: 1.0536 - val_acc: 0.9744\n",
      "Epoch 35/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0419 - acc: 1.0000 - val_loss: 1.0549 - val_acc: 0.9615\n",
      "Epoch 36/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0413 - acc: 1.0000 - val_loss: 1.0528 - val_acc: 0.9744\n",
      "Epoch 37/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0414 - acc: 1.0000 - val_loss: 1.0502 - val_acc: 0.9872\n",
      "Epoch 38/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0419 - acc: 0.9986 - val_loss: 1.0517 - val_acc: 0.9744\n",
      "Epoch 39/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0408 - acc: 1.0000 - val_loss: 1.0491 - val_acc: 0.9872\n",
      "Epoch 40/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0409 - acc: 1.0000 - val_loss: 1.0501 - val_acc: 0.9744\n",
      "Epoch 41/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0423 - acc: 0.9972 - val_loss: 1.0513 - val_acc: 0.9744\n",
      "Epoch 42/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0407 - acc: 1.0000 - val_loss: 1.0523 - val_acc: 0.9872\n",
      "Epoch 43/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0407 - acc: 1.0000 - val_loss: 1.0521 - val_acc: 0.9615\n",
      "Epoch 44/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0420 - acc: 0.9972 - val_loss: 1.0535 - val_acc: 0.9872\n",
      "Epoch 45/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0508 - val_acc: 0.9744\n",
      "Epoch 46/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0531 - val_acc: 0.9744\n",
      "Epoch 47/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0518 - val_acc: 0.9872\n",
      "Epoch 48/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0508 - val_acc: 0.9872\n",
      "Epoch 49/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0500 - val_acc: 0.9744\n",
      "Epoch 50/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0516 - val_acc: 0.9872\n",
      "Epoch 51/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0522 - val_acc: 0.9744\n",
      "Epoch 52/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0534 - val_acc: 0.9744\n",
      "Epoch 53/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0520 - val_acc: 0.9615\n",
      "Epoch 54/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0519 - val_acc: 0.9744\n",
      "Epoch 55/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0520 - val_acc: 0.9744\n",
      "Epoch 56/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0510 - val_acc: 0.9615\n",
      "Epoch 57/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0533 - val_acc: 0.9615\n",
      "Epoch 58/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0500 - val_acc: 0.9744\n",
      "Epoch 59/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0514 - val_acc: 0.9872\n",
      "Epoch 60/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0510 - val_acc: 0.9744\n",
      "Epoch 61/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0521 - val_acc: 0.9744\n",
      "Epoch 62/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0499 - val_acc: 0.9872\n",
      "Epoch 63/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0512 - val_acc: 0.9744\n",
      "Epoch 64/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0494 - val_acc: 0.9872\n",
      "Epoch 65/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0507 - val_acc: 0.9744\n",
      "Epoch 66/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0495 - val_acc: 0.9744\n",
      "Epoch 67/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0522 - val_acc: 0.9615\n",
      "Epoch 68/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0502 - val_acc: 0.9872\n",
      "Epoch 69/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0499 - val_acc: 0.9615\n",
      "Epoch 70/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0489 - val_acc: 0.9872\n",
      "Epoch 71/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0499 - val_acc: 0.9744\n",
      "Epoch 72/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0505 - val_acc: 0.9744\n",
      "Epoch 73/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0500 - val_acc: 0.9615\n",
      "Epoch 74/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0502 - val_acc: 0.9744\n",
      "Epoch 75/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0501 - val_acc: 0.9615\n",
      "Epoch 76/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0508 - val_acc: 0.9744\n",
      "Epoch 77/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0505 - val_acc: 0.9744\n",
      "Epoch 78/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0504 - val_acc: 0.9744\n",
      "Epoch 79/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0502 - val_acc: 0.9744\n",
      "Epoch 80/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0504 - val_acc: 0.9744\n",
      "Epoch 81/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0501 - val_acc: 0.9744\n",
      "Epoch 82/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0498 - val_acc: 0.9744\n",
      "Epoch 83/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0501 - val_acc: 0.9744\n",
      "Epoch 84/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0503 - val_acc: 0.9744\n",
      "Epoch 85/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0501 - val_acc: 0.9744\n",
      "Epoch 86/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0497 - val_acc: 0.9744\n",
      "Epoch 87/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0498 - val_acc: 0.9744\n",
      "Epoch 88/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0509 - val_acc: 0.9744\n",
      "Epoch 89/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0504 - val_acc: 0.9744\n",
      "Epoch 90/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0503 - val_acc: 0.9744\n",
      "Epoch 91/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0502 - val_acc: 0.9744\n",
      "Epoch 92/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0502 - val_acc: 0.9744\n",
      "Epoch 93/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0498 - val_acc: 0.9744\n",
      "Epoch 94/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0500 - val_acc: 0.9744\n",
      "Epoch 95/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0500 - val_acc: 0.9744\n",
      "Epoch 96/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0496 - val_acc: 0.9744\n",
      "Epoch 97/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0500 - val_acc: 0.9744\n",
      "Epoch 98/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0497 - val_acc: 0.9744\n",
      "Epoch 99/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0498 - val_acc: 0.9744\n",
      "Epoch 100/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0498 - val_acc: 0.9744\n",
      "78/78 [==============================] - 0s 314us/step\n",
      "Score for fold 6: loss of 1.0500762676581359; acc of 97.43589743589743%\n",
      "Train on 702 samples, validate on 78 samples\n",
      "Epoch 1/100\n",
      "702/702 [==============================] - 7s 10ms/step - loss: 2.3379 - acc: 0.2920 - val_loss: 2.0040 - val_acc: 0.4487\n",
      "Epoch 2/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.7276 - acc: 0.6296 - val_loss: 1.5748 - val_acc: 0.7564\n",
      "Epoch 3/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.4350 - acc: 0.7877 - val_loss: 1.2761 - val_acc: 0.9231\n",
      "Epoch 4/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.2639 - acc: 0.8761 - val_loss: 1.2822 - val_acc: 0.8077\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1828 - acc: 0.9117 - val_loss: 1.1845 - val_acc: 0.8974\n",
      "Epoch 6/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1499 - acc: 0.9074 - val_loss: 1.2887 - val_acc: 0.7564\n",
      "Epoch 7/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1541 - acc: 0.9003 - val_loss: 1.1295 - val_acc: 0.9359\n",
      "Epoch 8/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0940 - acc: 0.9587 - val_loss: 1.1251 - val_acc: 0.8718\n",
      "Epoch 9/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1166 - acc: 0.9245 - val_loss: 1.1100 - val_acc: 0.9872\n",
      "Epoch 10/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0806 - acc: 0.9644 - val_loss: 1.1170 - val_acc: 0.9103\n",
      "Epoch 11/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0729 - acc: 0.9715 - val_loss: 1.1188 - val_acc: 0.9359\n",
      "Epoch 12/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0765 - acc: 0.9672 - val_loss: 1.0919 - val_acc: 0.9487\n",
      "Epoch 13/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0589 - acc: 0.9858 - val_loss: 1.0869 - val_acc: 0.9744\n",
      "Epoch 14/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0727 - acc: 0.9644 - val_loss: 1.0987 - val_acc: 0.9615\n",
      "Epoch 15/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0730 - acc: 0.9801 - val_loss: 1.0821 - val_acc: 0.9231\n",
      "Epoch 16/100\n",
      "702/702 [==============================] - 3s 4ms/step - loss: 1.0644 - acc: 0.9829 - val_loss: 1.0672 - val_acc: 0.9615\n",
      "Epoch 17/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0524 - acc: 0.9900 - val_loss: 1.0702 - val_acc: 0.9744\n",
      "Epoch 18/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0520 - acc: 0.9886 - val_loss: 1.0756 - val_acc: 0.9744\n",
      "Epoch 19/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0503 - acc: 0.9929 - val_loss: 1.0738 - val_acc: 0.9744\n",
      "Epoch 20/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0545 - acc: 0.9929 - val_loss: 1.0675 - val_acc: 0.9744\n",
      "Epoch 21/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0483 - acc: 0.9972 - val_loss: 1.0675 - val_acc: 0.9744\n",
      "Epoch 22/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0455 - acc: 0.9972 - val_loss: 1.0800 - val_acc: 0.9744\n",
      "Epoch 23/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0451 - acc: 0.9986 - val_loss: 1.1329 - val_acc: 0.9487\n",
      "Epoch 24/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0442 - acc: 0.9972 - val_loss: 1.0673 - val_acc: 0.9744\n",
      "Epoch 25/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0442 - acc: 1.0000 - val_loss: 1.0729 - val_acc: 0.9615\n",
      "Epoch 26/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0434 - acc: 1.0000 - val_loss: 1.0837 - val_acc: 0.9615\n",
      "Epoch 27/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0433 - acc: 0.9986 - val_loss: 1.0754 - val_acc: 0.9744\n",
      "Epoch 28/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0435 - acc: 0.9972 - val_loss: 1.0700 - val_acc: 0.9872\n",
      "Epoch 29/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0420 - acc: 1.0000 - val_loss: 1.0739 - val_acc: 0.9872\n",
      "Epoch 30/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0424 - acc: 0.9986 - val_loss: 1.0710 - val_acc: 0.9872\n",
      "Epoch 31/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0419 - acc: 1.0000 - val_loss: 1.0656 - val_acc: 0.9615\n",
      "Epoch 32/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0425 - acc: 0.9986 - val_loss: 1.0903 - val_acc: 0.9744\n",
      "Epoch 33/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0416 - acc: 1.0000 - val_loss: 1.0768 - val_acc: 0.9744\n",
      "Epoch 34/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0417 - acc: 1.0000 - val_loss: 1.0817 - val_acc: 0.9744\n",
      "Epoch 35/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0414 - acc: 1.0000 - val_loss: 1.0682 - val_acc: 0.9872\n",
      "Epoch 36/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0411 - acc: 1.0000 - val_loss: 1.0783 - val_acc: 0.9615\n",
      "Epoch 37/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0411 - acc: 1.0000 - val_loss: 1.0707 - val_acc: 0.9615\n",
      "Epoch 38/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0412 - acc: 1.0000 - val_loss: 1.0700 - val_acc: 0.9744\n",
      "Epoch 39/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0411 - acc: 1.0000 - val_loss: 1.0784 - val_acc: 0.9615\n",
      "Epoch 40/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0416 - acc: 0.9986 - val_loss: 1.0743 - val_acc: 0.9872\n",
      "Epoch 41/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0408 - acc: 1.0000 - val_loss: 1.0707 - val_acc: 0.9744\n",
      "Epoch 42/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0408 - acc: 1.0000 - val_loss: 1.0670 - val_acc: 0.9872\n",
      "Epoch 43/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0408 - acc: 1.0000 - val_loss: 1.0764 - val_acc: 0.9744\n",
      "Epoch 44/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0408 - acc: 1.0000 - val_loss: 1.0640 - val_acc: 0.9615\n",
      "Epoch 45/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0407 - acc: 1.0000 - val_loss: 1.0819 - val_acc: 0.9872\n",
      "Epoch 46/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0409 - acc: 1.0000 - val_loss: 1.0910 - val_acc: 0.9744\n",
      "Epoch 47/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0765 - val_acc: 0.9615\n",
      "Epoch 48/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0725 - val_acc: 0.9872\n",
      "Epoch 49/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0794 - val_acc: 0.9744\n",
      "Epoch 50/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0726 - val_acc: 0.9744\n",
      "Epoch 51/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0762 - val_acc: 0.9744\n",
      "Epoch 52/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0821 - val_acc: 0.9744\n",
      "Epoch 53/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0408 - acc: 1.0000 - val_loss: 1.0717 - val_acc: 0.9744\n",
      "Epoch 54/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0822 - val_acc: 0.9615\n",
      "Epoch 55/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0673 - val_acc: 0.9872\n",
      "Epoch 56/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0704 - val_acc: 0.9744\n",
      "Epoch 57/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0741 - val_acc: 0.9744\n",
      "Epoch 58/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0842 - val_acc: 0.9744\n",
      "Epoch 59/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0770 - val_acc: 0.9744\n",
      "Epoch 60/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0779 - val_acc: 0.9744\n",
      "Epoch 61/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0823 - val_acc: 0.9872\n",
      "Epoch 62/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0809 - val_acc: 0.9744\n",
      "Epoch 63/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0793 - val_acc: 0.9615\n",
      "Epoch 64/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0865 - val_acc: 0.9615\n",
      "Epoch 65/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0784 - val_acc: 0.9872\n",
      "Epoch 66/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0832 - val_acc: 0.9872\n",
      "Epoch 67/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0821 - val_acc: 0.9744\n",
      "Epoch 68/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0803 - val_acc: 0.9744\n",
      "Epoch 69/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0820 - val_acc: 0.9872\n",
      "Epoch 70/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0820 - val_acc: 0.9872\n",
      "Epoch 71/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0811 - val_acc: 0.9615\n",
      "Epoch 72/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0861 - val_acc: 0.9744\n",
      "Epoch 73/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0853 - val_acc: 0.9615\n",
      "Epoch 74/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0796 - val_acc: 0.9615\n",
      "Epoch 75/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0834 - val_acc: 0.9872\n",
      "Epoch 76/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0847 - val_acc: 0.9615\n",
      "Epoch 77/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0815 - val_acc: 0.9872\n",
      "Epoch 78/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0820 - val_acc: 0.9872\n",
      "Epoch 79/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0824 - val_acc: 0.9872\n",
      "Epoch 80/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0791 - val_acc: 0.9872\n",
      "Epoch 81/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0813 - val_acc: 0.9615\n",
      "Epoch 82/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0828 - val_acc: 0.9872\n",
      "Epoch 83/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0819 - val_acc: 0.9872\n",
      "Epoch 84/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0821 - val_acc: 0.9872\n",
      "Epoch 85/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0811 - val_acc: 0.9872\n",
      "Epoch 86/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0833 - val_acc: 0.9744\n",
      "Epoch 87/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0866 - val_acc: 0.9872\n",
      "Epoch 88/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0871 - val_acc: 0.9872\n",
      "Epoch 89/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0882 - val_acc: 0.9872\n",
      "Epoch 90/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0853 - val_acc: 0.9872\n",
      "Epoch 91/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0845 - val_acc: 0.9744\n",
      "Epoch 92/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0828 - val_acc: 0.9872\n",
      "Epoch 93/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0854 - val_acc: 0.9872\n",
      "Epoch 94/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0847 - val_acc: 0.9872\n",
      "Epoch 95/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0854 - val_acc: 0.9872\n",
      "Epoch 96/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0848 - val_acc: 0.9872\n",
      "Epoch 97/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0861 - val_acc: 0.9872\n",
      "Epoch 98/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0870 - val_acc: 0.9872\n",
      "Epoch 99/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0863 - val_acc: 0.9744\n",
      "Epoch 100/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0861 - val_acc: 0.9744\n",
      "78/78 [==============================] - 0s 317us/step\n",
      "Score for fold 7: loss of 1.0818603558418078; acc of 97.43589743589743%\n",
      "Train on 702 samples, validate on 78 samples\n",
      "Epoch 1/100\n",
      "702/702 [==============================] - 7s 10ms/step - loss: 2.2176 - acc: 0.3946 - val_loss: 1.9495 - val_acc: 0.4231\n",
      "Epoch 2/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.6032 - acc: 0.7165 - val_loss: 1.4678 - val_acc: 0.7564\n",
      "Epoch 3/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.3666 - acc: 0.7963 - val_loss: 1.3174 - val_acc: 0.8077\n",
      "Epoch 4/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.2574 - acc: 0.8519 - val_loss: 1.1970 - val_acc: 0.8718\n",
      "Epoch 5/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1655 - acc: 0.9160 - val_loss: 1.2099 - val_acc: 0.8333\n",
      "Epoch 6/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1407 - acc: 0.9174 - val_loss: 1.1917 - val_acc: 0.9231\n",
      "Epoch 7/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1245 - acc: 0.9288 - val_loss: 1.1653 - val_acc: 0.9359\n",
      "Epoch 8/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1109 - acc: 0.9501 - val_loss: 1.0987 - val_acc: 0.9359\n",
      "Epoch 9/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1004 - acc: 0.9544 - val_loss: 1.1162 - val_acc: 0.9231\n",
      "Epoch 10/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0705 - acc: 0.9758 - val_loss: 1.1189 - val_acc: 0.8974\n",
      "Epoch 11/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1003 - acc: 0.9558 - val_loss: 1.1026 - val_acc: 0.9487\n",
      "Epoch 12/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0749 - acc: 0.9815 - val_loss: 1.3299 - val_acc: 0.7821\n",
      "Epoch 13/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0823 - acc: 0.9758 - val_loss: 1.0928 - val_acc: 0.9359\n",
      "Epoch 14/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0566 - acc: 0.9929 - val_loss: 1.0973 - val_acc: 0.9231\n",
      "Epoch 15/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0707 - acc: 0.9701 - val_loss: 1.0747 - val_acc: 0.9872\n",
      "Epoch 16/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0663 - acc: 0.9786 - val_loss: 1.0786 - val_acc: 0.9744\n",
      "Epoch 17/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0522 - acc: 0.9957 - val_loss: 1.1180 - val_acc: 0.9359\n",
      "Epoch 18/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0702 - acc: 0.9744 - val_loss: 1.0819 - val_acc: 0.9359\n",
      "Epoch 19/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0530 - acc: 0.9872 - val_loss: 1.1447 - val_acc: 0.8590\n",
      "Epoch 20/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0905 - acc: 0.9786 - val_loss: 1.0777 - val_acc: 0.9744\n",
      "Epoch 21/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0498 - acc: 0.9929 - val_loss: 1.0746 - val_acc: 0.9359\n",
      "Epoch 22/100\n",
      "702/702 [==============================] - 3s 4ms/step - loss: 1.0480 - acc: 0.9972 - val_loss: 1.0829 - val_acc: 0.9359\n",
      "Epoch 23/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0531 - acc: 0.9872 - val_loss: 1.0809 - val_acc: 0.9231\n",
      "Epoch 24/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0463 - acc: 0.9986 - val_loss: 1.0790 - val_acc: 0.9615\n",
      "Epoch 25/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0494 - acc: 0.9886 - val_loss: 1.0814 - val_acc: 0.9103\n",
      "Epoch 26/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0458 - acc: 0.9972 - val_loss: 1.0962 - val_acc: 0.9103\n",
      "Epoch 27/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0511 - acc: 0.9900 - val_loss: 1.0780 - val_acc: 0.9487\n",
      "Epoch 28/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0445 - acc: 0.9986 - val_loss: 1.0729 - val_acc: 0.9359\n",
      "Epoch 29/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0470 - acc: 0.9943 - val_loss: 1.0721 - val_acc: 0.9615\n",
      "Epoch 30/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0433 - acc: 0.9986 - val_loss: 1.0731 - val_acc: 0.9487\n",
      "Epoch 31/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0428 - acc: 0.9986 - val_loss: 1.0707 - val_acc: 0.9615\n",
      "Epoch 32/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0432 - acc: 0.9986 - val_loss: 1.0606 - val_acc: 0.9744\n",
      "Epoch 33/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0433 - acc: 0.9986 - val_loss: 1.0631 - val_acc: 0.9872\n",
      "Epoch 34/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0447 - acc: 0.9943 - val_loss: 1.0630 - val_acc: 0.9615\n",
      "Epoch 35/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0422 - acc: 0.9986 - val_loss: 1.0608 - val_acc: 0.9744\n",
      "Epoch 36/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0418 - acc: 0.9986 - val_loss: 1.0676 - val_acc: 0.9615\n",
      "Epoch 37/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0419 - acc: 0.9986 - val_loss: 1.0588 - val_acc: 0.9872\n",
      "Epoch 38/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0415 - acc: 1.0000 - val_loss: 1.0616 - val_acc: 0.9744\n",
      "Epoch 39/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0441 - acc: 0.9957 - val_loss: 1.0614 - val_acc: 0.9744\n",
      "Epoch 40/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0415 - acc: 0.9986 - val_loss: 1.0578 - val_acc: 0.9872\n",
      "Epoch 41/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0412 - acc: 1.0000 - val_loss: 1.0619 - val_acc: 0.9744\n",
      "Epoch 42/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0409 - acc: 1.0000 - val_loss: 1.0615 - val_acc: 0.9744\n",
      "Epoch 43/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0412 - acc: 1.0000 - val_loss: 1.0646 - val_acc: 0.9615\n",
      "Epoch 44/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0407 - acc: 1.0000 - val_loss: 1.0635 - val_acc: 0.9744\n",
      "Epoch 45/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0409 - acc: 1.0000 - val_loss: 1.0623 - val_acc: 0.9744\n",
      "Epoch 46/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0407 - acc: 1.0000 - val_loss: 1.0624 - val_acc: 0.9744\n",
      "Epoch 47/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0411 - acc: 1.0000 - val_loss: 1.0606 - val_acc: 0.9744\n",
      "Epoch 48/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0650 - val_acc: 0.9615\n",
      "Epoch 49/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0601 - val_acc: 0.9872\n",
      "Epoch 50/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0670 - val_acc: 0.9615\n",
      "Epoch 51/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0407 - acc: 1.0000 - val_loss: 1.0617 - val_acc: 0.9615\n",
      "Epoch 52/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0619 - val_acc: 0.9744\n",
      "Epoch 53/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0604 - val_acc: 0.9615\n",
      "Epoch 54/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0634 - val_acc: 0.9744\n",
      "Epoch 55/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0624 - val_acc: 0.9744\n",
      "Epoch 56/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0617 - val_acc: 0.9744\n",
      "Epoch 57/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0652 - val_acc: 0.9744\n",
      "Epoch 58/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0620 - val_acc: 0.9744\n",
      "Epoch 59/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0613 - val_acc: 0.9744\n",
      "Epoch 60/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0589 - val_acc: 0.9872\n",
      "Epoch 61/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0609 - val_acc: 0.9872\n",
      "Epoch 62/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0614 - val_acc: 0.9744\n",
      "Epoch 63/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0643 - val_acc: 0.9744\n",
      "Epoch 64/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0616 - val_acc: 0.9744\n",
      "Epoch 65/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0654 - val_acc: 0.9744\n",
      "Epoch 66/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0618 - val_acc: 0.9744\n",
      "Epoch 67/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0624 - val_acc: 0.9744\n",
      "Epoch 68/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0618 - val_acc: 0.9744\n",
      "Epoch 69/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0653 - val_acc: 0.9744\n",
      "Epoch 70/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0611 - val_acc: 0.9744\n",
      "Epoch 71/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0616 - val_acc: 0.9744\n",
      "Epoch 72/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0632 - val_acc: 0.9744\n",
      "Epoch 73/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0627 - val_acc: 0.9744\n",
      "Epoch 74/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0608 - val_acc: 0.9872\n",
      "Epoch 75/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0628 - val_acc: 0.9744\n",
      "Epoch 76/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0609 - val_acc: 0.9744\n",
      "Epoch 77/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0625 - val_acc: 0.9744\n",
      "Epoch 78/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0624 - val_acc: 0.9744\n",
      "Epoch 79/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0632 - val_acc: 0.9744\n",
      "Epoch 80/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0620 - val_acc: 0.9744\n",
      "Epoch 81/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0615 - val_acc: 0.9744\n",
      "Epoch 82/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0621 - val_acc: 0.9744\n",
      "Epoch 83/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0613 - val_acc: 0.9744\n",
      "Epoch 84/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0624 - val_acc: 0.9744\n",
      "Epoch 85/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0624 - val_acc: 0.9744\n",
      "Epoch 86/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0627 - val_acc: 0.9744\n",
      "Epoch 87/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0614 - val_acc: 0.9872\n",
      "Epoch 88/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0620 - val_acc: 0.9744\n",
      "Epoch 89/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0624 - val_acc: 0.9744\n",
      "Epoch 90/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0620 - val_acc: 0.9744\n",
      "Epoch 91/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0633 - val_acc: 0.9744\n",
      "Epoch 92/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0619 - val_acc: 0.9744\n",
      "Epoch 93/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0622 - val_acc: 0.9744\n",
      "Epoch 94/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0613 - val_acc: 0.9744\n",
      "Epoch 95/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0621 - val_acc: 0.9744\n",
      "Epoch 96/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0622 - val_acc: 0.9744\n",
      "Epoch 97/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0620 - val_acc: 0.9744\n",
      "Epoch 98/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0618 - val_acc: 0.9744\n",
      "Epoch 99/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0623 - val_acc: 0.9744\n",
      "Epoch 100/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0619 - val_acc: 0.9744\n",
      "78/78 [==============================] - 0s 335us/step\n",
      "Score for fold 8: loss of 1.0614565060688899; acc of 97.43589743589743%\n",
      "Train on 702 samples, validate on 78 samples\n",
      "Epoch 1/100\n",
      "702/702 [==============================] - 7s 10ms/step - loss: 2.2705 - acc: 0.3276 - val_loss: 1.7813 - val_acc: 0.5897\n",
      "Epoch 2/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.6251 - acc: 0.7137 - val_loss: 1.5049 - val_acc: 0.6795\n",
      "Epoch 3/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.3801 - acc: 0.8148 - val_loss: 1.4761 - val_acc: 0.5641\n",
      "Epoch 4/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.3048 - acc: 0.8291 - val_loss: 1.4317 - val_acc: 0.7051\n",
      "Epoch 5/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.2062 - acc: 0.8704 - val_loss: 1.2100 - val_acc: 0.7308\n",
      "Epoch 6/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1826 - acc: 0.8675 - val_loss: 1.2715 - val_acc: 0.8077\n",
      "Epoch 7/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1100 - acc: 0.9487 - val_loss: 1.0985 - val_acc: 0.9487\n",
      "Epoch 8/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1200 - acc: 0.9416 - val_loss: 1.1227 - val_acc: 0.9487\n",
      "Epoch 9/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1014 - acc: 0.9387 - val_loss: 1.0907 - val_acc: 0.9359\n",
      "Epoch 10/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0911 - acc: 0.9587 - val_loss: 1.0870 - val_acc: 0.9103\n",
      "Epoch 11/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0726 - acc: 0.9672 - val_loss: 1.0889 - val_acc: 0.9615\n",
      "Epoch 12/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0658 - acc: 0.9786 - val_loss: 1.0749 - val_acc: 0.9615\n",
      "Epoch 13/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0707 - acc: 0.9786 - val_loss: 1.0793 - val_acc: 0.9615\n",
      "Epoch 14/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0775 - acc: 0.9744 - val_loss: 1.0822 - val_acc: 0.9487\n",
      "Epoch 15/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0587 - acc: 0.9829 - val_loss: 1.1224 - val_acc: 0.8974\n",
      "Epoch 16/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0628 - acc: 0.9744 - val_loss: 1.0821 - val_acc: 0.9615\n",
      "Epoch 17/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0554 - acc: 0.9886 - val_loss: 1.0765 - val_acc: 0.9231\n",
      "Epoch 18/100\n",
      "702/702 [==============================] - 3s 4ms/step - loss: 1.0622 - acc: 0.9843 - val_loss: 1.0708 - val_acc: 0.9487\n",
      "Epoch 19/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0478 - acc: 0.9957 - val_loss: 1.1261 - val_acc: 0.9359\n",
      "Epoch 20/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0488 - acc: 0.9943 - val_loss: 1.0664 - val_acc: 0.9744\n",
      "Epoch 21/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0468 - acc: 0.9929 - val_loss: 1.0646 - val_acc: 0.9744\n",
      "Epoch 22/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0580 - acc: 0.9915 - val_loss: 1.0651 - val_acc: 0.9744\n",
      "Epoch 23/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0437 - acc: 1.0000 - val_loss: 1.0660 - val_acc: 0.9744\n",
      "Epoch 24/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0437 - acc: 1.0000 - val_loss: 1.0606 - val_acc: 0.9872\n",
      "Epoch 25/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0483 - acc: 0.9943 - val_loss: 1.0734 - val_acc: 0.9615\n",
      "Epoch 26/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0481 - acc: 0.9915 - val_loss: 1.0626 - val_acc: 0.9872\n",
      "Epoch 27/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0438 - acc: 0.9986 - val_loss: 1.0885 - val_acc: 0.9231\n",
      "Epoch 28/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0535 - acc: 0.9915 - val_loss: 1.0676 - val_acc: 0.9744\n",
      "Epoch 29/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0419 - acc: 1.0000 - val_loss: 1.0670 - val_acc: 0.9744\n",
      "Epoch 30/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0434 - acc: 0.9986 - val_loss: 1.0652 - val_acc: 0.9744\n",
      "Epoch 31/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0467 - acc: 0.9957 - val_loss: 1.0692 - val_acc: 0.9744\n",
      "Epoch 32/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0436 - acc: 0.9972 - val_loss: 1.0661 - val_acc: 0.9744\n",
      "Epoch 33/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0416 - acc: 1.0000 - val_loss: 1.0673 - val_acc: 0.9744\n",
      "Epoch 34/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0419 - acc: 0.9986 - val_loss: 1.0660 - val_acc: 0.9744\n",
      "Epoch 35/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0447 - acc: 0.9972 - val_loss: 1.0620 - val_acc: 0.9744\n",
      "Epoch 36/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0414 - acc: 1.0000 - val_loss: 1.0637 - val_acc: 0.9744\n",
      "Epoch 37/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0427 - acc: 0.9972 - val_loss: 1.0653 - val_acc: 0.9744\n",
      "Epoch 38/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0411 - acc: 1.0000 - val_loss: 1.0625 - val_acc: 0.9744\n",
      "Epoch 39/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0408 - acc: 1.0000 - val_loss: 1.0629 - val_acc: 0.9744\n",
      "Epoch 40/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0410 - acc: 1.0000 - val_loss: 1.0638 - val_acc: 0.9744\n",
      "Epoch 41/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0411 - acc: 1.0000 - val_loss: 1.0642 - val_acc: 0.9744\n",
      "Epoch 42/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0409 - acc: 1.0000 - val_loss: 1.0626 - val_acc: 0.9744\n",
      "Epoch 43/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0416 - acc: 0.9986 - val_loss: 1.0639 - val_acc: 0.9744\n",
      "Epoch 44/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0409 - acc: 1.0000 - val_loss: 1.0645 - val_acc: 0.9744\n",
      "Epoch 45/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0658 - val_acc: 0.9744\n",
      "Epoch 46/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0639 - val_acc: 0.9744\n",
      "Epoch 47/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0648 - val_acc: 0.9744\n",
      "Epoch 48/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0628 - val_acc: 0.9744\n",
      "Epoch 49/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0623 - val_acc: 0.9744\n",
      "Epoch 50/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0638 - val_acc: 0.9744\n",
      "Epoch 51/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0622 - val_acc: 0.9744\n",
      "Epoch 52/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0625 - val_acc: 0.9744\n",
      "Epoch 53/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0633 - val_acc: 0.9744\n",
      "Epoch 54/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0624 - val_acc: 0.9744\n",
      "Epoch 55/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0643 - val_acc: 0.9744\n",
      "Epoch 56/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0625 - val_acc: 0.9744\n",
      "Epoch 57/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0632 - val_acc: 0.9744\n",
      "Epoch 58/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0633 - val_acc: 0.9744\n",
      "Epoch 59/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0634 - val_acc: 0.9744\n",
      "Epoch 60/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0632 - val_acc: 0.9744\n",
      "Epoch 61/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0619 - val_acc: 0.9744\n",
      "Epoch 62/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0622 - val_acc: 0.9744\n",
      "Epoch 63/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0619 - val_acc: 0.9744\n",
      "Epoch 64/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0630 - val_acc: 0.9744\n",
      "Epoch 65/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0627 - val_acc: 0.9744\n",
      "Epoch 66/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0617 - val_acc: 0.9744\n",
      "Epoch 67/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0621 - val_acc: 0.9744\n",
      "Epoch 68/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0627 - val_acc: 0.9744\n",
      "Epoch 69/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0619 - val_acc: 0.9744\n",
      "Epoch 70/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0625 - val_acc: 0.9744\n",
      "Epoch 71/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0629 - val_acc: 0.9744\n",
      "Epoch 72/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0626 - val_acc: 0.9744\n",
      "Epoch 73/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0631 - val_acc: 0.9744\n",
      "Epoch 74/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0625 - val_acc: 0.9744\n",
      "Epoch 75/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0627 - val_acc: 0.9744\n",
      "Epoch 76/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0626 - val_acc: 0.9744\n",
      "Epoch 77/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0631 - val_acc: 0.9744\n",
      "Epoch 78/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0626 - val_acc: 0.9744\n",
      "Epoch 79/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0625 - val_acc: 0.9744\n",
      "Epoch 80/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0627 - val_acc: 0.9744\n",
      "Epoch 81/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0625 - val_acc: 0.9744\n",
      "Epoch 82/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0628 - val_acc: 0.9744\n",
      "Epoch 83/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0628 - val_acc: 0.9744\n",
      "Epoch 84/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0626 - val_acc: 0.9744\n",
      "Epoch 85/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0626 - val_acc: 0.9744\n",
      "Epoch 86/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0627 - val_acc: 0.9744\n",
      "Epoch 87/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0627 - val_acc: 0.9744\n",
      "Epoch 88/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0627 - val_acc: 0.9744\n",
      "Epoch 89/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0626 - val_acc: 0.9744\n",
      "Epoch 90/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0628 - val_acc: 0.9744\n",
      "Epoch 91/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0626 - val_acc: 0.9744\n",
      "Epoch 92/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0627 - val_acc: 0.9744\n",
      "Epoch 93/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0626 - val_acc: 0.9744\n",
      "Epoch 94/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0628 - val_acc: 0.9744\n",
      "Epoch 95/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0630 - val_acc: 0.9744\n",
      "Epoch 96/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0626 - val_acc: 0.9744\n",
      "Epoch 97/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0629 - val_acc: 0.9744\n",
      "Epoch 98/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0628 - val_acc: 0.9744\n",
      "Epoch 99/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0627 - val_acc: 0.9744\n",
      "Epoch 100/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0627 - val_acc: 0.9744\n",
      "78/78 [==============================] - 0s 320us/step\n",
      "Score for fold 9: loss of 1.062831313182146; acc of 97.43589697740018%\n",
      "Train on 702 samples, validate on 78 samples\n",
      "Epoch 1/100\n",
      "702/702 [==============================] - 8s 11ms/step - loss: 2.2592 - acc: 0.3519 - val_loss: 1.9308 - val_acc: 0.4872\n",
      "Epoch 2/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.6464 - acc: 0.6994 - val_loss: 1.4872 - val_acc: 0.7308\n",
      "Epoch 3/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.3566 - acc: 0.8575 - val_loss: 1.4425 - val_acc: 0.7179\n",
      "Epoch 4/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.2680 - acc: 0.8604 - val_loss: 1.2314 - val_acc: 0.8205\n",
      "Epoch 5/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.2056 - acc: 0.8960 - val_loss: 1.1770 - val_acc: 0.8974\n",
      "Epoch 6/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1287 - acc: 0.9245 - val_loss: 1.1560 - val_acc: 0.9615\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1373 - acc: 0.9217 - val_loss: 1.0969 - val_acc: 0.9744\n",
      "Epoch 8/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1225 - acc: 0.9558 - val_loss: 1.0988 - val_acc: 0.9615\n",
      "Epoch 9/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0768 - acc: 0.9687 - val_loss: 1.1261 - val_acc: 0.8462\n",
      "Epoch 10/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0962 - acc: 0.9587 - val_loss: 1.1455 - val_acc: 0.8718\n",
      "Epoch 11/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0738 - acc: 0.9744 - val_loss: 1.1172 - val_acc: 0.9359\n",
      "Epoch 12/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0777 - acc: 0.9729 - val_loss: 1.0994 - val_acc: 0.9231\n",
      "Epoch 13/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0632 - acc: 0.9815 - val_loss: 1.1680 - val_acc: 0.8333\n",
      "Epoch 14/100\n",
      "702/702 [==============================] - 3s 4ms/step - loss: 1.0755 - acc: 0.9758 - val_loss: 1.1996 - val_acc: 0.8462\n",
      "Epoch 15/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0646 - acc: 0.9801 - val_loss: 1.0589 - val_acc: 1.0000\n",
      "Epoch 16/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0535 - acc: 0.9929 - val_loss: 1.0826 - val_acc: 0.9615\n",
      "Epoch 17/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0525 - acc: 0.9929 - val_loss: 1.0800 - val_acc: 0.9615\n",
      "Epoch 18/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0499 - acc: 0.9943 - val_loss: 1.1502 - val_acc: 0.9231\n",
      "Epoch 19/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0574 - acc: 0.9915 - val_loss: 1.0589 - val_acc: 1.0000\n",
      "Epoch 20/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0744 - acc: 0.9872 - val_loss: 1.0666 - val_acc: 0.9744\n",
      "Epoch 21/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0466 - acc: 0.9957 - val_loss: 1.0654 - val_acc: 0.9872\n",
      "Epoch 22/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0494 - acc: 0.9900 - val_loss: 1.0577 - val_acc: 0.9872\n",
      "Epoch 23/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0453 - acc: 0.9972 - val_loss: 1.1059 - val_acc: 0.9487\n",
      "Epoch 24/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0461 - acc: 0.9972 - val_loss: 1.0601 - val_acc: 0.9744\n",
      "Epoch 25/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0441 - acc: 0.9986 - val_loss: 1.0708 - val_acc: 0.9744\n",
      "Epoch 26/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0469 - acc: 0.9915 - val_loss: 1.0560 - val_acc: 0.9872\n",
      "Epoch 27/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0452 - acc: 0.9972 - val_loss: 1.0579 - val_acc: 0.9744\n",
      "Epoch 28/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0442 - acc: 0.9986 - val_loss: 1.0883 - val_acc: 0.9487\n",
      "Epoch 29/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0430 - acc: 1.0000 - val_loss: 1.0665 - val_acc: 0.9744\n",
      "Epoch 30/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0428 - acc: 1.0000 - val_loss: 1.0705 - val_acc: 0.9744\n",
      "Epoch 31/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0422 - acc: 1.0000 - val_loss: 1.0667 - val_acc: 0.9744\n",
      "Epoch 32/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0423 - acc: 1.0000 - val_loss: 1.0604 - val_acc: 0.9744\n",
      "Epoch 33/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0432 - acc: 0.9986 - val_loss: 1.0509 - val_acc: 0.9872\n",
      "Epoch 34/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0418 - acc: 1.0000 - val_loss: 1.0508 - val_acc: 0.9872\n",
      "Epoch 35/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0414 - acc: 1.0000 - val_loss: 1.0599 - val_acc: 0.9744\n",
      "Epoch 36/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0415 - acc: 1.0000 - val_loss: 1.0581 - val_acc: 0.9744\n",
      "Epoch 37/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0416 - acc: 1.0000 - val_loss: 1.0617 - val_acc: 0.9744\n",
      "Epoch 38/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0413 - acc: 1.0000 - val_loss: 1.0542 - val_acc: 0.9744\n",
      "Epoch 39/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0412 - acc: 1.0000 - val_loss: 1.0874 - val_acc: 0.9872\n",
      "Epoch 40/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0412 - acc: 1.0000 - val_loss: 1.0602 - val_acc: 0.9744\n",
      "Epoch 41/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0410 - acc: 1.0000 - val_loss: 1.0512 - val_acc: 0.9744\n",
      "Epoch 42/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0410 - acc: 1.0000 - val_loss: 1.0506 - val_acc: 0.9872\n",
      "Epoch 43/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0409 - acc: 1.0000 - val_loss: 1.0569 - val_acc: 0.9744\n",
      "Epoch 44/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0409 - acc: 1.0000 - val_loss: 1.0599 - val_acc: 0.9872\n",
      "Epoch 45/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0409 - acc: 1.0000 - val_loss: 1.0472 - val_acc: 0.9872\n",
      "Epoch 46/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0410 - acc: 1.0000 - val_loss: 1.0474 - val_acc: 0.9872\n",
      "Epoch 47/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0407 - acc: 1.0000 - val_loss: 1.0519 - val_acc: 0.9744\n",
      "Epoch 48/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0407 - acc: 1.0000 - val_loss: 1.0525 - val_acc: 0.9744\n",
      "Epoch 49/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0407 - acc: 1.0000 - val_loss: 1.0518 - val_acc: 0.9744\n",
      "Epoch 50/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0407 - acc: 1.0000 - val_loss: 1.0540 - val_acc: 0.9872\n",
      "Epoch 51/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0407 - acc: 1.0000 - val_loss: 1.0477 - val_acc: 0.9744\n",
      "Epoch 52/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0410 - acc: 1.0000 - val_loss: 1.0485 - val_acc: 0.9872\n",
      "Epoch 53/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0470 - val_acc: 0.9872\n",
      "Epoch 54/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0510 - val_acc: 0.9744\n",
      "Epoch 55/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0498 - val_acc: 0.9744\n",
      "Epoch 56/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0481 - val_acc: 0.9872\n",
      "Epoch 57/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0472 - val_acc: 0.9872\n",
      "Epoch 58/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0481 - val_acc: 0.9872\n",
      "Epoch 59/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0480 - val_acc: 0.9872\n",
      "Epoch 60/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0471 - val_acc: 0.9872\n",
      "Epoch 61/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0506 - val_acc: 0.9744\n",
      "Epoch 62/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0481 - val_acc: 0.9872\n",
      "Epoch 63/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0534 - val_acc: 0.9744\n",
      "Epoch 64/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0494 - val_acc: 0.9744\n",
      "Epoch 65/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0485 - val_acc: 0.9744\n",
      "Epoch 66/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0481 - val_acc: 0.9744\n",
      "Epoch 67/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0479 - val_acc: 0.9744\n",
      "Epoch 68/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0505 - val_acc: 0.9744\n",
      "Epoch 69/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0512 - val_acc: 0.9872\n",
      "Epoch 70/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0508 - val_acc: 0.9744\n",
      "Epoch 71/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0496 - val_acc: 0.9744\n",
      "Epoch 72/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0485 - val_acc: 0.9744\n",
      "Epoch 73/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0497 - val_acc: 0.9744\n",
      "Epoch 74/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0507 - val_acc: 0.9744\n",
      "Epoch 75/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0483 - val_acc: 0.9744\n",
      "Epoch 76/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0491 - val_acc: 0.9744\n",
      "Epoch 77/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0497 - val_acc: 0.9744\n",
      "Epoch 78/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0503 - val_acc: 0.9872\n",
      "Epoch 79/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0498 - val_acc: 0.9744\n",
      "Epoch 80/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0492 - val_acc: 0.9744\n",
      "Epoch 81/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0517 - val_acc: 0.9744\n",
      "Epoch 82/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0500 - val_acc: 0.9744\n",
      "Epoch 83/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0492 - val_acc: 0.9744\n",
      "Epoch 84/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0491 - val_acc: 0.9744\n",
      "Epoch 85/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0504 - val_acc: 0.9744\n",
      "Epoch 86/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0501 - val_acc: 0.9744\n",
      "Epoch 87/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0503 - val_acc: 0.9744\n",
      "Epoch 88/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0501 - val_acc: 0.9744\n",
      "Epoch 89/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0506 - val_acc: 0.9744\n",
      "Epoch 90/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0492 - val_acc: 0.9744\n",
      "Epoch 91/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0492 - val_acc: 0.9744\n",
      "Epoch 92/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0492 - val_acc: 0.9744\n",
      "Epoch 93/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0498 - val_acc: 0.9744\n",
      "Epoch 94/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0511 - val_acc: 0.9744\n",
      "Epoch 95/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0499 - val_acc: 0.9744\n",
      "Epoch 96/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0501 - val_acc: 0.9744\n",
      "Epoch 97/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0503 - val_acc: 0.9744\n",
      "Epoch 98/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0500 - val_acc: 0.9744\n",
      "Epoch 99/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0496 - val_acc: 0.9744\n",
      "Epoch 100/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0498 - val_acc: 0.9744\n",
      "78/78 [==============================] - 0s 307us/step\n",
      "Score for fold 10: loss of 1.049278314297016; acc of 97.43589697740018%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1296x1296 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97.69230750890878\n",
      "Train on 702 samples, validate on 78 samples\n",
      "Epoch 1/100\n",
      "702/702 [==============================] - 8s 11ms/step - loss: 2.2749 - acc: 0.3319 - val_loss: 2.0612 - val_acc: 0.6026\n",
      "Epoch 2/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.6561 - acc: 0.6766 - val_loss: 1.5541 - val_acc: 0.6282\n",
      "Epoch 3/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.3996 - acc: 0.8034 - val_loss: 1.3903 - val_acc: 0.6026\n",
      "Epoch 4/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.2695 - acc: 0.8405 - val_loss: 1.2114 - val_acc: 0.8333\n",
      "Epoch 5/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1842 - acc: 0.8846 - val_loss: 1.4185 - val_acc: 0.7308\n",
      "Epoch 6/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1955 - acc: 0.8903 - val_loss: 1.3742 - val_acc: 0.7436\n",
      "Epoch 7/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1333 - acc: 0.9416 - val_loss: 1.1492 - val_acc: 0.8462\n",
      "Epoch 8/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1244 - acc: 0.9473 - val_loss: 1.1291 - val_acc: 0.8718\n",
      "Epoch 9/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0866 - acc: 0.9587 - val_loss: 1.9200 - val_acc: 0.7179\n",
      "Epoch 10/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1243 - acc: 0.9373 - val_loss: 1.0894 - val_acc: 0.9359\n",
      "Epoch 11/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0686 - acc: 0.9715 - val_loss: 1.0786 - val_acc: 0.9615\n",
      "Epoch 12/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0713 - acc: 0.9729 - val_loss: 1.3136 - val_acc: 0.7949\n",
      "Epoch 13/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0898 - acc: 0.9672 - val_loss: 1.0968 - val_acc: 0.9359\n",
      "Epoch 14/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0566 - acc: 0.9886 - val_loss: 1.1351 - val_acc: 0.8718\n",
      "Epoch 15/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0600 - acc: 0.9843 - val_loss: 1.0779 - val_acc: 0.9359\n",
      "Epoch 16/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0892 - acc: 0.9615 - val_loss: 1.1208 - val_acc: 0.8846\n",
      "Epoch 17/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0549 - acc: 0.9957 - val_loss: 1.0857 - val_acc: 0.9487\n",
      "Epoch 18/100\n",
      "702/702 [==============================] - 3s 4ms/step - loss: 1.0508 - acc: 0.9900 - val_loss: 1.0830 - val_acc: 0.9359\n",
      "Epoch 19/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0554 - acc: 0.9843 - val_loss: 1.0792 - val_acc: 0.9487\n",
      "Epoch 20/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0482 - acc: 0.9929 - val_loss: 1.0902 - val_acc: 0.9359\n",
      "Epoch 21/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0491 - acc: 0.9943 - val_loss: 1.0767 - val_acc: 0.9487\n",
      "Epoch 22/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0515 - acc: 0.9886 - val_loss: 1.0812 - val_acc: 0.9359\n",
      "Epoch 23/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0483 - acc: 0.9900 - val_loss: 1.1011 - val_acc: 0.8974\n",
      "Epoch 24/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0529 - acc: 0.9915 - val_loss: 1.0759 - val_acc: 0.9487\n",
      "Epoch 25/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0434 - acc: 0.9986 - val_loss: 1.0890 - val_acc: 0.9359\n",
      "Epoch 26/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0470 - acc: 0.9929 - val_loss: 1.0784 - val_acc: 0.9487\n",
      "Epoch 27/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0435 - acc: 0.9986 - val_loss: 1.0771 - val_acc: 0.9487\n",
      "Epoch 28/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0429 - acc: 1.0000 - val_loss: 1.0806 - val_acc: 0.9615\n",
      "Epoch 29/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0432 - acc: 1.0000 - val_loss: 1.0801 - val_acc: 0.9487\n",
      "Epoch 30/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0431 - acc: 0.9972 - val_loss: 1.0751 - val_acc: 0.9615\n",
      "Epoch 31/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0420 - acc: 1.0000 - val_loss: 1.0734 - val_acc: 0.9487\n",
      "Epoch 32/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0419 - acc: 1.0000 - val_loss: 1.0719 - val_acc: 0.9487\n",
      "Epoch 33/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0422 - acc: 1.0000 - val_loss: 1.0732 - val_acc: 0.9615\n",
      "Epoch 34/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0415 - acc: 1.0000 - val_loss: 1.0689 - val_acc: 0.9487\n",
      "Epoch 35/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0415 - acc: 1.0000 - val_loss: 1.0778 - val_acc: 0.9487\n",
      "Epoch 36/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0416 - acc: 1.0000 - val_loss: 1.0744 - val_acc: 0.9487\n",
      "Epoch 37/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0411 - acc: 1.0000 - val_loss: 1.0704 - val_acc: 0.9615\n",
      "Epoch 38/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0420 - acc: 0.9986 - val_loss: 1.0748 - val_acc: 0.9615\n",
      "Epoch 39/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0411 - acc: 1.0000 - val_loss: 1.0784 - val_acc: 0.9487\n",
      "Epoch 40/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0411 - acc: 1.0000 - val_loss: 1.0732 - val_acc: 0.9487\n",
      "Epoch 41/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0408 - acc: 1.0000 - val_loss: 1.0732 - val_acc: 0.9615\n",
      "Epoch 42/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0410 - acc: 1.0000 - val_loss: 1.0724 - val_acc: 0.9615\n",
      "Epoch 43/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0408 - acc: 1.0000 - val_loss: 1.0735 - val_acc: 0.9615\n",
      "Epoch 44/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0407 - acc: 1.0000 - val_loss: 1.0690 - val_acc: 0.9615\n",
      "Epoch 45/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0409 - acc: 1.0000 - val_loss: 1.0745 - val_acc: 0.9615\n",
      "Epoch 46/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0732 - val_acc: 0.9615\n",
      "Epoch 47/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0731 - val_acc: 0.9615\n",
      "Epoch 48/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0407 - acc: 1.0000 - val_loss: 1.0727 - val_acc: 0.9487\n",
      "Epoch 49/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0729 - val_acc: 0.9615\n",
      "Epoch 50/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0723 - val_acc: 0.9487\n",
      "Epoch 51/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0727 - val_acc: 0.9615\n",
      "Epoch 52/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0715 - val_acc: 0.9487\n",
      "Epoch 53/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0744 - val_acc: 0.9615\n",
      "Epoch 54/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0705 - val_acc: 0.9615\n",
      "Epoch 55/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0722 - val_acc: 0.9487\n",
      "Epoch 56/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0706 - val_acc: 0.9615\n",
      "Epoch 57/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0740 - val_acc: 0.9615\n",
      "Epoch 58/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0728 - val_acc: 0.9487\n",
      "Epoch 59/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0720 - val_acc: 0.9615\n",
      "Epoch 60/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0754 - val_acc: 0.9359\n",
      "Epoch 61/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0727 - val_acc: 0.9487\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 62/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0709 - val_acc: 0.9487\n",
      "Epoch 63/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0728 - val_acc: 0.9487\n",
      "Epoch 64/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0714 - val_acc: 0.9487\n",
      "Epoch 65/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0713 - val_acc: 0.9487\n",
      "Epoch 66/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0726 - val_acc: 0.9487\n",
      "Epoch 67/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0733 - val_acc: 0.9487\n",
      "Epoch 68/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0733 - val_acc: 0.9487\n",
      "Epoch 69/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0721 - val_acc: 0.9487\n",
      "Epoch 70/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0698 - val_acc: 0.9487\n",
      "Epoch 71/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0712 - val_acc: 0.9615\n",
      "Epoch 72/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0717 - val_acc: 0.9615\n",
      "Epoch 73/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0715 - val_acc: 0.9487\n",
      "Epoch 74/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0709 - val_acc: 0.9487\n",
      "Epoch 75/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0716 - val_acc: 0.9487\n",
      "Epoch 76/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0717 - val_acc: 0.9615\n",
      "Epoch 77/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0718 - val_acc: 0.9615\n",
      "Epoch 78/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0714 - val_acc: 0.9487\n",
      "Epoch 79/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0722 - val_acc: 0.9487\n",
      "Epoch 80/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0722 - val_acc: 0.9487\n",
      "Epoch 81/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0717 - val_acc: 0.9615\n",
      "Epoch 82/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0722 - val_acc: 0.9487\n",
      "Epoch 83/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0720 - val_acc: 0.9615\n",
      "Epoch 84/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0729 - val_acc: 0.9487\n",
      "Epoch 85/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0726 - val_acc: 0.9487\n",
      "Epoch 86/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0722 - val_acc: 0.9487\n",
      "Epoch 87/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0720 - val_acc: 0.9487\n",
      "Epoch 88/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0722 - val_acc: 0.9487\n",
      "Epoch 89/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0721 - val_acc: 0.9487\n",
      "Epoch 90/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0726 - val_acc: 0.9487\n",
      "Epoch 91/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0712 - val_acc: 0.9487\n",
      "Epoch 92/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0718 - val_acc: 0.9487\n",
      "Epoch 93/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0725 - val_acc: 0.9487\n",
      "Epoch 94/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0718 - val_acc: 0.9487\n",
      "Epoch 95/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0722 - val_acc: 0.9487\n",
      "Epoch 96/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0717 - val_acc: 0.9487\n",
      "Epoch 97/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0716 - val_acc: 0.9487\n",
      "Epoch 98/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0720 - val_acc: 0.9487\n",
      "Epoch 99/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0724 - val_acc: 0.9487\n",
      "Epoch 100/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0719 - val_acc: 0.9487\n",
      "78/78 [==============================] - 0s 302us/step\n",
      "Score for fold 1: loss of 1.072138725182949; acc of 94.87179487179486%\n",
      "Train on 702 samples, validate on 78 samples\n",
      "Epoch 1/100\n",
      "702/702 [==============================] - 8s 11ms/step - loss: 2.1607 - acc: 0.3675 - val_loss: 1.8972 - val_acc: 0.5000\n",
      "Epoch 2/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.5466 - acc: 0.7251 - val_loss: 1.5809 - val_acc: 0.6026\n",
      "Epoch 3/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.3329 - acc: 0.8063 - val_loss: 1.2890 - val_acc: 0.8590\n",
      "Epoch 4/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.2290 - acc: 0.8647 - val_loss: 1.2427 - val_acc: 0.8462\n",
      "Epoch 5/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1562 - acc: 0.9046 - val_loss: 1.3700 - val_acc: 0.7564\n",
      "Epoch 6/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1516 - acc: 0.9231 - val_loss: 1.1287 - val_acc: 0.9231\n",
      "Epoch 7/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1065 - acc: 0.9402 - val_loss: 1.1239 - val_acc: 0.8846\n",
      "Epoch 8/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1503 - acc: 0.9174 - val_loss: 1.2409 - val_acc: 0.7564\n",
      "Epoch 9/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0999 - acc: 0.9601 - val_loss: 1.3092 - val_acc: 0.8462\n",
      "Epoch 10/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0866 - acc: 0.9573 - val_loss: 1.1232 - val_acc: 0.8846\n",
      "Epoch 11/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0688 - acc: 0.9715 - val_loss: 1.1404 - val_acc: 0.9359\n",
      "Epoch 12/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0839 - acc: 0.9587 - val_loss: 1.0738 - val_acc: 0.9615\n",
      "Epoch 13/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0748 - acc: 0.9772 - val_loss: 1.0730 - val_acc: 0.9487\n",
      "Epoch 14/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0588 - acc: 0.9872 - val_loss: 1.0788 - val_acc: 0.9487\n",
      "Epoch 15/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0580 - acc: 0.9843 - val_loss: 1.1106 - val_acc: 0.9359\n",
      "Epoch 16/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0864 - acc: 0.9630 - val_loss: 1.0711 - val_acc: 0.9744\n",
      "Epoch 17/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0547 - acc: 0.9900 - val_loss: 1.0884 - val_acc: 0.9231\n",
      "Epoch 18/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0854 - acc: 0.9801 - val_loss: 1.0553 - val_acc: 1.0000\n",
      "Epoch 19/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0546 - acc: 0.9886 - val_loss: 1.0713 - val_acc: 0.9487\n",
      "Epoch 20/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0531 - acc: 0.9915 - val_loss: 1.0561 - val_acc: 0.9872\n",
      "Epoch 21/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0544 - acc: 0.9929 - val_loss: 1.1463 - val_acc: 0.9359\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0540 - acc: 0.9915 - val_loss: 1.0743 - val_acc: 0.9615\n",
      "Epoch 23/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0690 - acc: 0.9715 - val_loss: 1.0586 - val_acc: 0.9744\n",
      "Epoch 24/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0473 - acc: 0.9943 - val_loss: 1.0561 - val_acc: 0.9744\n",
      "Epoch 25/100\n",
      "702/702 [==============================] - 3s 4ms/step - loss: 1.0546 - acc: 0.9858 - val_loss: 1.0623 - val_acc: 0.9744\n",
      "Epoch 26/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0473 - acc: 0.9957 - val_loss: 1.0766 - val_acc: 0.9872\n",
      "Epoch 27/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0453 - acc: 0.9986 - val_loss: 1.0475 - val_acc: 1.0000\n",
      "Epoch 28/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0499 - acc: 0.9929 - val_loss: 1.0913 - val_acc: 0.9615\n",
      "Epoch 29/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0461 - acc: 0.9929 - val_loss: 1.0535 - val_acc: 0.9744\n",
      "Epoch 30/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0457 - acc: 0.9957 - val_loss: 1.0589 - val_acc: 0.9872\n",
      "Epoch 31/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0440 - acc: 0.9986 - val_loss: 1.0578 - val_acc: 0.9744\n",
      "Epoch 32/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0435 - acc: 0.9986 - val_loss: 1.0545 - val_acc: 0.9744\n",
      "Epoch 33/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0424 - acc: 1.0000 - val_loss: 1.0514 - val_acc: 0.9872\n",
      "Epoch 34/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0436 - acc: 0.9972 - val_loss: 1.0497 - val_acc: 0.9872\n",
      "Epoch 35/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0430 - acc: 0.9986 - val_loss: 1.0484 - val_acc: 0.9872\n",
      "Epoch 36/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0419 - acc: 1.0000 - val_loss: 1.0539 - val_acc: 0.9872\n",
      "Epoch 37/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0420 - acc: 1.0000 - val_loss: 1.0490 - val_acc: 0.9872\n",
      "Epoch 38/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0413 - acc: 1.0000 - val_loss: 1.0482 - val_acc: 0.9872\n",
      "Epoch 39/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0412 - acc: 1.0000 - val_loss: 1.0515 - val_acc: 0.9872\n",
      "Epoch 40/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0412 - acc: 1.0000 - val_loss: 1.0478 - val_acc: 0.9872\n",
      "Epoch 41/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0415 - acc: 1.0000 - val_loss: 1.0511 - val_acc: 0.9744\n",
      "Epoch 42/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0412 - acc: 1.0000 - val_loss: 1.0500 - val_acc: 0.9872\n",
      "Epoch 43/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0409 - acc: 1.0000 - val_loss: 1.0509 - val_acc: 0.9744\n",
      "Epoch 44/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0410 - acc: 1.0000 - val_loss: 1.0491 - val_acc: 0.9744\n",
      "Epoch 45/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0408 - acc: 1.0000 - val_loss: 1.0436 - val_acc: 1.0000\n",
      "Epoch 46/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0407 - acc: 1.0000 - val_loss: 1.0477 - val_acc: 0.9872\n",
      "Epoch 47/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0407 - acc: 1.0000 - val_loss: 1.0531 - val_acc: 0.9744\n",
      "Epoch 48/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0407 - acc: 1.0000 - val_loss: 1.0491 - val_acc: 0.9872\n",
      "Epoch 49/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0492 - val_acc: 0.9872\n",
      "Epoch 50/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0462 - val_acc: 0.9872\n",
      "Epoch 51/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0451 - val_acc: 0.9872\n",
      "Epoch 52/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0489 - val_acc: 0.9744\n",
      "Epoch 53/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0407 - acc: 1.0000 - val_loss: 1.0434 - val_acc: 1.0000\n",
      "Epoch 54/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0431 - val_acc: 1.0000\n",
      "Epoch 55/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0434 - val_acc: 1.0000\n",
      "Epoch 56/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0445 - val_acc: 1.0000\n",
      "Epoch 57/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0440 - val_acc: 1.0000\n",
      "Epoch 58/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0501 - val_acc: 0.9744\n",
      "Epoch 59/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0449 - val_acc: 0.9872\n",
      "Epoch 60/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0460 - val_acc: 0.9744\n",
      "Epoch 61/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0441 - val_acc: 1.0000\n",
      "Epoch 62/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0450 - val_acc: 0.9872\n",
      "Epoch 63/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0531 - val_acc: 0.9872\n",
      "Epoch 64/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0445 - val_acc: 1.0000\n",
      "Epoch 65/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0436 - val_acc: 1.0000\n",
      "Epoch 66/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0461 - val_acc: 0.9872\n",
      "Epoch 67/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0434 - val_acc: 1.0000\n",
      "Epoch 68/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0443 - val_acc: 0.9872\n",
      "Epoch 69/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0468 - val_acc: 0.9872\n",
      "Epoch 70/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0447 - val_acc: 1.0000\n",
      "Epoch 71/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0447 - val_acc: 0.9872\n",
      "Epoch 72/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0459 - val_acc: 0.9872\n",
      "Epoch 73/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0430 - val_acc: 1.0000\n",
      "Epoch 74/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0452 - val_acc: 0.9872\n",
      "Epoch 75/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0451 - val_acc: 0.9872\n",
      "Epoch 76/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0449 - val_acc: 0.9872\n",
      "Epoch 77/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0452 - val_acc: 0.9872\n",
      "Epoch 78/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0442 - val_acc: 0.9872\n",
      "Epoch 79/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0468 - val_acc: 0.9872\n",
      "Epoch 80/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0453 - val_acc: 0.9872\n",
      "Epoch 81/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0438 - val_acc: 1.0000\n",
      "Epoch 82/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0450 - val_acc: 1.0000\n",
      "Epoch 83/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0441 - val_acc: 1.0000\n",
      "Epoch 84/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0449 - val_acc: 1.0000\n",
      "Epoch 85/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0448 - val_acc: 1.0000\n",
      "Epoch 86/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0455 - val_acc: 0.9872\n",
      "Epoch 87/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0445 - val_acc: 1.0000\n",
      "Epoch 88/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0461 - val_acc: 0.9872\n",
      "Epoch 89/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0456 - val_acc: 0.9872\n",
      "Epoch 90/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0456 - val_acc: 0.9872\n",
      "Epoch 91/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0466 - val_acc: 0.9872\n",
      "Epoch 92/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0456 - val_acc: 0.9872\n",
      "Epoch 93/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0463 - val_acc: 0.9872\n",
      "Epoch 94/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0458 - val_acc: 0.9872\n",
      "Epoch 95/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0468 - val_acc: 0.9872\n",
      "Epoch 96/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0456 - val_acc: 0.9872\n",
      "Epoch 97/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0456 - val_acc: 0.9872\n",
      "Epoch 98/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0462 - val_acc: 0.9872\n",
      "Epoch 99/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0468 - val_acc: 0.9744\n",
      "Epoch 100/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0462 - val_acc: 0.9872\n",
      "78/78 [==============================] - 0s 304us/step\n",
      "Score for fold 2: loss of 1.0491988643621788; acc of 97.43589743589743%\n",
      "Train on 702 samples, validate on 78 samples\n",
      "Epoch 1/100\n",
      "702/702 [==============================] - 8s 12ms/step - loss: 2.2581 - acc: 0.3462 - val_loss: 1.8231 - val_acc: 0.5513\n",
      "Epoch 2/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.6543 - acc: 0.6937 - val_loss: 1.4842 - val_acc: 0.8077\n",
      "Epoch 3/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.3625 - acc: 0.8063 - val_loss: 1.2551 - val_acc: 0.8462\n",
      "Epoch 4/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.2475 - acc: 0.8405 - val_loss: 1.3676 - val_acc: 0.6538\n",
      "Epoch 5/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1990 - acc: 0.8889 - val_loss: 1.1595 - val_acc: 0.9103\n",
      "Epoch 6/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1318 - acc: 0.9274 - val_loss: 1.1663 - val_acc: 0.8333\n",
      "Epoch 7/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1172 - acc: 0.9288 - val_loss: 1.1546 - val_acc: 0.9231\n",
      "Epoch 8/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0944 - acc: 0.9544 - val_loss: 1.1104 - val_acc: 0.9103\n",
      "Epoch 9/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0898 - acc: 0.9516 - val_loss: 1.1629 - val_acc: 0.7949\n",
      "Epoch 10/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0927 - acc: 0.9587 - val_loss: 1.1029 - val_acc: 0.9487\n",
      "Epoch 11/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0665 - acc: 0.9701 - val_loss: 1.0804 - val_acc: 0.9615\n",
      "Epoch 12/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0945 - acc: 0.9473 - val_loss: 1.0934 - val_acc: 0.9744\n",
      "Epoch 13/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0599 - acc: 0.9858 - val_loss: 1.0897 - val_acc: 0.9615\n",
      "Epoch 14/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0868 - acc: 0.9715 - val_loss: 1.0736 - val_acc: 0.9615\n",
      "Epoch 15/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0826 - acc: 0.9801 - val_loss: 1.0730 - val_acc: 0.9615\n",
      "Epoch 16/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0562 - acc: 0.9972 - val_loss: 1.1082 - val_acc: 0.8974\n",
      "Epoch 17/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0547 - acc: 0.9886 - val_loss: 1.0785 - val_acc: 0.9487\n",
      "Epoch 18/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0567 - acc: 0.9886 - val_loss: 1.0793 - val_acc: 0.9615\n",
      "Epoch 19/100\n",
      "702/702 [==============================] - 3s 4ms/step - loss: 1.0662 - acc: 0.9758 - val_loss: 1.0734 - val_acc: 0.9744\n",
      "Epoch 20/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0495 - acc: 0.9957 - val_loss: 1.0643 - val_acc: 0.9615\n",
      "Epoch 21/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0478 - acc: 0.9957 - val_loss: 1.0691 - val_acc: 0.9615\n",
      "Epoch 22/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0477 - acc: 0.9943 - val_loss: 1.0724 - val_acc: 0.9487\n",
      "Epoch 23/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0486 - acc: 0.9957 - val_loss: 1.0671 - val_acc: 0.9744\n",
      "Epoch 24/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0460 - acc: 0.9972 - val_loss: 1.0717 - val_acc: 0.9615\n",
      "Epoch 25/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0482 - acc: 0.9929 - val_loss: 1.0664 - val_acc: 0.9744\n",
      "Epoch 26/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0447 - acc: 0.9972 - val_loss: 1.0597 - val_acc: 0.9872\n",
      "Epoch 27/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0449 - acc: 0.9986 - val_loss: 1.0709 - val_acc: 0.9744\n",
      "Epoch 28/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0440 - acc: 0.9986 - val_loss: 1.0671 - val_acc: 0.9744\n",
      "Epoch 29/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0442 - acc: 0.9957 - val_loss: 1.0627 - val_acc: 0.9744\n",
      "Epoch 30/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0434 - acc: 0.9986 - val_loss: 1.0696 - val_acc: 0.9744\n",
      "Epoch 31/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0454 - acc: 0.9929 - val_loss: 1.0660 - val_acc: 0.9615\n",
      "Epoch 32/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0429 - acc: 0.9986 - val_loss: 1.0656 - val_acc: 0.9744\n",
      "Epoch 33/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0427 - acc: 0.9986 - val_loss: 1.0564 - val_acc: 0.9872\n",
      "Epoch 34/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0434 - acc: 0.9957 - val_loss: 1.0598 - val_acc: 0.9744\n",
      "Epoch 35/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0419 - acc: 0.9986 - val_loss: 1.0664 - val_acc: 0.9744\n",
      "Epoch 36/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0441 - acc: 0.9972 - val_loss: 1.0528 - val_acc: 0.9872\n",
      "Epoch 37/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0416 - acc: 0.9986 - val_loss: 1.0624 - val_acc: 0.9615\n",
      "Epoch 38/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0419 - acc: 1.0000 - val_loss: 1.0537 - val_acc: 0.9744\n",
      "Epoch 39/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0414 - acc: 1.0000 - val_loss: 1.0539 - val_acc: 0.9744\n",
      "Epoch 40/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0412 - acc: 1.0000 - val_loss: 1.0613 - val_acc: 0.9872\n",
      "Epoch 41/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0411 - acc: 1.0000 - val_loss: 1.0549 - val_acc: 0.9615\n",
      "Epoch 42/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0410 - acc: 1.0000 - val_loss: 1.0499 - val_acc: 0.9744\n",
      "Epoch 43/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0413 - acc: 1.0000 - val_loss: 1.0530 - val_acc: 0.9872\n",
      "Epoch 44/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0409 - acc: 1.0000 - val_loss: 1.0551 - val_acc: 0.9872\n",
      "Epoch 45/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0408 - acc: 1.0000 - val_loss: 1.0535 - val_acc: 0.9872\n",
      "Epoch 46/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0407 - acc: 1.0000 - val_loss: 1.0513 - val_acc: 0.9872\n",
      "Epoch 47/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0407 - acc: 1.0000 - val_loss: 1.0543 - val_acc: 0.9872\n",
      "Epoch 48/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0407 - acc: 1.0000 - val_loss: 1.0524 - val_acc: 0.9744\n",
      "Epoch 49/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0407 - acc: 1.0000 - val_loss: 1.0498 - val_acc: 0.9872\n",
      "Epoch 50/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0407 - acc: 1.0000 - val_loss: 1.0508 - val_acc: 0.9744\n",
      "Epoch 51/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0574 - val_acc: 0.9744\n",
      "Epoch 52/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0502 - val_acc: 0.9744\n",
      "Epoch 53/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0530 - val_acc: 0.9744\n",
      "Epoch 54/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0502 - val_acc: 0.9744\n",
      "Epoch 55/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0490 - val_acc: 0.9872\n",
      "Epoch 56/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0529 - val_acc: 0.9744\n",
      "Epoch 57/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0514 - val_acc: 0.9872\n",
      "Epoch 58/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0531 - val_acc: 0.9744\n",
      "Epoch 59/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0547 - val_acc: 0.9744\n",
      "Epoch 60/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0548 - val_acc: 0.9744\n",
      "Epoch 61/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0532 - val_acc: 0.9744\n",
      "Epoch 62/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0513 - val_acc: 0.9744\n",
      "Epoch 63/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0535 - val_acc: 0.9744\n",
      "Epoch 64/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0521 - val_acc: 0.9744\n",
      "Epoch 65/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0514 - val_acc: 0.9744\n",
      "Epoch 66/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0508 - val_acc: 0.9744\n",
      "Epoch 67/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0513 - val_acc: 0.9744\n",
      "Epoch 68/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0517 - val_acc: 0.9744\n",
      "Epoch 69/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0494 - val_acc: 0.9744\n",
      "Epoch 70/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0502 - val_acc: 0.9744\n",
      "Epoch 71/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0514 - val_acc: 0.9744\n",
      "Epoch 72/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0504 - val_acc: 0.9744\n",
      "Epoch 73/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0498 - val_acc: 0.9744\n",
      "Epoch 74/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0510 - val_acc: 0.9744\n",
      "Epoch 75/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0501 - val_acc: 0.9744\n",
      "Epoch 76/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0520 - val_acc: 0.9744\n",
      "Epoch 77/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0527 - val_acc: 0.9744\n",
      "Epoch 78/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0519 - val_acc: 0.9744\n",
      "Epoch 79/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0509 - val_acc: 0.9744\n",
      "Epoch 80/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0513 - val_acc: 0.9744\n",
      "Epoch 81/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0510 - val_acc: 0.9872\n",
      "Epoch 82/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0514 - val_acc: 0.9744\n",
      "Epoch 83/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0520 - val_acc: 0.9744\n",
      "Epoch 84/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0514 - val_acc: 0.9744\n",
      "Epoch 85/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0518 - val_acc: 0.9744\n",
      "Epoch 86/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0512 - val_acc: 0.9744\n",
      "Epoch 87/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0506 - val_acc: 0.9744\n",
      "Epoch 88/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0510 - val_acc: 0.9744\n",
      "Epoch 89/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0509 - val_acc: 0.9872\n",
      "Epoch 90/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0501 - val_acc: 0.9872\n",
      "Epoch 91/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0506 - val_acc: 0.9872\n",
      "Epoch 92/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0514 - val_acc: 0.9744\n",
      "Epoch 93/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0516 - val_acc: 0.9744\n",
      "Epoch 94/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0506 - val_acc: 0.9872\n",
      "Epoch 95/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0508 - val_acc: 0.9872\n",
      "Epoch 96/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0512 - val_acc: 0.9744\n",
      "Epoch 97/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0510 - val_acc: 0.9744\n",
      "Epoch 98/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0509 - val_acc: 0.9744\n",
      "Epoch 99/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0513 - val_acc: 0.9744\n",
      "Epoch 100/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0511 - val_acc: 0.9744\n",
      "78/78 [==============================] - 0s 303us/step\n",
      "Score for fold 3: loss of 1.0528499254813561; acc of 97.43589697740018%\n",
      "Train on 702 samples, validate on 78 samples\n",
      "Epoch 1/100\n",
      "702/702 [==============================] - 8s 12ms/step - loss: 2.2660 - acc: 0.3248 - val_loss: 2.1957 - val_acc: 0.4359\n",
      "Epoch 2/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.6703 - acc: 0.6652 - val_loss: 1.5063 - val_acc: 0.7436\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.4046 - acc: 0.7806 - val_loss: 1.5095 - val_acc: 0.7308\n",
      "Epoch 4/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.2614 - acc: 0.8604 - val_loss: 1.3341 - val_acc: 0.7179\n",
      "Epoch 5/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.2247 - acc: 0.8604 - val_loss: 1.1610 - val_acc: 0.8974\n",
      "Epoch 6/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1463 - acc: 0.9174 - val_loss: 1.1322 - val_acc: 0.9615\n",
      "Epoch 7/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1412 - acc: 0.9330 - val_loss: 1.1588 - val_acc: 0.9487\n",
      "Epoch 8/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0951 - acc: 0.9487 - val_loss: 1.0824 - val_acc: 0.9359\n",
      "Epoch 9/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1217 - acc: 0.9245 - val_loss: 1.0842 - val_acc: 0.9359\n",
      "Epoch 10/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0743 - acc: 0.9786 - val_loss: 1.2770 - val_acc: 0.7949\n",
      "Epoch 11/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0827 - acc: 0.9744 - val_loss: 1.0772 - val_acc: 0.9744\n",
      "Epoch 12/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0699 - acc: 0.9829 - val_loss: 1.0657 - val_acc: 0.9872\n",
      "Epoch 13/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0772 - acc: 0.9630 - val_loss: 1.0989 - val_acc: 0.9487\n",
      "Epoch 14/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0582 - acc: 0.9900 - val_loss: 1.1365 - val_acc: 0.8590\n",
      "Epoch 15/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0635 - acc: 0.9758 - val_loss: 1.0650 - val_acc: 0.9487\n",
      "Epoch 16/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0613 - acc: 0.9829 - val_loss: 1.1756 - val_acc: 0.8846\n",
      "Epoch 17/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0633 - acc: 0.9786 - val_loss: 1.0569 - val_acc: 0.9872\n",
      "Epoch 18/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0564 - acc: 0.9900 - val_loss: 1.0677 - val_acc: 0.9615\n",
      "Epoch 19/100\n",
      "702/702 [==============================] - 3s 4ms/step - loss: 1.0507 - acc: 0.9915 - val_loss: 1.0611 - val_acc: 0.9872\n",
      "Epoch 20/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0494 - acc: 0.9900 - val_loss: 1.1142 - val_acc: 0.9103\n",
      "Epoch 21/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0529 - acc: 0.9900 - val_loss: 1.0578 - val_acc: 0.9872\n",
      "Epoch 22/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0466 - acc: 0.9943 - val_loss: 1.0848 - val_acc: 0.9615\n",
      "Epoch 23/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0500 - acc: 0.9886 - val_loss: 1.0663 - val_acc: 0.9615\n",
      "Epoch 24/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0472 - acc: 0.9929 - val_loss: 1.0525 - val_acc: 0.9872\n",
      "Epoch 25/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0462 - acc: 0.9957 - val_loss: 1.0527 - val_acc: 0.9872\n",
      "Epoch 26/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0442 - acc: 0.9986 - val_loss: 1.0557 - val_acc: 0.9744\n",
      "Epoch 27/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0435 - acc: 1.0000 - val_loss: 1.0518 - val_acc: 0.9872\n",
      "Epoch 28/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0445 - acc: 1.0000 - val_loss: 1.0493 - val_acc: 0.9872\n",
      "Epoch 29/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0424 - acc: 1.0000 - val_loss: 1.0487 - val_acc: 0.9744\n",
      "Epoch 30/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0430 - acc: 0.9972 - val_loss: 1.0505 - val_acc: 0.9744\n",
      "Epoch 31/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0443 - acc: 0.9972 - val_loss: 1.0541 - val_acc: 0.9744\n",
      "Epoch 32/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0417 - acc: 1.0000 - val_loss: 1.0530 - val_acc: 0.9744\n",
      "Epoch 33/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0429 - acc: 0.9986 - val_loss: 1.0504 - val_acc: 0.9744\n",
      "Epoch 34/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0413 - acc: 1.0000 - val_loss: 1.0483 - val_acc: 0.9872\n",
      "Epoch 35/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0414 - acc: 1.0000 - val_loss: 1.0533 - val_acc: 0.9615\n",
      "Epoch 36/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0418 - acc: 1.0000 - val_loss: 1.0459 - val_acc: 1.0000\n",
      "Epoch 37/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0415 - acc: 1.0000 - val_loss: 1.0535 - val_acc: 0.9872\n",
      "Epoch 38/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0417 - acc: 0.9986 - val_loss: 1.0432 - val_acc: 0.9872\n",
      "Epoch 39/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0411 - acc: 1.0000 - val_loss: 1.0509 - val_acc: 0.9872\n",
      "Epoch 40/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0415 - acc: 1.0000 - val_loss: 1.0450 - val_acc: 1.0000\n",
      "Epoch 41/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0411 - acc: 1.0000 - val_loss: 1.0530 - val_acc: 0.9744\n",
      "Epoch 42/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0414 - acc: 1.0000 - val_loss: 1.0527 - val_acc: 0.9872\n",
      "Epoch 43/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0412 - acc: 1.0000 - val_loss: 1.0444 - val_acc: 1.0000\n",
      "Epoch 44/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0408 - acc: 1.0000 - val_loss: 1.0466 - val_acc: 0.9872\n",
      "Epoch 45/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0409 - acc: 1.0000 - val_loss: 1.0479 - val_acc: 0.9872\n",
      "Epoch 46/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0409 - acc: 1.0000 - val_loss: 1.0453 - val_acc: 1.0000\n",
      "Epoch 47/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0407 - acc: 1.0000 - val_loss: 1.0486 - val_acc: 0.9872\n",
      "Epoch 48/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0408 - acc: 1.0000 - val_loss: 1.0456 - val_acc: 0.9744\n",
      "Epoch 49/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0407 - acc: 1.0000 - val_loss: 1.0463 - val_acc: 0.9744\n",
      "Epoch 50/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0509 - val_acc: 0.9872\n",
      "Epoch 51/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0510 - val_acc: 0.9872\n",
      "Epoch 52/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0486 - val_acc: 0.9872\n",
      "Epoch 53/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0461 - val_acc: 0.9872\n",
      "Epoch 54/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0451 - val_acc: 0.9872\n",
      "Epoch 55/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0490 - val_acc: 0.9615\n",
      "Epoch 56/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0430 - val_acc: 1.0000\n",
      "Epoch 57/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0512 - val_acc: 0.9872\n",
      "Epoch 58/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0514 - val_acc: 0.9872\n",
      "Epoch 59/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0480 - val_acc: 0.9872\n",
      "Epoch 60/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0457 - val_acc: 0.9744\n",
      "Epoch 61/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0473 - val_acc: 0.9872\n",
      "Epoch 62/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0497 - val_acc: 0.9872\n",
      "Epoch 63/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0421 - val_acc: 1.0000\n",
      "Epoch 64/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0458 - val_acc: 0.9872\n",
      "Epoch 65/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0503 - val_acc: 0.9872\n",
      "Epoch 66/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0469 - val_acc: 0.9872\n",
      "Epoch 67/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0498 - val_acc: 0.9872\n",
      "Epoch 68/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0497 - val_acc: 0.9872\n",
      "Epoch 69/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0510 - val_acc: 0.9872\n",
      "Epoch 70/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0502 - val_acc: 0.9872\n",
      "Epoch 71/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0467 - val_acc: 0.9744\n",
      "Epoch 72/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0468 - val_acc: 0.9744\n",
      "Epoch 73/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0466 - val_acc: 0.9744\n",
      "Epoch 74/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0447 - val_acc: 0.9872\n",
      "Epoch 75/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0440 - val_acc: 0.9872\n",
      "Epoch 76/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0432 - val_acc: 1.0000\n",
      "Epoch 77/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0475 - val_acc: 0.9872\n",
      "Epoch 78/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0505 - val_acc: 0.9872\n",
      "Epoch 79/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0429 - val_acc: 0.9872\n",
      "Epoch 80/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0450 - val_acc: 0.9744\n",
      "Epoch 81/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0439 - val_acc: 0.9744\n",
      "Epoch 82/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0444 - val_acc: 0.9872\n",
      "Epoch 83/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0467 - val_acc: 0.9872\n",
      "Epoch 84/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0464 - val_acc: 0.9744\n",
      "Epoch 85/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0449 - val_acc: 0.9744\n",
      "Epoch 86/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0455 - val_acc: 0.9744\n",
      "Epoch 87/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0459 - val_acc: 0.9744\n",
      "Epoch 88/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0453 - val_acc: 0.9744\n",
      "Epoch 89/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0466 - val_acc: 0.9744\n",
      "Epoch 90/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0463 - val_acc: 0.9744\n",
      "Epoch 91/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0471 - val_acc: 0.9744\n",
      "Epoch 92/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0470 - val_acc: 0.9744\n",
      "Epoch 93/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0463 - val_acc: 0.9744\n",
      "Epoch 94/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0455 - val_acc: 0.9744\n",
      "Epoch 95/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0461 - val_acc: 0.9872\n",
      "Epoch 96/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0454 - val_acc: 0.9744\n",
      "Epoch 97/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0472 - val_acc: 0.9872\n",
      "Epoch 98/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0469 - val_acc: 0.9744\n",
      "Epoch 99/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0467 - val_acc: 0.9744\n",
      "Epoch 100/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0456 - val_acc: 0.9744\n",
      "78/78 [==============================] - 0s 313us/step\n",
      "Score for fold 4: loss of 1.0500910129302587; acc of 98.71794825945145%\n",
      "Train on 702 samples, validate on 78 samples\n",
      "Epoch 1/100\n",
      "702/702 [==============================] - 9s 12ms/step - loss: 2.2570 - acc: 0.3148 - val_loss: 2.0025 - val_acc: 0.5385\n",
      "Epoch 2/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.6709 - acc: 0.6681 - val_loss: 1.5459 - val_acc: 0.6923\n",
      "Epoch 3/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.3959 - acc: 0.8405 - val_loss: 1.3928 - val_acc: 0.7564\n",
      "Epoch 4/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.2626 - acc: 0.8419 - val_loss: 1.2397 - val_acc: 0.8590\n",
      "Epoch 5/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1989 - acc: 0.8818 - val_loss: 1.1564 - val_acc: 0.9359\n",
      "Epoch 6/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1542 - acc: 0.8974 - val_loss: 1.1622 - val_acc: 0.9359\n",
      "Epoch 7/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1266 - acc: 0.9103 - val_loss: 1.1261 - val_acc: 0.9231\n",
      "Epoch 8/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1176 - acc: 0.9473 - val_loss: 1.1188 - val_acc: 0.9359\n",
      "Epoch 9/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1049 - acc: 0.9544 - val_loss: 1.1037 - val_acc: 0.9744\n",
      "Epoch 10/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0805 - acc: 0.9516 - val_loss: 1.0957 - val_acc: 0.9487\n",
      "Epoch 11/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0804 - acc: 0.9630 - val_loss: 1.1734 - val_acc: 0.8462\n",
      "Epoch 12/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0867 - acc: 0.9558 - val_loss: 1.2125 - val_acc: 0.7821\n",
      "Epoch 13/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0641 - acc: 0.9786 - val_loss: 1.0870 - val_acc: 0.9615\n",
      "Epoch 14/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0683 - acc: 0.9829 - val_loss: 1.1266 - val_acc: 0.9231\n",
      "Epoch 15/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0582 - acc: 0.9829 - val_loss: 1.1043 - val_acc: 0.9231\n",
      "Epoch 16/100\n",
      "702/702 [==============================] - 3s 5ms/step - loss: 1.0824 - acc: 0.9658 - val_loss: 1.0977 - val_acc: 0.9615\n",
      "Epoch 17/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0524 - acc: 0.9929 - val_loss: 1.0594 - val_acc: 0.9872\n",
      "Epoch 18/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0573 - acc: 0.9872 - val_loss: 1.0566 - val_acc: 0.9872\n",
      "Epoch 19/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0489 - acc: 0.9957 - val_loss: 1.0604 - val_acc: 0.9615\n",
      "Epoch 20/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0494 - acc: 0.9915 - val_loss: 1.0768 - val_acc: 0.9615\n",
      "Epoch 21/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0516 - acc: 0.9915 - val_loss: 1.0555 - val_acc: 0.9744\n",
      "Epoch 22/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0477 - acc: 0.9972 - val_loss: 1.0755 - val_acc: 0.9615\n",
      "Epoch 23/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0607 - acc: 0.9843 - val_loss: 1.0624 - val_acc: 0.9615\n",
      "Epoch 24/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0462 - acc: 0.9972 - val_loss: 1.0795 - val_acc: 0.9744\n",
      "Epoch 25/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0454 - acc: 0.9957 - val_loss: 1.0520 - val_acc: 0.9744\n",
      "Epoch 26/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0440 - acc: 0.9986 - val_loss: 1.0540 - val_acc: 0.9872\n",
      "Epoch 27/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0446 - acc: 0.9972 - val_loss: 1.0532 - val_acc: 0.9744\n",
      "Epoch 28/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0446 - acc: 0.9957 - val_loss: 1.0527 - val_acc: 0.9872\n",
      "Epoch 29/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0446 - acc: 0.9972 - val_loss: 1.0517 - val_acc: 0.9744\n",
      "Epoch 30/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0455 - acc: 0.9986 - val_loss: 1.1348 - val_acc: 0.9487\n",
      "Epoch 31/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0449 - acc: 0.9957 - val_loss: 1.0684 - val_acc: 0.9744\n",
      "Epoch 32/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0424 - acc: 0.9986 - val_loss: 1.0644 - val_acc: 0.9615\n",
      "Epoch 33/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0416 - acc: 1.0000 - val_loss: 1.0540 - val_acc: 0.9872\n",
      "Epoch 34/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0430 - acc: 0.9972 - val_loss: 1.0555 - val_acc: 0.9744\n",
      "Epoch 35/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0417 - acc: 1.0000 - val_loss: 1.0565 - val_acc: 0.9872\n",
      "Epoch 36/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0414 - acc: 1.0000 - val_loss: 1.0669 - val_acc: 0.9744\n",
      "Epoch 37/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0419 - acc: 1.0000 - val_loss: 1.0539 - val_acc: 0.9872\n",
      "Epoch 38/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0411 - acc: 1.0000 - val_loss: 1.0488 - val_acc: 0.9744\n",
      "Epoch 39/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0411 - acc: 1.0000 - val_loss: 1.0488 - val_acc: 0.9872\n",
      "Epoch 40/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0409 - acc: 1.0000 - val_loss: 1.0530 - val_acc: 0.9872\n",
      "Epoch 41/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0418 - acc: 0.9986 - val_loss: 1.0536 - val_acc: 0.9744\n",
      "Epoch 42/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0411 - acc: 1.0000 - val_loss: 1.0524 - val_acc: 0.9872\n",
      "Epoch 43/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0410 - acc: 1.0000 - val_loss: 1.0488 - val_acc: 0.9872\n",
      "Epoch 44/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0407 - acc: 1.0000 - val_loss: 1.0472 - val_acc: 0.9872\n",
      "Epoch 45/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0407 - acc: 1.0000 - val_loss: 1.0478 - val_acc: 0.9872\n",
      "Epoch 46/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0411 - acc: 0.9986 - val_loss: 1.0479 - val_acc: 0.9872\n",
      "Epoch 47/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0407 - acc: 1.0000 - val_loss: 1.0471 - val_acc: 0.9872\n",
      "Epoch 48/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0467 - val_acc: 0.9872\n",
      "Epoch 49/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0461 - val_acc: 0.9872\n",
      "Epoch 50/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0452 - val_acc: 0.9872\n",
      "Epoch 51/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0458 - val_acc: 0.9872\n",
      "Epoch 52/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0473 - val_acc: 0.9872\n",
      "Epoch 53/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0474 - val_acc: 1.0000\n",
      "Epoch 54/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0465 - val_acc: 0.9872\n",
      "Epoch 55/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0461 - val_acc: 0.9872\n",
      "Epoch 56/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0449 - val_acc: 1.0000\n",
      "Epoch 57/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0458 - val_acc: 0.9872\n",
      "Epoch 58/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0458 - val_acc: 0.9872\n",
      "Epoch 59/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0457 - val_acc: 1.0000\n",
      "Epoch 60/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0459 - val_acc: 1.0000\n",
      "Epoch 61/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0460 - val_acc: 0.9872\n",
      "Epoch 62/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0455 - val_acc: 0.9872\n",
      "Epoch 63/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0454 - val_acc: 1.0000\n",
      "Epoch 64/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0462 - val_acc: 0.9872\n",
      "Epoch 65/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0461 - val_acc: 1.0000\n",
      "Epoch 66/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0475 - val_acc: 0.9872\n",
      "Epoch 67/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0457 - val_acc: 1.0000\n",
      "Epoch 68/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0471 - val_acc: 1.0000\n",
      "Epoch 69/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0458 - val_acc: 1.0000\n",
      "Epoch 70/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0459 - val_acc: 1.0000\n",
      "Epoch 71/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0455 - val_acc: 1.0000\n",
      "Epoch 72/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0457 - val_acc: 1.0000\n",
      "Epoch 73/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0456 - val_acc: 1.0000\n",
      "Epoch 74/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0466 - val_acc: 1.0000\n",
      "Epoch 75/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0456 - val_acc: 1.0000\n",
      "Epoch 76/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0460 - val_acc: 1.0000\n",
      "Epoch 77/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0458 - val_acc: 1.0000\n",
      "Epoch 78/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0459 - val_acc: 1.0000\n",
      "Epoch 79/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0456 - val_acc: 1.0000\n",
      "Epoch 80/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0456 - val_acc: 1.0000\n",
      "Epoch 81/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0458 - val_acc: 1.0000\n",
      "Epoch 82/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0459 - val_acc: 1.0000\n",
      "Epoch 83/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0460 - val_acc: 1.0000\n",
      "Epoch 84/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0463 - val_acc: 1.0000\n",
      "Epoch 85/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0463 - val_acc: 1.0000\n",
      "Epoch 86/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0459 - val_acc: 1.0000\n",
      "Epoch 87/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0459 - val_acc: 1.0000\n",
      "Epoch 88/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0459 - val_acc: 1.0000\n",
      "Epoch 89/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0458 - val_acc: 1.0000\n",
      "Epoch 90/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0460 - val_acc: 1.0000\n",
      "Epoch 91/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0462 - val_acc: 1.0000\n",
      "Epoch 92/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0456 - val_acc: 1.0000\n",
      "Epoch 93/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0459 - val_acc: 1.0000\n",
      "Epoch 94/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0456 - val_acc: 1.0000\n",
      "Epoch 95/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0458 - val_acc: 1.0000\n",
      "Epoch 96/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0458 - val_acc: 1.0000\n",
      "Epoch 97/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0459 - val_acc: 1.0000\n",
      "Epoch 98/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0457 - val_acc: 1.0000\n",
      "Epoch 99/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0457 - val_acc: 1.0000\n",
      "Epoch 100/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0458 - val_acc: 1.0000\n",
      "78/78 [==============================] - 0s 301us/step\n",
      "Score for fold 5: loss of 1.0456902094376392; acc of 100.0%\n",
      "Train on 702 samples, validate on 78 samples\n",
      "Epoch 1/100\n",
      "702/702 [==============================] - 9s 13ms/step - loss: 2.2966 - acc: 0.3120 - val_loss: 1.9812 - val_acc: 0.5000\n",
      "Epoch 2/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.6123 - acc: 0.7407 - val_loss: 1.4914 - val_acc: 0.7821\n",
      "Epoch 3/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.3958 - acc: 0.7906 - val_loss: 1.5411 - val_acc: 0.6282\n",
      "Epoch 4/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.2552 - acc: 0.8276 - val_loss: 1.3866 - val_acc: 0.7308\n",
      "Epoch 5/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1867 - acc: 0.8974 - val_loss: 1.7084 - val_acc: 0.6282\n",
      "Epoch 6/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1588 - acc: 0.9217 - val_loss: 1.1523 - val_acc: 0.8974\n",
      "Epoch 7/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1347 - acc: 0.9103 - val_loss: 1.1060 - val_acc: 0.9615\n",
      "Epoch 8/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1000 - acc: 0.9487 - val_loss: 1.1359 - val_acc: 0.8846\n",
      "Epoch 9/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1178 - acc: 0.9245 - val_loss: 1.2750 - val_acc: 0.8077\n",
      "Epoch 10/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0840 - acc: 0.9601 - val_loss: 1.2649 - val_acc: 0.7949\n",
      "Epoch 11/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0959 - acc: 0.9573 - val_loss: 1.1178 - val_acc: 0.9231\n",
      "Epoch 12/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0667 - acc: 0.9758 - val_loss: 1.0779 - val_acc: 0.9615\n",
      "Epoch 13/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0859 - acc: 0.9701 - val_loss: 1.0750 - val_acc: 0.9615\n",
      "Epoch 14/100\n",
      "702/702 [==============================] - 3s 5ms/step - loss: 1.0637 - acc: 0.9772 - val_loss: 1.0766 - val_acc: 0.9487\n",
      "Epoch 15/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0548 - acc: 0.9929 - val_loss: 1.0723 - val_acc: 0.9487\n",
      "Epoch 16/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0554 - acc: 0.9872 - val_loss: 1.0750 - val_acc: 0.9872\n",
      "Epoch 17/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0499 - acc: 0.9957 - val_loss: 1.2998 - val_acc: 0.8462\n",
      "Epoch 18/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0721 - acc: 0.9801 - val_loss: 1.0642 - val_acc: 0.9615\n",
      "Epoch 19/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0519 - acc: 0.9900 - val_loss: 1.0569 - val_acc: 0.9744\n",
      "Epoch 20/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0488 - acc: 0.9957 - val_loss: 1.1298 - val_acc: 0.8974\n",
      "Epoch 21/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0652 - acc: 0.9929 - val_loss: 1.0655 - val_acc: 0.9744\n",
      "Epoch 22/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0463 - acc: 0.9957 - val_loss: 1.0574 - val_acc: 0.9744\n",
      "Epoch 23/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0491 - acc: 0.9900 - val_loss: 1.0818 - val_acc: 0.9231\n",
      "Epoch 24/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0457 - acc: 0.9972 - val_loss: 1.0539 - val_acc: 1.0000\n",
      "Epoch 25/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0462 - acc: 0.9972 - val_loss: 1.0863 - val_acc: 0.9359\n",
      "Epoch 26/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0443 - acc: 0.9986 - val_loss: 1.0601 - val_acc: 0.9744\n",
      "Epoch 27/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0451 - acc: 0.9972 - val_loss: 1.0518 - val_acc: 0.9872\n",
      "Epoch 28/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0450 - acc: 0.9957 - val_loss: 1.0546 - val_acc: 0.9872\n",
      "Epoch 29/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0432 - acc: 0.9986 - val_loss: 1.0625 - val_acc: 0.9487\n",
      "Epoch 30/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0444 - acc: 0.9986 - val_loss: 1.0727 - val_acc: 0.9487\n",
      "Epoch 31/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0428 - acc: 1.0000 - val_loss: 1.0731 - val_acc: 0.9487\n",
      "Epoch 32/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0428 - acc: 1.0000 - val_loss: 1.0530 - val_acc: 0.9744\n",
      "Epoch 33/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0426 - acc: 0.9986 - val_loss: 1.0699 - val_acc: 0.9615\n",
      "Epoch 34/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0422 - acc: 1.0000 - val_loss: 1.0625 - val_acc: 0.9744\n",
      "Epoch 35/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0421 - acc: 1.0000 - val_loss: 1.0562 - val_acc: 0.9487\n",
      "Epoch 36/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0419 - acc: 1.0000 - val_loss: 1.0566 - val_acc: 0.9615\n",
      "Epoch 37/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0421 - acc: 1.0000 - val_loss: 1.0552 - val_acc: 0.9744\n",
      "Epoch 38/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0413 - acc: 1.0000 - val_loss: 1.0757 - val_acc: 0.9487\n",
      "Epoch 39/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0414 - acc: 1.0000 - val_loss: 1.0539 - val_acc: 0.9744\n",
      "Epoch 40/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0412 - acc: 1.0000 - val_loss: 1.0512 - val_acc: 0.9744\n",
      "Epoch 41/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0412 - acc: 1.0000 - val_loss: 1.0575 - val_acc: 0.9615\n",
      "Epoch 42/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0412 - acc: 1.0000 - val_loss: 1.0552 - val_acc: 0.9744\n",
      "Epoch 43/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0410 - acc: 1.0000 - val_loss: 1.0583 - val_acc: 0.9615\n",
      "Epoch 44/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0410 - acc: 1.0000 - val_loss: 1.0559 - val_acc: 0.9615\n",
      "Epoch 45/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0408 - acc: 1.0000 - val_loss: 1.0547 - val_acc: 0.9744\n",
      "Epoch 46/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0408 - acc: 1.0000 - val_loss: 1.0559 - val_acc: 0.9615\n",
      "Epoch 47/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0409 - acc: 1.0000 - val_loss: 1.0536 - val_acc: 0.9744\n",
      "Epoch 48/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0408 - acc: 1.0000 - val_loss: 1.0588 - val_acc: 0.9615\n",
      "Epoch 49/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0408 - acc: 1.0000 - val_loss: 1.0597 - val_acc: 0.9615\n",
      "Epoch 50/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0551 - val_acc: 0.9744\n",
      "Epoch 51/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0569 - val_acc: 0.9615\n",
      "Epoch 52/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0551 - val_acc: 0.9744\n",
      "Epoch 53/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0577 - val_acc: 0.9615\n",
      "Epoch 54/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0539 - val_acc: 0.9487\n",
      "Epoch 55/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0575 - val_acc: 0.9615\n",
      "Epoch 56/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0545 - val_acc: 0.9615\n",
      "Epoch 57/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0563 - val_acc: 0.9615\n",
      "Epoch 58/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0573 - val_acc: 0.9615\n",
      "Epoch 59/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0549 - val_acc: 0.9615\n",
      "Epoch 60/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0564 - val_acc: 0.9615\n",
      "Epoch 61/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0554 - val_acc: 0.9615\n",
      "Epoch 62/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0548 - val_acc: 0.9615\n",
      "Epoch 63/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0558 - val_acc: 0.9615\n",
      "Epoch 64/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0551 - val_acc: 0.9615\n",
      "Epoch 65/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0552 - val_acc: 0.9615\n",
      "Epoch 66/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0542 - val_acc: 0.9615\n",
      "Epoch 67/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0537 - val_acc: 0.9744\n",
      "Epoch 68/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0535 - val_acc: 0.9615\n",
      "Epoch 69/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0544 - val_acc: 0.9872\n",
      "Epoch 70/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0554 - val_acc: 0.9615\n",
      "Epoch 71/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0547 - val_acc: 0.9615\n",
      "Epoch 72/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0545 - val_acc: 0.9615\n",
      "Epoch 73/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0545 - val_acc: 0.9615\n",
      "Epoch 74/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0548 - val_acc: 0.9615\n",
      "Epoch 75/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0540 - val_acc: 0.9615\n",
      "Epoch 76/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0542 - val_acc: 0.9615\n",
      "Epoch 77/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0540 - val_acc: 0.9615\n",
      "Epoch 78/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0533 - val_acc: 0.9615\n",
      "Epoch 79/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0536 - val_acc: 0.9744\n",
      "Epoch 80/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0536 - val_acc: 0.9744\n",
      "Epoch 81/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0540 - val_acc: 0.9615\n",
      "Epoch 82/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0540 - val_acc: 0.9615\n",
      "Epoch 83/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0542 - val_acc: 0.9615\n",
      "Epoch 84/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0537 - val_acc: 0.9615\n",
      "Epoch 85/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0539 - val_acc: 0.9615\n",
      "Epoch 86/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0533 - val_acc: 0.9615\n",
      "Epoch 87/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0538 - val_acc: 0.9615\n",
      "Epoch 88/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0534 - val_acc: 0.9615\n",
      "Epoch 89/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0539 - val_acc: 0.9615\n",
      "Epoch 90/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0540 - val_acc: 0.9744\n",
      "Epoch 91/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0536 - val_acc: 0.9615\n",
      "Epoch 92/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0534 - val_acc: 0.9744\n",
      "Epoch 93/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0535 - val_acc: 0.9615\n",
      "Epoch 94/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0535 - val_acc: 0.9615\n",
      "Epoch 95/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0535 - val_acc: 0.9615\n",
      "Epoch 96/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0535 - val_acc: 0.9615\n",
      "Epoch 97/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0535 - val_acc: 0.9615\n",
      "Epoch 98/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0539 - val_acc: 0.9744\n",
      "Epoch 99/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0542 - val_acc: 0.9615\n",
      "Epoch 100/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0538 - val_acc: 0.9615\n",
      "78/78 [==============================] - 0s 318us/step\n",
      "Score for fold 6: loss of 1.052814700664618; acc of 97.43589697740018%\n",
      "Train on 702 samples, validate on 78 samples\n",
      "Epoch 1/100\n",
      "702/702 [==============================] - 9s 13ms/step - loss: 2.2510 - acc: 0.3248 - val_loss: 1.7373 - val_acc: 0.5897\n",
      "Epoch 2/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.5981 - acc: 0.6937 - val_loss: 1.5380 - val_acc: 0.6538\n",
      "Epoch 3/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.3416 - acc: 0.8034 - val_loss: 1.3050 - val_acc: 0.8205\n",
      "Epoch 4/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.2554 - acc: 0.8647 - val_loss: 1.2439 - val_acc: 0.8718\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1760 - acc: 0.8875 - val_loss: 1.2744 - val_acc: 0.8077\n",
      "Epoch 6/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1370 - acc: 0.9160 - val_loss: 1.2593 - val_acc: 0.7949\n",
      "Epoch 7/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1355 - acc: 0.9160 - val_loss: 1.1190 - val_acc: 0.9103\n",
      "Epoch 8/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1233 - acc: 0.9188 - val_loss: 1.1108 - val_acc: 0.8590\n",
      "Epoch 9/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0976 - acc: 0.9573 - val_loss: 1.1136 - val_acc: 0.9231\n",
      "Epoch 10/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0838 - acc: 0.9573 - val_loss: 1.1790 - val_acc: 0.9231\n",
      "Epoch 11/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0817 - acc: 0.9701 - val_loss: 1.1176 - val_acc: 0.9359\n",
      "Epoch 12/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0921 - acc: 0.9558 - val_loss: 1.1179 - val_acc: 0.9359\n",
      "Epoch 13/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0650 - acc: 0.9858 - val_loss: 1.1414 - val_acc: 0.8846\n",
      "Epoch 14/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0915 - acc: 0.9729 - val_loss: 1.0894 - val_acc: 0.9615\n",
      "Epoch 15/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0553 - acc: 0.9943 - val_loss: 1.1211 - val_acc: 0.9615\n",
      "Epoch 16/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0864 - acc: 0.9658 - val_loss: 1.1364 - val_acc: 0.8846\n",
      "Epoch 17/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0667 - acc: 0.9772 - val_loss: 1.1203 - val_acc: 0.9103\n",
      "Epoch 18/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0572 - acc: 0.9829 - val_loss: 1.1787 - val_acc: 0.8846\n",
      "Epoch 19/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0715 - acc: 0.9701 - val_loss: 1.0902 - val_acc: 0.9744\n",
      "Epoch 20/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0590 - acc: 0.9815 - val_loss: 1.0980 - val_acc: 0.9615\n",
      "Epoch 21/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0506 - acc: 0.9943 - val_loss: 1.0797 - val_acc: 0.9615\n",
      "Epoch 22/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0615 - acc: 0.9801 - val_loss: 1.0947 - val_acc: 0.9744\n",
      "Epoch 23/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0495 - acc: 0.9900 - val_loss: 1.1126 - val_acc: 0.9487\n",
      "Epoch 24/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0491 - acc: 0.9929 - val_loss: 1.0965 - val_acc: 0.9744\n",
      "Epoch 25/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0598 - acc: 0.9843 - val_loss: 1.0679 - val_acc: 0.9744\n",
      "Epoch 26/100\n",
      "702/702 [==============================] - 3s 5ms/step - loss: 1.0490 - acc: 0.9943 - val_loss: 1.0669 - val_acc: 0.9744\n",
      "Epoch 27/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0446 - acc: 0.9972 - val_loss: 1.0786 - val_acc: 0.9615\n",
      "Epoch 28/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0473 - acc: 0.9957 - val_loss: 1.0742 - val_acc: 0.9744\n",
      "Epoch 29/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0446 - acc: 0.9957 - val_loss: 1.0781 - val_acc: 0.9744\n",
      "Epoch 30/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0442 - acc: 0.9986 - val_loss: 1.0830 - val_acc: 0.9744\n",
      "Epoch 31/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0443 - acc: 0.9986 - val_loss: 1.0741 - val_acc: 0.9744\n",
      "Epoch 32/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0434 - acc: 1.0000 - val_loss: 1.0924 - val_acc: 0.9487\n",
      "Epoch 33/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0436 - acc: 0.9972 - val_loss: 1.0860 - val_acc: 0.9744\n",
      "Epoch 34/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0432 - acc: 0.9986 - val_loss: 1.0857 - val_acc: 0.9744\n",
      "Epoch 35/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0418 - acc: 1.0000 - val_loss: 1.0971 - val_acc: 0.9744\n",
      "Epoch 36/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0421 - acc: 1.0000 - val_loss: 1.0872 - val_acc: 0.9615\n",
      "Epoch 37/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0444 - acc: 0.9972 - val_loss: 1.0732 - val_acc: 0.9744\n",
      "Epoch 38/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0415 - acc: 1.0000 - val_loss: 1.0760 - val_acc: 0.9744\n",
      "Epoch 39/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0412 - acc: 1.0000 - val_loss: 1.0748 - val_acc: 0.9744\n",
      "Epoch 40/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0410 - acc: 1.0000 - val_loss: 1.0754 - val_acc: 0.9744\n",
      "Epoch 41/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0414 - acc: 1.0000 - val_loss: 1.0843 - val_acc: 0.9744\n",
      "Epoch 42/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0410 - acc: 1.0000 - val_loss: 1.0741 - val_acc: 0.9744\n",
      "Epoch 43/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0409 - acc: 1.0000 - val_loss: 1.0763 - val_acc: 0.9615\n",
      "Epoch 44/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0411 - acc: 1.0000 - val_loss: 1.0767 - val_acc: 0.9744\n",
      "Epoch 45/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0408 - acc: 1.0000 - val_loss: 1.0740 - val_acc: 0.9744\n",
      "Epoch 46/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0407 - acc: 1.0000 - val_loss: 1.0795 - val_acc: 0.9744\n",
      "Epoch 47/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0407 - acc: 1.0000 - val_loss: 1.0701 - val_acc: 0.9744\n",
      "Epoch 48/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0863 - val_acc: 0.9744\n",
      "Epoch 49/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0407 - acc: 1.0000 - val_loss: 1.0736 - val_acc: 0.9744\n",
      "Epoch 50/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0738 - val_acc: 0.9744\n",
      "Epoch 51/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0723 - val_acc: 0.9744\n",
      "Epoch 52/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0701 - val_acc: 0.9744\n",
      "Epoch 53/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0786 - val_acc: 0.9615\n",
      "Epoch 54/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0774 - val_acc: 0.9744\n",
      "Epoch 55/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0752 - val_acc: 0.9744\n",
      "Epoch 56/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0756 - val_acc: 0.9744\n",
      "Epoch 57/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0746 - val_acc: 0.9744\n",
      "Epoch 58/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0772 - val_acc: 0.9744\n",
      "Epoch 59/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0747 - val_acc: 0.9744\n",
      "Epoch 60/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0779 - val_acc: 0.9744\n",
      "Epoch 61/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0696 - val_acc: 0.9744\n",
      "Epoch 62/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0770 - val_acc: 0.9744\n",
      "Epoch 63/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0775 - val_acc: 0.9744\n",
      "Epoch 64/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0759 - val_acc: 0.9744\n",
      "Epoch 65/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0759 - val_acc: 0.9744\n",
      "Epoch 66/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0748 - val_acc: 0.9744\n",
      "Epoch 67/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0761 - val_acc: 0.9744\n",
      "Epoch 68/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0765 - val_acc: 0.9744\n",
      "Epoch 69/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0767 - val_acc: 0.9744\n",
      "Epoch 70/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0763 - val_acc: 0.9744\n",
      "Epoch 71/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0777 - val_acc: 0.9744\n",
      "Epoch 72/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0777 - val_acc: 0.9744\n",
      "Epoch 73/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0804 - val_acc: 0.9744\n",
      "Epoch 74/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0771 - val_acc: 0.9744\n",
      "Epoch 75/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0736 - val_acc: 0.9744\n",
      "Epoch 76/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0741 - val_acc: 0.9744\n",
      "Epoch 77/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0768 - val_acc: 0.9744\n",
      "Epoch 78/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0769 - val_acc: 0.9744\n",
      "Epoch 79/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0755 - val_acc: 0.9744\n",
      "Epoch 80/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0782 - val_acc: 0.9744\n",
      "Epoch 81/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0766 - val_acc: 0.9744\n",
      "Epoch 82/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0752 - val_acc: 0.9744\n",
      "Epoch 83/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0758 - val_acc: 0.9744\n",
      "Epoch 84/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0777 - val_acc: 0.9744\n",
      "Epoch 85/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0769 - val_acc: 0.9744\n",
      "Epoch 86/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0760 - val_acc: 0.9744\n",
      "Epoch 87/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0754 - val_acc: 0.9744\n",
      "Epoch 88/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0768 - val_acc: 0.9744\n",
      "Epoch 89/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0767 - val_acc: 0.9744\n",
      "Epoch 90/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0774 - val_acc: 0.9744\n",
      "Epoch 91/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0778 - val_acc: 0.9744\n",
      "Epoch 92/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0770 - val_acc: 0.9744\n",
      "Epoch 93/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0784 - val_acc: 0.9744\n",
      "Epoch 94/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0773 - val_acc: 0.9744\n",
      "Epoch 95/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0763 - val_acc: 0.9744\n",
      "Epoch 96/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0760 - val_acc: 0.9744\n",
      "Epoch 97/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0767 - val_acc: 0.9744\n",
      "Epoch 98/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0759 - val_acc: 0.9744\n",
      "Epoch 99/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0776 - val_acc: 0.9744\n",
      "Epoch 100/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0773 - val_acc: 0.9744\n",
      "78/78 [==============================] - 0s 318us/step\n",
      "Score for fold 7: loss of 1.0815558708631074; acc of 97.43589697740018%\n",
      "Train on 702 samples, validate on 78 samples\n",
      "Epoch 1/100\n",
      "702/702 [==============================] - 9s 13ms/step - loss: 2.2424 - acc: 0.3134 - val_loss: 2.0007 - val_acc: 0.5769\n",
      "Epoch 2/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.6919 - acc: 0.6652 - val_loss: 1.5626 - val_acc: 0.7308\n",
      "Epoch 3/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.4445 - acc: 0.7749 - val_loss: 1.4441 - val_acc: 0.7692\n",
      "Epoch 4/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.2855 - acc: 0.8362 - val_loss: 1.5748 - val_acc: 0.7692\n",
      "Epoch 5/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.2043 - acc: 0.8746 - val_loss: 1.3439 - val_acc: 0.7692\n",
      "Epoch 6/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1406 - acc: 0.9245 - val_loss: 1.2253 - val_acc: 0.7949\n",
      "Epoch 7/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1575 - acc: 0.9188 - val_loss: 1.2509 - val_acc: 0.7564\n",
      "Epoch 8/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0968 - acc: 0.9516 - val_loss: 1.1546 - val_acc: 0.8846\n",
      "Epoch 9/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0943 - acc: 0.9459 - val_loss: 1.1151 - val_acc: 0.9231\n",
      "Epoch 10/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1374 - acc: 0.9245 - val_loss: 1.0894 - val_acc: 0.9487\n",
      "Epoch 11/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0709 - acc: 0.9829 - val_loss: 1.1073 - val_acc: 0.9103\n",
      "Epoch 12/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0763 - acc: 0.9744 - val_loss: 1.1335 - val_acc: 0.9103\n",
      "Epoch 13/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0717 - acc: 0.9772 - val_loss: 1.0885 - val_acc: 0.9615\n",
      "Epoch 14/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0605 - acc: 0.9786 - val_loss: 1.1062 - val_acc: 0.9103\n",
      "Epoch 15/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0934 - acc: 0.9501 - val_loss: 1.0951 - val_acc: 0.9487\n",
      "Epoch 16/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0551 - acc: 0.9900 - val_loss: 1.0866 - val_acc: 0.9744\n",
      "Epoch 17/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0739 - acc: 0.9744 - val_loss: 1.0829 - val_acc: 0.9744\n",
      "Epoch 18/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0685 - acc: 0.9886 - val_loss: 1.0800 - val_acc: 0.9615\n",
      "Epoch 19/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0531 - acc: 0.9929 - val_loss: 1.0897 - val_acc: 0.9359\n",
      "Epoch 20/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0591 - acc: 0.9758 - val_loss: 1.0828 - val_acc: 0.9487\n",
      "Epoch 21/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0495 - acc: 0.9972 - val_loss: 1.0951 - val_acc: 0.9615\n",
      "Epoch 22/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0690 - acc: 0.9843 - val_loss: 1.0740 - val_acc: 0.9615\n",
      "Epoch 23/100\n",
      "702/702 [==============================] - 3s 5ms/step - loss: 1.0476 - acc: 0.9957 - val_loss: 1.0935 - val_acc: 0.9615\n",
      "Epoch 24/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0564 - acc: 0.9886 - val_loss: 1.0902 - val_acc: 0.9615\n",
      "Epoch 25/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0482 - acc: 0.9943 - val_loss: 1.0724 - val_acc: 0.9615\n",
      "Epoch 26/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0486 - acc: 0.9943 - val_loss: 1.0716 - val_acc: 0.9615\n",
      "Epoch 27/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0460 - acc: 0.9972 - val_loss: 1.0861 - val_acc: 0.9487\n",
      "Epoch 28/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0947 - acc: 0.9829 - val_loss: 1.0744 - val_acc: 0.9744\n",
      "Epoch 29/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0437 - acc: 0.9986 - val_loss: 1.0683 - val_acc: 0.9744\n",
      "Epoch 30/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0436 - acc: 0.9972 - val_loss: 1.0648 - val_acc: 0.9744\n",
      "Epoch 31/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0486 - acc: 0.9943 - val_loss: 1.0678 - val_acc: 0.9744\n",
      "Epoch 32/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0426 - acc: 0.9986 - val_loss: 1.0726 - val_acc: 0.9615\n",
      "Epoch 33/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0425 - acc: 0.9986 - val_loss: 1.0752 - val_acc: 0.9744\n",
      "Epoch 34/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0429 - acc: 0.9986 - val_loss: 1.0804 - val_acc: 0.9615\n",
      "Epoch 35/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0442 - acc: 0.9957 - val_loss: 1.1309 - val_acc: 0.9487\n",
      "Epoch 36/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0439 - acc: 0.9972 - val_loss: 1.0644 - val_acc: 0.9744\n",
      "Epoch 37/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0413 - acc: 1.0000 - val_loss: 1.0679 - val_acc: 0.9744\n",
      "Epoch 38/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0417 - acc: 1.0000 - val_loss: 1.0679 - val_acc: 0.9744\n",
      "Epoch 39/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0433 - acc: 0.9957 - val_loss: 1.0699 - val_acc: 0.9744\n",
      "Epoch 40/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0414 - acc: 1.0000 - val_loss: 1.0691 - val_acc: 0.9615\n",
      "Epoch 41/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0415 - acc: 1.0000 - val_loss: 1.0628 - val_acc: 0.9744\n",
      "Epoch 42/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0409 - acc: 1.0000 - val_loss: 1.0664 - val_acc: 0.9744\n",
      "Epoch 43/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0409 - acc: 1.0000 - val_loss: 1.0661 - val_acc: 0.9744\n",
      "Epoch 44/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0410 - acc: 1.0000 - val_loss: 1.0667 - val_acc: 0.9744\n",
      "Epoch 45/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0412 - acc: 1.0000 - val_loss: 1.0638 - val_acc: 0.9744\n",
      "Epoch 46/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0408 - acc: 1.0000 - val_loss: 1.0647 - val_acc: 0.9744\n",
      "Epoch 47/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0408 - acc: 1.0000 - val_loss: 1.0682 - val_acc: 0.9615\n",
      "Epoch 48/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0654 - val_acc: 0.9744\n",
      "Epoch 49/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0407 - acc: 1.0000 - val_loss: 1.0650 - val_acc: 0.9744\n",
      "Epoch 50/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0407 - acc: 1.0000 - val_loss: 1.0661 - val_acc: 0.9744\n",
      "Epoch 51/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0407 - acc: 0.9986 - val_loss: 1.0659 - val_acc: 0.9744\n",
      "Epoch 52/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0674 - val_acc: 0.9615\n",
      "Epoch 53/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0407 - acc: 1.0000 - val_loss: 1.0664 - val_acc: 0.9744\n",
      "Epoch 54/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0682 - val_acc: 0.9615\n",
      "Epoch 55/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0652 - val_acc: 0.9744\n",
      "Epoch 56/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0660 - val_acc: 0.9744\n",
      "Epoch 57/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0669 - val_acc: 0.9615\n",
      "Epoch 58/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0654 - val_acc: 0.9744\n",
      "Epoch 59/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0671 - val_acc: 0.9744\n",
      "Epoch 60/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0555 - val_acc: 0.9872\n",
      "Epoch 61/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0642 - val_acc: 0.9744\n",
      "Epoch 62/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0623 - val_acc: 0.9744\n",
      "Epoch 63/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0648 - val_acc: 0.9744\n",
      "Epoch 64/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0646 - val_acc: 0.9744\n",
      "Epoch 65/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0632 - val_acc: 0.9744\n",
      "Epoch 66/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0604 - val_acc: 0.9744\n",
      "Epoch 67/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0640 - val_acc: 0.9744\n",
      "Epoch 68/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0640 - val_acc: 0.9744\n",
      "Epoch 69/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0642 - val_acc: 0.9744\n",
      "Epoch 70/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0632 - val_acc: 0.9744\n",
      "Epoch 71/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0641 - val_acc: 0.9744\n",
      "Epoch 72/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0644 - val_acc: 0.9744\n",
      "Epoch 73/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0638 - val_acc: 0.9744\n",
      "Epoch 74/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0632 - val_acc: 0.9744\n",
      "Epoch 75/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0643 - val_acc: 0.9744\n",
      "Epoch 76/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0645 - val_acc: 0.9744\n",
      "Epoch 77/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0641 - val_acc: 0.9744\n",
      "Epoch 78/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0649 - val_acc: 0.9744\n",
      "Epoch 79/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0637 - val_acc: 0.9744\n",
      "Epoch 80/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0645 - val_acc: 0.9744\n",
      "Epoch 81/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0632 - val_acc: 0.9744\n",
      "Epoch 82/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0638 - val_acc: 0.9744\n",
      "Epoch 83/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0636 - val_acc: 0.9744\n",
      "Epoch 84/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0634 - val_acc: 0.9744\n",
      "Epoch 85/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0637 - val_acc: 0.9744\n",
      "Epoch 86/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0635 - val_acc: 0.9744\n",
      "Epoch 87/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0641 - val_acc: 0.9744\n",
      "Epoch 88/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0637 - val_acc: 0.9744\n",
      "Epoch 89/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0639 - val_acc: 0.9744\n",
      "Epoch 90/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0639 - val_acc: 0.9744\n",
      "Epoch 91/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0638 - val_acc: 0.9744\n",
      "Epoch 92/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0639 - val_acc: 0.9744\n",
      "Epoch 93/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0639 - val_acc: 0.9744\n",
      "Epoch 94/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0633 - val_acc: 0.9744\n",
      "Epoch 95/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0640 - val_acc: 0.9744\n",
      "Epoch 96/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0636 - val_acc: 0.9744\n",
      "Epoch 97/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0639 - val_acc: 0.9744\n",
      "Epoch 98/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0636 - val_acc: 0.9744\n",
      "Epoch 99/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0635 - val_acc: 0.9744\n",
      "Epoch 100/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0634 - val_acc: 0.9744\n",
      "78/78 [==============================] - 0s 301us/step\n",
      "Score for fold 8: loss of 1.063201448856256; acc of 97.43589743589743%\n",
      "Train on 702 samples, validate on 78 samples\n",
      "Epoch 1/100\n",
      "702/702 [==============================] - 10s 14ms/step - loss: 2.2368 - acc: 0.3077 - val_loss: 1.9768 - val_acc: 0.4872\n",
      "Epoch 2/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.6442 - acc: 0.6581 - val_loss: 1.4502 - val_acc: 0.7179\n",
      "Epoch 3/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.3588 - acc: 0.7963 - val_loss: 1.3806 - val_acc: 0.7821\n",
      "Epoch 4/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.2562 - acc: 0.8661 - val_loss: 1.1577 - val_acc: 0.9359\n",
      "Epoch 5/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1859 - acc: 0.9003 - val_loss: 1.2486 - val_acc: 0.7436\n",
      "Epoch 6/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1363 - acc: 0.9416 - val_loss: 1.1229 - val_acc: 0.8462\n",
      "Epoch 7/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1274 - acc: 0.9259 - val_loss: 1.2347 - val_acc: 0.8846\n",
      "Epoch 8/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1080 - acc: 0.9530 - val_loss: 1.0991 - val_acc: 0.9231\n",
      "Epoch 9/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0821 - acc: 0.9558 - val_loss: 1.0911 - val_acc: 0.9359\n",
      "Epoch 10/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0888 - acc: 0.9544 - val_loss: 1.0930 - val_acc: 0.9231\n",
      "Epoch 11/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0779 - acc: 0.9715 - val_loss: 1.0822 - val_acc: 0.9744\n",
      "Epoch 12/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0681 - acc: 0.9729 - val_loss: 1.3952 - val_acc: 0.8333\n",
      "Epoch 13/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0913 - acc: 0.9687 - val_loss: 1.0968 - val_acc: 0.9487\n",
      "Epoch 14/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0677 - acc: 0.9744 - val_loss: 1.1452 - val_acc: 0.9359\n",
      "Epoch 15/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0613 - acc: 0.9915 - val_loss: 1.0794 - val_acc: 0.9615\n",
      "Epoch 16/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0581 - acc: 0.9829 - val_loss: 1.1870 - val_acc: 0.9103\n",
      "Epoch 17/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0758 - acc: 0.9758 - val_loss: 1.0792 - val_acc: 0.9744\n",
      "Epoch 18/100\n",
      "702/702 [==============================] - 3s 5ms/step - loss: 1.0589 - acc: 0.9858 - val_loss: 1.0859 - val_acc: 0.9744\n",
      "Epoch 19/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0500 - acc: 0.9957 - val_loss: 1.0719 - val_acc: 0.9615\n",
      "Epoch 20/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0451 - acc: 0.9972 - val_loss: 1.0743 - val_acc: 0.9615\n",
      "Epoch 21/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0508 - acc: 0.9915 - val_loss: 1.0772 - val_acc: 0.9615\n",
      "Epoch 22/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0480 - acc: 0.9900 - val_loss: 1.0672 - val_acc: 0.9615\n",
      "Epoch 23/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0464 - acc: 0.9972 - val_loss: 1.0687 - val_acc: 0.9744\n",
      "Epoch 24/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0455 - acc: 0.9986 - val_loss: 1.0730 - val_acc: 0.9744\n",
      "Epoch 25/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0436 - acc: 1.0000 - val_loss: 1.0698 - val_acc: 0.9744\n",
      "Epoch 26/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0424 - acc: 1.0000 - val_loss: 1.0701 - val_acc: 0.9615\n",
      "Epoch 27/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0433 - acc: 1.0000 - val_loss: 1.0660 - val_acc: 0.9744\n",
      "Epoch 28/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0435 - acc: 0.9986 - val_loss: 1.0685 - val_acc: 0.9744\n",
      "Epoch 29/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0432 - acc: 0.9972 - val_loss: 1.0704 - val_acc: 0.9487\n",
      "Epoch 30/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0422 - acc: 1.0000 - val_loss: 1.0684 - val_acc: 0.9744\n",
      "Epoch 31/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0418 - acc: 0.9986 - val_loss: 1.0675 - val_acc: 0.9744\n",
      "Epoch 32/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0416 - acc: 1.0000 - val_loss: 1.0688 - val_acc: 0.9744\n",
      "Epoch 33/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0420 - acc: 0.9986 - val_loss: 1.0660 - val_acc: 0.9744\n",
      "Epoch 34/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0412 - acc: 1.0000 - val_loss: 1.0680 - val_acc: 0.9744\n",
      "Epoch 35/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0412 - acc: 1.0000 - val_loss: 1.0691 - val_acc: 0.9744\n",
      "Epoch 36/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0419 - acc: 0.9972 - val_loss: 1.0663 - val_acc: 0.9744\n",
      "Epoch 37/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0410 - acc: 1.0000 - val_loss: 1.0671 - val_acc: 0.9744\n",
      "Epoch 38/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0408 - acc: 1.0000 - val_loss: 1.0676 - val_acc: 0.9744\n",
      "Epoch 39/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0408 - acc: 1.0000 - val_loss: 1.0662 - val_acc: 0.9744\n",
      "Epoch 40/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0408 - acc: 1.0000 - val_loss: 1.0676 - val_acc: 0.9744\n",
      "Epoch 41/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0409 - acc: 1.0000 - val_loss: 1.0660 - val_acc: 0.9744\n",
      "Epoch 42/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0415 - acc: 0.9986 - val_loss: 1.0673 - val_acc: 0.9744\n",
      "Epoch 43/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0666 - val_acc: 0.9744\n",
      "Epoch 44/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0672 - val_acc: 0.9744\n",
      "Epoch 45/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0408 - acc: 1.0000 - val_loss: 1.0663 - val_acc: 0.9744\n",
      "Epoch 46/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0663 - val_acc: 0.9744\n",
      "Epoch 47/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0663 - val_acc: 0.9744\n",
      "Epoch 48/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0670 - val_acc: 0.9744\n",
      "Epoch 49/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0664 - val_acc: 0.9744\n",
      "Epoch 50/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0665 - val_acc: 0.9744\n",
      "Epoch 51/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0664 - val_acc: 0.9744\n",
      "Epoch 52/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0661 - val_acc: 0.9744\n",
      "Epoch 53/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0664 - val_acc: 0.9744\n",
      "Epoch 54/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0661 - val_acc: 0.9744\n",
      "Epoch 55/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0665 - val_acc: 0.9744\n",
      "Epoch 56/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0666 - val_acc: 0.9744\n",
      "Epoch 57/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0664 - val_acc: 0.9744\n",
      "Epoch 58/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0666 - val_acc: 0.9744\n",
      "Epoch 59/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0670 - val_acc: 0.9744\n",
      "Epoch 60/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0671 - val_acc: 0.9744\n",
      "Epoch 61/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0663 - val_acc: 0.9744\n",
      "Epoch 62/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0669 - val_acc: 0.9744\n",
      "Epoch 63/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0673 - val_acc: 0.9744\n",
      "Epoch 64/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0672 - val_acc: 0.9744\n",
      "Epoch 65/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0674 - val_acc: 0.9744\n",
      "Epoch 66/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0668 - val_acc: 0.9744\n",
      "Epoch 67/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0669 - val_acc: 0.9744\n",
      "Epoch 68/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0670 - val_acc: 0.9744\n",
      "Epoch 69/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0667 - val_acc: 0.9744\n",
      "Epoch 70/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0668 - val_acc: 0.9744\n",
      "Epoch 71/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0667 - val_acc: 0.9744\n",
      "Epoch 72/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0670 - val_acc: 0.9744\n",
      "Epoch 73/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0668 - val_acc: 0.9744\n",
      "Epoch 74/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0668 - val_acc: 0.9744\n",
      "Epoch 75/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0668 - val_acc: 0.9744\n",
      "Epoch 76/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0668 - val_acc: 0.9744\n",
      "Epoch 77/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0668 - val_acc: 0.9744\n",
      "Epoch 78/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0669 - val_acc: 0.9744\n",
      "Epoch 79/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0667 - val_acc: 0.9744\n",
      "Epoch 80/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0666 - val_acc: 0.9744\n",
      "Epoch 81/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0666 - val_acc: 0.9744\n",
      "Epoch 82/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0667 - val_acc: 0.9744\n",
      "Epoch 83/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0668 - val_acc: 0.9744\n",
      "Epoch 84/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0666 - val_acc: 0.9744\n",
      "Epoch 85/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0667 - val_acc: 0.9744\n",
      "Epoch 86/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0664 - val_acc: 0.9744\n",
      "Epoch 87/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0666 - val_acc: 0.9744\n",
      "Epoch 88/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0668 - val_acc: 0.9744\n",
      "Epoch 89/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0666 - val_acc: 0.9744\n",
      "Epoch 90/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0666 - val_acc: 0.9744\n",
      "Epoch 91/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0666 - val_acc: 0.9744\n",
      "Epoch 92/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0664 - val_acc: 0.9744\n",
      "Epoch 93/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0665 - val_acc: 0.9744\n",
      "Epoch 94/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0666 - val_acc: 0.9744\n",
      "Epoch 95/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0667 - val_acc: 0.9744\n",
      "Epoch 96/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0666 - val_acc: 0.9744\n",
      "Epoch 97/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0667 - val_acc: 0.9744\n",
      "Epoch 98/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0668 - val_acc: 0.9744\n",
      "Epoch 99/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0670 - val_acc: 0.9744\n",
      "Epoch 100/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0666 - val_acc: 0.9744\n",
      "78/78 [==============================] - 0s 325us/step\n",
      "Score for fold 9: loss of 1.0664793711442213; acc of 97.43589697740018%\n",
      "Train on 702 samples, validate on 78 samples\n",
      "Epoch 1/100\n",
      "702/702 [==============================] - 10s 14ms/step - loss: 2.2509 - acc: 0.3433 - val_loss: 1.8666 - val_acc: 0.5897\n",
      "Epoch 2/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.6297 - acc: 0.7037 - val_loss: 1.4781 - val_acc: 0.7179\n",
      "Epoch 3/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.3585 - acc: 0.8020 - val_loss: 1.5935 - val_acc: 0.7051\n",
      "Epoch 4/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.2613 - acc: 0.8704 - val_loss: 1.1838 - val_acc: 0.9231\n",
      "Epoch 5/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1807 - acc: 0.8818 - val_loss: 1.3059 - val_acc: 0.7564\n",
      "Epoch 6/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1479 - acc: 0.9202 - val_loss: 1.2038 - val_acc: 0.8462\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1345 - acc: 0.9288 - val_loss: 1.1242 - val_acc: 0.9487\n",
      "Epoch 8/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1347 - acc: 0.9302 - val_loss: 1.1333 - val_acc: 0.8974\n",
      "Epoch 9/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0746 - acc: 0.9829 - val_loss: 1.1504 - val_acc: 0.8462\n",
      "Epoch 10/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0899 - acc: 0.9558 - val_loss: 1.1090 - val_acc: 0.9487\n",
      "Epoch 11/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0703 - acc: 0.9744 - val_loss: 1.0843 - val_acc: 0.9487\n",
      "Epoch 12/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0962 - acc: 0.9587 - val_loss: 1.0759 - val_acc: 0.9744\n",
      "Epoch 13/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0693 - acc: 0.9772 - val_loss: 1.0713 - val_acc: 0.9872\n",
      "Epoch 14/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0776 - acc: 0.9701 - val_loss: 1.0900 - val_acc: 0.9744\n",
      "Epoch 15/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0591 - acc: 0.9843 - val_loss: 1.0881 - val_acc: 0.9487\n",
      "Epoch 16/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0608 - acc: 0.9744 - val_loss: 1.0719 - val_acc: 0.9744\n",
      "Epoch 17/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0550 - acc: 0.9915 - val_loss: 1.0787 - val_acc: 0.9487\n",
      "Epoch 18/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0636 - acc: 0.9772 - val_loss: 1.0881 - val_acc: 0.9615\n",
      "Epoch 19/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0574 - acc: 0.9886 - val_loss: 1.0688 - val_acc: 0.9615\n",
      "Epoch 20/100\n",
      "702/702 [==============================] - 3s 5ms/step - loss: 1.0533 - acc: 0.9900 - val_loss: 1.5301 - val_acc: 0.8205\n",
      "Epoch 21/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0674 - acc: 0.9843 - val_loss: 1.0745 - val_acc: 0.9744\n",
      "Epoch 22/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0485 - acc: 0.9957 - val_loss: 1.0608 - val_acc: 1.0000\n",
      "Epoch 23/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0468 - acc: 0.9986 - val_loss: 1.0895 - val_acc: 0.9872\n",
      "Epoch 24/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0502 - acc: 0.9929 - val_loss: 1.0552 - val_acc: 1.0000\n",
      "Epoch 25/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0486 - acc: 0.9929 - val_loss: 1.0730 - val_acc: 0.9872\n",
      "Epoch 26/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0477 - acc: 0.9929 - val_loss: 1.0700 - val_acc: 0.9872\n",
      "Epoch 27/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0615 - acc: 0.9915 - val_loss: 1.0616 - val_acc: 1.0000\n",
      "Epoch 28/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0452 - acc: 0.9986 - val_loss: 1.0664 - val_acc: 0.9872\n",
      "Epoch 29/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0457 - acc: 0.9943 - val_loss: 1.0586 - val_acc: 0.9872\n",
      "Epoch 30/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0436 - acc: 0.9972 - val_loss: 1.0608 - val_acc: 0.9872\n",
      "Epoch 31/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0431 - acc: 0.9986 - val_loss: 1.0553 - val_acc: 1.0000\n",
      "Epoch 32/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0428 - acc: 1.0000 - val_loss: 1.0538 - val_acc: 1.0000\n",
      "Epoch 33/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0437 - acc: 0.9957 - val_loss: 1.0534 - val_acc: 1.0000\n",
      "Epoch 34/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0428 - acc: 1.0000 - val_loss: 1.0618 - val_acc: 1.0000\n",
      "Epoch 35/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0419 - acc: 1.0000 - val_loss: 1.0710 - val_acc: 0.9872\n",
      "Epoch 36/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0417 - acc: 1.0000 - val_loss: 1.0580 - val_acc: 1.0000\n",
      "Epoch 37/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0420 - acc: 1.0000 - val_loss: 1.0899 - val_acc: 0.9744\n",
      "Epoch 38/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0414 - acc: 1.0000 - val_loss: 1.0683 - val_acc: 0.9744\n",
      "Epoch 39/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0412 - acc: 1.0000 - val_loss: 1.0694 - val_acc: 0.9872\n",
      "Epoch 40/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0413 - acc: 1.0000 - val_loss: 1.0697 - val_acc: 0.9872\n",
      "Epoch 41/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0412 - acc: 1.0000 - val_loss: 1.0773 - val_acc: 0.9744\n",
      "Epoch 42/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0414 - acc: 1.0000 - val_loss: 1.0669 - val_acc: 0.9872\n",
      "Epoch 43/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0407 - acc: 1.0000 - val_loss: 1.0713 - val_acc: 0.9872\n",
      "Epoch 44/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0408 - acc: 1.0000 - val_loss: 1.0688 - val_acc: 0.9872\n",
      "Epoch 45/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0408 - acc: 1.0000 - val_loss: 1.0830 - val_acc: 0.9872\n",
      "Epoch 46/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0410 - acc: 1.0000 - val_loss: 1.0675 - val_acc: 0.9872\n",
      "Epoch 47/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0407 - acc: 1.0000 - val_loss: 1.0678 - val_acc: 0.9872\n",
      "Epoch 48/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0407 - acc: 1.0000 - val_loss: 1.0458 - val_acc: 1.0000\n",
      "Epoch 49/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0590 - val_acc: 0.9872\n",
      "Epoch 50/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0408 - acc: 1.0000 - val_loss: 1.0512 - val_acc: 1.0000\n",
      "Epoch 51/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0500 - val_acc: 0.9872\n",
      "Epoch 52/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0451 - val_acc: 1.0000\n",
      "Epoch 53/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0471 - val_acc: 1.0000\n",
      "Epoch 54/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0460 - val_acc: 1.0000\n",
      "Epoch 55/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0456 - val_acc: 1.0000\n",
      "Epoch 56/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0486 - val_acc: 1.0000\n",
      "Epoch 57/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0456 - val_acc: 1.0000\n",
      "Epoch 58/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0565 - val_acc: 1.0000\n",
      "Epoch 59/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0472 - val_acc: 1.0000\n",
      "Epoch 60/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0504 - val_acc: 1.0000\n",
      "Epoch 61/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0526 - val_acc: 1.0000\n",
      "Epoch 62/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0541 - val_acc: 1.0000\n",
      "Epoch 63/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0482 - val_acc: 1.0000\n",
      "Epoch 64/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0471 - val_acc: 1.0000\n",
      "Epoch 65/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0459 - val_acc: 1.0000\n",
      "Epoch 66/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0475 - val_acc: 1.0000\n",
      "Epoch 67/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0505 - val_acc: 1.0000\n",
      "Epoch 68/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0465 - val_acc: 1.0000\n",
      "Epoch 69/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0509 - val_acc: 0.9872\n",
      "Epoch 70/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0487 - val_acc: 1.0000\n",
      "Epoch 71/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0498 - val_acc: 1.0000\n",
      "Epoch 72/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0489 - val_acc: 1.0000\n",
      "Epoch 73/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0479 - val_acc: 1.0000\n",
      "Epoch 74/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0483 - val_acc: 1.0000\n",
      "Epoch 75/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0484 - val_acc: 1.0000\n",
      "Epoch 76/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0503 - val_acc: 0.9872\n",
      "Epoch 77/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0487 - val_acc: 1.0000\n",
      "Epoch 78/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0489 - val_acc: 1.0000\n",
      "Epoch 79/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0491 - val_acc: 1.0000\n",
      "Epoch 80/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0481 - val_acc: 1.0000\n",
      "Epoch 81/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0497 - val_acc: 1.0000\n",
      "Epoch 82/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0483 - val_acc: 1.0000\n",
      "Epoch 83/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0469 - val_acc: 1.0000\n",
      "Epoch 84/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0474 - val_acc: 1.0000\n",
      "Epoch 85/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0493 - val_acc: 1.0000\n",
      "Epoch 86/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0483 - val_acc: 1.0000\n",
      "Epoch 87/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0508 - val_acc: 0.9872\n",
      "Epoch 88/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0466 - val_acc: 1.0000\n",
      "Epoch 89/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0488 - val_acc: 1.0000\n",
      "Epoch 90/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0494 - val_acc: 1.0000\n",
      "Epoch 91/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0486 - val_acc: 1.0000\n",
      "Epoch 92/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0493 - val_acc: 1.0000\n",
      "Epoch 93/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0501 - val_acc: 0.9872\n",
      "Epoch 94/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0496 - val_acc: 1.0000\n",
      "Epoch 95/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0494 - val_acc: 1.0000\n",
      "Epoch 96/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0490 - val_acc: 1.0000\n",
      "Epoch 97/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0494 - val_acc: 1.0000\n",
      "Epoch 98/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0483 - val_acc: 1.0000\n",
      "Epoch 99/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0477 - val_acc: 1.0000\n",
      "Epoch 100/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0477 - val_acc: 1.0000\n",
      "78/78 [==============================] - 0s 320us/step\n",
      "Score for fold 10: loss of 1.0475176588082924; acc of 100.0%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1296x1296 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97.82051259126419\n",
      "Train on 702 samples, validate on 78 samples\n",
      "Epoch 1/100\n",
      "702/702 [==============================] - 10s 14ms/step - loss: 2.2071 - acc: 0.3761 - val_loss: 2.0258 - val_acc: 0.3590\n",
      "Epoch 2/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.6022 - acc: 0.6966 - val_loss: 1.4803 - val_acc: 0.6667\n",
      "Epoch 3/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.3676 - acc: 0.8234 - val_loss: 1.2665 - val_acc: 0.8974\n",
      "Epoch 4/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.2463 - acc: 0.8561 - val_loss: 1.1970 - val_acc: 0.8846\n",
      "Epoch 5/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1933 - acc: 0.8917 - val_loss: 1.1416 - val_acc: 0.9359\n",
      "Epoch 6/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1572 - acc: 0.9003 - val_loss: 1.1826 - val_acc: 0.8846\n",
      "Epoch 7/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0996 - acc: 0.9558 - val_loss: 1.2201 - val_acc: 0.7821\n",
      "Epoch 8/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1178 - acc: 0.9473 - val_loss: 1.1500 - val_acc: 0.9103\n",
      "Epoch 9/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1057 - acc: 0.9558 - val_loss: 1.2038 - val_acc: 0.8205\n",
      "Epoch 10/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0847 - acc: 0.9672 - val_loss: 1.1488 - val_acc: 0.8718\n",
      "Epoch 11/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0797 - acc: 0.9644 - val_loss: 1.0823 - val_acc: 0.9359\n",
      "Epoch 12/100\n",
      "702/702 [==============================] - 4s 5ms/step - loss: 1.0696 - acc: 0.9701 - val_loss: 1.2142 - val_acc: 0.8846\n",
      "Epoch 13/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0803 - acc: 0.9772 - val_loss: 1.1000 - val_acc: 0.8974\n",
      "Epoch 14/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0538 - acc: 0.9929 - val_loss: 1.3215 - val_acc: 0.8846\n",
      "Epoch 15/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0673 - acc: 0.9872 - val_loss: 1.0797 - val_acc: 0.9487\n",
      "Epoch 16/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0515 - acc: 0.9915 - val_loss: 1.1338 - val_acc: 0.8590\n",
      "Epoch 17/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0596 - acc: 0.9801 - val_loss: 1.0782 - val_acc: 0.9231\n",
      "Epoch 18/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0499 - acc: 0.9929 - val_loss: 1.1318 - val_acc: 0.8974\n",
      "Epoch 19/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0549 - acc: 0.9900 - val_loss: 1.1454 - val_acc: 0.8846\n",
      "Epoch 20/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0558 - acc: 0.9872 - val_loss: 1.1355 - val_acc: 0.9103\n",
      "Epoch 21/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0498 - acc: 0.9943 - val_loss: 1.0860 - val_acc: 0.9615\n",
      "Epoch 22/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0491 - acc: 0.9943 - val_loss: 1.0855 - val_acc: 0.9487\n",
      "Epoch 23/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0507 - acc: 0.9886 - val_loss: 1.0793 - val_acc: 0.9487\n",
      "Epoch 24/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0478 - acc: 0.9943 - val_loss: 1.0782 - val_acc: 0.9615\n",
      "Epoch 25/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0501 - acc: 0.9915 - val_loss: 1.0826 - val_acc: 0.9359\n",
      "Epoch 26/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0456 - acc: 0.9957 - val_loss: 1.0717 - val_acc: 0.9615\n",
      "Epoch 27/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0478 - acc: 0.9929 - val_loss: 1.0907 - val_acc: 0.9103\n",
      "Epoch 28/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0445 - acc: 0.9986 - val_loss: 1.0803 - val_acc: 0.9487\n",
      "Epoch 29/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0438 - acc: 0.9986 - val_loss: 1.0641 - val_acc: 0.9487\n",
      "Epoch 30/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0446 - acc: 0.9957 - val_loss: 1.0700 - val_acc: 0.9487\n",
      "Epoch 31/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0432 - acc: 0.9986 - val_loss: 1.0685 - val_acc: 0.9615\n",
      "Epoch 32/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0431 - acc: 0.9986 - val_loss: 1.0747 - val_acc: 0.9615\n",
      "Epoch 33/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0422 - acc: 1.0000 - val_loss: 1.0712 - val_acc: 0.9487\n",
      "Epoch 34/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0441 - acc: 0.9957 - val_loss: 1.0773 - val_acc: 0.9487\n",
      "Epoch 35/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0421 - acc: 0.9986 - val_loss: 1.0690 - val_acc: 0.9615\n",
      "Epoch 36/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0434 - acc: 0.9972 - val_loss: 1.0679 - val_acc: 0.9615\n",
      "Epoch 37/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0416 - acc: 1.0000 - val_loss: 1.0657 - val_acc: 0.9615\n",
      "Epoch 38/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0413 - acc: 1.0000 - val_loss: 1.0704 - val_acc: 0.9487\n",
      "Epoch 39/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0415 - acc: 1.0000 - val_loss: 1.0720 - val_acc: 0.9615\n",
      "Epoch 40/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0414 - acc: 1.0000 - val_loss: 1.0691 - val_acc: 0.9487\n",
      "Epoch 41/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0413 - acc: 1.0000 - val_loss: 1.0699 - val_acc: 0.9487\n",
      "Epoch 42/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0411 - acc: 1.0000 - val_loss: 1.0713 - val_acc: 0.9615\n",
      "Epoch 43/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0408 - acc: 1.0000 - val_loss: 1.0673 - val_acc: 0.9487\n",
      "Epoch 44/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0408 - acc: 1.0000 - val_loss: 1.0665 - val_acc: 0.9615\n",
      "Epoch 45/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0409 - acc: 1.0000 - val_loss: 1.0723 - val_acc: 0.9487\n",
      "Epoch 46/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0410 - acc: 0.9986 - val_loss: 1.0701 - val_acc: 0.9615\n",
      "Epoch 47/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0407 - acc: 1.0000 - val_loss: 1.0649 - val_acc: 0.9615\n",
      "Epoch 48/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0693 - val_acc: 0.9615\n",
      "Epoch 49/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0407 - acc: 1.0000 - val_loss: 1.0704 - val_acc: 0.9615\n",
      "Epoch 50/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0710 - val_acc: 0.9615\n",
      "Epoch 51/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0711 - val_acc: 0.9615\n",
      "Epoch 52/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0687 - val_acc: 0.9615\n",
      "Epoch 53/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0703 - val_acc: 0.9615\n",
      "Epoch 54/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0684 - val_acc: 0.9615\n",
      "Epoch 55/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0679 - val_acc: 0.9615\n",
      "Epoch 56/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0643 - val_acc: 0.9615\n",
      "Epoch 57/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0664 - val_acc: 0.9615\n",
      "Epoch 58/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0665 - val_acc: 0.9615\n",
      "Epoch 59/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0664 - val_acc: 0.9615\n",
      "Epoch 60/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0683 - val_acc: 0.9615\n",
      "Epoch 61/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0714 - val_acc: 0.9487\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 62/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0694 - val_acc: 0.9615\n",
      "Epoch 63/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0703 - val_acc: 0.9615\n",
      "Epoch 64/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0692 - val_acc: 0.9615\n",
      "Epoch 65/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0707 - val_acc: 0.9615\n",
      "Epoch 66/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0694 - val_acc: 0.9615\n",
      "Epoch 67/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0701 - val_acc: 0.9615\n",
      "Epoch 68/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0696 - val_acc: 0.9615\n",
      "Epoch 69/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0714 - val_acc: 0.9615\n",
      "Epoch 70/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0718 - val_acc: 0.9615\n",
      "Epoch 71/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0704 - val_acc: 0.9615\n",
      "Epoch 72/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0708 - val_acc: 0.9615\n",
      "Epoch 73/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0707 - val_acc: 0.9615\n",
      "Epoch 74/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0700 - val_acc: 0.9615\n",
      "Epoch 75/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0709 - val_acc: 0.9615\n",
      "Epoch 76/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0703 - val_acc: 0.9615\n",
      "Epoch 77/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0697 - val_acc: 0.9615\n",
      "Epoch 78/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0697 - val_acc: 0.9615\n",
      "Epoch 79/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0701 - val_acc: 0.9615\n",
      "Epoch 80/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0699 - val_acc: 0.9615\n",
      "Epoch 81/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0707 - val_acc: 0.9615\n",
      "Epoch 82/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0694 - val_acc: 0.9615\n",
      "Epoch 83/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0696 - val_acc: 0.9615\n",
      "Epoch 84/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0693 - val_acc: 0.9615\n",
      "Epoch 85/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0689 - val_acc: 0.9615\n",
      "Epoch 86/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0688 - val_acc: 0.9615\n",
      "Epoch 87/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0694 - val_acc: 0.9615\n",
      "Epoch 88/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0694 - val_acc: 0.9615\n",
      "Epoch 89/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0697 - val_acc: 0.9615\n",
      "Epoch 90/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0691 - val_acc: 0.9615\n",
      "Epoch 91/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0692 - val_acc: 0.9615\n",
      "Epoch 92/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0703 - val_acc: 0.9615\n",
      "Epoch 93/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0694 - val_acc: 0.9615\n",
      "Epoch 94/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0692 - val_acc: 0.9615\n",
      "Epoch 95/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0693 - val_acc: 0.9615\n",
      "Epoch 96/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0693 - val_acc: 0.9615\n",
      "Epoch 97/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0692 - val_acc: 0.9615\n",
      "Epoch 98/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0689 - val_acc: 0.9615\n",
      "Epoch 99/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0686 - val_acc: 0.9615\n",
      "Epoch 100/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0688 - val_acc: 0.9615\n",
      "78/78 [==============================] - 0s 321us/step\n",
      "Score for fold 1: loss of 1.0690729648638995; acc of 96.15384615384616%\n",
      "Train on 702 samples, validate on 78 samples\n",
      "Epoch 1/100\n",
      "702/702 [==============================] - 10s 14ms/step - loss: 2.2103 - acc: 0.3590 - val_loss: 1.9072 - val_acc: 0.5897\n",
      "Epoch 2/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.5519 - acc: 0.7365 - val_loss: 1.5075 - val_acc: 0.7564\n",
      "Epoch 3/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.3344 - acc: 0.8063 - val_loss: 1.3103 - val_acc: 0.8590\n",
      "Epoch 4/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.2345 - acc: 0.8704 - val_loss: 1.3959 - val_acc: 0.6795\n",
      "Epoch 5/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.2053 - acc: 0.8803 - val_loss: 1.1848 - val_acc: 0.9103\n",
      "Epoch 6/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1185 - acc: 0.9359 - val_loss: 1.2124 - val_acc: 0.7436\n",
      "Epoch 7/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1247 - acc: 0.9202 - val_loss: 1.1514 - val_acc: 0.9103\n",
      "Epoch 8/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1107 - acc: 0.9601 - val_loss: 1.2938 - val_acc: 0.7949\n",
      "Epoch 9/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1076 - acc: 0.9444 - val_loss: 1.1508 - val_acc: 0.8846\n",
      "Epoch 10/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0785 - acc: 0.9672 - val_loss: 1.3205 - val_acc: 0.8333\n",
      "Epoch 11/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0864 - acc: 0.9587 - val_loss: 1.0815 - val_acc: 0.9744\n",
      "Epoch 12/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0669 - acc: 0.9758 - val_loss: 1.0857 - val_acc: 0.9744\n",
      "Epoch 13/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0873 - acc: 0.9687 - val_loss: 1.0800 - val_acc: 0.9744\n",
      "Epoch 14/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0555 - acc: 0.9900 - val_loss: 1.0721 - val_acc: 0.9487\n",
      "Epoch 15/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0700 - acc: 0.9843 - val_loss: 1.0687 - val_acc: 0.9615\n",
      "Epoch 16/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0662 - acc: 0.9801 - val_loss: 1.0689 - val_acc: 0.9615\n",
      "Epoch 17/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0527 - acc: 0.9900 - val_loss: 1.0876 - val_acc: 0.9487\n",
      "Epoch 18/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0531 - acc: 0.9943 - val_loss: 1.0520 - val_acc: 1.0000\n",
      "Epoch 19/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0658 - acc: 0.9929 - val_loss: 1.0877 - val_acc: 0.9359\n",
      "Epoch 20/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0564 - acc: 0.9843 - val_loss: 1.0533 - val_acc: 0.9872\n",
      "Epoch 21/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0581 - acc: 0.9858 - val_loss: 1.3027 - val_acc: 0.8333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0551 - acc: 0.9915 - val_loss: 1.0574 - val_acc: 0.9872\n",
      "Epoch 23/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0486 - acc: 0.9943 - val_loss: 1.0510 - val_acc: 0.9744\n",
      "Epoch 24/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0527 - acc: 0.9858 - val_loss: 1.0695 - val_acc: 0.9872\n",
      "Epoch 25/100\n",
      "702/702 [==============================] - 4s 5ms/step - loss: 1.0498 - acc: 0.9900 - val_loss: 1.0540 - val_acc: 0.9872\n",
      "Epoch 26/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0456 - acc: 0.9972 - val_loss: 1.0565 - val_acc: 0.9872\n",
      "Epoch 27/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0496 - acc: 0.9957 - val_loss: 1.0494 - val_acc: 0.9872\n",
      "Epoch 28/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0436 - acc: 0.9986 - val_loss: 1.0549 - val_acc: 0.9615\n",
      "Epoch 29/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0452 - acc: 0.9972 - val_loss: 1.0587 - val_acc: 0.9744\n",
      "Epoch 30/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0441 - acc: 0.9986 - val_loss: 1.0655 - val_acc: 0.9487\n",
      "Epoch 31/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0471 - acc: 0.9929 - val_loss: 1.0678 - val_acc: 0.9744\n",
      "Epoch 32/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0426 - acc: 0.9986 - val_loss: 1.0621 - val_acc: 0.9744\n",
      "Epoch 33/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0429 - acc: 0.9986 - val_loss: 1.0501 - val_acc: 0.9872\n",
      "Epoch 34/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0441 - acc: 0.9943 - val_loss: 1.0527 - val_acc: 0.9872\n",
      "Epoch 35/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0421 - acc: 0.9986 - val_loss: 1.0668 - val_acc: 0.9615\n",
      "Epoch 36/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0437 - acc: 0.9972 - val_loss: 1.0519 - val_acc: 0.9744\n",
      "Epoch 37/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0417 - acc: 1.0000 - val_loss: 1.0609 - val_acc: 0.9872\n",
      "Epoch 38/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0415 - acc: 1.0000 - val_loss: 1.0452 - val_acc: 1.0000\n",
      "Epoch 39/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0414 - acc: 1.0000 - val_loss: 1.0481 - val_acc: 0.9872\n",
      "Epoch 40/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0412 - acc: 1.0000 - val_loss: 1.0451 - val_acc: 1.0000\n",
      "Epoch 41/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0416 - acc: 1.0000 - val_loss: 1.0506 - val_acc: 0.9872\n",
      "Epoch 42/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0412 - acc: 1.0000 - val_loss: 1.0465 - val_acc: 0.9872\n",
      "Epoch 43/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0426 - acc: 0.9972 - val_loss: 1.0454 - val_acc: 0.9872\n",
      "Epoch 44/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0411 - acc: 0.9986 - val_loss: 1.0441 - val_acc: 1.0000\n",
      "Epoch 45/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0408 - acc: 1.0000 - val_loss: 1.0444 - val_acc: 1.0000\n",
      "Epoch 46/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0408 - acc: 1.0000 - val_loss: 1.0452 - val_acc: 1.0000\n",
      "Epoch 47/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0407 - acc: 1.0000 - val_loss: 1.0446 - val_acc: 0.9872\n",
      "Epoch 48/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0408 - acc: 1.0000 - val_loss: 1.0473 - val_acc: 0.9872\n",
      "Epoch 49/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0407 - acc: 1.0000 - val_loss: 1.0466 - val_acc: 0.9744\n",
      "Epoch 50/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0454 - val_acc: 0.9872\n",
      "Epoch 51/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0482 - val_acc: 0.9744\n",
      "Epoch 52/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0458 - val_acc: 0.9872\n",
      "Epoch 53/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0458 - val_acc: 0.9744\n",
      "Epoch 54/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0474 - val_acc: 0.9872\n",
      "Epoch 55/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0465 - val_acc: 0.9872\n",
      "Epoch 56/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0460 - val_acc: 0.9744\n",
      "Epoch 57/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0456 - val_acc: 0.9872\n",
      "Epoch 58/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0464 - val_acc: 0.9744\n",
      "Epoch 59/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0457 - val_acc: 0.9744\n",
      "Epoch 60/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0462 - val_acc: 0.9872\n",
      "Epoch 61/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0454 - val_acc: 0.9872\n",
      "Epoch 62/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0454 - val_acc: 0.9744\n",
      "Epoch 63/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0463 - val_acc: 0.9744\n",
      "Epoch 64/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0473 - val_acc: 0.9872\n",
      "Epoch 65/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0479 - val_acc: 0.9744\n",
      "Epoch 66/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0459 - val_acc: 0.9872\n",
      "Epoch 67/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0456 - val_acc: 0.9744\n",
      "Epoch 68/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0462 - val_acc: 0.9744\n",
      "Epoch 69/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0466 - val_acc: 0.9744\n",
      "Epoch 70/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0468 - val_acc: 0.9744\n",
      "Epoch 71/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0460 - val_acc: 0.9872\n",
      "Epoch 72/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0463 - val_acc: 0.9872\n",
      "Epoch 73/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0478 - val_acc: 0.9744\n",
      "Epoch 74/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0480 - val_acc: 0.9744\n",
      "Epoch 75/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0470 - val_acc: 0.9744\n",
      "Epoch 76/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0472 - val_acc: 0.9744\n",
      "Epoch 77/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0481 - val_acc: 0.9744\n",
      "Epoch 78/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0473 - val_acc: 0.9744\n",
      "Epoch 79/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0478 - val_acc: 0.9744\n",
      "Epoch 80/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0478 - val_acc: 0.9744\n",
      "Epoch 81/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0478 - val_acc: 0.9744\n",
      "Epoch 82/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0460 - val_acc: 0.9872\n",
      "Epoch 83/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0466 - val_acc: 0.9744\n",
      "Epoch 84/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0477 - val_acc: 0.9744\n",
      "Epoch 85/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0472 - val_acc: 0.9872\n",
      "Epoch 86/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0472 - val_acc: 0.9744\n",
      "Epoch 87/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0476 - val_acc: 0.9744\n",
      "Epoch 88/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0466 - val_acc: 0.9872\n",
      "Epoch 89/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0467 - val_acc: 0.9744\n",
      "Epoch 90/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0466 - val_acc: 0.9744\n",
      "Epoch 91/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0465 - val_acc: 0.9744\n",
      "Epoch 92/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0457 - val_acc: 0.9744\n",
      "Epoch 93/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0461 - val_acc: 0.9744\n",
      "Epoch 94/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0457 - val_acc: 0.9744\n",
      "Epoch 95/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0462 - val_acc: 0.9744\n",
      "Epoch 96/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0460 - val_acc: 0.9744\n",
      "Epoch 97/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0459 - val_acc: 0.9744\n",
      "Epoch 98/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0457 - val_acc: 0.9744\n",
      "Epoch 99/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0460 - val_acc: 0.9744\n",
      "Epoch 100/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0459 - val_acc: 0.9744\n",
      "78/78 [==============================] - 0s 315us/step\n",
      "Score for fold 2: loss of 1.045475267446958; acc of 97.43589743589743%\n",
      "Train on 702 samples, validate on 78 samples\n",
      "Epoch 1/100\n",
      "702/702 [==============================] - 10s 15ms/step - loss: 2.2622 - acc: 0.3291 - val_loss: 2.1676 - val_acc: 0.2821\n",
      "Epoch 2/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.6551 - acc: 0.6610 - val_loss: 1.4693 - val_acc: 0.8077\n",
      "Epoch 3/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.4023 - acc: 0.7849 - val_loss: 1.3499 - val_acc: 0.7821\n",
      "Epoch 4/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.2660 - acc: 0.8604 - val_loss: 1.2849 - val_acc: 0.7436\n",
      "Epoch 5/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1782 - acc: 0.9103 - val_loss: 1.5808 - val_acc: 0.7051\n",
      "Epoch 6/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1696 - acc: 0.8932 - val_loss: 1.1462 - val_acc: 0.8974\n",
      "Epoch 7/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1229 - acc: 0.9416 - val_loss: 1.2014 - val_acc: 0.8333\n",
      "Epoch 8/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0928 - acc: 0.9487 - val_loss: 1.2657 - val_acc: 0.8077\n",
      "Epoch 9/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1090 - acc: 0.9316 - val_loss: 1.1138 - val_acc: 0.9231\n",
      "Epoch 10/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0771 - acc: 0.9601 - val_loss: 1.0834 - val_acc: 0.9615\n",
      "Epoch 11/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0905 - acc: 0.9644 - val_loss: 1.0838 - val_acc: 0.9615\n",
      "Epoch 12/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0642 - acc: 0.9886 - val_loss: 1.1439 - val_acc: 0.9231\n",
      "Epoch 13/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0866 - acc: 0.9701 - val_loss: 1.3415 - val_acc: 0.8333\n",
      "Epoch 14/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0936 - acc: 0.9772 - val_loss: 1.0674 - val_acc: 0.9487\n",
      "Epoch 15/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0921 - acc: 0.9729 - val_loss: 1.1546 - val_acc: 0.8718\n",
      "Epoch 16/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0691 - acc: 0.9815 - val_loss: 1.0808 - val_acc: 0.9744\n",
      "Epoch 17/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0505 - acc: 0.9957 - val_loss: 1.0749 - val_acc: 0.9744\n",
      "Epoch 18/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0726 - acc: 0.9758 - val_loss: 1.0732 - val_acc: 0.9359\n",
      "Epoch 19/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0502 - acc: 0.9957 - val_loss: 1.0800 - val_acc: 0.9487\n",
      "Epoch 20/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0699 - acc: 0.9744 - val_loss: 1.0584 - val_acc: 0.9872\n",
      "Epoch 21/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0522 - acc: 0.9929 - val_loss: 1.0619 - val_acc: 0.9872\n",
      "Epoch 22/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0486 - acc: 0.9943 - val_loss: 1.0813 - val_acc: 0.9487\n",
      "Epoch 23/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0579 - acc: 0.9929 - val_loss: 1.0907 - val_acc: 0.9487\n",
      "Epoch 24/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0639 - acc: 0.9843 - val_loss: 1.0980 - val_acc: 0.9615\n",
      "Epoch 25/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0461 - acc: 0.9986 - val_loss: 1.1134 - val_acc: 0.9744\n",
      "Epoch 26/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0515 - acc: 0.9886 - val_loss: 1.0790 - val_acc: 0.9615\n",
      "Epoch 27/100\n",
      "702/702 [==============================] - 4s 5ms/step - loss: 1.0494 - acc: 0.9929 - val_loss: 1.1301 - val_acc: 0.9487\n",
      "Epoch 28/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0440 - acc: 0.9986 - val_loss: 1.0676 - val_acc: 0.9615\n",
      "Epoch 29/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0453 - acc: 0.9943 - val_loss: 1.0906 - val_acc: 0.9744\n",
      "Epoch 30/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0449 - acc: 0.9957 - val_loss: 1.1088 - val_acc: 0.9359\n",
      "Epoch 31/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0438 - acc: 0.9986 - val_loss: 1.0584 - val_acc: 0.9872\n",
      "Epoch 32/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0450 - acc: 0.9972 - val_loss: 1.1175 - val_acc: 0.9615\n",
      "Epoch 33/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0428 - acc: 0.9972 - val_loss: 1.0639 - val_acc: 0.9744\n",
      "Epoch 34/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0423 - acc: 1.0000 - val_loss: 1.1678 - val_acc: 0.9744\n",
      "Epoch 35/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0417 - acc: 1.0000 - val_loss: 1.1489 - val_acc: 0.9744\n",
      "Epoch 36/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0429 - acc: 0.9986 - val_loss: 1.0561 - val_acc: 0.9744\n",
      "Epoch 37/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0417 - acc: 0.9986 - val_loss: 1.0870 - val_acc: 0.9615\n",
      "Epoch 38/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0413 - acc: 1.0000 - val_loss: 1.1148 - val_acc: 0.9615\n",
      "Epoch 39/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0412 - acc: 1.0000 - val_loss: 1.1581 - val_acc: 0.9615\n",
      "Epoch 40/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0411 - acc: 1.0000 - val_loss: 1.1419 - val_acc: 0.9615\n",
      "Epoch 41/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0410 - acc: 1.0000 - val_loss: 1.0768 - val_acc: 0.9744\n",
      "Epoch 42/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0410 - acc: 1.0000 - val_loss: 1.1012 - val_acc: 0.9615\n",
      "Epoch 43/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0410 - acc: 1.0000 - val_loss: 1.1412 - val_acc: 0.9487\n",
      "Epoch 44/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0408 - acc: 1.0000 - val_loss: 1.1578 - val_acc: 0.9487\n",
      "Epoch 45/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0407 - acc: 1.0000 - val_loss: 1.1522 - val_acc: 0.9487\n",
      "Epoch 46/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0407 - acc: 1.0000 - val_loss: 1.1377 - val_acc: 0.9487\n",
      "Epoch 47/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0508 - val_acc: 0.9744\n",
      "Epoch 48/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0521 - val_acc: 0.9744\n",
      "Epoch 49/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0599 - val_acc: 0.9744\n",
      "Epoch 50/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0718 - val_acc: 0.9744\n",
      "Epoch 51/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0678 - val_acc: 0.9744\n",
      "Epoch 52/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.1330 - val_acc: 0.9487\n",
      "Epoch 53/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0563 - val_acc: 0.9744\n",
      "Epoch 54/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0533 - val_acc: 0.9744\n",
      "Epoch 55/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0899 - val_acc: 0.9615\n",
      "Epoch 56/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0485 - val_acc: 0.9744\n",
      "Epoch 57/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.1149 - val_acc: 0.9615\n",
      "Epoch 58/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0956 - val_acc: 0.9615\n",
      "Epoch 59/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0915 - val_acc: 0.9615\n",
      "Epoch 60/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0497 - val_acc: 0.9744\n",
      "Epoch 61/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0929 - val_acc: 0.9615\n",
      "Epoch 62/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0837 - val_acc: 0.9744\n",
      "Epoch 63/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0861 - val_acc: 0.9615\n",
      "Epoch 64/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0545 - val_acc: 0.9872\n",
      "Epoch 65/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0933 - val_acc: 0.9744\n",
      "Epoch 66/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0507 - val_acc: 0.9872\n",
      "Epoch 67/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0785 - val_acc: 0.9744\n",
      "Epoch 68/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0701 - val_acc: 0.9615\n",
      "Epoch 69/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0819 - val_acc: 0.9615\n",
      "Epoch 70/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0951 - val_acc: 0.9615\n",
      "Epoch 71/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0689 - val_acc: 0.9615\n",
      "Epoch 72/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0558 - val_acc: 0.9744\n",
      "Epoch 73/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0558 - val_acc: 0.9744\n",
      "Epoch 74/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0622 - val_acc: 0.9744\n",
      "Epoch 75/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0712 - val_acc: 0.9615\n",
      "Epoch 76/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0936 - val_acc: 0.9615\n",
      "Epoch 77/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0588 - val_acc: 0.9872\n",
      "Epoch 78/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0643 - val_acc: 0.9872\n",
      "Epoch 79/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0547 - val_acc: 0.9744\n",
      "Epoch 80/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0539 - val_acc: 0.9744\n",
      "Epoch 81/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0783 - val_acc: 0.9615\n",
      "Epoch 82/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0795 - val_acc: 0.9615\n",
      "Epoch 83/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0604 - val_acc: 0.9744\n",
      "Epoch 84/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0718 - val_acc: 0.9615\n",
      "Epoch 85/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0721 - val_acc: 0.9615\n",
      "Epoch 86/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0652 - val_acc: 0.9744\n",
      "Epoch 87/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0556 - val_acc: 0.9744\n",
      "Epoch 88/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0571 - val_acc: 0.9744\n",
      "Epoch 89/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0793 - val_acc: 0.9615\n",
      "Epoch 90/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0624 - val_acc: 0.9744\n",
      "Epoch 91/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0603 - val_acc: 0.9744\n",
      "Epoch 92/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0663 - val_acc: 0.9615\n",
      "Epoch 93/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0768 - val_acc: 0.9615\n",
      "Epoch 94/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0643 - val_acc: 0.9744\n",
      "Epoch 95/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0621 - val_acc: 0.9744\n",
      "Epoch 96/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0635 - val_acc: 0.9744\n",
      "Epoch 97/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0615 - val_acc: 0.9744\n",
      "Epoch 98/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0797 - val_acc: 0.9615\n",
      "Epoch 99/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0745 - val_acc: 0.9615\n",
      "Epoch 100/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0697 - val_acc: 0.9615\n",
      "78/78 [==============================] - 0s 322us/step\n",
      "Score for fold 3: loss of 1.0628010156827095; acc of 97.43589697740018%\n",
      "Train on 702 samples, validate on 78 samples\n",
      "Epoch 1/100\n",
      "702/702 [==============================] - 11s 15ms/step - loss: 2.2121 - acc: 0.3689 - val_loss: 1.9112 - val_acc: 0.4744\n",
      "Epoch 2/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.6649 - acc: 0.6510 - val_loss: 1.4127 - val_acc: 0.8333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.3714 - acc: 0.7977 - val_loss: 1.3592 - val_acc: 0.8846\n",
      "Epoch 4/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.2455 - acc: 0.8618 - val_loss: 1.2009 - val_acc: 0.8846\n",
      "Epoch 5/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1919 - acc: 0.9060 - val_loss: 1.1775 - val_acc: 0.8846\n",
      "Epoch 6/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1572 - acc: 0.9060 - val_loss: 1.1671 - val_acc: 0.8077\n",
      "Epoch 7/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1253 - acc: 0.9387 - val_loss: 1.1576 - val_acc: 0.8974\n",
      "Epoch 8/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1068 - acc: 0.9359 - val_loss: 1.2438 - val_acc: 0.8974\n",
      "Epoch 9/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0850 - acc: 0.9687 - val_loss: 1.0815 - val_acc: 0.9744\n",
      "Epoch 10/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0913 - acc: 0.9729 - val_loss: 1.2317 - val_acc: 0.8974\n",
      "Epoch 11/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0991 - acc: 0.9430 - val_loss: 1.0598 - val_acc: 0.9744\n",
      "Epoch 12/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0649 - acc: 0.9786 - val_loss: 1.0660 - val_acc: 1.0000\n",
      "Epoch 13/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1029 - acc: 0.9587 - val_loss: 1.0609 - val_acc: 0.9744\n",
      "Epoch 14/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0663 - acc: 0.9872 - val_loss: 1.0614 - val_acc: 0.9615\n",
      "Epoch 15/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0761 - acc: 0.9729 - val_loss: 1.0600 - val_acc: 0.9872\n",
      "Epoch 16/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0605 - acc: 0.9815 - val_loss: 1.0553 - val_acc: 1.0000\n",
      "Epoch 17/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0643 - acc: 0.9829 - val_loss: 1.1429 - val_acc: 0.8974\n",
      "Epoch 18/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0616 - acc: 0.9886 - val_loss: 1.0745 - val_acc: 0.9615\n",
      "Epoch 19/100\n",
      "702/702 [==============================] - 4s 5ms/step - loss: 1.0522 - acc: 0.9929 - val_loss: 1.0537 - val_acc: 0.9872\n",
      "Epoch 20/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0549 - acc: 0.9843 - val_loss: 1.0488 - val_acc: 1.0000\n",
      "Epoch 21/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0496 - acc: 0.9929 - val_loss: 1.0626 - val_acc: 0.9615\n",
      "Epoch 22/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0474 - acc: 0.9986 - val_loss: 1.0675 - val_acc: 0.9231\n",
      "Epoch 23/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0520 - acc: 0.9858 - val_loss: 1.0486 - val_acc: 0.9872\n",
      "Epoch 24/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0463 - acc: 0.9986 - val_loss: 1.0467 - val_acc: 1.0000\n",
      "Epoch 25/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0475 - acc: 0.9929 - val_loss: 1.0480 - val_acc: 1.0000\n",
      "Epoch 26/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0446 - acc: 0.9986 - val_loss: 1.0440 - val_acc: 1.0000\n",
      "Epoch 27/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0445 - acc: 0.9972 - val_loss: 1.0480 - val_acc: 0.9872\n",
      "Epoch 28/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0442 - acc: 0.9957 - val_loss: 1.0552 - val_acc: 0.9744\n",
      "Epoch 29/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0434 - acc: 0.9986 - val_loss: 1.0437 - val_acc: 1.0000\n",
      "Epoch 30/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0429 - acc: 0.9986 - val_loss: 1.0463 - val_acc: 1.0000\n",
      "Epoch 31/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0427 - acc: 0.9986 - val_loss: 1.0585 - val_acc: 0.9872\n",
      "Epoch 32/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0437 - acc: 0.9986 - val_loss: 1.0456 - val_acc: 0.9872\n",
      "Epoch 33/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0418 - acc: 1.0000 - val_loss: 1.0444 - val_acc: 1.0000\n",
      "Epoch 34/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0422 - acc: 1.0000 - val_loss: 1.0428 - val_acc: 1.0000\n",
      "Epoch 35/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0418 - acc: 1.0000 - val_loss: 1.0434 - val_acc: 1.0000\n",
      "Epoch 36/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0424 - acc: 1.0000 - val_loss: 1.0502 - val_acc: 0.9872\n",
      "Epoch 37/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0415 - acc: 1.0000 - val_loss: 1.0430 - val_acc: 1.0000\n",
      "Epoch 38/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0411 - acc: 1.0000 - val_loss: 1.0426 - val_acc: 1.0000\n",
      "Epoch 39/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0409 - acc: 1.0000 - val_loss: 1.0483 - val_acc: 1.0000\n",
      "Epoch 40/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0411 - acc: 1.0000 - val_loss: 1.0435 - val_acc: 1.0000\n",
      "Epoch 41/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0410 - acc: 1.0000 - val_loss: 1.0449 - val_acc: 0.9872\n",
      "Epoch 42/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0409 - acc: 1.0000 - val_loss: 1.0479 - val_acc: 0.9872\n",
      "Epoch 43/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0409 - acc: 1.0000 - val_loss: 1.0418 - val_acc: 1.0000\n",
      "Epoch 44/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0408 - acc: 1.0000 - val_loss: 1.0423 - val_acc: 1.0000\n",
      "Epoch 45/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0407 - acc: 1.0000 - val_loss: 1.0439 - val_acc: 1.0000\n",
      "Epoch 46/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0407 - acc: 1.0000 - val_loss: 1.0417 - val_acc: 1.0000\n",
      "Epoch 47/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0407 - acc: 1.0000 - val_loss: 1.0424 - val_acc: 1.0000\n",
      "Epoch 48/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0407 - acc: 1.0000 - val_loss: 1.0423 - val_acc: 1.0000\n",
      "Epoch 49/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0422 - val_acc: 1.0000\n",
      "Epoch 50/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0421 - val_acc: 1.0000\n",
      "Epoch 51/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0414 - val_acc: 1.0000\n",
      "Epoch 52/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0457 - val_acc: 0.9872\n",
      "Epoch 53/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0419 - val_acc: 1.0000\n",
      "Epoch 54/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0415 - val_acc: 1.0000\n",
      "Epoch 55/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0414 - val_acc: 1.0000\n",
      "Epoch 56/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0414 - val_acc: 1.0000\n",
      "Epoch 57/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0415 - val_acc: 1.0000\n",
      "Epoch 58/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0416 - val_acc: 1.0000\n",
      "Epoch 59/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0418 - val_acc: 1.0000\n",
      "Epoch 60/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0415 - val_acc: 1.0000\n",
      "Epoch 61/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0414 - val_acc: 1.0000\n",
      "Epoch 62/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0414 - val_acc: 1.0000\n",
      "Epoch 63/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0414 - val_acc: 1.0000\n",
      "Epoch 64/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0415 - val_acc: 1.0000\n",
      "Epoch 65/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0412 - val_acc: 1.0000\n",
      "Epoch 66/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0413 - val_acc: 1.0000\n",
      "Epoch 67/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0412 - val_acc: 1.0000\n",
      "Epoch 68/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0414 - val_acc: 1.0000\n",
      "Epoch 69/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0412 - val_acc: 1.0000\n",
      "Epoch 70/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0414 - val_acc: 1.0000\n",
      "Epoch 71/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0414 - val_acc: 1.0000\n",
      "Epoch 72/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0412 - val_acc: 1.0000\n",
      "Epoch 73/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0414 - val_acc: 1.0000\n",
      "Epoch 74/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0414 - val_acc: 1.0000\n",
      "Epoch 75/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0414 - val_acc: 1.0000\n",
      "Epoch 76/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0414 - val_acc: 1.0000\n",
      "Epoch 77/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0415 - val_acc: 1.0000\n",
      "Epoch 78/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0415 - val_acc: 1.0000\n",
      "Epoch 79/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0412 - val_acc: 1.0000\n",
      "Epoch 80/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0412 - val_acc: 1.0000\n",
      "Epoch 81/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0413 - val_acc: 1.0000\n",
      "Epoch 82/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0412 - val_acc: 1.0000\n",
      "Epoch 83/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0412 - val_acc: 1.0000\n",
      "Epoch 84/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0412 - val_acc: 1.0000\n",
      "Epoch 85/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0413 - val_acc: 1.0000\n",
      "Epoch 86/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0414 - val_acc: 1.0000\n",
      "Epoch 87/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0413 - val_acc: 1.0000\n",
      "Epoch 88/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0412 - val_acc: 1.0000\n",
      "Epoch 89/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0412 - val_acc: 1.0000\n",
      "Epoch 90/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0413 - val_acc: 1.0000\n",
      "Epoch 91/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0413 - val_acc: 1.0000\n",
      "Epoch 92/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0412 - val_acc: 1.0000\n",
      "Epoch 93/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0413 - val_acc: 1.0000\n",
      "Epoch 94/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0413 - val_acc: 1.0000\n",
      "Epoch 95/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0413 - val_acc: 1.0000\n",
      "Epoch 96/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0413 - val_acc: 1.0000\n",
      "Epoch 97/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0413 - val_acc: 1.0000\n",
      "Epoch 98/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0412 - val_acc: 1.0000\n",
      "Epoch 99/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0413 - val_acc: 1.0000\n",
      "Epoch 100/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0413 - val_acc: 1.0000\n",
      "78/78 [==============================] - 0s 318us/step\n",
      "Score for fold 4: loss of 1.0414379743429332; acc of 100.0%\n",
      "Train on 702 samples, validate on 78 samples\n",
      "Epoch 1/100\n",
      "702/702 [==============================] - 11s 15ms/step - loss: 2.1484 - acc: 0.3989 - val_loss: 1.8571 - val_acc: 0.5256\n",
      "Epoch 2/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.5290 - acc: 0.7464 - val_loss: 1.9226 - val_acc: 0.4103\n",
      "Epoch 3/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.3664 - acc: 0.8105 - val_loss: 1.4055 - val_acc: 0.7436\n",
      "Epoch 4/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.2551 - acc: 0.8490 - val_loss: 1.3301 - val_acc: 0.7564\n",
      "Epoch 5/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1792 - acc: 0.8860 - val_loss: 1.3100 - val_acc: 0.8333\n",
      "Epoch 6/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1415 - acc: 0.9145 - val_loss: 1.1383 - val_acc: 0.9103\n",
      "Epoch 7/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1200 - acc: 0.9473 - val_loss: 1.7710 - val_acc: 0.5897\n",
      "Epoch 8/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1086 - acc: 0.9544 - val_loss: 1.1540 - val_acc: 0.8590\n",
      "Epoch 9/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0970 - acc: 0.9402 - val_loss: 1.1496 - val_acc: 0.8590\n",
      "Epoch 10/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0915 - acc: 0.9558 - val_loss: 1.1052 - val_acc: 0.9359\n",
      "Epoch 11/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0942 - acc: 0.9615 - val_loss: 1.1058 - val_acc: 0.9487\n",
      "Epoch 12/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0842 - acc: 0.9687 - val_loss: 1.0937 - val_acc: 0.9487\n",
      "Epoch 13/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0720 - acc: 0.9630 - val_loss: 1.0802 - val_acc: 0.9744\n",
      "Epoch 14/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0686 - acc: 0.9744 - val_loss: 1.1354 - val_acc: 0.8718\n",
      "Epoch 15/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0609 - acc: 0.9786 - val_loss: 1.0651 - val_acc: 0.9872\n",
      "Epoch 16/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0546 - acc: 0.9929 - val_loss: 1.1658 - val_acc: 0.8590\n",
      "Epoch 17/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0605 - acc: 0.9843 - val_loss: 1.0799 - val_acc: 0.9615\n",
      "Epoch 18/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0559 - acc: 0.9829 - val_loss: 1.0918 - val_acc: 0.9615\n",
      "Epoch 19/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0549 - acc: 0.9858 - val_loss: 1.0740 - val_acc: 0.9744\n",
      "Epoch 20/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0978 - acc: 0.9801 - val_loss: 1.0645 - val_acc: 0.9872\n",
      "Epoch 21/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0502 - acc: 0.9915 - val_loss: 1.0710 - val_acc: 0.9615\n",
      "Epoch 22/100\n",
      "702/702 [==============================] - 4s 6ms/step - loss: 1.0609 - acc: 0.9843 - val_loss: 1.0528 - val_acc: 0.9872\n",
      "Epoch 23/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0461 - acc: 0.9957 - val_loss: 1.0543 - val_acc: 0.9872\n",
      "Epoch 24/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0482 - acc: 0.9957 - val_loss: 1.0542 - val_acc: 0.9872\n",
      "Epoch 25/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0450 - acc: 0.9986 - val_loss: 1.0543 - val_acc: 0.9872\n",
      "Epoch 26/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0503 - acc: 0.9929 - val_loss: 1.0542 - val_acc: 0.9872\n",
      "Epoch 27/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0455 - acc: 0.9957 - val_loss: 1.0629 - val_acc: 0.9744\n",
      "Epoch 28/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0460 - acc: 0.9957 - val_loss: 1.0603 - val_acc: 0.9744\n",
      "Epoch 29/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0436 - acc: 0.9986 - val_loss: 1.0543 - val_acc: 0.9872\n",
      "Epoch 30/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0430 - acc: 0.9986 - val_loss: 1.0549 - val_acc: 0.9872\n",
      "Epoch 31/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0433 - acc: 0.9986 - val_loss: 1.0745 - val_acc: 0.9744\n",
      "Epoch 32/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0424 - acc: 0.9986 - val_loss: 1.0498 - val_acc: 0.9872\n",
      "Epoch 33/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0419 - acc: 0.9986 - val_loss: 1.0555 - val_acc: 0.9872\n",
      "Epoch 34/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0430 - acc: 0.9986 - val_loss: 1.0513 - val_acc: 0.9872\n",
      "Epoch 35/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0429 - acc: 0.9972 - val_loss: 1.0534 - val_acc: 0.9872\n",
      "Epoch 36/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0415 - acc: 1.0000 - val_loss: 1.0504 - val_acc: 0.9872\n",
      "Epoch 37/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0416 - acc: 1.0000 - val_loss: 1.0517 - val_acc: 0.9872\n",
      "Epoch 38/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0413 - acc: 1.0000 - val_loss: 1.0513 - val_acc: 0.9872\n",
      "Epoch 39/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0412 - acc: 1.0000 - val_loss: 1.0522 - val_acc: 0.9872\n",
      "Epoch 40/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0412 - acc: 1.0000 - val_loss: 1.0491 - val_acc: 0.9872\n",
      "Epoch 41/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0410 - acc: 1.0000 - val_loss: 1.0493 - val_acc: 0.9872\n",
      "Epoch 42/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0408 - acc: 1.0000 - val_loss: 1.0486 - val_acc: 0.9872\n",
      "Epoch 43/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0410 - acc: 1.0000 - val_loss: 1.0528 - val_acc: 0.9744\n",
      "Epoch 44/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0408 - acc: 1.0000 - val_loss: 1.0501 - val_acc: 0.9872\n",
      "Epoch 45/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0407 - acc: 1.0000 - val_loss: 1.0534 - val_acc: 0.9872\n",
      "Epoch 46/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0408 - acc: 1.0000 - val_loss: 1.0499 - val_acc: 0.9872\n",
      "Epoch 47/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0497 - val_acc: 0.9872\n",
      "Epoch 48/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0511 - val_acc: 0.9872\n",
      "Epoch 49/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0510 - val_acc: 0.9872\n",
      "Epoch 50/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0490 - val_acc: 0.9872\n",
      "Epoch 51/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0496 - val_acc: 0.9872\n",
      "Epoch 52/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0516 - val_acc: 0.9872\n",
      "Epoch 53/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0497 - val_acc: 0.9872\n",
      "Epoch 54/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0496 - val_acc: 0.9872\n",
      "Epoch 55/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0508 - val_acc: 0.9872\n",
      "Epoch 56/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0514 - val_acc: 0.9872\n",
      "Epoch 57/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0497 - val_acc: 0.9872\n",
      "Epoch 58/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0497 - val_acc: 0.9872\n",
      "Epoch 59/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0499 - val_acc: 0.9872\n",
      "Epoch 60/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0503 - val_acc: 0.9872\n",
      "Epoch 61/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0500 - val_acc: 0.9872\n",
      "Epoch 62/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0497 - val_acc: 0.9872\n",
      "Epoch 63/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0492 - val_acc: 0.9872\n",
      "Epoch 64/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0501 - val_acc: 0.9872\n",
      "Epoch 65/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0492 - val_acc: 0.9872\n",
      "Epoch 66/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0496 - val_acc: 0.9872\n",
      "Epoch 67/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0498 - val_acc: 0.9872\n",
      "Epoch 68/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0496 - val_acc: 0.9872\n",
      "Epoch 69/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0494 - val_acc: 0.9872\n",
      "Epoch 70/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0494 - val_acc: 0.9872\n",
      "Epoch 71/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0499 - val_acc: 0.9872\n",
      "Epoch 72/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0496 - val_acc: 0.9872\n",
      "Epoch 73/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0497 - val_acc: 0.9872\n",
      "Epoch 74/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0498 - val_acc: 0.9872\n",
      "Epoch 75/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0495 - val_acc: 0.9872\n",
      "Epoch 76/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0491 - val_acc: 0.9872\n",
      "Epoch 77/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0498 - val_acc: 0.9872\n",
      "Epoch 78/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0496 - val_acc: 0.9872\n",
      "Epoch 79/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0499 - val_acc: 0.9872\n",
      "Epoch 80/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0498 - val_acc: 0.9872\n",
      "Epoch 81/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0500 - val_acc: 0.9872\n",
      "Epoch 82/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0501 - val_acc: 0.9872\n",
      "Epoch 83/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0501 - val_acc: 0.9872\n",
      "Epoch 84/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0498 - val_acc: 0.9872\n",
      "Epoch 85/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0497 - val_acc: 0.9872\n",
      "Epoch 86/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0498 - val_acc: 0.9872\n",
      "Epoch 87/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0497 - val_acc: 0.9872\n",
      "Epoch 88/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0495 - val_acc: 0.9872\n",
      "Epoch 89/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0495 - val_acc: 0.9872\n",
      "Epoch 90/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0498 - val_acc: 0.9872\n",
      "Epoch 91/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0497 - val_acc: 0.9872\n",
      "Epoch 92/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0498 - val_acc: 0.9872\n",
      "Epoch 93/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0498 - val_acc: 0.9872\n",
      "Epoch 94/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0501 - val_acc: 0.9872\n",
      "Epoch 95/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0499 - val_acc: 0.9872\n",
      "Epoch 96/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0497 - val_acc: 0.9872\n",
      "Epoch 97/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0495 - val_acc: 0.9872\n",
      "Epoch 98/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0498 - val_acc: 0.9872\n",
      "Epoch 99/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0496 - val_acc: 0.9872\n",
      "Epoch 100/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0496 - val_acc: 0.9872\n",
      "78/78 [==============================] - 0s 331us/step\n",
      "Score for fold 5: loss of 1.0494694098448143; acc of 98.71794871794873%\n",
      "Train on 702 samples, validate on 78 samples\n",
      "Epoch 1/100\n",
      "702/702 [==============================] - 11s 16ms/step - loss: 2.2505 - acc: 0.3504 - val_loss: 1.8791 - val_acc: 0.4744\n",
      "Epoch 2/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.6262 - acc: 0.6638 - val_loss: 1.3951 - val_acc: 0.8590\n",
      "Epoch 3/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.3403 - acc: 0.8276 - val_loss: 1.5364 - val_acc: 0.6538\n",
      "Epoch 4/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.2446 - acc: 0.8761 - val_loss: 1.2503 - val_acc: 0.8846\n",
      "Epoch 5/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1702 - acc: 0.9088 - val_loss: 1.2856 - val_acc: 0.6795\n",
      "Epoch 6/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1540 - acc: 0.9259 - val_loss: 1.1233 - val_acc: 0.9615\n",
      "Epoch 7/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1094 - acc: 0.9501 - val_loss: 1.2109 - val_acc: 0.8462\n",
      "Epoch 8/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1125 - acc: 0.9430 - val_loss: 1.1258 - val_acc: 0.9615\n",
      "Epoch 9/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1047 - acc: 0.9573 - val_loss: 1.0907 - val_acc: 0.9872\n",
      "Epoch 10/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0750 - acc: 0.9829 - val_loss: 1.1555 - val_acc: 0.8974\n",
      "Epoch 11/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0752 - acc: 0.9630 - val_loss: 1.0951 - val_acc: 0.9872\n",
      "Epoch 12/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1022 - acc: 0.9473 - val_loss: 1.1110 - val_acc: 0.9103\n",
      "Epoch 13/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0702 - acc: 0.9758 - val_loss: 1.0870 - val_acc: 0.9487\n",
      "Epoch 14/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0705 - acc: 0.9801 - val_loss: 1.0870 - val_acc: 0.9487\n",
      "Epoch 15/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0624 - acc: 0.9801 - val_loss: 1.0573 - val_acc: 1.0000\n",
      "Epoch 16/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0594 - acc: 0.9801 - val_loss: 1.0663 - val_acc: 0.9872\n",
      "Epoch 17/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0576 - acc: 0.9900 - val_loss: 1.0730 - val_acc: 0.9615\n",
      "Epoch 18/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0557 - acc: 0.9829 - val_loss: 1.0578 - val_acc: 0.9744\n",
      "Epoch 19/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0678 - acc: 0.9801 - val_loss: 1.0682 - val_acc: 0.9615\n",
      "Epoch 20/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0509 - acc: 0.9900 - val_loss: 1.0565 - val_acc: 1.0000\n",
      "Epoch 21/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0534 - acc: 0.9872 - val_loss: 1.0657 - val_acc: 0.9744\n",
      "Epoch 22/100\n",
      "702/702 [==============================] - 4s 6ms/step - loss: 1.0507 - acc: 0.9900 - val_loss: 1.0753 - val_acc: 0.9744\n",
      "Epoch 23/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0509 - acc: 0.9900 - val_loss: 1.0577 - val_acc: 0.9872\n",
      "Epoch 24/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0467 - acc: 0.9957 - val_loss: 1.0525 - val_acc: 0.9872\n",
      "Epoch 25/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0469 - acc: 0.9943 - val_loss: 1.0550 - val_acc: 0.9872\n",
      "Epoch 26/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0489 - acc: 0.9900 - val_loss: 1.0514 - val_acc: 0.9872\n",
      "Epoch 27/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0433 - acc: 0.9986 - val_loss: 1.0525 - val_acc: 0.9872\n",
      "Epoch 28/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0438 - acc: 0.9986 - val_loss: 1.0525 - val_acc: 1.0000\n",
      "Epoch 29/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0437 - acc: 0.9972 - val_loss: 1.0658 - val_acc: 0.9744\n",
      "Epoch 30/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0446 - acc: 0.9957 - val_loss: 1.0574 - val_acc: 0.9615\n",
      "Epoch 31/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0424 - acc: 1.0000 - val_loss: 1.0487 - val_acc: 1.0000\n",
      "Epoch 32/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0436 - acc: 0.9972 - val_loss: 1.0526 - val_acc: 0.9872\n",
      "Epoch 33/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0425 - acc: 1.0000 - val_loss: 1.0567 - val_acc: 0.9872\n",
      "Epoch 34/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0420 - acc: 1.0000 - val_loss: 1.0498 - val_acc: 1.0000\n",
      "Epoch 35/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0421 - acc: 1.0000 - val_loss: 1.0506 - val_acc: 0.9872\n",
      "Epoch 36/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0416 - acc: 0.9986 - val_loss: 1.0496 - val_acc: 1.0000\n",
      "Epoch 37/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0415 - acc: 0.9986 - val_loss: 1.0458 - val_acc: 0.9872\n",
      "Epoch 38/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0414 - acc: 1.0000 - val_loss: 1.0473 - val_acc: 0.9744\n",
      "Epoch 39/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0416 - acc: 1.0000 - val_loss: 1.0470 - val_acc: 1.0000\n",
      "Epoch 40/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0410 - acc: 1.0000 - val_loss: 1.0464 - val_acc: 1.0000\n",
      "Epoch 41/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0409 - acc: 1.0000 - val_loss: 1.0499 - val_acc: 0.9872\n",
      "Epoch 42/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0408 - acc: 1.0000 - val_loss: 1.0493 - val_acc: 0.9872\n",
      "Epoch 43/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0409 - acc: 1.0000 - val_loss: 1.0472 - val_acc: 1.0000\n",
      "Epoch 44/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0408 - acc: 1.0000 - val_loss: 1.0468 - val_acc: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0408 - acc: 1.0000 - val_loss: 1.0472 - val_acc: 1.0000\n",
      "Epoch 46/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0407 - acc: 1.0000 - val_loss: 1.0456 - val_acc: 1.0000\n",
      "Epoch 47/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0407 - acc: 1.0000 - val_loss: 1.0446 - val_acc: 1.0000\n",
      "Epoch 48/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0445 - val_acc: 1.0000\n",
      "Epoch 49/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0484 - val_acc: 1.0000\n",
      "Epoch 50/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0466 - val_acc: 1.0000\n",
      "Epoch 51/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0456 - val_acc: 1.0000\n",
      "Epoch 52/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0447 - val_acc: 1.0000\n",
      "Epoch 53/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0445 - val_acc: 1.0000\n",
      "Epoch 54/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0441 - val_acc: 1.0000\n",
      "Epoch 55/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0442 - val_acc: 1.0000\n",
      "Epoch 56/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0452 - val_acc: 1.0000\n",
      "Epoch 57/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0439 - val_acc: 1.0000\n",
      "Epoch 58/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0462 - val_acc: 1.0000\n",
      "Epoch 59/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0455 - val_acc: 1.0000\n",
      "Epoch 60/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0455 - val_acc: 1.0000\n",
      "Epoch 61/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0447 - val_acc: 1.0000\n",
      "Epoch 62/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0441 - val_acc: 1.0000\n",
      "Epoch 63/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0457 - val_acc: 1.0000\n",
      "Epoch 64/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0469 - val_acc: 1.0000\n",
      "Epoch 65/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0461 - val_acc: 1.0000\n",
      "Epoch 66/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0455 - val_acc: 1.0000\n",
      "Epoch 67/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0455 - val_acc: 1.0000\n",
      "Epoch 68/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0448 - val_acc: 1.0000\n",
      "Epoch 69/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0454 - val_acc: 1.0000\n",
      "Epoch 70/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0463 - val_acc: 1.0000\n",
      "Epoch 71/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0454 - val_acc: 1.0000\n",
      "Epoch 72/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0457 - val_acc: 1.0000\n",
      "Epoch 73/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0451 - val_acc: 1.0000\n",
      "Epoch 74/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0458 - val_acc: 1.0000\n",
      "Epoch 75/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0452 - val_acc: 1.0000\n",
      "Epoch 76/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0445 - val_acc: 1.0000\n",
      "Epoch 77/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0445 - val_acc: 1.0000\n",
      "Epoch 78/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0447 - val_acc: 1.0000\n",
      "Epoch 79/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0448 - val_acc: 1.0000\n",
      "Epoch 80/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0443 - val_acc: 1.0000\n",
      "Epoch 81/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0445 - val_acc: 1.0000\n",
      "Epoch 82/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0440 - val_acc: 1.0000\n",
      "Epoch 83/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0442 - val_acc: 1.0000\n",
      "Epoch 84/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0444 - val_acc: 1.0000\n",
      "Epoch 85/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0446 - val_acc: 1.0000\n",
      "Epoch 86/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0443 - val_acc: 1.0000\n",
      "Epoch 87/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0443 - val_acc: 1.0000\n",
      "Epoch 88/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0443 - val_acc: 1.0000\n",
      "Epoch 89/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0442 - val_acc: 1.0000\n",
      "Epoch 90/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0441 - val_acc: 1.0000\n",
      "Epoch 91/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0442 - val_acc: 1.0000\n",
      "Epoch 92/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0441 - val_acc: 1.0000\n",
      "Epoch 93/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0444 - val_acc: 1.0000\n",
      "Epoch 94/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0443 - val_acc: 1.0000\n",
      "Epoch 95/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0444 - val_acc: 1.0000\n",
      "Epoch 96/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0442 - val_acc: 1.0000\n",
      "Epoch 97/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0442 - val_acc: 1.0000\n",
      "Epoch 98/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0443 - val_acc: 1.0000\n",
      "Epoch 99/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0442 - val_acc: 1.0000\n",
      "Epoch 100/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0442 - val_acc: 1.0000\n",
      "78/78 [==============================] - 0s 431us/step\n",
      "Score for fold 6: loss of 1.0445560369736109; acc of 100.0%\n",
      "Train on 702 samples, validate on 78 samples\n",
      "Epoch 1/100\n",
      "702/702 [==============================] - 11s 16ms/step - loss: 2.3165 - acc: 0.3148 - val_loss: 1.9125 - val_acc: 0.5769\n",
      "Epoch 2/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.6751 - acc: 0.6766 - val_loss: 1.5908 - val_acc: 0.7564\n",
      "Epoch 3/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.3857 - acc: 0.8006 - val_loss: 1.2566 - val_acc: 0.8846\n",
      "Epoch 4/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.2334 - acc: 0.9017 - val_loss: 1.3337 - val_acc: 0.8077\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1914 - acc: 0.9003 - val_loss: 1.3522 - val_acc: 0.7179\n",
      "Epoch 6/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1536 - acc: 0.9202 - val_loss: 1.1868 - val_acc: 0.8718\n",
      "Epoch 7/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1132 - acc: 0.9188 - val_loss: 1.1677 - val_acc: 0.8205\n",
      "Epoch 8/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0962 - acc: 0.9516 - val_loss: 1.1610 - val_acc: 0.8077\n",
      "Epoch 9/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0937 - acc: 0.9444 - val_loss: 1.1230 - val_acc: 0.9231\n",
      "Epoch 10/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1043 - acc: 0.9615 - val_loss: 1.0879 - val_acc: 0.9231\n",
      "Epoch 11/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0746 - acc: 0.9658 - val_loss: 1.0974 - val_acc: 0.9487\n",
      "Epoch 12/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0731 - acc: 0.9744 - val_loss: 1.1043 - val_acc: 0.9103\n",
      "Epoch 13/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0639 - acc: 0.9744 - val_loss: 1.1869 - val_acc: 0.8590\n",
      "Epoch 14/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0976 - acc: 0.9658 - val_loss: 1.1157 - val_acc: 0.9744\n",
      "Epoch 15/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0568 - acc: 0.9843 - val_loss: 1.0984 - val_acc: 0.9744\n",
      "Epoch 16/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0632 - acc: 0.9772 - val_loss: 1.1021 - val_acc: 0.9744\n",
      "Epoch 17/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0617 - acc: 0.9915 - val_loss: 1.1133 - val_acc: 0.9744\n",
      "Epoch 18/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0772 - acc: 0.9801 - val_loss: 1.0988 - val_acc: 0.9615\n",
      "Epoch 19/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0477 - acc: 0.9972 - val_loss: 1.0948 - val_acc: 0.9744\n",
      "Epoch 20/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0745 - acc: 0.9786 - val_loss: 1.0781 - val_acc: 0.9615\n",
      "Epoch 21/100\n",
      "702/702 [==============================] - 4s 6ms/step - loss: 1.0494 - acc: 0.9943 - val_loss: 1.0720 - val_acc: 0.9615\n",
      "Epoch 22/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0473 - acc: 0.9957 - val_loss: 1.0866 - val_acc: 0.9615\n",
      "Epoch 23/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0492 - acc: 0.9886 - val_loss: 1.0977 - val_acc: 0.9615\n",
      "Epoch 24/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0447 - acc: 0.9986 - val_loss: 1.1012 - val_acc: 0.9615\n",
      "Epoch 25/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0468 - acc: 0.9957 - val_loss: 1.0879 - val_acc: 0.9872\n",
      "Epoch 26/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0516 - acc: 0.9915 - val_loss: 1.1016 - val_acc: 0.9744\n",
      "Epoch 27/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0441 - acc: 1.0000 - val_loss: 1.1062 - val_acc: 0.9744\n",
      "Epoch 28/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0430 - acc: 1.0000 - val_loss: 1.0864 - val_acc: 0.9744\n",
      "Epoch 29/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0521 - acc: 0.9886 - val_loss: 1.0939 - val_acc: 0.9744\n",
      "Epoch 30/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0430 - acc: 1.0000 - val_loss: 1.0923 - val_acc: 0.9744\n",
      "Epoch 31/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0458 - acc: 0.9957 - val_loss: 1.0982 - val_acc: 0.9744\n",
      "Epoch 32/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0440 - acc: 0.9986 - val_loss: 1.1380 - val_acc: 0.8974\n",
      "Epoch 33/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0459 - acc: 0.9915 - val_loss: 1.0902 - val_acc: 0.9744\n",
      "Epoch 34/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0419 - acc: 1.0000 - val_loss: 1.0883 - val_acc: 0.9744\n",
      "Epoch 35/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0434 - acc: 0.9972 - val_loss: 1.0878 - val_acc: 0.9744\n",
      "Epoch 36/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0417 - acc: 1.0000 - val_loss: 1.1154 - val_acc: 0.9359\n",
      "Epoch 37/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0425 - acc: 0.9986 - val_loss: 1.0955 - val_acc: 0.9872\n",
      "Epoch 38/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0414 - acc: 1.0000 - val_loss: 1.0969 - val_acc: 0.9744\n",
      "Epoch 39/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0413 - acc: 1.0000 - val_loss: 1.0951 - val_acc: 0.9744\n",
      "Epoch 40/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0419 - acc: 0.9972 - val_loss: 1.0891 - val_acc: 0.9744\n",
      "Epoch 41/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0411 - acc: 1.0000 - val_loss: 1.0751 - val_acc: 0.9872\n",
      "Epoch 42/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0410 - acc: 1.0000 - val_loss: 1.0796 - val_acc: 0.9744\n",
      "Epoch 43/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0411 - acc: 1.0000 - val_loss: 1.0829 - val_acc: 0.9744\n",
      "Epoch 44/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0410 - acc: 1.0000 - val_loss: 1.0690 - val_acc: 0.9744\n",
      "Epoch 45/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0407 - acc: 1.0000 - val_loss: 1.0847 - val_acc: 0.9744\n",
      "Epoch 46/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0408 - acc: 1.0000 - val_loss: 1.0851 - val_acc: 0.9744\n",
      "Epoch 47/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0408 - acc: 1.0000 - val_loss: 1.0797 - val_acc: 0.9744\n",
      "Epoch 48/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0408 - acc: 1.0000 - val_loss: 1.0857 - val_acc: 0.9744\n",
      "Epoch 49/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0918 - val_acc: 0.9744\n",
      "Epoch 50/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0407 - acc: 1.0000 - val_loss: 1.0832 - val_acc: 0.9744\n",
      "Epoch 51/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0909 - val_acc: 0.9744\n",
      "Epoch 52/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0872 - val_acc: 0.9744\n",
      "Epoch 53/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0782 - val_acc: 0.9744\n",
      "Epoch 54/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0844 - val_acc: 0.9744\n",
      "Epoch 55/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0778 - val_acc: 0.9744\n",
      "Epoch 56/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0932 - val_acc: 0.9744\n",
      "Epoch 57/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0871 - val_acc: 0.9744\n",
      "Epoch 58/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0903 - val_acc: 0.9744\n",
      "Epoch 59/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0957 - val_acc: 0.9744\n",
      "Epoch 60/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0916 - val_acc: 0.9744\n",
      "Epoch 61/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0927 - val_acc: 0.9744\n",
      "Epoch 62/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0798 - val_acc: 0.9872\n",
      "Epoch 63/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0823 - val_acc: 0.9744\n",
      "Epoch 64/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0857 - val_acc: 0.9744\n",
      "Epoch 65/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0912 - val_acc: 0.9744\n",
      "Epoch 66/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0905 - val_acc: 0.9744\n",
      "Epoch 67/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0951 - val_acc: 0.9744\n",
      "Epoch 68/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0912 - val_acc: 0.9744\n",
      "Epoch 69/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0935 - val_acc: 0.9744\n",
      "Epoch 70/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0831 - val_acc: 0.9744\n",
      "Epoch 71/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0956 - val_acc: 0.9744\n",
      "Epoch 72/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0958 - val_acc: 0.9744\n",
      "Epoch 73/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0905 - val_acc: 0.9744\n",
      "Epoch 74/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0952 - val_acc: 0.9744\n",
      "Epoch 75/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0914 - val_acc: 0.9744\n",
      "Epoch 76/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0911 - val_acc: 0.9744\n",
      "Epoch 77/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0908 - val_acc: 0.9744\n",
      "Epoch 78/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0899 - val_acc: 0.9744\n",
      "Epoch 79/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0892 - val_acc: 0.9744\n",
      "Epoch 80/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0902 - val_acc: 0.9744\n",
      "Epoch 81/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0863 - val_acc: 0.9744\n",
      "Epoch 82/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0901 - val_acc: 0.9744\n",
      "Epoch 83/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0894 - val_acc: 0.9744\n",
      "Epoch 84/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0902 - val_acc: 0.9744\n",
      "Epoch 85/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0901 - val_acc: 0.9744\n",
      "Epoch 86/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0929 - val_acc: 0.9744\n",
      "Epoch 87/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0915 - val_acc: 0.9744\n",
      "Epoch 88/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0932 - val_acc: 0.9744\n",
      "Epoch 89/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0930 - val_acc: 0.9744\n",
      "Epoch 90/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0913 - val_acc: 0.9744\n",
      "Epoch 91/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0884 - val_acc: 0.9744\n",
      "Epoch 92/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0923 - val_acc: 0.9744\n",
      "Epoch 93/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0881 - val_acc: 0.9744\n",
      "Epoch 94/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0896 - val_acc: 0.9744\n",
      "Epoch 95/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0900 - val_acc: 0.9744\n",
      "Epoch 96/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0930 - val_acc: 0.9744\n",
      "Epoch 97/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0935 - val_acc: 0.9744\n",
      "Epoch 98/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0918 - val_acc: 0.9744\n",
      "Epoch 99/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0931 - val_acc: 0.9744\n",
      "Epoch 100/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0931 - val_acc: 0.9744\n",
      "78/78 [==============================] - 0s 322us/step\n",
      "Score for fold 7: loss of 1.0906498707257783; acc of 97.43589697740018%\n",
      "Train on 702 samples, validate on 78 samples\n",
      "Epoch 1/100\n",
      "702/702 [==============================] - 11s 16ms/step - loss: 2.2657 - acc: 0.3191 - val_loss: 1.9354 - val_acc: 0.5256\n",
      "Epoch 2/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.6366 - acc: 0.6652 - val_loss: 1.5190 - val_acc: 0.7051\n",
      "Epoch 3/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.3769 - acc: 0.8205 - val_loss: 1.4195 - val_acc: 0.7051\n",
      "Epoch 4/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.2678 - acc: 0.8618 - val_loss: 1.5017 - val_acc: 0.6795\n",
      "Epoch 5/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1841 - acc: 0.9131 - val_loss: 1.2444 - val_acc: 0.8333\n",
      "Epoch 6/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1306 - acc: 0.9288 - val_loss: 1.2476 - val_acc: 0.8974\n",
      "Epoch 7/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1804 - acc: 0.9117 - val_loss: 1.1649 - val_acc: 0.8205\n",
      "Epoch 8/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1076 - acc: 0.9316 - val_loss: 1.1263 - val_acc: 0.8846\n",
      "Epoch 9/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0880 - acc: 0.9516 - val_loss: 1.1397 - val_acc: 0.9103\n",
      "Epoch 10/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1180 - acc: 0.9459 - val_loss: 1.1169 - val_acc: 0.9359\n",
      "Epoch 11/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0731 - acc: 0.9658 - val_loss: 1.1014 - val_acc: 0.9487\n",
      "Epoch 12/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0942 - acc: 0.9501 - val_loss: 1.0890 - val_acc: 0.9359\n",
      "Epoch 13/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0629 - acc: 0.9815 - val_loss: 1.1180 - val_acc: 0.8846\n",
      "Epoch 14/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1076 - acc: 0.9601 - val_loss: 1.0939 - val_acc: 0.9231\n",
      "Epoch 15/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0645 - acc: 0.9744 - val_loss: 1.1020 - val_acc: 0.9231\n",
      "Epoch 16/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0543 - acc: 0.9986 - val_loss: 1.1125 - val_acc: 0.9744\n",
      "Epoch 17/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0631 - acc: 0.9744 - val_loss: 1.0806 - val_acc: 0.9487\n",
      "Epoch 18/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0560 - acc: 0.9929 - val_loss: 1.0960 - val_acc: 0.9615\n",
      "Epoch 19/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0524 - acc: 0.9915 - val_loss: 1.0726 - val_acc: 0.9744\n",
      "Epoch 20/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0549 - acc: 0.9843 - val_loss: 1.0885 - val_acc: 0.9487\n",
      "Epoch 21/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0527 - acc: 0.9858 - val_loss: 1.0726 - val_acc: 0.9487\n",
      "Epoch 22/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0516 - acc: 0.9943 - val_loss: 1.0753 - val_acc: 0.9615\n",
      "Epoch 23/100\n",
      "702/702 [==============================] - 4s 6ms/step - loss: 1.0467 - acc: 0.9986 - val_loss: 1.0783 - val_acc: 0.9231\n",
      "Epoch 24/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0506 - acc: 0.9858 - val_loss: 1.0678 - val_acc: 0.9487\n",
      "Epoch 25/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0470 - acc: 0.9929 - val_loss: 1.0652 - val_acc: 0.9744\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0441 - acc: 0.9986 - val_loss: 1.0632 - val_acc: 0.9872\n",
      "Epoch 27/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0488 - acc: 0.9929 - val_loss: 1.0639 - val_acc: 0.9487\n",
      "Epoch 28/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0463 - acc: 0.9957 - val_loss: 1.0892 - val_acc: 0.9231\n",
      "Epoch 29/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0457 - acc: 0.9957 - val_loss: 1.0670 - val_acc: 0.9744\n",
      "Epoch 30/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0440 - acc: 0.9972 - val_loss: 1.0687 - val_acc: 0.9744\n",
      "Epoch 31/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0453 - acc: 0.9972 - val_loss: 1.0808 - val_acc: 0.9487\n",
      "Epoch 32/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0461 - acc: 0.9957 - val_loss: 1.0685 - val_acc: 0.9744\n",
      "Epoch 33/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0437 - acc: 0.9986 - val_loss: 1.0619 - val_acc: 0.9872\n",
      "Epoch 34/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0429 - acc: 0.9986 - val_loss: 1.0744 - val_acc: 0.9487\n",
      "Epoch 35/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0432 - acc: 0.9957 - val_loss: 1.0671 - val_acc: 0.9615\n",
      "Epoch 36/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0427 - acc: 0.9986 - val_loss: 1.0636 - val_acc: 0.9615\n",
      "Epoch 37/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0421 - acc: 0.9986 - val_loss: 1.0652 - val_acc: 0.9744\n",
      "Epoch 38/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0417 - acc: 0.9986 - val_loss: 1.1102 - val_acc: 0.9615\n",
      "Epoch 39/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0424 - acc: 0.9986 - val_loss: 1.0675 - val_acc: 0.9615\n",
      "Epoch 40/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0420 - acc: 0.9986 - val_loss: 1.0639 - val_acc: 0.9487\n",
      "Epoch 41/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0418 - acc: 0.9986 - val_loss: 1.0704 - val_acc: 0.9744\n",
      "Epoch 42/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0412 - acc: 1.0000 - val_loss: 1.0636 - val_acc: 0.9744\n",
      "Epoch 43/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0412 - acc: 1.0000 - val_loss: 1.0621 - val_acc: 0.9744\n",
      "Epoch 44/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0423 - acc: 0.9986 - val_loss: 1.0638 - val_acc: 0.9744\n",
      "Epoch 45/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0411 - acc: 1.0000 - val_loss: 1.0679 - val_acc: 0.9487\n",
      "Epoch 46/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0408 - acc: 1.0000 - val_loss: 1.0664 - val_acc: 0.9615\n",
      "Epoch 47/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0407 - acc: 1.0000 - val_loss: 1.0627 - val_acc: 0.9744\n",
      "Epoch 48/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0407 - acc: 1.0000 - val_loss: 1.0669 - val_acc: 0.9744\n",
      "Epoch 49/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0413 - acc: 0.9986 - val_loss: 1.0586 - val_acc: 0.9744\n",
      "Epoch 50/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0601 - val_acc: 0.9615\n",
      "Epoch 51/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0583 - val_acc: 0.9615\n",
      "Epoch 52/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0407 - acc: 1.0000 - val_loss: 1.0629 - val_acc: 0.9744\n",
      "Epoch 53/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0608 - val_acc: 0.9615\n",
      "Epoch 54/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0571 - val_acc: 0.9872\n",
      "Epoch 55/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0613 - val_acc: 0.9744\n",
      "Epoch 56/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0588 - val_acc: 0.9744\n",
      "Epoch 57/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0603 - val_acc: 0.9615\n",
      "Epoch 58/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0600 - val_acc: 0.9615\n",
      "Epoch 59/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0600 - val_acc: 0.9615\n",
      "Epoch 60/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0587 - val_acc: 0.9744\n",
      "Epoch 61/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0602 - val_acc: 0.9615\n",
      "Epoch 62/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0604 - val_acc: 0.9744\n",
      "Epoch 63/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0568 - val_acc: 0.9744\n",
      "Epoch 64/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0578 - val_acc: 0.9744\n",
      "Epoch 65/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0602 - val_acc: 0.9744\n",
      "Epoch 66/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0573 - val_acc: 0.9744\n",
      "Epoch 67/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0570 - val_acc: 0.9744\n",
      "Epoch 68/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0560 - val_acc: 0.9744\n",
      "Epoch 69/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0613 - val_acc: 0.9744\n",
      "Epoch 70/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0570 - val_acc: 0.9744\n",
      "Epoch 71/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0588 - val_acc: 0.9744\n",
      "Epoch 72/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0573 - val_acc: 0.9744\n",
      "Epoch 73/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0574 - val_acc: 0.9744\n",
      "Epoch 74/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0586 - val_acc: 0.9744\n",
      "Epoch 75/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0578 - val_acc: 0.9744\n",
      "Epoch 76/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0586 - val_acc: 0.9744\n",
      "Epoch 77/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0606 - val_acc: 0.9744\n",
      "Epoch 78/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0569 - val_acc: 0.9744\n",
      "Epoch 79/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0588 - val_acc: 0.9615\n",
      "Epoch 80/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0584 - val_acc: 0.9744\n",
      "Epoch 81/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0581 - val_acc: 0.9744\n",
      "Epoch 82/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0610 - val_acc: 0.9744\n",
      "Epoch 83/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0610 - val_acc: 0.9744\n",
      "Epoch 84/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0610 - val_acc: 0.9615\n",
      "Epoch 85/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0608 - val_acc: 0.9615\n",
      "Epoch 86/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0584 - val_acc: 0.9615\n",
      "Epoch 87/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0589 - val_acc: 0.9615\n",
      "Epoch 88/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0584 - val_acc: 0.9744\n",
      "Epoch 89/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0581 - val_acc: 0.9615\n",
      "Epoch 90/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0595 - val_acc: 0.9615\n",
      "Epoch 91/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0584 - val_acc: 0.9615\n",
      "Epoch 92/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0589 - val_acc: 0.9615\n",
      "Epoch 93/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0607 - val_acc: 0.9615\n",
      "Epoch 94/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0601 - val_acc: 0.9615\n",
      "Epoch 95/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0601 - val_acc: 0.9615\n",
      "Epoch 96/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0600 - val_acc: 0.9615\n",
      "Epoch 97/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0594 - val_acc: 0.9615\n",
      "Epoch 98/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0620 - val_acc: 0.9615\n",
      "Epoch 99/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0608 - val_acc: 0.9615\n",
      "Epoch 100/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0607 - val_acc: 0.9615\n",
      "78/78 [==============================] - 0s 321us/step\n",
      "Score for fold 8: loss of 1.0607681671778362; acc of 96.15384615384616%\n",
      "Train on 702 samples, validate on 78 samples\n",
      "Epoch 1/100\n",
      "702/702 [==============================] - 12s 17ms/step - loss: 2.3097 - acc: 0.3219 - val_loss: 1.8370 - val_acc: 0.6282\n",
      "Epoch 2/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.6466 - acc: 0.6752 - val_loss: 1.3741 - val_acc: 0.7179\n",
      "Epoch 3/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.3300 - acc: 0.8376 - val_loss: 1.3047 - val_acc: 0.8333\n",
      "Epoch 4/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.2535 - acc: 0.8746 - val_loss: 1.1405 - val_acc: 0.8846\n",
      "Epoch 5/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1562 - acc: 0.9160 - val_loss: 1.1408 - val_acc: 0.8590\n",
      "Epoch 6/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1167 - acc: 0.9402 - val_loss: 1.1189 - val_acc: 0.9359\n",
      "Epoch 7/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1218 - acc: 0.9516 - val_loss: 1.0882 - val_acc: 0.9615\n",
      "Epoch 8/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1084 - acc: 0.9430 - val_loss: 1.1727 - val_acc: 0.8590\n",
      "Epoch 9/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0830 - acc: 0.9658 - val_loss: 1.1281 - val_acc: 0.9231\n",
      "Epoch 10/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0744 - acc: 0.9715 - val_loss: 1.1893 - val_acc: 0.8462\n",
      "Epoch 11/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0988 - acc: 0.9658 - val_loss: 1.0759 - val_acc: 0.9615\n",
      "Epoch 12/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0725 - acc: 0.9644 - val_loss: 1.0826 - val_acc: 0.9487\n",
      "Epoch 13/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0657 - acc: 0.9801 - val_loss: 1.0813 - val_acc: 0.9615\n",
      "Epoch 14/100\n",
      "702/702 [==============================] - 4s 6ms/step - loss: 1.0568 - acc: 0.9900 - val_loss: 1.0881 - val_acc: 0.9103\n",
      "Epoch 15/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0575 - acc: 0.9815 - val_loss: 1.0971 - val_acc: 0.9487\n",
      "Epoch 16/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0569 - acc: 0.9929 - val_loss: 1.0701 - val_acc: 0.9744\n",
      "Epoch 17/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0490 - acc: 0.9957 - val_loss: 1.0690 - val_acc: 0.9744\n",
      "Epoch 18/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0494 - acc: 0.9929 - val_loss: 1.0728 - val_acc: 0.9615\n",
      "Epoch 19/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0489 - acc: 0.9957 - val_loss: 1.1103 - val_acc: 0.8974\n",
      "Epoch 20/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0515 - acc: 0.9929 - val_loss: 1.0771 - val_acc: 0.9615\n",
      "Epoch 21/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0477 - acc: 0.9957 - val_loss: 1.0893 - val_acc: 0.9615\n",
      "Epoch 22/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0474 - acc: 0.9972 - val_loss: 1.0694 - val_acc: 0.9744\n",
      "Epoch 23/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0465 - acc: 0.9957 - val_loss: 1.0660 - val_acc: 0.9615\n",
      "Epoch 24/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0450 - acc: 0.9986 - val_loss: 1.0730 - val_acc: 0.9487\n",
      "Epoch 25/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0433 - acc: 1.0000 - val_loss: 1.0693 - val_acc: 0.9615\n",
      "Epoch 26/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0427 - acc: 0.9986 - val_loss: 1.0668 - val_acc: 0.9744\n",
      "Epoch 27/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0434 - acc: 1.0000 - val_loss: 1.0681 - val_acc: 0.9744\n",
      "Epoch 28/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0437 - acc: 0.9957 - val_loss: 1.0652 - val_acc: 0.9615\n",
      "Epoch 29/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0417 - acc: 1.0000 - val_loss: 1.0664 - val_acc: 0.9744\n",
      "Epoch 30/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0415 - acc: 1.0000 - val_loss: 1.0652 - val_acc: 0.9744\n",
      "Epoch 31/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0428 - acc: 0.9986 - val_loss: 1.0651 - val_acc: 0.9744\n",
      "Epoch 32/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0413 - acc: 1.0000 - val_loss: 1.0649 - val_acc: 0.9744\n",
      "Epoch 33/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0411 - acc: 1.0000 - val_loss: 1.0655 - val_acc: 0.9744\n",
      "Epoch 34/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0421 - acc: 1.0000 - val_loss: 1.0646 - val_acc: 0.9744\n",
      "Epoch 35/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0412 - acc: 1.0000 - val_loss: 1.0643 - val_acc: 0.9744\n",
      "Epoch 36/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0412 - acc: 1.0000 - val_loss: 1.0639 - val_acc: 0.9744\n",
      "Epoch 37/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0409 - acc: 1.0000 - val_loss: 1.0649 - val_acc: 0.9744\n",
      "Epoch 38/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0410 - acc: 1.0000 - val_loss: 1.0670 - val_acc: 0.9744\n",
      "Epoch 39/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0408 - acc: 1.0000 - val_loss: 1.0655 - val_acc: 0.9744\n",
      "Epoch 40/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0408 - acc: 1.0000 - val_loss: 1.0646 - val_acc: 0.9744\n",
      "Epoch 41/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0408 - acc: 1.0000 - val_loss: 1.0645 - val_acc: 0.9744\n",
      "Epoch 42/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0407 - acc: 1.0000 - val_loss: 1.0643 - val_acc: 0.9744\n",
      "Epoch 43/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0654 - val_acc: 0.9744\n",
      "Epoch 44/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0634 - val_acc: 0.9744\n",
      "Epoch 45/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0644 - val_acc: 0.9744\n",
      "Epoch 46/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0636 - val_acc: 0.9744\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0634 - val_acc: 0.9744\n",
      "Epoch 48/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0407 - acc: 1.0000 - val_loss: 1.0643 - val_acc: 0.9744\n",
      "Epoch 49/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0641 - val_acc: 0.9744\n",
      "Epoch 50/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0643 - val_acc: 0.9744\n",
      "Epoch 51/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0637 - val_acc: 0.9744\n",
      "Epoch 52/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0634 - val_acc: 0.9744\n",
      "Epoch 53/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0643 - val_acc: 0.9744\n",
      "Epoch 54/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0641 - val_acc: 0.9744\n",
      "Epoch 55/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0633 - val_acc: 0.9744\n",
      "Epoch 56/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0635 - val_acc: 0.9744\n",
      "Epoch 57/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0639 - val_acc: 0.9744\n",
      "Epoch 58/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0626 - val_acc: 0.9744\n",
      "Epoch 59/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0632 - val_acc: 0.9744\n",
      "Epoch 60/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0635 - val_acc: 0.9744\n",
      "Epoch 61/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0644 - val_acc: 0.9744\n",
      "Epoch 62/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0637 - val_acc: 0.9744\n",
      "Epoch 63/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0641 - val_acc: 0.9744\n",
      "Epoch 64/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0634 - val_acc: 0.9744\n",
      "Epoch 65/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0637 - val_acc: 0.9744\n",
      "Epoch 66/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0633 - val_acc: 0.9744\n",
      "Epoch 67/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0638 - val_acc: 0.9744\n",
      "Epoch 68/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0636 - val_acc: 0.9744\n",
      "Epoch 69/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0634 - val_acc: 0.9744\n",
      "Epoch 70/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0635 - val_acc: 0.9744\n",
      "Epoch 71/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0635 - val_acc: 0.9744\n",
      "Epoch 72/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0635 - val_acc: 0.9744\n",
      "Epoch 73/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0640 - val_acc: 0.9744\n",
      "Epoch 74/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0636 - val_acc: 0.9744\n",
      "Epoch 75/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0639 - val_acc: 0.9744\n",
      "Epoch 76/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0640 - val_acc: 0.9744\n",
      "Epoch 77/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0632 - val_acc: 0.9744\n",
      "Epoch 78/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0638 - val_acc: 0.9744\n",
      "Epoch 79/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0636 - val_acc: 0.9744\n",
      "Epoch 80/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0641 - val_acc: 0.9744\n",
      "Epoch 81/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0639 - val_acc: 0.9744\n",
      "Epoch 82/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0639 - val_acc: 0.9744\n",
      "Epoch 83/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0638 - val_acc: 0.9744\n",
      "Epoch 84/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0640 - val_acc: 0.9744\n",
      "Epoch 85/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0638 - val_acc: 0.9744\n",
      "Epoch 86/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0636 - val_acc: 0.9744\n",
      "Epoch 87/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0639 - val_acc: 0.9744\n",
      "Epoch 88/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0638 - val_acc: 0.9744\n",
      "Epoch 89/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0639 - val_acc: 0.9744\n",
      "Epoch 90/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0637 - val_acc: 0.9744\n",
      "Epoch 91/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0636 - val_acc: 0.9744\n",
      "Epoch 92/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0637 - val_acc: 0.9744\n",
      "Epoch 93/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0638 - val_acc: 0.9744\n",
      "Epoch 94/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0639 - val_acc: 0.9744\n",
      "Epoch 95/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0638 - val_acc: 0.9744\n",
      "Epoch 96/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0639 - val_acc: 0.9744\n",
      "Epoch 97/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0638 - val_acc: 0.9744\n",
      "Epoch 98/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0638 - val_acc: 0.9744\n",
      "Epoch 99/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0639 - val_acc: 0.9744\n",
      "Epoch 100/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0639 - val_acc: 0.9744\n",
      "78/78 [==============================] - 0s 332us/step\n",
      "Score for fold 9: loss of 1.0640052618124547; acc of 97.43589697740018%\n",
      "Train on 702 samples, validate on 78 samples\n",
      "Epoch 1/100\n",
      "702/702 [==============================] - 12s 17ms/step - loss: 2.2367 - acc: 0.3533 - val_loss: 1.9084 - val_acc: 0.4615\n",
      "Epoch 2/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.6241 - acc: 0.7023 - val_loss: 1.5668 - val_acc: 0.7436\n",
      "Epoch 3/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.3408 - acc: 0.8376 - val_loss: 1.2848 - val_acc: 0.8462\n",
      "Epoch 4/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.2274 - acc: 0.8704 - val_loss: 1.3637 - val_acc: 0.7564\n",
      "Epoch 5/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1987 - acc: 0.8789 - val_loss: 1.1238 - val_acc: 0.9744\n",
      "Epoch 6/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1541 - acc: 0.9046 - val_loss: 1.2011 - val_acc: 0.7436\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1092 - acc: 0.9387 - val_loss: 1.1499 - val_acc: 0.8974\n",
      "Epoch 8/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1196 - acc: 0.9387 - val_loss: 1.1159 - val_acc: 0.9487\n",
      "Epoch 9/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1296 - acc: 0.9202 - val_loss: 1.1310 - val_acc: 0.8846\n",
      "Epoch 10/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0759 - acc: 0.9672 - val_loss: 1.0845 - val_acc: 0.9744\n",
      "Epoch 11/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0993 - acc: 0.9587 - val_loss: 1.0803 - val_acc: 0.9744\n",
      "Epoch 12/100\n",
      "702/702 [==============================] - 4s 6ms/step - loss: 1.0930 - acc: 0.9516 - val_loss: 1.0782 - val_acc: 0.9487\n",
      "Epoch 13/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0565 - acc: 0.9929 - val_loss: 1.0800 - val_acc: 0.9615\n",
      "Epoch 14/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0672 - acc: 0.9801 - val_loss: 1.0671 - val_acc: 0.9615\n",
      "Epoch 15/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0733 - acc: 0.9886 - val_loss: 1.0678 - val_acc: 0.9487\n",
      "Epoch 16/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0543 - acc: 0.9915 - val_loss: 1.0979 - val_acc: 0.9231\n",
      "Epoch 17/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0524 - acc: 0.9900 - val_loss: 1.0629 - val_acc: 0.9615\n",
      "Epoch 18/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0596 - acc: 0.9872 - val_loss: 1.2044 - val_acc: 0.8846\n",
      "Epoch 19/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0583 - acc: 0.9886 - val_loss: 1.0575 - val_acc: 0.9744\n",
      "Epoch 20/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0480 - acc: 0.9957 - val_loss: 1.0635 - val_acc: 0.9487\n",
      "Epoch 21/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0488 - acc: 0.9957 - val_loss: 1.0602 - val_acc: 0.9487\n",
      "Epoch 22/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0515 - acc: 0.9929 - val_loss: 1.0561 - val_acc: 0.9744\n",
      "Epoch 23/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0456 - acc: 0.9986 - val_loss: 1.0559 - val_acc: 0.9615\n",
      "Epoch 24/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0456 - acc: 0.9986 - val_loss: 1.0575 - val_acc: 0.9872\n",
      "Epoch 25/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0480 - acc: 0.9957 - val_loss: 1.0947 - val_acc: 0.9487\n",
      "Epoch 26/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0463 - acc: 0.9957 - val_loss: 1.0470 - val_acc: 1.0000\n",
      "Epoch 27/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0445 - acc: 0.9986 - val_loss: 1.0628 - val_acc: 0.9615\n",
      "Epoch 28/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0465 - acc: 0.9943 - val_loss: 1.0546 - val_acc: 1.0000\n",
      "Epoch 29/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0465 - acc: 0.9929 - val_loss: 1.0671 - val_acc: 0.9872\n",
      "Epoch 30/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0455 - acc: 0.9957 - val_loss: 1.0559 - val_acc: 0.9872\n",
      "Epoch 31/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0438 - acc: 0.9986 - val_loss: 1.0996 - val_acc: 0.9103\n",
      "Epoch 32/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0463 - acc: 0.9957 - val_loss: 1.0527 - val_acc: 0.9744\n",
      "Epoch 33/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0461 - acc: 0.9943 - val_loss: 1.0768 - val_acc: 0.9615\n",
      "Epoch 34/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0437 - acc: 0.9972 - val_loss: 1.0550 - val_acc: 0.9615\n",
      "Epoch 35/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0426 - acc: 0.9986 - val_loss: 1.0563 - val_acc: 0.9615\n",
      "Epoch 36/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0426 - acc: 0.9986 - val_loss: 1.0510 - val_acc: 0.9744\n",
      "Epoch 37/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0424 - acc: 0.9986 - val_loss: 1.0542 - val_acc: 0.9872\n",
      "Epoch 38/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0439 - acc: 0.9972 - val_loss: 1.0557 - val_acc: 0.9744\n",
      "Epoch 39/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0422 - acc: 0.9986 - val_loss: 1.0489 - val_acc: 0.9872\n",
      "Epoch 40/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0421 - acc: 0.9986 - val_loss: 1.0482 - val_acc: 0.9744\n",
      "Epoch 41/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0417 - acc: 0.9986 - val_loss: 1.0492 - val_acc: 0.9872\n",
      "Epoch 42/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0417 - acc: 0.9986 - val_loss: 1.0488 - val_acc: 0.9744\n",
      "Epoch 43/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0420 - acc: 0.9986 - val_loss: 1.0487 - val_acc: 0.9872\n",
      "Epoch 44/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0415 - acc: 1.0000 - val_loss: 1.0478 - val_acc: 0.9744\n",
      "Epoch 45/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0414 - acc: 1.0000 - val_loss: 1.0655 - val_acc: 0.9872\n",
      "Epoch 46/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0414 - acc: 1.0000 - val_loss: 1.0458 - val_acc: 1.0000\n",
      "Epoch 47/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0411 - acc: 1.0000 - val_loss: 1.0476 - val_acc: 0.9872\n",
      "Epoch 48/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0411 - acc: 1.0000 - val_loss: 1.0457 - val_acc: 0.9872\n",
      "Epoch 49/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0409 - acc: 1.0000 - val_loss: 1.0479 - val_acc: 0.9872\n",
      "Epoch 50/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0412 - acc: 0.9986 - val_loss: 1.0491 - val_acc: 0.9872\n",
      "Epoch 51/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0410 - acc: 1.0000 - val_loss: 1.0483 - val_acc: 1.0000\n",
      "Epoch 52/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0407 - acc: 1.0000 - val_loss: 1.0476 - val_acc: 0.9872\n",
      "Epoch 53/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0407 - acc: 1.0000 - val_loss: 1.0514 - val_acc: 0.9872\n",
      "Epoch 54/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0408 - acc: 1.0000 - val_loss: 1.0489 - val_acc: 1.0000\n",
      "Epoch 55/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0407 - acc: 1.0000 - val_loss: 1.0469 - val_acc: 0.9872\n",
      "Epoch 56/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0457 - val_acc: 1.0000\n",
      "Epoch 57/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0408 - acc: 1.0000 - val_loss: 1.0459 - val_acc: 1.0000\n",
      "Epoch 58/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0455 - val_acc: 1.0000\n",
      "Epoch 59/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0472 - val_acc: 1.0000\n",
      "Epoch 60/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0462 - val_acc: 1.0000\n",
      "Epoch 61/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0407 - acc: 1.0000 - val_loss: 1.0476 - val_acc: 1.0000\n",
      "Epoch 62/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0487 - val_acc: 0.9872\n",
      "Epoch 63/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0469 - val_acc: 1.0000\n",
      "Epoch 64/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0464 - val_acc: 1.0000\n",
      "Epoch 65/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0441 - val_acc: 1.0000\n",
      "Epoch 66/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0469 - val_acc: 1.0000\n",
      "Epoch 67/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0484 - val_acc: 1.0000\n",
      "Epoch 68/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0494 - val_acc: 0.9872\n",
      "Epoch 69/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0516 - val_acc: 0.9872\n",
      "Epoch 70/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0454 - val_acc: 1.0000\n",
      "Epoch 71/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0452 - val_acc: 1.0000\n",
      "Epoch 72/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0481 - val_acc: 1.0000\n",
      "Epoch 73/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0470 - val_acc: 0.9872\n",
      "Epoch 74/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0530 - val_acc: 1.0000\n",
      "Epoch 75/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0476 - val_acc: 0.9872\n",
      "Epoch 76/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0474 - val_acc: 1.0000\n",
      "Epoch 77/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0498 - val_acc: 1.0000\n",
      "Epoch 78/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0463 - val_acc: 1.0000\n",
      "Epoch 79/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0465 - val_acc: 1.0000\n",
      "Epoch 80/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0530 - val_acc: 1.0000\n",
      "Epoch 81/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0495 - val_acc: 1.0000\n",
      "Epoch 82/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0512 - val_acc: 1.0000\n",
      "Epoch 83/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0537 - val_acc: 1.0000\n",
      "Epoch 84/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0497 - val_acc: 1.0000\n",
      "Epoch 85/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0477 - val_acc: 1.0000\n",
      "Epoch 86/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0522 - val_acc: 1.0000\n",
      "Epoch 87/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0498 - val_acc: 1.0000\n",
      "Epoch 88/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0502 - val_acc: 1.0000\n",
      "Epoch 89/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0501 - val_acc: 1.0000\n",
      "Epoch 90/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0519 - val_acc: 1.0000\n",
      "Epoch 91/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0492 - val_acc: 1.0000\n",
      "Epoch 92/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0500 - val_acc: 1.0000\n",
      "Epoch 93/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0482 - val_acc: 1.0000\n",
      "Epoch 94/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0480 - val_acc: 1.0000\n",
      "Epoch 95/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0465 - val_acc: 1.0000\n",
      "Epoch 96/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0482 - val_acc: 1.0000\n",
      "Epoch 97/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0482 - val_acc: 1.0000\n",
      "Epoch 98/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0483 - val_acc: 1.0000\n",
      "Epoch 99/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0495 - val_acc: 1.0000\n",
      "Epoch 100/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0471 - val_acc: 1.0000\n",
      "78/78 [==============================] - 0s 321us/step\n",
      "Score for fold 10: loss of 1.0443033805260291; acc of 100.0%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1296x1296 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98.0769229393739\n",
      "Train on 702 samples, validate on 78 samples\n",
      "Epoch 1/100\n",
      "702/702 [==============================] - 12s 17ms/step - loss: 2.2713 - acc: 0.3006 - val_loss: 2.1327 - val_acc: 0.3077\n",
      "Epoch 2/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.6385 - acc: 0.6923 - val_loss: 1.5318 - val_acc: 0.7179\n",
      "Epoch 3/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.3888 - acc: 0.8291 - val_loss: 1.5244 - val_acc: 0.6154\n",
      "Epoch 4/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.2630 - acc: 0.8718 - val_loss: 1.2058 - val_acc: 0.8718\n",
      "Epoch 5/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1997 - acc: 0.8889 - val_loss: 1.1956 - val_acc: 0.7949\n",
      "Epoch 6/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1386 - acc: 0.9359 - val_loss: 1.3009 - val_acc: 0.8205\n",
      "Epoch 7/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1049 - acc: 0.9573 - val_loss: 1.1280 - val_acc: 0.8846\n",
      "Epoch 8/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1135 - acc: 0.9544 - val_loss: 1.1284 - val_acc: 0.8462\n",
      "Epoch 9/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1185 - acc: 0.9487 - val_loss: 1.1195 - val_acc: 0.9359\n",
      "Epoch 10/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0698 - acc: 0.9815 - val_loss: 1.1091 - val_acc: 0.9103\n",
      "Epoch 11/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1000 - acc: 0.9587 - val_loss: 1.0951 - val_acc: 0.9103\n",
      "Epoch 12/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0780 - acc: 0.9829 - val_loss: 1.0876 - val_acc: 0.9231\n",
      "Epoch 13/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0609 - acc: 0.9886 - val_loss: 1.1622 - val_acc: 0.8205\n",
      "Epoch 14/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0809 - acc: 0.9729 - val_loss: 1.0854 - val_acc: 0.9487\n",
      "Epoch 15/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0549 - acc: 0.9886 - val_loss: 1.0963 - val_acc: 0.9359\n",
      "Epoch 16/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0666 - acc: 0.9772 - val_loss: 1.0901 - val_acc: 0.9359\n",
      "Epoch 17/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0701 - acc: 0.9786 - val_loss: 1.0909 - val_acc: 0.9359\n",
      "Epoch 18/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0542 - acc: 0.9815 - val_loss: 1.0743 - val_acc: 0.9615\n",
      "Epoch 19/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0593 - acc: 0.9858 - val_loss: 1.0854 - val_acc: 0.9615\n",
      "Epoch 20/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0511 - acc: 0.9957 - val_loss: 1.0816 - val_acc: 0.9487\n",
      "Epoch 21/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0479 - acc: 0.9972 - val_loss: 1.0933 - val_acc: 0.9359\n",
      "Epoch 22/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0741 - acc: 0.9729 - val_loss: 1.0822 - val_acc: 0.9615\n",
      "Epoch 23/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0572 - acc: 0.9915 - val_loss: 1.0787 - val_acc: 0.9615\n",
      "Epoch 24/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0475 - acc: 0.9957 - val_loss: 1.0789 - val_acc: 0.9487\n",
      "Epoch 25/100\n",
      "702/702 [==============================] - 4s 6ms/step - loss: 1.0490 - acc: 0.9972 - val_loss: 1.0930 - val_acc: 0.9231\n",
      "Epoch 26/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0464 - acc: 0.9972 - val_loss: 1.0929 - val_acc: 0.9231\n",
      "Epoch 27/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0531 - acc: 0.9872 - val_loss: 1.1007 - val_acc: 0.9359\n",
      "Epoch 28/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0477 - acc: 0.9943 - val_loss: 1.0782 - val_acc: 0.9359\n",
      "Epoch 29/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0450 - acc: 0.9957 - val_loss: 1.0790 - val_acc: 0.9231\n",
      "Epoch 30/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0522 - acc: 0.9900 - val_loss: 1.0787 - val_acc: 0.9359\n",
      "Epoch 31/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0430 - acc: 1.0000 - val_loss: 1.0779 - val_acc: 0.9487\n",
      "Epoch 32/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0430 - acc: 1.0000 - val_loss: 1.0872 - val_acc: 0.9231\n",
      "Epoch 33/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0429 - acc: 0.9986 - val_loss: 1.0782 - val_acc: 0.9359\n",
      "Epoch 34/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0454 - acc: 0.9957 - val_loss: 1.0784 - val_acc: 0.9359\n",
      "Epoch 35/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0432 - acc: 0.9972 - val_loss: 1.0797 - val_acc: 0.9487\n",
      "Epoch 36/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0421 - acc: 1.0000 - val_loss: 1.0774 - val_acc: 0.9359\n",
      "Epoch 37/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0423 - acc: 1.0000 - val_loss: 1.0828 - val_acc: 0.9487\n",
      "Epoch 38/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0416 - acc: 1.0000 - val_loss: 1.0804 - val_acc: 0.9487\n",
      "Epoch 39/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0427 - acc: 0.9986 - val_loss: 1.0771 - val_acc: 0.9487\n",
      "Epoch 40/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0412 - acc: 1.0000 - val_loss: 1.0776 - val_acc: 0.9487\n",
      "Epoch 41/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0411 - acc: 1.0000 - val_loss: 1.0780 - val_acc: 0.9487\n",
      "Epoch 42/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0412 - acc: 1.0000 - val_loss: 1.0790 - val_acc: 0.9359\n",
      "Epoch 43/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0413 - acc: 1.0000 - val_loss: 1.0902 - val_acc: 0.9231\n",
      "Epoch 44/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0426 - acc: 0.9972 - val_loss: 1.0780 - val_acc: 0.9487\n",
      "Epoch 45/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0408 - acc: 1.0000 - val_loss: 1.0784 - val_acc: 0.9487\n",
      "Epoch 46/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0408 - acc: 1.0000 - val_loss: 1.0755 - val_acc: 0.9487\n",
      "Epoch 47/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0408 - acc: 1.0000 - val_loss: 1.0789 - val_acc: 0.9487\n",
      "Epoch 48/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0416 - acc: 1.0000 - val_loss: 1.0740 - val_acc: 0.9487\n",
      "Epoch 49/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0407 - acc: 1.0000 - val_loss: 1.0765 - val_acc: 0.9487\n",
      "Epoch 50/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0769 - val_acc: 0.9359\n",
      "Epoch 51/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0762 - val_acc: 0.9487\n",
      "Epoch 52/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0766 - val_acc: 0.9487\n",
      "Epoch 53/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0771 - val_acc: 0.9487\n",
      "Epoch 54/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0761 - val_acc: 0.9487\n",
      "Epoch 55/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0748 - val_acc: 0.9487\n",
      "Epoch 56/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0763 - val_acc: 0.9487\n",
      "Epoch 57/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0755 - val_acc: 0.9487\n",
      "Epoch 58/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0766 - val_acc: 0.9487\n",
      "Epoch 59/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0783 - val_acc: 0.9487\n",
      "Epoch 60/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0766 - val_acc: 0.9487\n",
      "Epoch 61/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0777 - val_acc: 0.9487\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 62/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0770 - val_acc: 0.9487\n",
      "Epoch 63/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0743 - val_acc: 0.9487\n",
      "Epoch 64/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0745 - val_acc: 0.9487\n",
      "Epoch 65/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0773 - val_acc: 0.9487\n",
      "Epoch 66/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0776 - val_acc: 0.9487\n",
      "Epoch 67/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0762 - val_acc: 0.9487\n",
      "Epoch 68/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0755 - val_acc: 0.9487\n",
      "Epoch 69/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0794 - val_acc: 0.9487\n",
      "Epoch 70/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0745 - val_acc: 0.9487\n",
      "Epoch 71/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0745 - val_acc: 0.9487\n",
      "Epoch 72/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0751 - val_acc: 0.9487\n",
      "Epoch 73/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0750 - val_acc: 0.9487\n",
      "Epoch 74/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0753 - val_acc: 0.9487\n",
      "Epoch 75/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0758 - val_acc: 0.9487\n",
      "Epoch 76/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0763 - val_acc: 0.9487\n",
      "Epoch 77/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0770 - val_acc: 0.9487\n",
      "Epoch 78/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0758 - val_acc: 0.9487\n",
      "Epoch 79/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0732 - val_acc: 0.9487\n",
      "Epoch 80/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0750 - val_acc: 0.9487\n",
      "Epoch 81/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0756 - val_acc: 0.9487\n",
      "Epoch 82/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0753 - val_acc: 0.9487\n",
      "Epoch 83/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0764 - val_acc: 0.9487\n",
      "Epoch 84/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0760 - val_acc: 0.9487\n",
      "Epoch 85/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0752 - val_acc: 0.9487\n",
      "Epoch 86/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0753 - val_acc: 0.9487\n",
      "Epoch 87/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0753 - val_acc: 0.9487\n",
      "Epoch 88/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0762 - val_acc: 0.9487\n",
      "Epoch 89/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0765 - val_acc: 0.9487\n",
      "Epoch 90/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0754 - val_acc: 0.9487\n",
      "Epoch 91/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0759 - val_acc: 0.9487\n",
      "Epoch 92/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0754 - val_acc: 0.9487\n",
      "Epoch 93/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0763 - val_acc: 0.9487\n",
      "Epoch 94/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0762 - val_acc: 0.9487\n",
      "Epoch 95/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0759 - val_acc: 0.9487\n",
      "Epoch 96/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0762 - val_acc: 0.9487\n",
      "Epoch 97/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0760 - val_acc: 0.9487\n",
      "Epoch 98/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0759 - val_acc: 0.9487\n",
      "Epoch 99/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0751 - val_acc: 0.9487\n",
      "Epoch 100/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0751 - val_acc: 0.9487\n",
      "78/78 [==============================] - 0s 350us/step\n",
      "Score for fold 1: loss of 1.0750177793013744; acc of 94.87179487179486%\n",
      "Train on 702 samples, validate on 78 samples\n",
      "Epoch 1/100\n",
      "702/702 [==============================] - 12s 18ms/step - loss: 2.2213 - acc: 0.3561 - val_loss: 2.0277 - val_acc: 0.3333\n",
      "Epoch 2/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.6791 - acc: 0.6496 - val_loss: 1.5614 - val_acc: 0.6923\n",
      "Epoch 3/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.3875 - acc: 0.8105 - val_loss: 1.4497 - val_acc: 0.6795\n",
      "Epoch 4/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.2557 - acc: 0.8476 - val_loss: 1.3931 - val_acc: 0.8333\n",
      "Epoch 5/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.2075 - acc: 0.8803 - val_loss: 1.2084 - val_acc: 0.7949\n",
      "Epoch 6/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1494 - acc: 0.9046 - val_loss: 1.1498 - val_acc: 0.8974\n",
      "Epoch 7/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1207 - acc: 0.9330 - val_loss: 1.2659 - val_acc: 0.7692\n",
      "Epoch 8/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1090 - acc: 0.9316 - val_loss: 1.2227 - val_acc: 0.8077\n",
      "Epoch 9/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0887 - acc: 0.9644 - val_loss: 1.3225 - val_acc: 0.8077\n",
      "Epoch 10/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1354 - acc: 0.9387 - val_loss: 1.1840 - val_acc: 0.8205\n",
      "Epoch 11/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0724 - acc: 0.9829 - val_loss: 1.0743 - val_acc: 0.9744\n",
      "Epoch 12/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0835 - acc: 0.9658 - val_loss: 1.0808 - val_acc: 0.9615\n",
      "Epoch 13/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0752 - acc: 0.9729 - val_loss: 1.1040 - val_acc: 0.9359\n",
      "Epoch 14/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0609 - acc: 0.9843 - val_loss: 1.0895 - val_acc: 0.9744\n",
      "Epoch 15/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0633 - acc: 0.9772 - val_loss: 1.0708 - val_acc: 0.9872\n",
      "Epoch 16/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0689 - acc: 0.9758 - val_loss: 1.0666 - val_acc: 0.9872\n",
      "Epoch 17/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0677 - acc: 0.9786 - val_loss: 1.1872 - val_acc: 0.8846\n",
      "Epoch 18/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0682 - acc: 0.9786 - val_loss: 1.0698 - val_acc: 0.9615\n",
      "Epoch 19/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0538 - acc: 0.9915 - val_loss: 1.0810 - val_acc: 0.9744\n",
      "Epoch 20/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0614 - acc: 0.9758 - val_loss: 1.4464 - val_acc: 0.8333\n",
      "Epoch 21/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0645 - acc: 0.9843 - val_loss: 1.0549 - val_acc: 0.9744\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/100\n",
      "702/702 [==============================] - 4s 6ms/step - loss: 1.0625 - acc: 0.9829 - val_loss: 1.0677 - val_acc: 0.9744\n",
      "Epoch 23/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0476 - acc: 0.9972 - val_loss: 1.0656 - val_acc: 0.9744\n",
      "Epoch 24/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0467 - acc: 0.9972 - val_loss: 1.0613 - val_acc: 0.9744\n",
      "Epoch 25/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0497 - acc: 0.9900 - val_loss: 1.0833 - val_acc: 0.9359\n",
      "Epoch 26/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0467 - acc: 0.9957 - val_loss: 1.0618 - val_acc: 0.9487\n",
      "Epoch 27/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0472 - acc: 0.9943 - val_loss: 1.0523 - val_acc: 0.9744\n",
      "Epoch 28/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0489 - acc: 0.9900 - val_loss: 1.0560 - val_acc: 0.9872\n",
      "Epoch 29/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0457 - acc: 0.9972 - val_loss: 1.0463 - val_acc: 1.0000\n",
      "Epoch 30/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0437 - acc: 0.9986 - val_loss: 1.0532 - val_acc: 0.9872\n",
      "Epoch 31/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0434 - acc: 0.9986 - val_loss: 1.0541 - val_acc: 0.9744\n",
      "Epoch 32/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0432 - acc: 0.9986 - val_loss: 1.0499 - val_acc: 0.9744\n",
      "Epoch 33/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0453 - acc: 0.9943 - val_loss: 1.0456 - val_acc: 0.9872\n",
      "Epoch 34/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0425 - acc: 0.9986 - val_loss: 1.0467 - val_acc: 0.9872\n",
      "Epoch 35/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0428 - acc: 0.9986 - val_loss: 1.0722 - val_acc: 0.9744\n",
      "Epoch 36/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0423 - acc: 0.9986 - val_loss: 1.0489 - val_acc: 0.9872\n",
      "Epoch 37/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0418 - acc: 1.0000 - val_loss: 1.0474 - val_acc: 0.9744\n",
      "Epoch 38/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0420 - acc: 1.0000 - val_loss: 1.0470 - val_acc: 0.9744\n",
      "Epoch 39/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0414 - acc: 1.0000 - val_loss: 1.0485 - val_acc: 0.9744\n",
      "Epoch 40/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0422 - acc: 1.0000 - val_loss: 1.0480 - val_acc: 1.0000\n",
      "Epoch 41/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0414 - acc: 1.0000 - val_loss: 1.0502 - val_acc: 0.9872\n",
      "Epoch 42/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0415 - acc: 1.0000 - val_loss: 1.0485 - val_acc: 0.9872\n",
      "Epoch 43/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0410 - acc: 1.0000 - val_loss: 1.0520 - val_acc: 0.9744\n",
      "Epoch 44/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0410 - acc: 1.0000 - val_loss: 1.0689 - val_acc: 0.9744\n",
      "Epoch 45/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0413 - acc: 1.0000 - val_loss: 1.0451 - val_acc: 1.0000\n",
      "Epoch 46/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0411 - acc: 1.0000 - val_loss: 1.0435 - val_acc: 1.0000\n",
      "Epoch 47/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0409 - acc: 1.0000 - val_loss: 1.0439 - val_acc: 1.0000\n",
      "Epoch 48/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0409 - acc: 1.0000 - val_loss: 1.0512 - val_acc: 0.9872\n",
      "Epoch 49/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0409 - acc: 1.0000 - val_loss: 1.0446 - val_acc: 0.9872\n",
      "Epoch 50/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0407 - acc: 1.0000 - val_loss: 1.0455 - val_acc: 0.9872\n",
      "Epoch 51/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0475 - val_acc: 0.9872\n",
      "Epoch 52/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0407 - acc: 1.0000 - val_loss: 1.0484 - val_acc: 0.9872\n",
      "Epoch 53/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0407 - acc: 1.0000 - val_loss: 1.0451 - val_acc: 0.9872\n",
      "Epoch 54/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0441 - val_acc: 0.9872\n",
      "Epoch 55/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0407 - acc: 1.0000 - val_loss: 1.0443 - val_acc: 1.0000\n",
      "Epoch 56/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0445 - val_acc: 0.9872\n",
      "Epoch 57/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0441 - val_acc: 1.0000\n",
      "Epoch 58/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0449 - val_acc: 1.0000\n",
      "Epoch 59/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0435 - val_acc: 1.0000\n",
      "Epoch 60/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0461 - val_acc: 0.9872\n",
      "Epoch 61/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0444 - val_acc: 0.9872\n",
      "Epoch 62/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0447 - val_acc: 0.9872\n",
      "Epoch 63/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0451 - val_acc: 0.9872\n",
      "Epoch 64/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0460 - val_acc: 0.9872\n",
      "Epoch 65/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0446 - val_acc: 1.0000\n",
      "Epoch 66/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0442 - val_acc: 1.0000\n",
      "Epoch 67/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0434 - val_acc: 1.0000\n",
      "Epoch 68/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0435 - val_acc: 1.0000\n",
      "Epoch 69/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0438 - val_acc: 0.9872\n",
      "Epoch 70/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0438 - val_acc: 1.0000\n",
      "Epoch 71/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0430 - val_acc: 1.0000\n",
      "Epoch 72/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0434 - val_acc: 1.0000\n",
      "Epoch 73/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0440 - val_acc: 0.9872\n",
      "Epoch 74/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0432 - val_acc: 1.0000\n",
      "Epoch 75/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0436 - val_acc: 1.0000\n",
      "Epoch 76/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0437 - val_acc: 1.0000\n",
      "Epoch 77/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0441 - val_acc: 1.0000\n",
      "Epoch 78/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0434 - val_acc: 1.0000\n",
      "Epoch 79/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0429 - val_acc: 1.0000\n",
      "Epoch 80/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0436 - val_acc: 1.0000\n",
      "Epoch 81/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0436 - val_acc: 1.0000\n",
      "Epoch 82/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0432 - val_acc: 1.0000\n",
      "Epoch 83/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0435 - val_acc: 1.0000\n",
      "Epoch 84/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0440 - val_acc: 1.0000\n",
      "Epoch 85/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0442 - val_acc: 0.9872\n",
      "Epoch 86/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0454 - val_acc: 0.9872\n",
      "Epoch 87/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0457 - val_acc: 0.9872\n",
      "Epoch 88/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0454 - val_acc: 0.9872\n",
      "Epoch 89/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0456 - val_acc: 0.9872\n",
      "Epoch 90/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0449 - val_acc: 0.9872\n",
      "Epoch 91/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0453 - val_acc: 0.9872\n",
      "Epoch 92/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0450 - val_acc: 0.9872\n",
      "Epoch 93/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0452 - val_acc: 0.9872\n",
      "Epoch 94/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0459 - val_acc: 0.9872\n",
      "Epoch 95/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0450 - val_acc: 0.9872\n",
      "Epoch 96/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0449 - val_acc: 0.9872\n",
      "Epoch 97/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0447 - val_acc: 0.9872\n",
      "Epoch 98/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0451 - val_acc: 0.9872\n",
      "Epoch 99/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0450 - val_acc: 0.9872\n",
      "Epoch 100/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0452 - val_acc: 0.9872\n",
      "78/78 [==============================] - 0s 334us/step\n",
      "Score for fold 2: loss of 1.0457770029703777; acc of 98.71794871794873%\n",
      "Train on 702 samples, validate on 78 samples\n",
      "Epoch 1/100\n",
      "702/702 [==============================] - 13s 18ms/step - loss: 2.3229 - acc: 0.2906 - val_loss: 1.8552 - val_acc: 0.5000\n",
      "Epoch 2/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.6627 - acc: 0.6795 - val_loss: 1.7026 - val_acc: 0.5513\n",
      "Epoch 3/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.3880 - acc: 0.8034 - val_loss: 1.6889 - val_acc: 0.5897\n",
      "Epoch 4/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.2726 - acc: 0.8632 - val_loss: 1.3442 - val_acc: 0.7436\n",
      "Epoch 5/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1993 - acc: 0.9088 - val_loss: 1.2184 - val_acc: 0.8462\n",
      "Epoch 6/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1591 - acc: 0.9131 - val_loss: 1.1546 - val_acc: 0.9103\n",
      "Epoch 7/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1393 - acc: 0.9231 - val_loss: 1.1640 - val_acc: 0.8974\n",
      "Epoch 8/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0969 - acc: 0.9573 - val_loss: 1.1528 - val_acc: 0.8077\n",
      "Epoch 9/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0920 - acc: 0.9501 - val_loss: 1.1366 - val_acc: 0.9359\n",
      "Epoch 10/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0880 - acc: 0.9615 - val_loss: 1.1140 - val_acc: 0.9231\n",
      "Epoch 11/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0705 - acc: 0.9729 - val_loss: 1.0917 - val_acc: 0.8974\n",
      "Epoch 12/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0905 - acc: 0.9473 - val_loss: 1.0971 - val_acc: 0.9487\n",
      "Epoch 13/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0799 - acc: 0.9758 - val_loss: 1.0762 - val_acc: 0.9872\n",
      "Epoch 14/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0755 - acc: 0.9715 - val_loss: 1.0969 - val_acc: 0.8974\n",
      "Epoch 15/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0607 - acc: 0.9786 - val_loss: 1.0872 - val_acc: 0.9744\n",
      "Epoch 16/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0599 - acc: 0.9801 - val_loss: 1.0668 - val_acc: 0.9872\n",
      "Epoch 17/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0633 - acc: 0.9786 - val_loss: 1.1041 - val_acc: 0.9487\n",
      "Epoch 18/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0550 - acc: 0.9900 - val_loss: 1.1019 - val_acc: 0.9615\n",
      "Epoch 19/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0531 - acc: 0.9886 - val_loss: 1.0577 - val_acc: 0.9872\n",
      "Epoch 20/100\n",
      "702/702 [==============================] - 4s 6ms/step - loss: 1.0579 - acc: 0.9815 - val_loss: 1.0981 - val_acc: 0.8974\n",
      "Epoch 21/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0497 - acc: 0.9943 - val_loss: 1.0615 - val_acc: 0.9615\n",
      "Epoch 22/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0489 - acc: 0.9915 - val_loss: 1.0572 - val_acc: 0.9615\n",
      "Epoch 23/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0461 - acc: 0.9972 - val_loss: 1.0524 - val_acc: 0.9872\n",
      "Epoch 24/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0439 - acc: 1.0000 - val_loss: 1.0564 - val_acc: 0.9744\n",
      "Epoch 25/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0477 - acc: 0.9972 - val_loss: 1.0803 - val_acc: 0.9744\n",
      "Epoch 26/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0460 - acc: 0.9957 - val_loss: 1.1296 - val_acc: 0.9487\n",
      "Epoch 27/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0546 - acc: 0.9943 - val_loss: 1.0570 - val_acc: 0.9615\n",
      "Epoch 28/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0423 - acc: 1.0000 - val_loss: 1.0576 - val_acc: 1.0000\n",
      "Epoch 29/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0421 - acc: 1.0000 - val_loss: 1.0615 - val_acc: 0.9744\n",
      "Epoch 30/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0445 - acc: 0.9972 - val_loss: 1.0583 - val_acc: 0.9872\n",
      "Epoch 31/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0428 - acc: 0.9972 - val_loss: 1.0752 - val_acc: 0.9872\n",
      "Epoch 32/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0422 - acc: 1.0000 - val_loss: 1.0600 - val_acc: 1.0000\n",
      "Epoch 33/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0418 - acc: 1.0000 - val_loss: 1.0479 - val_acc: 0.9872\n",
      "Epoch 34/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0425 - acc: 1.0000 - val_loss: 1.0525 - val_acc: 0.9872\n",
      "Epoch 35/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0433 - acc: 0.9957 - val_loss: 1.1234 - val_acc: 0.9487\n",
      "Epoch 36/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0414 - acc: 1.0000 - val_loss: 1.0495 - val_acc: 0.9872\n",
      "Epoch 37/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0415 - acc: 0.9986 - val_loss: 1.0555 - val_acc: 0.9872\n",
      "Epoch 38/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0414 - acc: 1.0000 - val_loss: 1.0488 - val_acc: 0.9872\n",
      "Epoch 39/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0410 - acc: 1.0000 - val_loss: 1.0620 - val_acc: 0.9744\n",
      "Epoch 40/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0419 - acc: 1.0000 - val_loss: 1.0477 - val_acc: 0.9872\n",
      "Epoch 41/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0416 - acc: 0.9986 - val_loss: 1.0646 - val_acc: 0.9744\n",
      "Epoch 42/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0407 - acc: 1.0000 - val_loss: 1.0484 - val_acc: 0.9872\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0409 - acc: 1.0000 - val_loss: 1.0465 - val_acc: 1.0000\n",
      "Epoch 44/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0419 - acc: 0.9972 - val_loss: 1.0469 - val_acc: 0.9872\n",
      "Epoch 45/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0407 - acc: 1.0000 - val_loss: 1.0478 - val_acc: 0.9872\n",
      "Epoch 46/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0407 - acc: 1.0000 - val_loss: 1.0508 - val_acc: 0.9872\n",
      "Epoch 47/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0407 - acc: 1.0000 - val_loss: 1.0467 - val_acc: 0.9872\n",
      "Epoch 48/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0407 - acc: 1.0000 - val_loss: 1.0489 - val_acc: 0.9872\n",
      "Epoch 49/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0525 - val_acc: 0.9872\n",
      "Epoch 50/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0481 - val_acc: 0.9872\n",
      "Epoch 51/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0467 - val_acc: 0.9872\n",
      "Epoch 52/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0472 - val_acc: 0.9872\n",
      "Epoch 53/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0479 - val_acc: 0.9744\n",
      "Epoch 54/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0477 - val_acc: 0.9872\n",
      "Epoch 55/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0467 - val_acc: 0.9872\n",
      "Epoch 56/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0509 - val_acc: 0.9872\n",
      "Epoch 57/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0465 - val_acc: 0.9872\n",
      "Epoch 58/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0485 - val_acc: 0.9872\n",
      "Epoch 59/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0471 - val_acc: 0.9744\n",
      "Epoch 60/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0476 - val_acc: 0.9872\n",
      "Epoch 61/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0465 - val_acc: 0.9872\n",
      "Epoch 62/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0461 - val_acc: 0.9872\n",
      "Epoch 63/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0459 - val_acc: 0.9872\n",
      "Epoch 64/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0464 - val_acc: 0.9872\n",
      "Epoch 65/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0485 - val_acc: 0.9872\n",
      "Epoch 66/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0466 - val_acc: 0.9872\n",
      "Epoch 67/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0462 - val_acc: 0.9872\n",
      "Epoch 68/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0459 - val_acc: 0.9872\n",
      "Epoch 69/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0470 - val_acc: 0.9872\n",
      "Epoch 70/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0463 - val_acc: 0.9872\n",
      "Epoch 71/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0455 - val_acc: 0.9872\n",
      "Epoch 72/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0462 - val_acc: 0.9872\n",
      "Epoch 73/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0459 - val_acc: 0.9872\n",
      "Epoch 74/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0462 - val_acc: 0.9872\n",
      "Epoch 75/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0458 - val_acc: 0.9872\n",
      "Epoch 76/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0450 - val_acc: 0.9872\n",
      "Epoch 77/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0452 - val_acc: 0.9872\n",
      "Epoch 78/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0451 - val_acc: 0.9872\n",
      "Epoch 79/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0457 - val_acc: 0.9872\n",
      "Epoch 80/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0459 - val_acc: 0.9872\n",
      "Epoch 81/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0451 - val_acc: 0.9872\n",
      "Epoch 82/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0453 - val_acc: 0.9872\n",
      "Epoch 83/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0456 - val_acc: 0.9872\n",
      "Epoch 84/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0453 - val_acc: 0.9872\n",
      "Epoch 85/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0454 - val_acc: 0.9872\n",
      "Epoch 86/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0451 - val_acc: 0.9872\n",
      "Epoch 87/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0453 - val_acc: 0.9872\n",
      "Epoch 88/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0456 - val_acc: 0.9872\n",
      "Epoch 89/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0457 - val_acc: 0.9872\n",
      "Epoch 90/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0457 - val_acc: 0.9872\n",
      "Epoch 91/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0454 - val_acc: 0.9872\n",
      "Epoch 92/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0456 - val_acc: 0.9872\n",
      "Epoch 93/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0453 - val_acc: 0.9872\n",
      "Epoch 94/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0454 - val_acc: 0.9872\n",
      "Epoch 95/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0457 - val_acc: 0.9872\n",
      "Epoch 96/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0455 - val_acc: 0.9872\n",
      "Epoch 97/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0453 - val_acc: 0.9872\n",
      "Epoch 98/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0456 - val_acc: 0.9872\n",
      "Epoch 99/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0456 - val_acc: 0.9872\n",
      "Epoch 100/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0453 - val_acc: 0.9872\n",
      "78/78 [==============================] - 0s 323us/step\n",
      "Score for fold 3: loss of 1.0461487861780019; acc of 98.71794871794873%\n",
      "Train on 702 samples, validate on 78 samples\n",
      "Epoch 1/100\n",
      "702/702 [==============================] - 13s 18ms/step - loss: 2.2405 - acc: 0.2963 - val_loss: 1.8125 - val_acc: 0.4615\n",
      "Epoch 2/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.6582 - acc: 0.6410 - val_loss: 1.5685 - val_acc: 0.6795\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.4177 - acc: 0.7336 - val_loss: 1.3659 - val_acc: 0.8462\n",
      "Epoch 4/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.2462 - acc: 0.8590 - val_loss: 1.4019 - val_acc: 0.7949\n",
      "Epoch 5/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1886 - acc: 0.8989 - val_loss: 1.1787 - val_acc: 0.8718\n",
      "Epoch 6/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.2025 - acc: 0.8618 - val_loss: 1.3232 - val_acc: 0.7692\n",
      "Epoch 7/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1331 - acc: 0.9202 - val_loss: 1.0796 - val_acc: 1.0000\n",
      "Epoch 8/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1305 - acc: 0.9231 - val_loss: 1.1365 - val_acc: 0.8846\n",
      "Epoch 9/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1003 - acc: 0.9487 - val_loss: 1.1534 - val_acc: 0.9103\n",
      "Epoch 10/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0893 - acc: 0.9573 - val_loss: 1.0781 - val_acc: 0.9487\n",
      "Epoch 11/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0810 - acc: 0.9658 - val_loss: 1.1516 - val_acc: 0.7692\n",
      "Epoch 12/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0795 - acc: 0.9687 - val_loss: 1.0618 - val_acc: 0.9872\n",
      "Epoch 13/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0833 - acc: 0.9516 - val_loss: 1.1078 - val_acc: 0.8590\n",
      "Epoch 14/100\n",
      "702/702 [==============================] - 5s 6ms/step - loss: 1.0838 - acc: 0.9729 - val_loss: 1.0717 - val_acc: 0.9615\n",
      "Epoch 15/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0536 - acc: 0.9943 - val_loss: 1.1288 - val_acc: 0.8590\n",
      "Epoch 16/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0702 - acc: 0.9758 - val_loss: 1.0656 - val_acc: 0.9744\n",
      "Epoch 17/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0516 - acc: 0.9943 - val_loss: 1.0546 - val_acc: 0.9872\n",
      "Epoch 18/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0596 - acc: 0.9900 - val_loss: 1.0796 - val_acc: 0.9487\n",
      "Epoch 19/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0513 - acc: 0.9943 - val_loss: 1.2269 - val_acc: 0.8718\n",
      "Epoch 20/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0607 - acc: 0.9900 - val_loss: 1.0502 - val_acc: 0.9744\n",
      "Epoch 21/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0456 - acc: 0.9986 - val_loss: 1.0529 - val_acc: 0.9872\n",
      "Epoch 22/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0491 - acc: 0.9915 - val_loss: 1.0516 - val_acc: 0.9744\n",
      "Epoch 23/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0501 - acc: 0.9929 - val_loss: 1.0634 - val_acc: 0.9615\n",
      "Epoch 24/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0461 - acc: 0.9986 - val_loss: 1.0508 - val_acc: 0.9872\n",
      "Epoch 25/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0508 - acc: 0.9886 - val_loss: 1.0494 - val_acc: 1.0000\n",
      "Epoch 26/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0459 - acc: 0.9986 - val_loss: 1.0473 - val_acc: 0.9872\n",
      "Epoch 27/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0442 - acc: 0.9986 - val_loss: 1.0488 - val_acc: 1.0000\n",
      "Epoch 28/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0437 - acc: 0.9986 - val_loss: 1.0617 - val_acc: 0.9744\n",
      "Epoch 29/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0438 - acc: 0.9986 - val_loss: 1.0501 - val_acc: 0.9872\n",
      "Epoch 30/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0443 - acc: 0.9986 - val_loss: 1.0477 - val_acc: 0.9872\n",
      "Epoch 31/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0441 - acc: 0.9972 - val_loss: 1.0547 - val_acc: 0.9615\n",
      "Epoch 32/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0433 - acc: 0.9986 - val_loss: 1.0453 - val_acc: 0.9872\n",
      "Epoch 33/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0429 - acc: 0.9986 - val_loss: 1.0435 - val_acc: 1.0000\n",
      "Epoch 34/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0424 - acc: 0.9986 - val_loss: 1.0465 - val_acc: 0.9872\n",
      "Epoch 35/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0423 - acc: 0.9986 - val_loss: 1.0634 - val_acc: 0.9744\n",
      "Epoch 36/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0426 - acc: 1.0000 - val_loss: 1.0556 - val_acc: 0.9744\n",
      "Epoch 37/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0431 - acc: 0.9972 - val_loss: 1.0452 - val_acc: 0.9872\n",
      "Epoch 38/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0418 - acc: 1.0000 - val_loss: 1.0494 - val_acc: 0.9872\n",
      "Epoch 39/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0418 - acc: 1.0000 - val_loss: 1.0447 - val_acc: 0.9872\n",
      "Epoch 40/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0416 - acc: 1.0000 - val_loss: 1.0499 - val_acc: 0.9744\n",
      "Epoch 41/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0414 - acc: 1.0000 - val_loss: 1.0501 - val_acc: 0.9872\n",
      "Epoch 42/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0416 - acc: 1.0000 - val_loss: 1.0497 - val_acc: 0.9872\n",
      "Epoch 43/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0412 - acc: 1.0000 - val_loss: 1.0466 - val_acc: 0.9872\n",
      "Epoch 44/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0412 - acc: 1.0000 - val_loss: 1.0497 - val_acc: 0.9744\n",
      "Epoch 45/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0411 - acc: 1.0000 - val_loss: 1.0477 - val_acc: 0.9872\n",
      "Epoch 46/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0410 - acc: 1.0000 - val_loss: 1.0459 - val_acc: 0.9872\n",
      "Epoch 47/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0410 - acc: 1.0000 - val_loss: 1.0494 - val_acc: 0.9872\n",
      "Epoch 48/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0409 - acc: 1.0000 - val_loss: 1.0462 - val_acc: 0.9872\n",
      "Epoch 49/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0408 - acc: 1.0000 - val_loss: 1.0484 - val_acc: 0.9872\n",
      "Epoch 50/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0408 - acc: 1.0000 - val_loss: 1.0432 - val_acc: 1.0000\n",
      "Epoch 51/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0408 - acc: 1.0000 - val_loss: 1.0453 - val_acc: 0.9872\n",
      "Epoch 52/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0453 - val_acc: 0.9872\n",
      "Epoch 53/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0407 - acc: 1.0000 - val_loss: 1.0508 - val_acc: 0.9872\n",
      "Epoch 54/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0488 - val_acc: 0.9872\n",
      "Epoch 55/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0466 - val_acc: 0.9872\n",
      "Epoch 56/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0443 - val_acc: 0.9872\n",
      "Epoch 57/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0426 - val_acc: 1.0000\n",
      "Epoch 58/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0451 - val_acc: 0.9872\n",
      "Epoch 59/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0463 - val_acc: 0.9872\n",
      "Epoch 60/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0446 - val_acc: 0.9872\n",
      "Epoch 61/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0451 - val_acc: 0.9872\n",
      "Epoch 62/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0456 - val_acc: 0.9872\n",
      "Epoch 63/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0438 - val_acc: 0.9872\n",
      "Epoch 64/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0438 - val_acc: 0.9872\n",
      "Epoch 65/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0451 - val_acc: 0.9872\n",
      "Epoch 66/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0438 - val_acc: 0.9872\n",
      "Epoch 67/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0450 - val_acc: 0.9872\n",
      "Epoch 68/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0445 - val_acc: 0.9872\n",
      "Epoch 69/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0439 - val_acc: 0.9872\n",
      "Epoch 70/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0442 - val_acc: 0.9872\n",
      "Epoch 71/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0446 - val_acc: 0.9872\n",
      "Epoch 72/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0430 - val_acc: 1.0000\n",
      "Epoch 73/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0426 - val_acc: 1.0000\n",
      "Epoch 74/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0427 - val_acc: 1.0000\n",
      "Epoch 75/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0438 - val_acc: 0.9872\n",
      "Epoch 76/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0435 - val_acc: 0.9872\n",
      "Epoch 77/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0439 - val_acc: 0.9872\n",
      "Epoch 78/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0430 - val_acc: 1.0000\n",
      "Epoch 79/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0430 - val_acc: 1.0000\n",
      "Epoch 80/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0447 - val_acc: 0.9872\n",
      "Epoch 81/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0431 - val_acc: 1.0000\n",
      "Epoch 82/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0430 - val_acc: 1.0000\n",
      "Epoch 83/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0433 - val_acc: 1.0000\n",
      "Epoch 84/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0432 - val_acc: 1.0000\n",
      "Epoch 85/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0432 - val_acc: 1.0000\n",
      "Epoch 86/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0428 - val_acc: 1.0000\n",
      "Epoch 87/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0432 - val_acc: 1.0000\n",
      "Epoch 88/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0426 - val_acc: 1.0000\n",
      "Epoch 89/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0433 - val_acc: 0.9872\n",
      "Epoch 90/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0439 - val_acc: 0.9872\n",
      "Epoch 91/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0437 - val_acc: 0.9872\n",
      "Epoch 92/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0437 - val_acc: 0.9872\n",
      "Epoch 93/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0429 - val_acc: 1.0000\n",
      "Epoch 94/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0430 - val_acc: 1.0000\n",
      "Epoch 95/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0432 - val_acc: 1.0000\n",
      "Epoch 96/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0431 - val_acc: 1.0000\n",
      "Epoch 97/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0436 - val_acc: 0.9872\n",
      "Epoch 98/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0433 - val_acc: 0.9872\n",
      "Epoch 99/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0432 - val_acc: 0.9872\n",
      "Epoch 100/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0434 - val_acc: 0.9872\n",
      "78/78 [==============================] - 0s 354us/step\n",
      "Score for fold 4: loss of 1.04726308125716; acc of 98.71794825945145%\n",
      "Train on 702 samples, validate on 78 samples\n",
      "Epoch 1/100\n",
      "702/702 [==============================] - 13s 19ms/step - loss: 2.2595 - acc: 0.3604 - val_loss: 1.9379 - val_acc: 0.5256\n",
      "Epoch 2/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.6786 - acc: 0.6652 - val_loss: 1.4873 - val_acc: 0.6923\n",
      "Epoch 3/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.3693 - acc: 0.7778 - val_loss: 1.4433 - val_acc: 0.8205\n",
      "Epoch 4/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.2224 - acc: 0.8561 - val_loss: 1.2384 - val_acc: 0.8846\n",
      "Epoch 5/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1969 - acc: 0.8746 - val_loss: 1.2520 - val_acc: 0.8205\n",
      "Epoch 6/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1566 - acc: 0.8860 - val_loss: 1.2801 - val_acc: 0.7949\n",
      "Epoch 7/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1117 - acc: 0.9444 - val_loss: 1.1288 - val_acc: 0.9359\n",
      "Epoch 8/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1169 - acc: 0.9387 - val_loss: 1.1595 - val_acc: 0.9231\n",
      "Epoch 9/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0859 - acc: 0.9658 - val_loss: 1.0876 - val_acc: 0.9744\n",
      "Epoch 10/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0901 - acc: 0.9516 - val_loss: 1.0942 - val_acc: 0.9744\n",
      "Epoch 11/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0669 - acc: 0.9801 - val_loss: 1.2000 - val_acc: 0.9103\n",
      "Epoch 12/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1155 - acc: 0.9587 - val_loss: 1.2516 - val_acc: 0.7949\n",
      "Epoch 13/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0672 - acc: 0.9801 - val_loss: 1.0811 - val_acc: 0.9744\n",
      "Epoch 14/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0561 - acc: 0.9858 - val_loss: 1.0824 - val_acc: 0.9615\n",
      "Epoch 15/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0781 - acc: 0.9701 - val_loss: 1.1015 - val_acc: 0.8974\n",
      "Epoch 16/100\n",
      "702/702 [==============================] - 5s 7ms/step - loss: 1.0598 - acc: 0.9872 - val_loss: 1.0748 - val_acc: 0.9744\n",
      "Epoch 17/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0558 - acc: 0.9886 - val_loss: 1.0884 - val_acc: 0.9615\n",
      "Epoch 18/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0533 - acc: 0.9872 - val_loss: 1.1338 - val_acc: 0.9487\n",
      "Epoch 19/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0481 - acc: 0.9972 - val_loss: 1.0700 - val_acc: 0.9744\n",
      "Epoch 20/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0494 - acc: 0.9915 - val_loss: 1.0671 - val_acc: 0.9744\n",
      "Epoch 21/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0552 - acc: 0.9858 - val_loss: 1.0635 - val_acc: 0.9615\n",
      "Epoch 22/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0506 - acc: 0.9886 - val_loss: 1.1088 - val_acc: 0.9615\n",
      "Epoch 23/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0460 - acc: 0.9957 - val_loss: 1.0670 - val_acc: 0.9744\n",
      "Epoch 24/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0450 - acc: 0.9986 - val_loss: 1.0717 - val_acc: 0.9615\n",
      "Epoch 25/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0467 - acc: 0.9957 - val_loss: 1.0637 - val_acc: 0.9615\n",
      "Epoch 26/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0441 - acc: 0.9986 - val_loss: 1.0732 - val_acc: 0.9744\n",
      "Epoch 27/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0483 - acc: 0.9929 - val_loss: 1.0573 - val_acc: 0.9872\n",
      "Epoch 28/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0438 - acc: 0.9986 - val_loss: 1.0656 - val_acc: 0.9744\n",
      "Epoch 29/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0436 - acc: 0.9986 - val_loss: 1.0621 - val_acc: 0.9487\n",
      "Epoch 30/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0446 - acc: 0.9986 - val_loss: 1.0581 - val_acc: 0.9872\n",
      "Epoch 31/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0439 - acc: 0.9972 - val_loss: 1.0566 - val_acc: 0.9872\n",
      "Epoch 32/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0429 - acc: 0.9986 - val_loss: 1.0577 - val_acc: 0.9744\n",
      "Epoch 33/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0485 - acc: 0.9929 - val_loss: 1.0526 - val_acc: 0.9744\n",
      "Epoch 34/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0421 - acc: 1.0000 - val_loss: 1.0552 - val_acc: 0.9872\n",
      "Epoch 35/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0418 - acc: 1.0000 - val_loss: 1.0555 - val_acc: 0.9744\n",
      "Epoch 36/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0416 - acc: 1.0000 - val_loss: 1.0509 - val_acc: 0.9872\n",
      "Epoch 37/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0419 - acc: 1.0000 - val_loss: 1.0537 - val_acc: 0.9744\n",
      "Epoch 38/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0415 - acc: 1.0000 - val_loss: 1.0529 - val_acc: 0.9872\n",
      "Epoch 39/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0415 - acc: 1.0000 - val_loss: 1.0551 - val_acc: 0.9872\n",
      "Epoch 40/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0416 - acc: 1.0000 - val_loss: 1.0588 - val_acc: 0.9872\n",
      "Epoch 41/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0410 - acc: 1.0000 - val_loss: 1.0629 - val_acc: 0.9615\n",
      "Epoch 42/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0411 - acc: 1.0000 - val_loss: 1.0535 - val_acc: 0.9872\n",
      "Epoch 43/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0409 - acc: 1.0000 - val_loss: 1.0537 - val_acc: 0.9744\n",
      "Epoch 44/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0410 - acc: 1.0000 - val_loss: 1.0529 - val_acc: 0.9744\n",
      "Epoch 45/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0409 - acc: 1.0000 - val_loss: 1.0496 - val_acc: 0.9872\n",
      "Epoch 46/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0408 - acc: 1.0000 - val_loss: 1.0757 - val_acc: 0.9872\n",
      "Epoch 47/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0408 - acc: 1.0000 - val_loss: 1.0478 - val_acc: 1.0000\n",
      "Epoch 48/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0407 - acc: 1.0000 - val_loss: 1.0530 - val_acc: 0.9872\n",
      "Epoch 49/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0407 - acc: 1.0000 - val_loss: 1.0522 - val_acc: 0.9872\n",
      "Epoch 50/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0492 - val_acc: 0.9872\n",
      "Epoch 51/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0499 - val_acc: 0.9872\n",
      "Epoch 52/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0407 - acc: 1.0000 - val_loss: 1.0461 - val_acc: 1.0000\n",
      "Epoch 53/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0507 - val_acc: 0.9872\n",
      "Epoch 54/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0482 - val_acc: 1.0000\n",
      "Epoch 55/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0486 - val_acc: 1.0000\n",
      "Epoch 56/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0475 - val_acc: 1.0000\n",
      "Epoch 57/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0471 - val_acc: 1.0000\n",
      "Epoch 58/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0492 - val_acc: 1.0000\n",
      "Epoch 59/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0479 - val_acc: 1.0000\n",
      "Epoch 60/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0474 - val_acc: 1.0000\n",
      "Epoch 61/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0472 - val_acc: 1.0000\n",
      "Epoch 62/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0463 - val_acc: 1.0000\n",
      "Epoch 63/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0482 - val_acc: 0.9872\n",
      "Epoch 64/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0468 - val_acc: 1.0000\n",
      "Epoch 65/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0478 - val_acc: 1.0000\n",
      "Epoch 66/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0488 - val_acc: 1.0000\n",
      "Epoch 67/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0495 - val_acc: 1.0000\n",
      "Epoch 68/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0490 - val_acc: 1.0000\n",
      "Epoch 69/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0471 - val_acc: 1.0000\n",
      "Epoch 70/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0463 - val_acc: 1.0000\n",
      "Epoch 71/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0467 - val_acc: 1.0000\n",
      "Epoch 72/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0507 - val_acc: 1.0000\n",
      "Epoch 73/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0496 - val_acc: 1.0000\n",
      "Epoch 74/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0509 - val_acc: 0.9872\n",
      "Epoch 75/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0481 - val_acc: 1.0000\n",
      "Epoch 76/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0503 - val_acc: 0.9872\n",
      "Epoch 77/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0497 - val_acc: 0.9872\n",
      "Epoch 78/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0493 - val_acc: 0.9872\n",
      "Epoch 79/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0491 - val_acc: 0.9872\n",
      "Epoch 80/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0487 - val_acc: 0.9872\n",
      "Epoch 81/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0482 - val_acc: 0.9872\n",
      "Epoch 82/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0482 - val_acc: 0.9872\n",
      "Epoch 83/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0493 - val_acc: 0.9872\n",
      "Epoch 84/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0493 - val_acc: 0.9872\n",
      "Epoch 85/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0477 - val_acc: 1.0000\n",
      "Epoch 86/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0488 - val_acc: 0.9872\n",
      "Epoch 87/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0490 - val_acc: 0.9872\n",
      "Epoch 88/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0487 - val_acc: 0.9872\n",
      "Epoch 89/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0469 - val_acc: 1.0000\n",
      "Epoch 90/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0472 - val_acc: 1.0000\n",
      "Epoch 91/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0468 - val_acc: 1.0000\n",
      "Epoch 92/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0474 - val_acc: 1.0000\n",
      "Epoch 93/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0474 - val_acc: 1.0000\n",
      "Epoch 94/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0483 - val_acc: 1.0000\n",
      "Epoch 95/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0482 - val_acc: 1.0000\n",
      "Epoch 96/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0479 - val_acc: 1.0000\n",
      "Epoch 97/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0480 - val_acc: 1.0000\n",
      "Epoch 98/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0480 - val_acc: 1.0000\n",
      "Epoch 99/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0481 - val_acc: 1.0000\n",
      "Epoch 100/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0478 - val_acc: 1.0000\n",
      "78/78 [==============================] - 0s 338us/step\n",
      "Score for fold 5: loss of 1.0459548968535204; acc of 100.0%\n",
      "Train on 702 samples, validate on 78 samples\n",
      "Epoch 1/100\n",
      "702/702 [==============================] - 13s 19ms/step - loss: 2.2733 - acc: 0.3533 - val_loss: 2.1653 - val_acc: 0.4487\n",
      "Epoch 2/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.7245 - acc: 0.6581 - val_loss: 1.4544 - val_acc: 0.7692\n",
      "Epoch 3/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.3680 - acc: 0.8234 - val_loss: 1.3434 - val_acc: 0.7436\n",
      "Epoch 4/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.2650 - acc: 0.8462 - val_loss: 1.1617 - val_acc: 0.9744\n",
      "Epoch 5/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1659 - acc: 0.8917 - val_loss: 1.1883 - val_acc: 0.8974\n",
      "Epoch 6/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1440 - acc: 0.9103 - val_loss: 1.1367 - val_acc: 0.9359\n",
      "Epoch 7/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1463 - acc: 0.9174 - val_loss: 1.1559 - val_acc: 0.9359\n",
      "Epoch 8/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0908 - acc: 0.9630 - val_loss: 1.1428 - val_acc: 0.8718\n",
      "Epoch 9/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0980 - acc: 0.9359 - val_loss: 1.1016 - val_acc: 0.9487\n",
      "Epoch 10/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0831 - acc: 0.9615 - val_loss: 1.1090 - val_acc: 0.8974\n",
      "Epoch 11/100\n",
      "702/702 [==============================] - 5s 7ms/step - loss: 1.0847 - acc: 0.9672 - val_loss: 1.0987 - val_acc: 0.8974\n",
      "Epoch 12/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0635 - acc: 0.9815 - val_loss: 1.1255 - val_acc: 0.8590\n",
      "Epoch 13/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0641 - acc: 0.9829 - val_loss: 1.0584 - val_acc: 1.0000\n",
      "Epoch 14/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0710 - acc: 0.9815 - val_loss: 1.0628 - val_acc: 0.9744\n",
      "Epoch 15/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0569 - acc: 0.9872 - val_loss: 1.0562 - val_acc: 1.0000\n",
      "Epoch 16/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0518 - acc: 0.9929 - val_loss: 1.0562 - val_acc: 0.9872\n",
      "Epoch 17/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0548 - acc: 0.9843 - val_loss: 1.0540 - val_acc: 1.0000\n",
      "Epoch 18/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0523 - acc: 0.9915 - val_loss: 1.0584 - val_acc: 1.0000\n",
      "Epoch 19/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0497 - acc: 0.9972 - val_loss: 1.0566 - val_acc: 0.9872\n",
      "Epoch 20/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0520 - acc: 0.9872 - val_loss: 1.0607 - val_acc: 0.9615\n",
      "Epoch 21/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0477 - acc: 0.9943 - val_loss: 1.0879 - val_acc: 0.9615\n",
      "Epoch 22/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0478 - acc: 0.9929 - val_loss: 1.0536 - val_acc: 1.0000\n",
      "Epoch 23/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0461 - acc: 0.9957 - val_loss: 1.0609 - val_acc: 0.9744\n",
      "Epoch 24/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0454 - acc: 0.9972 - val_loss: 1.0693 - val_acc: 0.9744\n",
      "Epoch 25/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0470 - acc: 0.9900 - val_loss: 1.0620 - val_acc: 1.0000\n",
      "Epoch 26/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0447 - acc: 0.9986 - val_loss: 1.0536 - val_acc: 1.0000\n",
      "Epoch 27/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0440 - acc: 0.9986 - val_loss: 1.0577 - val_acc: 0.9744\n",
      "Epoch 28/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0446 - acc: 0.9972 - val_loss: 1.0498 - val_acc: 1.0000\n",
      "Epoch 29/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0429 - acc: 0.9972 - val_loss: 1.0544 - val_acc: 0.9872\n",
      "Epoch 30/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0427 - acc: 0.9986 - val_loss: 1.0659 - val_acc: 0.9487\n",
      "Epoch 31/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0434 - acc: 0.9986 - val_loss: 1.0507 - val_acc: 0.9872\n",
      "Epoch 32/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0430 - acc: 0.9986 - val_loss: 1.0592 - val_acc: 0.9872\n",
      "Epoch 33/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0419 - acc: 1.0000 - val_loss: 1.0586 - val_acc: 0.9744\n",
      "Epoch 34/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0417 - acc: 1.0000 - val_loss: 1.0566 - val_acc: 0.9744\n",
      "Epoch 35/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0417 - acc: 1.0000 - val_loss: 1.0563 - val_acc: 0.9872\n",
      "Epoch 36/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0420 - acc: 1.0000 - val_loss: 1.0781 - val_acc: 0.9487\n",
      "Epoch 37/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0441 - acc: 0.9972 - val_loss: 1.0579 - val_acc: 0.9744\n",
      "Epoch 38/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0420 - acc: 0.9986 - val_loss: 1.0582 - val_acc: 0.9744\n",
      "Epoch 39/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0414 - acc: 1.0000 - val_loss: 1.0518 - val_acc: 1.0000\n",
      "Epoch 40/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0412 - acc: 1.0000 - val_loss: 1.0495 - val_acc: 1.0000\n",
      "Epoch 41/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0411 - acc: 1.0000 - val_loss: 1.0530 - val_acc: 0.9872\n",
      "Epoch 42/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0411 - acc: 1.0000 - val_loss: 1.0645 - val_acc: 0.9872\n",
      "Epoch 43/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0413 - acc: 1.0000 - val_loss: 1.0664 - val_acc: 0.9744\n",
      "Epoch 44/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0409 - acc: 1.0000 - val_loss: 1.0582 - val_acc: 0.9872\n",
      "Epoch 45/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0408 - acc: 1.0000 - val_loss: 1.0551 - val_acc: 0.9744\n",
      "Epoch 46/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0408 - acc: 1.0000 - val_loss: 1.0519 - val_acc: 0.9872\n",
      "Epoch 47/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0407 - acc: 1.0000 - val_loss: 1.0593 - val_acc: 0.9744\n",
      "Epoch 48/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0408 - acc: 1.0000 - val_loss: 1.0564 - val_acc: 0.9872\n",
      "Epoch 49/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0407 - acc: 1.0000 - val_loss: 1.0565 - val_acc: 0.9872\n",
      "Epoch 50/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0407 - acc: 1.0000 - val_loss: 1.0549 - val_acc: 0.9872\n",
      "Epoch 51/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0516 - val_acc: 0.9872\n",
      "Epoch 52/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0513 - val_acc: 0.9872\n",
      "Epoch 53/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0551 - val_acc: 0.9872\n",
      "Epoch 54/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0541 - val_acc: 0.9872\n",
      "Epoch 55/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0513 - val_acc: 0.9872\n",
      "Epoch 56/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0509 - val_acc: 1.0000\n",
      "Epoch 57/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0501 - val_acc: 1.0000\n",
      "Epoch 58/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0491 - val_acc: 1.0000\n",
      "Epoch 59/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0510 - val_acc: 1.0000\n",
      "Epoch 60/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0495 - val_acc: 1.0000\n",
      "Epoch 61/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0493 - val_acc: 1.0000\n",
      "Epoch 62/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0501 - val_acc: 0.9872\n",
      "Epoch 63/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0499 - val_acc: 0.9872\n",
      "Epoch 64/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0498 - val_acc: 1.0000\n",
      "Epoch 65/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0498 - val_acc: 1.0000\n",
      "Epoch 66/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0500 - val_acc: 0.9872\n",
      "Epoch 67/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0496 - val_acc: 1.0000\n",
      "Epoch 68/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0499 - val_acc: 1.0000\n",
      "Epoch 69/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0502 - val_acc: 0.9872\n",
      "Epoch 70/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0500 - val_acc: 1.0000\n",
      "Epoch 71/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0507 - val_acc: 1.0000\n",
      "Epoch 72/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0511 - val_acc: 0.9872\n",
      "Epoch 73/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0514 - val_acc: 0.9872\n",
      "Epoch 74/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0498 - val_acc: 1.0000\n",
      "Epoch 75/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0499 - val_acc: 0.9872\n",
      "Epoch 76/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0500 - val_acc: 0.9872\n",
      "Epoch 77/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0512 - val_acc: 0.9872\n",
      "Epoch 78/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0505 - val_acc: 0.9872\n",
      "Epoch 79/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0509 - val_acc: 0.9872\n",
      "Epoch 80/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0519 - val_acc: 0.9872\n",
      "Epoch 81/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0521 - val_acc: 0.9872\n",
      "Epoch 82/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0511 - val_acc: 0.9872\n",
      "Epoch 83/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0507 - val_acc: 0.9872\n",
      "Epoch 84/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0514 - val_acc: 0.9872\n",
      "Epoch 85/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0514 - val_acc: 0.9872\n",
      "Epoch 86/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0509 - val_acc: 0.9872\n",
      "Epoch 87/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0514 - val_acc: 0.9872\n",
      "Epoch 88/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0511 - val_acc: 0.9872\n",
      "Epoch 89/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0516 - val_acc: 0.9872\n",
      "Epoch 90/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0512 - val_acc: 0.9872\n",
      "Epoch 91/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0510 - val_acc: 0.9872\n",
      "Epoch 92/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0524 - val_acc: 0.9744\n",
      "Epoch 93/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0529 - val_acc: 0.9744\n",
      "Epoch 94/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0514 - val_acc: 0.9872\n",
      "Epoch 95/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0523 - val_acc: 0.9872\n",
      "Epoch 96/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0515 - val_acc: 0.9872\n",
      "Epoch 97/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0508 - val_acc: 0.9872\n",
      "Epoch 98/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0518 - val_acc: 0.9872\n",
      "Epoch 99/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0515 - val_acc: 0.9872\n",
      "Epoch 100/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0513 - val_acc: 0.9872\n",
      "78/78 [==============================] - 0s 345us/step\n",
      "Score for fold 6: loss of 1.049520153265733; acc of 98.71794825945145%\n",
      "Train on 702 samples, validate on 78 samples\n",
      "Epoch 1/100\n",
      "702/702 [==============================] - 14s 19ms/step - loss: 2.2740 - acc: 0.3390 - val_loss: 1.8616 - val_acc: 0.5897\n",
      "Epoch 2/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.6662 - acc: 0.6923 - val_loss: 1.6268 - val_acc: 0.6282\n",
      "Epoch 3/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.4056 - acc: 0.7877 - val_loss: 1.2965 - val_acc: 0.8205\n",
      "Epoch 4/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.2552 - acc: 0.8390 - val_loss: 1.3011 - val_acc: 0.7949\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1851 - acc: 0.8946 - val_loss: 1.2288 - val_acc: 0.8462\n",
      "Epoch 6/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1407 - acc: 0.9145 - val_loss: 1.1256 - val_acc: 0.9103\n",
      "Epoch 7/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1743 - acc: 0.8960 - val_loss: 1.1004 - val_acc: 0.9359\n",
      "Epoch 8/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1046 - acc: 0.9544 - val_loss: 1.1262 - val_acc: 0.9103\n",
      "Epoch 9/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0847 - acc: 0.9587 - val_loss: 1.0814 - val_acc: 0.9615\n",
      "Epoch 10/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0907 - acc: 0.9501 - val_loss: 1.0950 - val_acc: 0.9103\n",
      "Epoch 11/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0840 - acc: 0.9658 - val_loss: 1.1312 - val_acc: 0.9103\n",
      "Epoch 12/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0663 - acc: 0.9744 - val_loss: 1.4360 - val_acc: 0.7949\n",
      "Epoch 13/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0801 - acc: 0.9687 - val_loss: 1.0950 - val_acc: 0.9615\n",
      "Epoch 14/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0650 - acc: 0.9772 - val_loss: 1.0713 - val_acc: 0.9487\n",
      "Epoch 15/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0586 - acc: 0.9915 - val_loss: 1.0683 - val_acc: 0.9615\n",
      "Epoch 16/100\n",
      "702/702 [==============================] - 5s 7ms/step - loss: 1.0660 - acc: 0.9744 - val_loss: 1.0741 - val_acc: 0.9615\n",
      "Epoch 17/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0528 - acc: 0.9886 - val_loss: 1.0865 - val_acc: 0.9615\n",
      "Epoch 18/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0521 - acc: 0.9929 - val_loss: 1.0753 - val_acc: 0.9744\n",
      "Epoch 19/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0572 - acc: 0.9886 - val_loss: 1.0702 - val_acc: 0.9744\n",
      "Epoch 20/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0490 - acc: 0.9943 - val_loss: 1.1141 - val_acc: 0.9487\n",
      "Epoch 21/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0507 - acc: 0.9915 - val_loss: 1.0654 - val_acc: 0.9744\n",
      "Epoch 22/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0479 - acc: 0.9957 - val_loss: 1.0752 - val_acc: 0.9487\n",
      "Epoch 23/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0510 - acc: 0.9843 - val_loss: 1.0576 - val_acc: 0.9872\n",
      "Epoch 24/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0461 - acc: 0.9972 - val_loss: 1.0689 - val_acc: 0.9744\n",
      "Epoch 25/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0468 - acc: 0.9957 - val_loss: 1.1361 - val_acc: 0.8846\n",
      "Epoch 26/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0524 - acc: 0.9872 - val_loss: 1.0578 - val_acc: 0.9744\n",
      "Epoch 27/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0442 - acc: 0.9986 - val_loss: 1.0772 - val_acc: 0.9615\n",
      "Epoch 28/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0443 - acc: 0.9972 - val_loss: 1.0621 - val_acc: 0.9744\n",
      "Epoch 29/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0451 - acc: 0.9957 - val_loss: 1.0638 - val_acc: 0.9744\n",
      "Epoch 30/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0454 - acc: 1.0000 - val_loss: 1.0635 - val_acc: 0.9744\n",
      "Epoch 31/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0425 - acc: 0.9986 - val_loss: 1.0734 - val_acc: 0.9744\n",
      "Epoch 32/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0421 - acc: 1.0000 - val_loss: 1.0670 - val_acc: 0.9744\n",
      "Epoch 33/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0441 - acc: 0.9972 - val_loss: 1.0628 - val_acc: 0.9744\n",
      "Epoch 34/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0434 - acc: 0.9972 - val_loss: 1.0572 - val_acc: 0.9615\n",
      "Epoch 35/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0422 - acc: 0.9986 - val_loss: 1.0659 - val_acc: 0.9744\n",
      "Epoch 36/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0414 - acc: 1.0000 - val_loss: 1.0589 - val_acc: 0.9744\n",
      "Epoch 37/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0416 - acc: 1.0000 - val_loss: 1.0583 - val_acc: 0.9744\n",
      "Epoch 38/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0438 - acc: 0.9957 - val_loss: 1.0573 - val_acc: 0.9744\n",
      "Epoch 39/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0409 - acc: 1.0000 - val_loss: 1.0560 - val_acc: 0.9744\n",
      "Epoch 40/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0410 - acc: 1.0000 - val_loss: 1.0630 - val_acc: 0.9744\n",
      "Epoch 41/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0411 - acc: 1.0000 - val_loss: 1.0532 - val_acc: 0.9744\n",
      "Epoch 42/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0410 - acc: 1.0000 - val_loss: 1.0548 - val_acc: 0.9744\n",
      "Epoch 43/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0408 - acc: 1.0000 - val_loss: 1.0581 - val_acc: 0.9744\n",
      "Epoch 44/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0407 - acc: 1.0000 - val_loss: 1.0676 - val_acc: 0.9744\n",
      "Epoch 45/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0408 - acc: 1.0000 - val_loss: 1.0662 - val_acc: 0.9744\n",
      "Epoch 46/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0409 - acc: 1.0000 - val_loss: 1.0587 - val_acc: 0.9744\n",
      "Epoch 47/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0642 - val_acc: 0.9744\n",
      "Epoch 48/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0658 - val_acc: 0.9744\n",
      "Epoch 49/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0671 - val_acc: 0.9744\n",
      "Epoch 50/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0603 - val_acc: 0.9744\n",
      "Epoch 51/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0622 - val_acc: 0.9744\n",
      "Epoch 52/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0585 - val_acc: 0.9744\n",
      "Epoch 53/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0746 - val_acc: 0.9615\n",
      "Epoch 54/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0593 - val_acc: 0.9744\n",
      "Epoch 55/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0592 - val_acc: 0.9744\n",
      "Epoch 56/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0546 - val_acc: 0.9744\n",
      "Epoch 57/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0640 - val_acc: 0.9744\n",
      "Epoch 58/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0587 - val_acc: 0.9744\n",
      "Epoch 59/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0609 - val_acc: 0.9744\n",
      "Epoch 60/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0539 - val_acc: 0.9744\n",
      "Epoch 61/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0578 - val_acc: 0.9744\n",
      "Epoch 62/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0558 - val_acc: 0.9744\n",
      "Epoch 63/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0585 - val_acc: 0.9744\n",
      "Epoch 64/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0572 - val_acc: 0.9744\n",
      "Epoch 65/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0564 - val_acc: 0.9744\n",
      "Epoch 66/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0584 - val_acc: 0.9744\n",
      "Epoch 67/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0581 - val_acc: 0.9744\n",
      "Epoch 68/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0621 - val_acc: 0.9744\n",
      "Epoch 69/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0609 - val_acc: 0.9744\n",
      "Epoch 70/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0590 - val_acc: 0.9744\n",
      "Epoch 71/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0623 - val_acc: 0.9744\n",
      "Epoch 72/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0627 - val_acc: 0.9744\n",
      "Epoch 73/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0635 - val_acc: 0.9744\n",
      "Epoch 74/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0582 - val_acc: 0.9744\n",
      "Epoch 75/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0606 - val_acc: 0.9744\n",
      "Epoch 76/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0586 - val_acc: 0.9744\n",
      "Epoch 77/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0588 - val_acc: 0.9744\n",
      "Epoch 78/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0593 - val_acc: 0.9744\n",
      "Epoch 79/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0560 - val_acc: 0.9744\n",
      "Epoch 80/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0596 - val_acc: 0.9744\n",
      "Epoch 81/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0576 - val_acc: 0.9744\n",
      "Epoch 82/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0610 - val_acc: 0.9744\n",
      "Epoch 83/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0593 - val_acc: 0.9744\n",
      "Epoch 84/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0566 - val_acc: 0.9744\n",
      "Epoch 85/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0590 - val_acc: 0.9744\n",
      "Epoch 86/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0564 - val_acc: 0.9744\n",
      "Epoch 87/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0573 - val_acc: 0.9744\n",
      "Epoch 88/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0568 - val_acc: 0.9744\n",
      "Epoch 89/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0570 - val_acc: 0.9744\n",
      "Epoch 90/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0569 - val_acc: 0.9744\n",
      "Epoch 91/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0561 - val_acc: 0.9744\n",
      "Epoch 92/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0564 - val_acc: 0.9744\n",
      "Epoch 93/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0593 - val_acc: 0.9744\n",
      "Epoch 94/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0582 - val_acc: 0.9744\n",
      "Epoch 95/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0563 - val_acc: 0.9744\n",
      "Epoch 96/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0576 - val_acc: 0.9744\n",
      "Epoch 97/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0583 - val_acc: 0.9744\n",
      "Epoch 98/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0576 - val_acc: 0.9744\n",
      "Epoch 99/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0585 - val_acc: 0.9744\n",
      "Epoch 100/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0597 - val_acc: 0.9744\n",
      "78/78 [==============================] - 0s 340us/step\n",
      "Score for fold 7: loss of 1.0587392372962756; acc of 97.43589697740018%\n",
      "Train on 702 samples, validate on 78 samples\n",
      "Epoch 1/100\n",
      "702/702 [==============================] - 14s 20ms/step - loss: 2.2784 - acc: 0.3006 - val_loss: 1.9827 - val_acc: 0.4359\n",
      "Epoch 2/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.6887 - acc: 0.6538 - val_loss: 1.6278 - val_acc: 0.7179\n",
      "Epoch 3/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.3867 - acc: 0.8148 - val_loss: 1.6582 - val_acc: 0.5897\n",
      "Epoch 4/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.3071 - acc: 0.8305 - val_loss: 1.2838 - val_acc: 0.8462\n",
      "Epoch 5/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1806 - acc: 0.9103 - val_loss: 1.1754 - val_acc: 0.8590\n",
      "Epoch 6/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1359 - acc: 0.9231 - val_loss: 1.4521 - val_acc: 0.7564\n",
      "Epoch 7/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1330 - acc: 0.9288 - val_loss: 1.5279 - val_acc: 0.7436\n",
      "Epoch 8/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1391 - acc: 0.9117 - val_loss: 1.1024 - val_acc: 0.8846\n",
      "Epoch 9/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1335 - acc: 0.9387 - val_loss: 1.3770 - val_acc: 0.8590\n",
      "Epoch 10/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1219 - acc: 0.9444 - val_loss: 1.1090 - val_acc: 0.9103\n",
      "Epoch 11/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0711 - acc: 0.9772 - val_loss: 1.1197 - val_acc: 0.8846\n",
      "Epoch 12/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0704 - acc: 0.9744 - val_loss: 1.0871 - val_acc: 0.9744\n",
      "Epoch 13/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0830 - acc: 0.9630 - val_loss: 1.0951 - val_acc: 0.9103\n",
      "Epoch 14/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0585 - acc: 0.9886 - val_loss: 1.0935 - val_acc: 0.9231\n",
      "Epoch 15/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0680 - acc: 0.9815 - val_loss: 1.0790 - val_acc: 0.9231\n",
      "Epoch 16/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0596 - acc: 0.9815 - val_loss: 1.0767 - val_acc: 0.9487\n",
      "Epoch 17/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0847 - acc: 0.9772 - val_loss: 1.1117 - val_acc: 0.9231\n",
      "Epoch 18/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0551 - acc: 0.9915 - val_loss: 1.1383 - val_acc: 0.9231\n",
      "Epoch 19/100\n",
      "702/702 [==============================] - 5s 7ms/step - loss: 1.0519 - acc: 0.9915 - val_loss: 1.0754 - val_acc: 0.9487\n",
      "Epoch 20/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0508 - acc: 0.9943 - val_loss: 1.0778 - val_acc: 0.9615\n",
      "Epoch 21/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0486 - acc: 0.9957 - val_loss: 1.0656 - val_acc: 0.9744\n",
      "Epoch 22/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0522 - acc: 0.9929 - val_loss: 1.0733 - val_acc: 0.9359\n",
      "Epoch 23/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0481 - acc: 0.9929 - val_loss: 1.0715 - val_acc: 0.9615\n",
      "Epoch 24/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0469 - acc: 0.9943 - val_loss: 1.1005 - val_acc: 0.9615\n",
      "Epoch 25/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0477 - acc: 0.9929 - val_loss: 1.0732 - val_acc: 0.9359\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0463 - acc: 0.9943 - val_loss: 1.0594 - val_acc: 0.9744\n",
      "Epoch 27/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0440 - acc: 0.9986 - val_loss: 1.0827 - val_acc: 0.9487\n",
      "Epoch 28/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0464 - acc: 0.9957 - val_loss: 1.0686 - val_acc: 0.9487\n",
      "Epoch 29/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0435 - acc: 0.9986 - val_loss: 1.0630 - val_acc: 0.9744\n",
      "Epoch 30/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0431 - acc: 0.9986 - val_loss: 1.0638 - val_acc: 0.9744\n",
      "Epoch 31/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0437 - acc: 0.9972 - val_loss: 1.0635 - val_acc: 0.9615\n",
      "Epoch 32/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0423 - acc: 1.0000 - val_loss: 1.0612 - val_acc: 0.9487\n",
      "Epoch 33/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0416 - acc: 1.0000 - val_loss: 1.0582 - val_acc: 0.9744\n",
      "Epoch 34/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0422 - acc: 1.0000 - val_loss: 1.0694 - val_acc: 0.9615\n",
      "Epoch 35/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0416 - acc: 1.0000 - val_loss: 1.0594 - val_acc: 0.9615\n",
      "Epoch 36/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0414 - acc: 1.0000 - val_loss: 1.0731 - val_acc: 0.9487\n",
      "Epoch 37/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0420 - acc: 1.0000 - val_loss: 1.0546 - val_acc: 0.9872\n",
      "Epoch 38/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0429 - acc: 0.9972 - val_loss: 1.0618 - val_acc: 0.9615\n",
      "Epoch 39/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0411 - acc: 1.0000 - val_loss: 1.0592 - val_acc: 0.9744\n",
      "Epoch 40/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0410 - acc: 1.0000 - val_loss: 1.0581 - val_acc: 0.9744\n",
      "Epoch 41/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0413 - acc: 1.0000 - val_loss: 1.0599 - val_acc: 0.9615\n",
      "Epoch 42/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0419 - acc: 1.0000 - val_loss: 1.0619 - val_acc: 0.9487\n",
      "Epoch 43/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0418 - acc: 0.9986 - val_loss: 1.0646 - val_acc: 0.9359\n",
      "Epoch 44/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0409 - acc: 1.0000 - val_loss: 1.0570 - val_acc: 0.9744\n",
      "Epoch 45/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0410 - acc: 1.0000 - val_loss: 1.0735 - val_acc: 0.9359\n",
      "Epoch 46/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0409 - acc: 1.0000 - val_loss: 1.0590 - val_acc: 0.9744\n",
      "Epoch 47/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0407 - acc: 1.0000 - val_loss: 1.0577 - val_acc: 0.9872\n",
      "Epoch 48/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0407 - acc: 1.0000 - val_loss: 1.0605 - val_acc: 0.9487\n",
      "Epoch 49/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0407 - acc: 1.0000 - val_loss: 1.0578 - val_acc: 0.9615\n",
      "Epoch 50/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0411 - acc: 0.9986 - val_loss: 1.0601 - val_acc: 0.9487\n",
      "Epoch 51/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0548 - val_acc: 0.9744\n",
      "Epoch 52/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0581 - val_acc: 0.9615\n",
      "Epoch 53/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0589 - val_acc: 0.9615\n",
      "Epoch 54/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0594 - val_acc: 0.9615\n",
      "Epoch 55/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0565 - val_acc: 0.9744\n",
      "Epoch 56/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0544 - val_acc: 0.9744\n",
      "Epoch 57/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0526 - val_acc: 0.9872\n",
      "Epoch 58/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0551 - val_acc: 0.9744\n",
      "Epoch 59/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0528 - val_acc: 0.9744\n",
      "Epoch 60/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0606 - val_acc: 0.9615\n",
      "Epoch 61/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0578 - val_acc: 0.9744\n",
      "Epoch 62/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0535 - val_acc: 0.9615\n",
      "Epoch 63/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0588 - val_acc: 0.9615\n",
      "Epoch 64/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0529 - val_acc: 0.9744\n",
      "Epoch 65/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0543 - val_acc: 0.9744\n",
      "Epoch 66/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0537 - val_acc: 0.9744\n",
      "Epoch 67/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0567 - val_acc: 0.9744\n",
      "Epoch 68/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0539 - val_acc: 0.9744\n",
      "Epoch 69/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0565 - val_acc: 0.9744\n",
      "Epoch 70/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0526 - val_acc: 0.9744\n",
      "Epoch 71/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0573 - val_acc: 0.9615\n",
      "Epoch 72/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0528 - val_acc: 0.9744\n",
      "Epoch 73/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0554 - val_acc: 0.9744\n",
      "Epoch 74/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0517 - val_acc: 0.9872\n",
      "Epoch 75/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0531 - val_acc: 0.9744\n",
      "Epoch 76/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0535 - val_acc: 0.9744\n",
      "Epoch 77/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0527 - val_acc: 0.9744\n",
      "Epoch 78/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0551 - val_acc: 0.9744\n",
      "Epoch 79/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0530 - val_acc: 0.9744\n",
      "Epoch 80/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0536 - val_acc: 0.9744\n",
      "Epoch 81/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0538 - val_acc: 0.9744\n",
      "Epoch 82/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0530 - val_acc: 0.9744\n",
      "Epoch 83/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0527 - val_acc: 0.9744\n",
      "Epoch 84/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0538 - val_acc: 0.9744\n",
      "Epoch 85/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0542 - val_acc: 0.9744\n",
      "Epoch 86/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0520 - val_acc: 0.9744\n",
      "Epoch 87/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0535 - val_acc: 0.9744\n",
      "Epoch 88/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0531 - val_acc: 0.9744\n",
      "Epoch 89/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0536 - val_acc: 0.9744\n",
      "Epoch 90/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0531 - val_acc: 0.9744\n",
      "Epoch 91/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0524 - val_acc: 0.9744\n",
      "Epoch 92/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0538 - val_acc: 0.9744\n",
      "Epoch 93/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0528 - val_acc: 0.9744\n",
      "Epoch 94/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0529 - val_acc: 0.9744\n",
      "Epoch 95/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0527 - val_acc: 0.9744\n",
      "Epoch 96/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0524 - val_acc: 0.9744\n",
      "Epoch 97/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0536 - val_acc: 0.9744\n",
      "Epoch 98/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0524 - val_acc: 0.9744\n",
      "Epoch 99/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0523 - val_acc: 0.9744\n",
      "Epoch 100/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0522 - val_acc: 0.9744\n",
      "78/78 [==============================] - 0s 334us/step\n",
      "Score for fold 8: loss of 1.0517392372473693; acc of 97.43589743589743%\n",
      "Train on 702 samples, validate on 78 samples\n",
      "Epoch 1/100\n",
      "702/702 [==============================] - 14s 20ms/step - loss: 2.3167 - acc: 0.3191 - val_loss: 1.9019 - val_acc: 0.6923\n",
      "Epoch 2/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.6612 - acc: 0.6937 - val_loss: 1.4712 - val_acc: 0.8590\n",
      "Epoch 3/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.3783 - acc: 0.8034 - val_loss: 1.2568 - val_acc: 0.8718\n",
      "Epoch 4/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.2192 - acc: 0.8704 - val_loss: 1.2426 - val_acc: 0.8333\n",
      "Epoch 5/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.2033 - acc: 0.8932 - val_loss: 1.2459 - val_acc: 0.7692\n",
      "Epoch 6/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1531 - acc: 0.9088 - val_loss: 1.2515 - val_acc: 0.7436\n",
      "Epoch 7/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1265 - acc: 0.9288 - val_loss: 1.1210 - val_acc: 0.9103\n",
      "Epoch 8/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1094 - acc: 0.9416 - val_loss: 1.0909 - val_acc: 0.9615\n",
      "Epoch 9/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0983 - acc: 0.9530 - val_loss: 1.0961 - val_acc: 0.9615\n",
      "Epoch 10/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0814 - acc: 0.9687 - val_loss: 1.1913 - val_acc: 0.9103\n",
      "Epoch 11/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0758 - acc: 0.9672 - val_loss: 1.1315 - val_acc: 0.8718\n",
      "Epoch 12/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0681 - acc: 0.9815 - val_loss: 1.0840 - val_acc: 0.9487\n",
      "Epoch 13/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0763 - acc: 0.9729 - val_loss: 1.0750 - val_acc: 0.9615\n",
      "Epoch 14/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0690 - acc: 0.9715 - val_loss: 1.1008 - val_acc: 0.8974\n",
      "Epoch 15/100\n",
      "702/702 [==============================] - 5s 7ms/step - loss: 1.0587 - acc: 0.9872 - val_loss: 1.0826 - val_acc: 0.9615\n",
      "Epoch 16/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0567 - acc: 0.9900 - val_loss: 1.0695 - val_acc: 0.9744\n",
      "Epoch 17/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0518 - acc: 0.9929 - val_loss: 1.0748 - val_acc: 0.9744\n",
      "Epoch 18/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0491 - acc: 0.9943 - val_loss: 1.0776 - val_acc: 0.9744\n",
      "Epoch 19/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0529 - acc: 0.9929 - val_loss: 1.0763 - val_acc: 0.9744\n",
      "Epoch 20/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0531 - acc: 0.9843 - val_loss: 1.0683 - val_acc: 0.9744\n",
      "Epoch 21/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0471 - acc: 0.9972 - val_loss: 1.0697 - val_acc: 0.9744\n",
      "Epoch 22/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0470 - acc: 0.9957 - val_loss: 1.0689 - val_acc: 0.9615\n",
      "Epoch 23/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0443 - acc: 1.0000 - val_loss: 1.0720 - val_acc: 0.9744\n",
      "Epoch 24/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0466 - acc: 0.9972 - val_loss: 1.0658 - val_acc: 0.9744\n",
      "Epoch 25/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0448 - acc: 0.9986 - val_loss: 1.0690 - val_acc: 0.9744\n",
      "Epoch 26/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0430 - acc: 0.9986 - val_loss: 1.0654 - val_acc: 0.9744\n",
      "Epoch 27/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0432 - acc: 0.9972 - val_loss: 1.0668 - val_acc: 0.9744\n",
      "Epoch 28/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0499 - acc: 0.9972 - val_loss: 1.0669 - val_acc: 0.9744\n",
      "Epoch 29/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0428 - acc: 1.0000 - val_loss: 1.0717 - val_acc: 0.9615\n",
      "Epoch 30/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0450 - acc: 0.9972 - val_loss: 1.0666 - val_acc: 0.9744\n",
      "Epoch 31/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0417 - acc: 1.0000 - val_loss: 1.0650 - val_acc: 0.9744\n",
      "Epoch 32/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0429 - acc: 0.9972 - val_loss: 1.0680 - val_acc: 0.9744\n",
      "Epoch 33/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0417 - acc: 1.0000 - val_loss: 1.0673 - val_acc: 0.9744\n",
      "Epoch 34/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0417 - acc: 1.0000 - val_loss: 1.0666 - val_acc: 0.9744\n",
      "Epoch 35/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0414 - acc: 1.0000 - val_loss: 1.0684 - val_acc: 0.9744\n",
      "Epoch 36/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0416 - acc: 1.0000 - val_loss: 1.0650 - val_acc: 0.9744\n",
      "Epoch 37/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0415 - acc: 1.0000 - val_loss: 1.0663 - val_acc: 0.9744\n",
      "Epoch 38/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0410 - acc: 1.0000 - val_loss: 1.0658 - val_acc: 0.9744\n",
      "Epoch 39/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0412 - acc: 1.0000 - val_loss: 1.0663 - val_acc: 0.9744\n",
      "Epoch 40/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0412 - acc: 1.0000 - val_loss: 1.0647 - val_acc: 0.9744\n",
      "Epoch 41/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0409 - acc: 1.0000 - val_loss: 1.0667 - val_acc: 0.9744\n",
      "Epoch 42/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0409 - acc: 1.0000 - val_loss: 1.0655 - val_acc: 0.9744\n",
      "Epoch 43/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0408 - acc: 1.0000 - val_loss: 1.0651 - val_acc: 0.9744\n",
      "Epoch 44/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0408 - acc: 1.0000 - val_loss: 1.0656 - val_acc: 0.9744\n",
      "Epoch 45/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0408 - acc: 1.0000 - val_loss: 1.0652 - val_acc: 0.9744\n",
      "Epoch 46/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0407 - acc: 1.0000 - val_loss: 1.0658 - val_acc: 0.9744\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0407 - acc: 1.0000 - val_loss: 1.0656 - val_acc: 0.9744\n",
      "Epoch 48/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0407 - acc: 1.0000 - val_loss: 1.0648 - val_acc: 0.9744\n",
      "Epoch 49/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0651 - val_acc: 0.9744\n",
      "Epoch 50/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0656 - val_acc: 0.9744\n",
      "Epoch 51/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0647 - val_acc: 0.9744\n",
      "Epoch 52/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0650 - val_acc: 0.9744\n",
      "Epoch 53/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0656 - val_acc: 0.9744\n",
      "Epoch 54/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0650 - val_acc: 0.9744\n",
      "Epoch 55/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0653 - val_acc: 0.9744\n",
      "Epoch 56/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0648 - val_acc: 0.9744\n",
      "Epoch 57/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0656 - val_acc: 0.9744\n",
      "Epoch 58/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0652 - val_acc: 0.9744\n",
      "Epoch 59/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0650 - val_acc: 0.9744\n",
      "Epoch 60/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0656 - val_acc: 0.9744\n",
      "Epoch 61/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0656 - val_acc: 0.9744\n",
      "Epoch 62/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0656 - val_acc: 0.9744\n",
      "Epoch 63/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0655 - val_acc: 0.9744\n",
      "Epoch 64/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0659 - val_acc: 0.9744\n",
      "Epoch 65/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0654 - val_acc: 0.9744\n",
      "Epoch 66/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0656 - val_acc: 0.9744\n",
      "Epoch 67/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0651 - val_acc: 0.9744\n",
      "Epoch 68/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0652 - val_acc: 0.9744\n",
      "Epoch 69/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0651 - val_acc: 0.9744\n",
      "Epoch 70/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0655 - val_acc: 0.9744\n",
      "Epoch 71/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0653 - val_acc: 0.9744\n",
      "Epoch 72/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0653 - val_acc: 0.9744\n",
      "Epoch 73/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0653 - val_acc: 0.9744\n",
      "Epoch 74/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0656 - val_acc: 0.9744\n",
      "Epoch 75/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0655 - val_acc: 0.9744\n",
      "Epoch 76/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0654 - val_acc: 0.9744\n",
      "Epoch 77/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0651 - val_acc: 0.9744\n",
      "Epoch 78/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0652 - val_acc: 0.9744\n",
      "Epoch 79/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0653 - val_acc: 0.9744\n",
      "Epoch 80/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0655 - val_acc: 0.9744\n",
      "Epoch 81/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0653 - val_acc: 0.9744\n",
      "Epoch 82/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0654 - val_acc: 0.9744\n",
      "Epoch 83/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0655 - val_acc: 0.9744\n",
      "Epoch 84/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0655 - val_acc: 0.9744\n",
      "Epoch 85/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0656 - val_acc: 0.9744\n",
      "Epoch 86/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0657 - val_acc: 0.9744\n",
      "Epoch 87/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0657 - val_acc: 0.9744\n",
      "Epoch 88/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0655 - val_acc: 0.9744\n",
      "Epoch 89/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0658 - val_acc: 0.9744\n",
      "Epoch 90/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0658 - val_acc: 0.9744\n",
      "Epoch 91/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0657 - val_acc: 0.9744\n",
      "Epoch 92/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0657 - val_acc: 0.9744\n",
      "Epoch 93/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0660 - val_acc: 0.9744\n",
      "Epoch 94/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0658 - val_acc: 0.9744\n",
      "Epoch 95/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0659 - val_acc: 0.9744\n",
      "Epoch 96/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0658 - val_acc: 0.9744\n",
      "Epoch 97/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0660 - val_acc: 0.9744\n",
      "Epoch 98/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0659 - val_acc: 0.9744\n",
      "Epoch 99/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0659 - val_acc: 0.9744\n",
      "Epoch 100/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0660 - val_acc: 0.9744\n",
      "78/78 [==============================] - 0s 322us/step\n",
      "Score for fold 9: loss of 1.0659292447261322; acc of 97.43589697740018%\n",
      "Train on 702 samples, validate on 78 samples\n",
      "Epoch 1/100\n",
      "702/702 [==============================] - 14s 20ms/step - loss: 2.2204 - acc: 0.3134 - val_loss: 2.1134 - val_acc: 0.3718\n",
      "Epoch 2/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.6083 - acc: 0.6952 - val_loss: 1.5580 - val_acc: 0.6667\n",
      "Epoch 3/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.3940 - acc: 0.7877 - val_loss: 1.4969 - val_acc: 0.6667\n",
      "Epoch 4/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.2700 - acc: 0.8462 - val_loss: 1.2887 - val_acc: 0.8462\n",
      "Epoch 5/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.2037 - acc: 0.8647 - val_loss: 1.2654 - val_acc: 0.7179\n",
      "Epoch 6/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1526 - acc: 0.8989 - val_loss: 1.5341 - val_acc: 0.6026\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1494 - acc: 0.9131 - val_loss: 1.1268 - val_acc: 0.9744\n",
      "Epoch 8/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0884 - acc: 0.9615 - val_loss: 1.3059 - val_acc: 0.7692\n",
      "Epoch 9/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1128 - acc: 0.9345 - val_loss: 1.1532 - val_acc: 0.9103\n",
      "Epoch 10/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1038 - acc: 0.9387 - val_loss: 1.2594 - val_acc: 0.8462\n",
      "Epoch 11/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0855 - acc: 0.9630 - val_loss: 1.0730 - val_acc: 0.9872\n",
      "Epoch 12/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0920 - acc: 0.9544 - val_loss: 1.1052 - val_acc: 0.9744\n",
      "Epoch 13/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0725 - acc: 0.9829 - val_loss: 1.0658 - val_acc: 0.9744\n",
      "Epoch 14/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0886 - acc: 0.9544 - val_loss: 1.0718 - val_acc: 0.9872\n",
      "Epoch 15/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0582 - acc: 0.9858 - val_loss: 1.0767 - val_acc: 0.9487\n",
      "Epoch 16/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0681 - acc: 0.9744 - val_loss: 1.2551 - val_acc: 0.8718\n",
      "Epoch 17/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0565 - acc: 0.9886 - val_loss: 1.0564 - val_acc: 0.9872\n",
      "Epoch 18/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1006 - acc: 0.9658 - val_loss: 1.0767 - val_acc: 1.0000\n",
      "Epoch 19/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0542 - acc: 0.9900 - val_loss: 1.0689 - val_acc: 0.9744\n",
      "Epoch 20/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0672 - acc: 0.9786 - val_loss: 1.0540 - val_acc: 0.9872\n",
      "Epoch 21/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0478 - acc: 0.9986 - val_loss: 1.0795 - val_acc: 0.9615\n",
      "Epoch 22/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0562 - acc: 0.9886 - val_loss: 1.3255 - val_acc: 0.8462\n",
      "Epoch 23/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0576 - acc: 0.9843 - val_loss: 1.1673 - val_acc: 0.8974\n",
      "Epoch 24/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0588 - acc: 0.9886 - val_loss: 1.0803 - val_acc: 0.9872\n",
      "Epoch 25/100\n",
      "702/702 [==============================] - 5s 7ms/step - loss: 1.0488 - acc: 0.9929 - val_loss: 1.0497 - val_acc: 1.0000\n",
      "Epoch 26/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0474 - acc: 0.9929 - val_loss: 1.0491 - val_acc: 1.0000\n",
      "Epoch 27/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0500 - acc: 0.9929 - val_loss: 1.0615 - val_acc: 0.9872\n",
      "Epoch 28/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0449 - acc: 0.9986 - val_loss: 1.0484 - val_acc: 0.9872\n",
      "Epoch 29/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0507 - acc: 0.9943 - val_loss: 1.0575 - val_acc: 0.9744\n",
      "Epoch 30/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0449 - acc: 0.9957 - val_loss: 1.0542 - val_acc: 0.9744\n",
      "Epoch 31/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0442 - acc: 0.9986 - val_loss: 1.0698 - val_acc: 0.9615\n",
      "Epoch 32/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0442 - acc: 0.9986 - val_loss: 1.0471 - val_acc: 0.9744\n",
      "Epoch 33/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0440 - acc: 0.9972 - val_loss: 1.0453 - val_acc: 1.0000\n",
      "Epoch 34/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0429 - acc: 0.9986 - val_loss: 1.0523 - val_acc: 0.9872\n",
      "Epoch 35/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0459 - acc: 0.9957 - val_loss: 1.0461 - val_acc: 0.9872\n",
      "Epoch 36/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0431 - acc: 0.9957 - val_loss: 1.0459 - val_acc: 0.9872\n",
      "Epoch 37/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0451 - acc: 0.9943 - val_loss: 1.0465 - val_acc: 0.9872\n",
      "Epoch 38/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0420 - acc: 0.9986 - val_loss: 1.0442 - val_acc: 0.9872\n",
      "Epoch 39/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0418 - acc: 1.0000 - val_loss: 1.0455 - val_acc: 0.9872\n",
      "Epoch 40/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0421 - acc: 0.9986 - val_loss: 1.0550 - val_acc: 0.9744\n",
      "Epoch 41/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0415 - acc: 1.0000 - val_loss: 1.0446 - val_acc: 1.0000\n",
      "Epoch 42/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0420 - acc: 1.0000 - val_loss: 1.0440 - val_acc: 1.0000\n",
      "Epoch 43/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0415 - acc: 1.0000 - val_loss: 1.0437 - val_acc: 1.0000\n",
      "Epoch 44/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0413 - acc: 1.0000 - val_loss: 1.0428 - val_acc: 1.0000\n",
      "Epoch 45/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0409 - acc: 1.0000 - val_loss: 1.0441 - val_acc: 1.0000\n",
      "Epoch 46/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0409 - acc: 1.0000 - val_loss: 1.0481 - val_acc: 0.9872\n",
      "Epoch 47/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0411 - acc: 1.0000 - val_loss: 1.0437 - val_acc: 1.0000\n",
      "Epoch 48/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0422 - acc: 0.9986 - val_loss: 1.0443 - val_acc: 0.9872\n",
      "Epoch 49/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0409 - acc: 1.0000 - val_loss: 1.0440 - val_acc: 1.0000\n",
      "Epoch 50/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0408 - acc: 1.0000 - val_loss: 1.0448 - val_acc: 1.0000\n",
      "Epoch 51/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0407 - acc: 1.0000 - val_loss: 1.0443 - val_acc: 0.9872\n",
      "Epoch 52/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0408 - acc: 1.0000 - val_loss: 1.0423 - val_acc: 1.0000\n",
      "Epoch 53/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0438 - val_acc: 0.9872\n",
      "Epoch 54/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0407 - acc: 1.0000 - val_loss: 1.0511 - val_acc: 0.9744\n",
      "Epoch 55/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0418 - val_acc: 1.0000\n",
      "Epoch 56/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0428 - val_acc: 0.9872\n",
      "Epoch 57/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0421 - val_acc: 1.0000\n",
      "Epoch 58/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0422 - val_acc: 1.0000\n",
      "Epoch 59/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0416 - val_acc: 1.0000\n",
      "Epoch 60/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0417 - val_acc: 1.0000\n",
      "Epoch 61/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0420 - val_acc: 1.0000\n",
      "Epoch 62/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0423 - val_acc: 1.0000\n",
      "Epoch 63/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0418 - val_acc: 1.0000\n",
      "Epoch 64/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0420 - val_acc: 1.0000\n",
      "Epoch 65/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0422 - val_acc: 1.0000\n",
      "Epoch 66/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0416 - val_acc: 1.0000\n",
      "Epoch 67/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0418 - val_acc: 1.0000\n",
      "Epoch 68/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0417 - val_acc: 1.0000\n",
      "Epoch 69/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0423 - val_acc: 1.0000\n",
      "Epoch 70/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0427 - val_acc: 0.9872\n",
      "Epoch 71/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0424 - val_acc: 1.0000\n",
      "Epoch 72/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0417 - val_acc: 1.0000\n",
      "Epoch 73/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0423 - val_acc: 1.0000\n",
      "Epoch 74/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0429 - val_acc: 1.0000\n",
      "Epoch 75/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0418 - val_acc: 1.0000\n",
      "Epoch 76/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0422 - val_acc: 1.0000\n",
      "Epoch 77/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0423 - val_acc: 1.0000\n",
      "Epoch 78/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0425 - val_acc: 0.9872\n",
      "Epoch 79/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0425 - val_acc: 1.0000\n",
      "Epoch 80/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0421 - val_acc: 1.0000\n",
      "Epoch 81/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0417 - val_acc: 1.0000\n",
      "Epoch 82/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0422 - val_acc: 1.0000\n",
      "Epoch 83/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0428 - val_acc: 1.0000\n",
      "Epoch 84/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0419 - val_acc: 1.0000\n",
      "Epoch 85/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0420 - val_acc: 1.0000\n",
      "Epoch 86/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0420 - val_acc: 1.0000\n",
      "Epoch 87/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0422 - val_acc: 1.0000\n",
      "Epoch 88/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0418 - val_acc: 1.0000\n",
      "Epoch 89/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0420 - val_acc: 1.0000\n",
      "Epoch 90/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0422 - val_acc: 1.0000\n",
      "Epoch 91/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0418 - val_acc: 1.0000\n",
      "Epoch 92/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0420 - val_acc: 1.0000\n",
      "Epoch 93/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0422 - val_acc: 1.0000\n",
      "Epoch 94/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0423 - val_acc: 1.0000\n",
      "Epoch 95/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0419 - val_acc: 1.0000\n",
      "Epoch 96/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0420 - val_acc: 1.0000\n",
      "Epoch 97/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0420 - val_acc: 1.0000\n",
      "Epoch 98/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0423 - val_acc: 1.0000\n",
      "Epoch 99/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0420 - val_acc: 1.0000\n",
      "Epoch 100/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0420 - val_acc: 1.0000\n",
      "78/78 [==============================] - 0s 374us/step\n",
      "Score for fold 10: loss of 1.0420331817406874; acc of 100.0%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1296x1296 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98.2051280217293\n",
      "Train on 702 samples, validate on 78 samples\n",
      "Epoch 1/100\n",
      "702/702 [==============================] - 15s 21ms/step - loss: 2.2975 - acc: 0.3063 - val_loss: 1.8629 - val_acc: 0.5769\n",
      "Epoch 2/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.6555 - acc: 0.6909 - val_loss: 1.5850 - val_acc: 0.6923\n",
      "Epoch 3/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.3948 - acc: 0.7934 - val_loss: 1.4220 - val_acc: 0.7821\n",
      "Epoch 4/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.2661 - acc: 0.8704 - val_loss: 1.4216 - val_acc: 0.7179\n",
      "Epoch 5/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.2021 - acc: 0.9074 - val_loss: 1.2112 - val_acc: 0.8846\n",
      "Epoch 6/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1500 - acc: 0.9188 - val_loss: 1.3162 - val_acc: 0.7436\n",
      "Epoch 7/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1188 - acc: 0.9501 - val_loss: 1.1196 - val_acc: 0.9359\n",
      "Epoch 8/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0967 - acc: 0.9530 - val_loss: 1.1302 - val_acc: 0.8846\n",
      "Epoch 9/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1010 - acc: 0.9459 - val_loss: 1.1072 - val_acc: 0.9103\n",
      "Epoch 10/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0981 - acc: 0.9758 - val_loss: 1.1045 - val_acc: 0.9359\n",
      "Epoch 11/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0750 - acc: 0.9744 - val_loss: 1.0841 - val_acc: 0.9487\n",
      "Epoch 12/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0659 - acc: 0.9744 - val_loss: 1.1094 - val_acc: 0.9359\n",
      "Epoch 13/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0694 - acc: 0.9715 - val_loss: 1.1025 - val_acc: 0.9103\n",
      "Epoch 14/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0852 - acc: 0.9630 - val_loss: 1.0864 - val_acc: 0.9487\n",
      "Epoch 15/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0604 - acc: 0.9843 - val_loss: 1.0950 - val_acc: 0.9103\n",
      "Epoch 16/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0647 - acc: 0.9801 - val_loss: 1.1424 - val_acc: 0.8718\n",
      "Epoch 17/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0665 - acc: 0.9715 - val_loss: 1.0804 - val_acc: 0.9615\n",
      "Epoch 18/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0545 - acc: 0.9929 - val_loss: 1.0867 - val_acc: 0.9359\n",
      "Epoch 19/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0553 - acc: 0.9900 - val_loss: 1.0849 - val_acc: 0.9359\n",
      "Epoch 20/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0588 - acc: 0.9915 - val_loss: 1.0806 - val_acc: 0.9487\n",
      "Epoch 21/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0483 - acc: 0.9972 - val_loss: 1.0895 - val_acc: 0.9487\n",
      "Epoch 22/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0517 - acc: 0.9858 - val_loss: 1.0769 - val_acc: 0.9359\n",
      "Epoch 23/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0505 - acc: 0.9886 - val_loss: 1.0799 - val_acc: 0.9487\n",
      "Epoch 24/100\n",
      "702/702 [==============================] - 5s 7ms/step - loss: 1.0567 - acc: 0.9943 - val_loss: 1.0761 - val_acc: 0.9487\n",
      "Epoch 25/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0455 - acc: 0.9972 - val_loss: 1.0856 - val_acc: 0.9231\n",
      "Epoch 26/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0475 - acc: 0.9972 - val_loss: 1.0893 - val_acc: 0.9359\n",
      "Epoch 27/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0438 - acc: 0.9972 - val_loss: 1.0745 - val_acc: 0.9231\n",
      "Epoch 28/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0442 - acc: 0.9972 - val_loss: 1.0761 - val_acc: 0.9359\n",
      "Epoch 29/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0443 - acc: 0.9972 - val_loss: 1.0742 - val_acc: 0.9487\n",
      "Epoch 30/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0437 - acc: 0.9986 - val_loss: 1.0717 - val_acc: 0.9615\n",
      "Epoch 31/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0450 - acc: 0.9972 - val_loss: 1.0710 - val_acc: 0.9615\n",
      "Epoch 32/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0428 - acc: 0.9986 - val_loss: 1.0695 - val_acc: 0.9615\n",
      "Epoch 33/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0420 - acc: 0.9986 - val_loss: 1.0692 - val_acc: 0.9487\n",
      "Epoch 34/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0425 - acc: 0.9986 - val_loss: 1.0697 - val_acc: 0.9487\n",
      "Epoch 35/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0415 - acc: 1.0000 - val_loss: 1.0697 - val_acc: 0.9615\n",
      "Epoch 36/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0414 - acc: 1.0000 - val_loss: 1.0716 - val_acc: 0.9487\n",
      "Epoch 37/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0416 - acc: 1.0000 - val_loss: 1.0720 - val_acc: 0.9487\n",
      "Epoch 38/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0411 - acc: 1.0000 - val_loss: 1.0712 - val_acc: 0.9487\n",
      "Epoch 39/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0410 - acc: 1.0000 - val_loss: 1.0728 - val_acc: 0.9615\n",
      "Epoch 40/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0409 - acc: 1.0000 - val_loss: 1.0710 - val_acc: 0.9615\n",
      "Epoch 41/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0422 - acc: 0.9986 - val_loss: 1.0719 - val_acc: 0.9487\n",
      "Epoch 42/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0411 - acc: 1.0000 - val_loss: 1.0687 - val_acc: 0.9487\n",
      "Epoch 43/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0407 - acc: 1.0000 - val_loss: 1.0678 - val_acc: 0.9487\n",
      "Epoch 44/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0407 - acc: 1.0000 - val_loss: 1.0725 - val_acc: 0.9487\n",
      "Epoch 45/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0407 - acc: 1.0000 - val_loss: 1.0736 - val_acc: 0.9487\n",
      "Epoch 46/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0408 - acc: 1.0000 - val_loss: 1.0711 - val_acc: 0.9487\n",
      "Epoch 47/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0702 - val_acc: 0.9615\n",
      "Epoch 48/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0728 - val_acc: 0.9487\n",
      "Epoch 49/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0704 - val_acc: 0.9615\n",
      "Epoch 50/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0714 - val_acc: 0.9487\n",
      "Epoch 51/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0698 - val_acc: 0.9487\n",
      "Epoch 52/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0736 - val_acc: 0.9487\n",
      "Epoch 53/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0707 - val_acc: 0.9487\n",
      "Epoch 54/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0687 - val_acc: 0.9487\n",
      "Epoch 55/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0706 - val_acc: 0.9615\n",
      "Epoch 56/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0710 - val_acc: 0.9487\n",
      "Epoch 57/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0684 - val_acc: 0.9615\n",
      "Epoch 58/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0715 - val_acc: 0.9487\n",
      "Epoch 59/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0682 - val_acc: 0.9615\n",
      "Epoch 60/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0687 - val_acc: 0.9487\n",
      "Epoch 61/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0698 - val_acc: 0.9615\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 62/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0713 - val_acc: 0.9487\n",
      "Epoch 63/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0662 - val_acc: 0.9487\n",
      "Epoch 64/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0712 - val_acc: 0.9487\n",
      "Epoch 65/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0695 - val_acc: 0.9487\n",
      "Epoch 66/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0701 - val_acc: 0.9487\n",
      "Epoch 67/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0677 - val_acc: 0.9487\n",
      "Epoch 68/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0689 - val_acc: 0.9487\n",
      "Epoch 69/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0708 - val_acc: 0.9487\n",
      "Epoch 70/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0713 - val_acc: 0.9615\n",
      "Epoch 71/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0703 - val_acc: 0.9487\n",
      "Epoch 72/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0719 - val_acc: 0.9487\n",
      "Epoch 73/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0696 - val_acc: 0.9487\n",
      "Epoch 74/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0698 - val_acc: 0.9615\n",
      "Epoch 75/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0708 - val_acc: 0.9487\n",
      "Epoch 76/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0708 - val_acc: 0.9615\n",
      "Epoch 77/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0688 - val_acc: 0.9487\n",
      "Epoch 78/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0702 - val_acc: 0.9487\n",
      "Epoch 79/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0684 - val_acc: 0.9615\n",
      "Epoch 80/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0695 - val_acc: 0.9615\n",
      "Epoch 81/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0687 - val_acc: 0.9615\n",
      "Epoch 82/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0698 - val_acc: 0.9615\n",
      "Epoch 83/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0698 - val_acc: 0.9487\n",
      "Epoch 84/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0685 - val_acc: 0.9615\n",
      "Epoch 85/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0700 - val_acc: 0.9487\n",
      "Epoch 86/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0705 - val_acc: 0.9487\n",
      "Epoch 87/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0702 - val_acc: 0.9487\n",
      "Epoch 88/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0703 - val_acc: 0.9615\n",
      "Epoch 89/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0702 - val_acc: 0.9487\n",
      "Epoch 90/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0698 - val_acc: 0.9487\n",
      "Epoch 91/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0703 - val_acc: 0.9487\n",
      "Epoch 92/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0706 - val_acc: 0.9487\n",
      "Epoch 93/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0697 - val_acc: 0.9487\n",
      "Epoch 94/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0698 - val_acc: 0.9487\n",
      "Epoch 95/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0702 - val_acc: 0.9487\n",
      "Epoch 96/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0691 - val_acc: 0.9615\n",
      "Epoch 97/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0700 - val_acc: 0.9487\n",
      "Epoch 98/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0697 - val_acc: 0.9487\n",
      "Epoch 99/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0700 - val_acc: 0.9487\n",
      "Epoch 100/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0703 - val_acc: 0.9487\n",
      "78/78 [==============================] - 0s 330us/step\n",
      "Score for fold 1: loss of 1.070901131018614; acc of 94.87179487179486%\n",
      "Train on 702 samples, validate on 78 samples\n",
      "Epoch 1/100\n",
      "702/702 [==============================] - 15s 21ms/step - loss: 2.2620 - acc: 0.3234 - val_loss: 2.0093 - val_acc: 0.4487\n",
      "Epoch 2/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.6513 - acc: 0.6595 - val_loss: 1.5356 - val_acc: 0.7821\n",
      "Epoch 3/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.3420 - acc: 0.8405 - val_loss: 1.4407 - val_acc: 0.7949\n",
      "Epoch 4/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.2568 - acc: 0.8689 - val_loss: 1.5594 - val_acc: 0.6923\n",
      "Epoch 5/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1682 - acc: 0.9074 - val_loss: 1.2374 - val_acc: 0.7949\n",
      "Epoch 6/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1490 - acc: 0.9046 - val_loss: 1.2088 - val_acc: 0.8846\n",
      "Epoch 7/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1317 - acc: 0.9402 - val_loss: 1.2492 - val_acc: 0.8205\n",
      "Epoch 8/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1026 - acc: 0.9473 - val_loss: 1.1072 - val_acc: 0.9487\n",
      "Epoch 9/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1106 - acc: 0.9487 - val_loss: 1.1410 - val_acc: 0.8718\n",
      "Epoch 10/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0661 - acc: 0.9829 - val_loss: 1.2352 - val_acc: 0.8974\n",
      "Epoch 11/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0712 - acc: 0.9744 - val_loss: 1.0883 - val_acc: 0.9487\n",
      "Epoch 12/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0976 - acc: 0.9387 - val_loss: 1.0901 - val_acc: 0.9615\n",
      "Epoch 13/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0571 - acc: 0.9915 - val_loss: 1.0957 - val_acc: 0.9487\n",
      "Epoch 14/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0601 - acc: 0.9786 - val_loss: 1.1198 - val_acc: 0.9359\n",
      "Epoch 15/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0628 - acc: 0.9801 - val_loss: 1.0778 - val_acc: 0.9615\n",
      "Epoch 16/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0580 - acc: 0.9900 - val_loss: 1.0742 - val_acc: 0.9744\n",
      "Epoch 17/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0866 - acc: 0.9715 - val_loss: 1.0923 - val_acc: 0.9487\n",
      "Epoch 18/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0520 - acc: 0.9915 - val_loss: 1.0802 - val_acc: 0.9231\n",
      "Epoch 19/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0517 - acc: 0.9929 - val_loss: 1.0867 - val_acc: 0.9615\n",
      "Epoch 20/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0635 - acc: 0.9815 - val_loss: 1.0624 - val_acc: 0.9359\n",
      "Epoch 21/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0483 - acc: 0.9943 - val_loss: 1.0955 - val_acc: 0.9744\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0545 - acc: 0.9886 - val_loss: 1.0829 - val_acc: 0.9231\n",
      "Epoch 23/100\n",
      "702/702 [==============================] - 5s 7ms/step - loss: 1.0552 - acc: 0.9858 - val_loss: 1.1080 - val_acc: 0.9359\n",
      "Epoch 24/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0454 - acc: 0.9986 - val_loss: 1.1224 - val_acc: 0.9231\n",
      "Epoch 25/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0469 - acc: 0.9929 - val_loss: 1.0792 - val_acc: 0.9615\n",
      "Epoch 26/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0456 - acc: 0.9972 - val_loss: 1.0877 - val_acc: 0.9615\n",
      "Epoch 27/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0440 - acc: 0.9972 - val_loss: 1.0606 - val_acc: 0.9872\n",
      "Epoch 28/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0542 - acc: 0.9943 - val_loss: 1.0834 - val_acc: 0.9615\n",
      "Epoch 29/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0432 - acc: 0.9986 - val_loss: 1.0804 - val_acc: 0.9615\n",
      "Epoch 30/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0547 - acc: 0.9886 - val_loss: 1.0544 - val_acc: 0.9872\n",
      "Epoch 31/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0426 - acc: 1.0000 - val_loss: 1.0577 - val_acc: 0.9744\n",
      "Epoch 32/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0435 - acc: 0.9986 - val_loss: 1.0766 - val_acc: 0.9744\n",
      "Epoch 33/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0423 - acc: 1.0000 - val_loss: 1.0651 - val_acc: 0.9615\n",
      "Epoch 34/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0460 - acc: 0.9943 - val_loss: 1.0666 - val_acc: 0.9744\n",
      "Epoch 35/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0421 - acc: 0.9986 - val_loss: 1.0570 - val_acc: 0.9872\n",
      "Epoch 36/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0417 - acc: 1.0000 - val_loss: 1.0598 - val_acc: 0.9744\n",
      "Epoch 37/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0423 - acc: 0.9972 - val_loss: 1.0848 - val_acc: 0.9744\n",
      "Epoch 38/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0415 - acc: 1.0000 - val_loss: 1.0644 - val_acc: 0.9744\n",
      "Epoch 39/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0436 - acc: 0.9957 - val_loss: 1.0577 - val_acc: 0.9744\n",
      "Epoch 40/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0412 - acc: 1.0000 - val_loss: 1.0607 - val_acc: 0.9615\n",
      "Epoch 41/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0422 - acc: 0.9972 - val_loss: 1.0519 - val_acc: 0.9872\n",
      "Epoch 42/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0408 - acc: 1.0000 - val_loss: 1.0535 - val_acc: 0.9744\n",
      "Epoch 43/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0408 - acc: 1.0000 - val_loss: 1.0609 - val_acc: 0.9744\n",
      "Epoch 44/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0409 - acc: 1.0000 - val_loss: 1.0573 - val_acc: 0.9744\n",
      "Epoch 45/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0418 - acc: 0.9986 - val_loss: 1.0495 - val_acc: 0.9872\n",
      "Epoch 46/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0411 - acc: 1.0000 - val_loss: 1.0576 - val_acc: 0.9872\n",
      "Epoch 47/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0571 - val_acc: 0.9744\n",
      "Epoch 48/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0737 - val_acc: 0.9744\n",
      "Epoch 49/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0409 - acc: 1.0000 - val_loss: 1.0523 - val_acc: 0.9872\n",
      "Epoch 50/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0527 - val_acc: 0.9872\n",
      "Epoch 51/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0748 - val_acc: 0.9744\n",
      "Epoch 52/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0510 - val_acc: 0.9872\n",
      "Epoch 53/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0499 - val_acc: 0.9872\n",
      "Epoch 54/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0521 - val_acc: 0.9744\n",
      "Epoch 55/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0532 - val_acc: 0.9744\n",
      "Epoch 56/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0630 - val_acc: 0.9744\n",
      "Epoch 57/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0629 - val_acc: 0.9744\n",
      "Epoch 58/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0550 - val_acc: 0.9872\n",
      "Epoch 59/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0604 - val_acc: 0.9744\n",
      "Epoch 60/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0620 - val_acc: 0.9744\n",
      "Epoch 61/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0571 - val_acc: 0.9744\n",
      "Epoch 62/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0605 - val_acc: 0.9744\n",
      "Epoch 63/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0544 - val_acc: 0.9744\n",
      "Epoch 64/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0561 - val_acc: 0.9744\n",
      "Epoch 65/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0515 - val_acc: 0.9872\n",
      "Epoch 66/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0499 - val_acc: 0.9872\n",
      "Epoch 67/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0532 - val_acc: 0.9744\n",
      "Epoch 68/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0498 - val_acc: 0.9872\n",
      "Epoch 69/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0574 - val_acc: 0.9744\n",
      "Epoch 70/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0654 - val_acc: 0.9744\n",
      "Epoch 71/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0604 - val_acc: 0.9744\n",
      "Epoch 72/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0527 - val_acc: 0.9744\n",
      "Epoch 73/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0585 - val_acc: 0.9744\n",
      "Epoch 74/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0582 - val_acc: 0.9744\n",
      "Epoch 75/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0508 - val_acc: 0.9872\n",
      "Epoch 76/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0601 - val_acc: 0.9744\n",
      "Epoch 77/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0586 - val_acc: 0.9744\n",
      "Epoch 78/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0589 - val_acc: 0.9744\n",
      "Epoch 79/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0577 - val_acc: 0.9744\n",
      "Epoch 80/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0556 - val_acc: 0.9744\n",
      "Epoch 81/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0539 - val_acc: 0.9744\n",
      "Epoch 82/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0584 - val_acc: 0.9744\n",
      "Epoch 83/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0539 - val_acc: 0.9744\n",
      "Epoch 84/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0593 - val_acc: 0.9744\n",
      "Epoch 85/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0580 - val_acc: 0.9744\n",
      "Epoch 86/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0587 - val_acc: 0.9744\n",
      "Epoch 87/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0607 - val_acc: 0.9744\n",
      "Epoch 88/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0574 - val_acc: 0.9744\n",
      "Epoch 89/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0591 - val_acc: 0.9744\n",
      "Epoch 90/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0548 - val_acc: 0.9744\n",
      "Epoch 91/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0538 - val_acc: 0.9744\n",
      "Epoch 92/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0609 - val_acc: 0.9744\n",
      "Epoch 93/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0557 - val_acc: 0.9744\n",
      "Epoch 94/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0575 - val_acc: 0.9744\n",
      "Epoch 95/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0593 - val_acc: 0.9744\n",
      "Epoch 96/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0569 - val_acc: 0.9744\n",
      "Epoch 97/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0602 - val_acc: 0.9744\n",
      "Epoch 98/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0553 - val_acc: 0.9744\n",
      "Epoch 99/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0575 - val_acc: 0.9744\n",
      "Epoch 100/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0589 - val_acc: 0.9744\n",
      "78/78 [==============================] - 0s 319us/step\n",
      "Score for fold 2: loss of 1.060818190758045; acc of 97.43589743589743%\n",
      "Train on 702 samples, validate on 78 samples\n",
      "Epoch 1/100\n",
      "702/702 [==============================] - 15s 21ms/step - loss: 2.2537 - acc: 0.3348 - val_loss: 1.9544 - val_acc: 0.4872\n",
      "Epoch 2/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.6503 - acc: 0.6553 - val_loss: 1.4913 - val_acc: 0.6667\n",
      "Epoch 3/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.3668 - acc: 0.8191 - val_loss: 1.2331 - val_acc: 0.9359\n",
      "Epoch 4/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.2784 - acc: 0.8519 - val_loss: 1.3576 - val_acc: 0.7692\n",
      "Epoch 5/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1928 - acc: 0.9074 - val_loss: 1.2331 - val_acc: 0.7949\n",
      "Epoch 6/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1473 - acc: 0.8917 - val_loss: 1.3333 - val_acc: 0.6667\n",
      "Epoch 7/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1301 - acc: 0.9288 - val_loss: 1.1295 - val_acc: 0.8718\n",
      "Epoch 8/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0956 - acc: 0.9587 - val_loss: 1.1959 - val_acc: 0.9231\n",
      "Epoch 9/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1128 - acc: 0.9473 - val_loss: 1.1194 - val_acc: 0.9103\n",
      "Epoch 10/100\n",
      "702/702 [==============================] - 5s 7ms/step - loss: 1.0854 - acc: 0.9644 - val_loss: 1.1053 - val_acc: 0.9359\n",
      "Epoch 11/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0766 - acc: 0.9758 - val_loss: 1.0870 - val_acc: 0.9487\n",
      "Epoch 12/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0617 - acc: 0.9858 - val_loss: 1.0697 - val_acc: 0.9744\n",
      "Epoch 13/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0632 - acc: 0.9772 - val_loss: 1.1065 - val_acc: 0.9359\n",
      "Epoch 14/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0641 - acc: 0.9829 - val_loss: 1.1163 - val_acc: 0.8974\n",
      "Epoch 15/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0616 - acc: 0.9829 - val_loss: 1.0788 - val_acc: 0.9744\n",
      "Epoch 16/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0547 - acc: 0.9915 - val_loss: 1.0813 - val_acc: 0.9103\n",
      "Epoch 17/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0532 - acc: 0.9929 - val_loss: 1.0656 - val_acc: 0.9872\n",
      "Epoch 18/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0554 - acc: 0.9929 - val_loss: 1.0562 - val_acc: 0.9872\n",
      "Epoch 19/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0560 - acc: 0.9829 - val_loss: 1.1340 - val_acc: 0.8846\n",
      "Epoch 20/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0555 - acc: 0.9843 - val_loss: 1.0681 - val_acc: 0.9744\n",
      "Epoch 21/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0489 - acc: 0.9943 - val_loss: 1.1165 - val_acc: 0.9359\n",
      "Epoch 22/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0538 - acc: 0.9900 - val_loss: 1.0756 - val_acc: 0.9487\n",
      "Epoch 23/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0506 - acc: 0.9858 - val_loss: 1.0737 - val_acc: 0.9744\n",
      "Epoch 24/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0488 - acc: 0.9929 - val_loss: 1.0691 - val_acc: 0.9615\n",
      "Epoch 25/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0456 - acc: 0.9943 - val_loss: 1.0845 - val_acc: 0.9359\n",
      "Epoch 26/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0464 - acc: 0.9972 - val_loss: 1.0837 - val_acc: 0.9744\n",
      "Epoch 27/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0449 - acc: 0.9972 - val_loss: 1.0705 - val_acc: 0.9744\n",
      "Epoch 28/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0447 - acc: 1.0000 - val_loss: 1.0582 - val_acc: 0.9872\n",
      "Epoch 29/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0481 - acc: 0.9957 - val_loss: 1.0534 - val_acc: 0.9872\n",
      "Epoch 30/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0426 - acc: 1.0000 - val_loss: 1.0554 - val_acc: 0.9872\n",
      "Epoch 31/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0432 - acc: 1.0000 - val_loss: 1.0625 - val_acc: 0.9744\n",
      "Epoch 32/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0433 - acc: 0.9986 - val_loss: 1.0509 - val_acc: 1.0000\n",
      "Epoch 33/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0427 - acc: 0.9986 - val_loss: 1.0469 - val_acc: 1.0000\n",
      "Epoch 34/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0417 - acc: 1.0000 - val_loss: 1.0526 - val_acc: 0.9872\n",
      "Epoch 35/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0419 - acc: 1.0000 - val_loss: 1.0501 - val_acc: 1.0000\n",
      "Epoch 36/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0425 - acc: 0.9986 - val_loss: 1.0493 - val_acc: 0.9872\n",
      "Epoch 37/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0415 - acc: 1.0000 - val_loss: 1.0511 - val_acc: 0.9872\n",
      "Epoch 38/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0416 - acc: 1.0000 - val_loss: 1.0477 - val_acc: 0.9872\n",
      "Epoch 39/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0418 - acc: 1.0000 - val_loss: 1.0459 - val_acc: 1.0000\n",
      "Epoch 40/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0423 - acc: 1.0000 - val_loss: 1.0488 - val_acc: 0.9872\n",
      "Epoch 41/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0410 - acc: 1.0000 - val_loss: 1.0479 - val_acc: 1.0000\n",
      "Epoch 42/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0414 - acc: 1.0000 - val_loss: 1.0560 - val_acc: 0.9872\n",
      "Epoch 43/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0410 - acc: 1.0000 - val_loss: 1.0618 - val_acc: 0.9744\n",
      "Epoch 44/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0411 - acc: 1.0000 - val_loss: 1.0533 - val_acc: 0.9872\n",
      "Epoch 45/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0409 - acc: 1.0000 - val_loss: 1.0476 - val_acc: 1.0000\n",
      "Epoch 46/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0411 - acc: 1.0000 - val_loss: 1.0519 - val_acc: 0.9872\n",
      "Epoch 47/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0407 - acc: 1.0000 - val_loss: 1.0473 - val_acc: 0.9872\n",
      "Epoch 48/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0409 - acc: 1.0000 - val_loss: 1.0487 - val_acc: 0.9872\n",
      "Epoch 49/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0408 - acc: 1.0000 - val_loss: 1.0480 - val_acc: 1.0000\n",
      "Epoch 50/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0407 - acc: 1.0000 - val_loss: 1.0499 - val_acc: 0.9872\n",
      "Epoch 51/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0412 - acc: 0.9986 - val_loss: 1.0487 - val_acc: 0.9872\n",
      "Epoch 52/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0408 - acc: 1.0000 - val_loss: 1.0466 - val_acc: 0.9872\n",
      "Epoch 53/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0488 - val_acc: 0.9872\n",
      "Epoch 54/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0448 - val_acc: 0.9872\n",
      "Epoch 55/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0490 - val_acc: 0.9872\n",
      "Epoch 56/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0443 - val_acc: 1.0000\n",
      "Epoch 57/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0475 - val_acc: 0.9872\n",
      "Epoch 58/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0456 - val_acc: 1.0000\n",
      "Epoch 59/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0500 - val_acc: 0.9872\n",
      "Epoch 60/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0452 - val_acc: 1.0000\n",
      "Epoch 61/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0502 - val_acc: 0.9872\n",
      "Epoch 62/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0508 - val_acc: 0.9872\n",
      "Epoch 63/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0490 - val_acc: 0.9872\n",
      "Epoch 64/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0488 - val_acc: 0.9872\n",
      "Epoch 65/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0509 - val_acc: 0.9872\n",
      "Epoch 66/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0462 - val_acc: 1.0000\n",
      "Epoch 67/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0479 - val_acc: 0.9872\n",
      "Epoch 68/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0461 - val_acc: 0.9872\n",
      "Epoch 69/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0458 - val_acc: 1.0000\n",
      "Epoch 70/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0488 - val_acc: 1.0000\n",
      "Epoch 71/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0466 - val_acc: 1.0000\n",
      "Epoch 72/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0458 - val_acc: 1.0000\n",
      "Epoch 73/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0463 - val_acc: 1.0000\n",
      "Epoch 74/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0450 - val_acc: 1.0000\n",
      "Epoch 75/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0468 - val_acc: 0.9872\n",
      "Epoch 76/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0467 - val_acc: 0.9872\n",
      "Epoch 77/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0463 - val_acc: 1.0000\n",
      "Epoch 78/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0455 - val_acc: 1.0000\n",
      "Epoch 79/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0471 - val_acc: 0.9872\n",
      "Epoch 80/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0466 - val_acc: 0.9872\n",
      "Epoch 81/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0485 - val_acc: 0.9872\n",
      "Epoch 82/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0463 - val_acc: 1.0000\n",
      "Epoch 83/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0464 - val_acc: 1.0000\n",
      "Epoch 84/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0463 - val_acc: 1.0000\n",
      "Epoch 85/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0464 - val_acc: 0.9872\n",
      "Epoch 86/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0472 - val_acc: 0.9872\n",
      "Epoch 87/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0460 - val_acc: 0.9872\n",
      "Epoch 88/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0465 - val_acc: 0.9872\n",
      "Epoch 89/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0463 - val_acc: 1.0000\n",
      "Epoch 90/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0467 - val_acc: 0.9872\n",
      "Epoch 91/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0470 - val_acc: 0.9872\n",
      "Epoch 92/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0469 - val_acc: 0.9872\n",
      "Epoch 93/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0469 - val_acc: 0.9872\n",
      "Epoch 94/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0460 - val_acc: 1.0000\n",
      "Epoch 95/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0467 - val_acc: 0.9872\n",
      "Epoch 96/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0465 - val_acc: 1.0000\n",
      "Epoch 97/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0466 - val_acc: 0.9872\n",
      "Epoch 98/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0471 - val_acc: 0.9872\n",
      "Epoch 99/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0465 - val_acc: 0.9872\n",
      "Epoch 100/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0465 - val_acc: 0.9872\n",
      "78/78 [==============================] - 0s 343us/step\n",
      "Score for fold 3: loss of 1.046442640133393; acc of 98.71794871794873%\n",
      "Train on 702 samples, validate on 78 samples\n",
      "Epoch 1/100\n",
      "702/702 [==============================] - 15s 22ms/step - loss: 2.2835 - acc: 0.3276 - val_loss: 2.0662 - val_acc: 0.5128\n",
      "Epoch 2/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.6855 - acc: 0.6496 - val_loss: 1.5141 - val_acc: 0.7564\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.3538 - acc: 0.8177 - val_loss: 1.3933 - val_acc: 0.7051\n",
      "Epoch 4/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.2651 - acc: 0.8490 - val_loss: 1.3050 - val_acc: 0.8205\n",
      "Epoch 5/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1767 - acc: 0.9003 - val_loss: 1.1815 - val_acc: 0.8974\n",
      "Epoch 6/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1414 - acc: 0.9245 - val_loss: 1.1145 - val_acc: 0.8846\n",
      "Epoch 7/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1137 - acc: 0.9302 - val_loss: 1.2547 - val_acc: 0.7821\n",
      "Epoch 8/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1009 - acc: 0.9473 - val_loss: 1.1809 - val_acc: 0.8718\n",
      "Epoch 9/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1025 - acc: 0.9601 - val_loss: 1.0902 - val_acc: 0.9359\n",
      "Epoch 10/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0818 - acc: 0.9473 - val_loss: 1.0936 - val_acc: 0.9231\n",
      "Epoch 11/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0802 - acc: 0.9544 - val_loss: 1.1262 - val_acc: 0.8205\n",
      "Epoch 12/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0919 - acc: 0.9444 - val_loss: 1.0948 - val_acc: 0.9103\n",
      "Epoch 13/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0723 - acc: 0.9772 - val_loss: 1.0618 - val_acc: 0.9744\n",
      "Epoch 14/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0547 - acc: 0.9900 - val_loss: 1.0651 - val_acc: 0.9872\n",
      "Epoch 15/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0664 - acc: 0.9772 - val_loss: 1.0800 - val_acc: 0.9615\n",
      "Epoch 16/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0650 - acc: 0.9858 - val_loss: 1.3322 - val_acc: 0.8205\n",
      "Epoch 17/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0694 - acc: 0.9772 - val_loss: 1.3554 - val_acc: 0.8846\n",
      "Epoch 18/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0594 - acc: 0.9900 - val_loss: 1.0675 - val_acc: 0.9744\n",
      "Epoch 19/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0539 - acc: 0.9886 - val_loss: 1.0652 - val_acc: 0.9615\n",
      "Epoch 20/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0555 - acc: 0.9772 - val_loss: 1.0551 - val_acc: 0.9872\n",
      "Epoch 21/100\n",
      "702/702 [==============================] - 5s 8ms/step - loss: 1.0621 - acc: 0.9744 - val_loss: 1.0579 - val_acc: 0.9872\n",
      "Epoch 22/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0472 - acc: 0.9957 - val_loss: 1.0535 - val_acc: 1.0000\n",
      "Epoch 23/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0487 - acc: 0.9957 - val_loss: 1.0526 - val_acc: 0.9872\n",
      "Epoch 24/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0543 - acc: 0.9900 - val_loss: 1.0594 - val_acc: 0.9872\n",
      "Epoch 25/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0458 - acc: 0.9957 - val_loss: 1.0556 - val_acc: 0.9744\n",
      "Epoch 26/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0449 - acc: 0.9972 - val_loss: 1.0522 - val_acc: 0.9744\n",
      "Epoch 27/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0469 - acc: 0.9957 - val_loss: 1.0518 - val_acc: 0.9872\n",
      "Epoch 28/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0440 - acc: 0.9986 - val_loss: 1.0614 - val_acc: 0.9872\n",
      "Epoch 29/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0473 - acc: 0.9915 - val_loss: 1.0451 - val_acc: 0.9872\n",
      "Epoch 30/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0432 - acc: 0.9986 - val_loss: 1.0473 - val_acc: 0.9872\n",
      "Epoch 31/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0426 - acc: 0.9986 - val_loss: 1.0574 - val_acc: 0.9744\n",
      "Epoch 32/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0444 - acc: 0.9957 - val_loss: 1.0535 - val_acc: 0.9872\n",
      "Epoch 33/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0439 - acc: 0.9986 - val_loss: 1.0549 - val_acc: 0.9744\n",
      "Epoch 34/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0424 - acc: 1.0000 - val_loss: 1.0463 - val_acc: 1.0000\n",
      "Epoch 35/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0419 - acc: 1.0000 - val_loss: 1.0561 - val_acc: 0.9744\n",
      "Epoch 36/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0421 - acc: 1.0000 - val_loss: 1.0437 - val_acc: 1.0000\n",
      "Epoch 37/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0412 - acc: 1.0000 - val_loss: 1.0464 - val_acc: 0.9872\n",
      "Epoch 38/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0424 - acc: 0.9986 - val_loss: 1.0455 - val_acc: 1.0000\n",
      "Epoch 39/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0411 - acc: 1.0000 - val_loss: 1.0434 - val_acc: 1.0000\n",
      "Epoch 40/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0411 - acc: 1.0000 - val_loss: 1.0427 - val_acc: 1.0000\n",
      "Epoch 41/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0417 - acc: 0.9986 - val_loss: 1.0511 - val_acc: 0.9744\n",
      "Epoch 42/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0409 - acc: 1.0000 - val_loss: 1.0434 - val_acc: 1.0000\n",
      "Epoch 43/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0408 - acc: 1.0000 - val_loss: 1.0432 - val_acc: 1.0000\n",
      "Epoch 44/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0408 - acc: 1.0000 - val_loss: 1.0442 - val_acc: 1.0000\n",
      "Epoch 45/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0409 - acc: 1.0000 - val_loss: 1.0432 - val_acc: 1.0000\n",
      "Epoch 46/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0407 - acc: 1.0000 - val_loss: 1.0430 - val_acc: 1.0000\n",
      "Epoch 47/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0411 - acc: 1.0000 - val_loss: 1.0433 - val_acc: 1.0000\n",
      "Epoch 48/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0440 - val_acc: 0.9872\n",
      "Epoch 49/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0425 - val_acc: 1.0000\n",
      "Epoch 50/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0432 - val_acc: 1.0000\n",
      "Epoch 51/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0439 - val_acc: 1.0000\n",
      "Epoch 52/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0422 - val_acc: 1.0000\n",
      "Epoch 53/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0416 - val_acc: 1.0000\n",
      "Epoch 54/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0434 - val_acc: 1.0000\n",
      "Epoch 55/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0407 - acc: 0.9986 - val_loss: 1.0435 - val_acc: 1.0000\n",
      "Epoch 56/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0516 - val_acc: 0.9872\n",
      "Epoch 57/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0416 - val_acc: 1.0000\n",
      "Epoch 58/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0433 - val_acc: 1.0000\n",
      "Epoch 59/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0442 - val_acc: 1.0000\n",
      "Epoch 60/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0421 - val_acc: 1.0000\n",
      "Epoch 61/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0425 - val_acc: 1.0000\n",
      "Epoch 62/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0424 - val_acc: 1.0000\n",
      "Epoch 63/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0428 - val_acc: 1.0000\n",
      "Epoch 64/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0430 - val_acc: 1.0000\n",
      "Epoch 65/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0438 - val_acc: 1.0000\n",
      "Epoch 66/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0431 - val_acc: 1.0000\n",
      "Epoch 67/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0423 - val_acc: 1.0000\n",
      "Epoch 68/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0432 - val_acc: 1.0000\n",
      "Epoch 69/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0421 - val_acc: 1.0000\n",
      "Epoch 70/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0424 - val_acc: 1.0000\n",
      "Epoch 71/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0424 - val_acc: 1.0000\n",
      "Epoch 72/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0430 - val_acc: 1.0000\n",
      "Epoch 73/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0452 - val_acc: 0.9872\n",
      "Epoch 74/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0443 - val_acc: 1.0000\n",
      "Epoch 75/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0459 - val_acc: 0.9872\n",
      "Epoch 76/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0433 - val_acc: 1.0000\n",
      "Epoch 77/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0441 - val_acc: 1.0000\n",
      "Epoch 78/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0439 - val_acc: 1.0000\n",
      "Epoch 79/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0445 - val_acc: 0.9872\n",
      "Epoch 80/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0461 - val_acc: 0.9872\n",
      "Epoch 81/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0421 - val_acc: 1.0000\n",
      "Epoch 82/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0424 - val_acc: 1.0000\n",
      "Epoch 83/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0452 - val_acc: 0.9872\n",
      "Epoch 84/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0445 - val_acc: 0.9872\n",
      "Epoch 85/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0444 - val_acc: 0.9872\n",
      "Epoch 86/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0433 - val_acc: 1.0000\n",
      "Epoch 87/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0435 - val_acc: 1.0000\n",
      "Epoch 88/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0448 - val_acc: 0.9872\n",
      "Epoch 89/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0447 - val_acc: 0.9872\n",
      "Epoch 90/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0455 - val_acc: 0.9872\n",
      "Epoch 91/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0459 - val_acc: 0.9872\n",
      "Epoch 92/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0450 - val_acc: 0.9872\n",
      "Epoch 93/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0452 - val_acc: 0.9872\n",
      "Epoch 94/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0457 - val_acc: 0.9872\n",
      "Epoch 95/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0458 - val_acc: 0.9872\n",
      "Epoch 96/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0453 - val_acc: 0.9872\n",
      "Epoch 97/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0457 - val_acc: 0.9872\n",
      "Epoch 98/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0454 - val_acc: 0.9872\n",
      "Epoch 99/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0458 - val_acc: 0.9872\n",
      "Epoch 100/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0444 - val_acc: 0.9872\n",
      "78/78 [==============================] - 0s 329us/step\n",
      "Score for fold 4: loss of 1.0488538161302223; acc of 98.71794825945145%\n",
      "Train on 702 samples, validate on 78 samples\n",
      "Epoch 1/100\n",
      "702/702 [==============================] - 15s 22ms/step - loss: 2.2916 - acc: 0.3191 - val_loss: 1.9919 - val_acc: 0.3718\n",
      "Epoch 2/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.6750 - acc: 0.6396 - val_loss: 1.7133 - val_acc: 0.7179\n",
      "Epoch 3/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.3607 - acc: 0.8419 - val_loss: 1.3947 - val_acc: 0.7179\n",
      "Epoch 4/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.2480 - acc: 0.8718 - val_loss: 1.4164 - val_acc: 0.7821\n",
      "Epoch 5/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1718 - acc: 0.9117 - val_loss: 1.2113 - val_acc: 0.7949\n",
      "Epoch 6/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1321 - acc: 0.9231 - val_loss: 1.1412 - val_acc: 0.9359\n",
      "Epoch 7/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1141 - acc: 0.9416 - val_loss: 1.1278 - val_acc: 0.9744\n",
      "Epoch 8/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1146 - acc: 0.9530 - val_loss: 1.1382 - val_acc: 0.8846\n",
      "Epoch 9/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0814 - acc: 0.9701 - val_loss: 1.1232 - val_acc: 0.9103\n",
      "Epoch 10/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0740 - acc: 0.9658 - val_loss: 1.2097 - val_acc: 0.8718\n",
      "Epoch 11/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0909 - acc: 0.9630 - val_loss: 1.1465 - val_acc: 0.9231\n",
      "Epoch 12/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0658 - acc: 0.9786 - val_loss: 1.4935 - val_acc: 0.8462\n",
      "Epoch 13/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0839 - acc: 0.9587 - val_loss: 1.1206 - val_acc: 0.9487\n",
      "Epoch 14/100\n",
      "702/702 [==============================] - 5s 8ms/step - loss: 1.0544 - acc: 0.9957 - val_loss: 1.0806 - val_acc: 0.9744\n",
      "Epoch 15/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0557 - acc: 0.9872 - val_loss: 1.1030 - val_acc: 0.9615\n",
      "Epoch 16/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0527 - acc: 0.9872 - val_loss: 1.1001 - val_acc: 0.9103\n",
      "Epoch 17/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0643 - acc: 0.9843 - val_loss: 1.0829 - val_acc: 0.9615\n",
      "Epoch 18/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0506 - acc: 0.9929 - val_loss: 1.0653 - val_acc: 0.9744\n",
      "Epoch 19/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0492 - acc: 0.9929 - val_loss: 1.0594 - val_acc: 0.9872\n",
      "Epoch 20/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0497 - acc: 0.9943 - val_loss: 1.0597 - val_acc: 0.9744\n",
      "Epoch 21/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0607 - acc: 0.9815 - val_loss: 1.0570 - val_acc: 0.9744\n",
      "Epoch 22/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0487 - acc: 0.9957 - val_loss: 1.0577 - val_acc: 0.9615\n",
      "Epoch 23/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0458 - acc: 0.9972 - val_loss: 1.0549 - val_acc: 0.9872\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0464 - acc: 0.9957 - val_loss: 1.0555 - val_acc: 0.9615\n",
      "Epoch 25/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0473 - acc: 0.9957 - val_loss: 1.0892 - val_acc: 0.9615\n",
      "Epoch 26/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0441 - acc: 0.9972 - val_loss: 1.0563 - val_acc: 0.9872\n",
      "Epoch 27/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0438 - acc: 0.9986 - val_loss: 1.0547 - val_acc: 0.9872\n",
      "Epoch 28/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0439 - acc: 0.9972 - val_loss: 1.0597 - val_acc: 0.9872\n",
      "Epoch 29/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0426 - acc: 0.9986 - val_loss: 1.0669 - val_acc: 0.9744\n",
      "Epoch 30/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0424 - acc: 1.0000 - val_loss: 1.0551 - val_acc: 0.9872\n",
      "Epoch 31/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0427 - acc: 0.9986 - val_loss: 1.0511 - val_acc: 0.9872\n",
      "Epoch 32/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0455 - acc: 0.9929 - val_loss: 1.0522 - val_acc: 0.9872\n",
      "Epoch 33/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0413 - acc: 1.0000 - val_loss: 1.0541 - val_acc: 0.9872\n",
      "Epoch 34/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0417 - acc: 1.0000 - val_loss: 1.0675 - val_acc: 0.9744\n",
      "Epoch 35/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0414 - acc: 1.0000 - val_loss: 1.0649 - val_acc: 0.9744\n",
      "Epoch 36/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0414 - acc: 1.0000 - val_loss: 1.0548 - val_acc: 0.9744\n",
      "Epoch 37/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0415 - acc: 1.0000 - val_loss: 1.0498 - val_acc: 0.9872\n",
      "Epoch 38/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0422 - acc: 0.9986 - val_loss: 1.0556 - val_acc: 0.9872\n",
      "Epoch 39/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0412 - acc: 1.0000 - val_loss: 1.0488 - val_acc: 0.9872\n",
      "Epoch 40/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0408 - acc: 1.0000 - val_loss: 1.0488 - val_acc: 0.9872\n",
      "Epoch 41/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0411 - acc: 1.0000 - val_loss: 1.0539 - val_acc: 0.9872\n",
      "Epoch 42/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0408 - acc: 1.0000 - val_loss: 1.0495 - val_acc: 0.9872\n",
      "Epoch 43/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0408 - acc: 1.0000 - val_loss: 1.0500 - val_acc: 0.9872\n",
      "Epoch 44/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0416 - acc: 0.9986 - val_loss: 1.0588 - val_acc: 0.9744\n",
      "Epoch 45/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0407 - acc: 1.0000 - val_loss: 1.0575 - val_acc: 0.9744\n",
      "Epoch 46/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0562 - val_acc: 0.9744\n",
      "Epoch 47/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0551 - val_acc: 0.9744\n",
      "Epoch 48/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0519 - val_acc: 0.9744\n",
      "Epoch 49/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0407 - acc: 1.0000 - val_loss: 1.0561 - val_acc: 0.9744\n",
      "Epoch 50/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0407 - acc: 1.0000 - val_loss: 1.0527 - val_acc: 0.9872\n",
      "Epoch 51/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0521 - val_acc: 0.9744\n",
      "Epoch 52/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0511 - val_acc: 0.9872\n",
      "Epoch 53/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0499 - val_acc: 0.9872\n",
      "Epoch 54/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0516 - val_acc: 0.9744\n",
      "Epoch 55/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0528 - val_acc: 0.9744\n",
      "Epoch 56/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0521 - val_acc: 0.9872\n",
      "Epoch 57/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0555 - val_acc: 0.9744\n",
      "Epoch 58/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0543 - val_acc: 0.9744\n",
      "Epoch 59/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0492 - val_acc: 0.9872\n",
      "Epoch 60/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0555 - val_acc: 0.9744\n",
      "Epoch 61/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0534 - val_acc: 0.9615\n",
      "Epoch 62/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0547 - val_acc: 0.9615\n",
      "Epoch 63/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0522 - val_acc: 0.9872\n",
      "Epoch 64/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0590 - val_acc: 0.9744\n",
      "Epoch 65/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0568 - val_acc: 0.9744\n",
      "Epoch 66/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0539 - val_acc: 0.9744\n",
      "Epoch 67/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0569 - val_acc: 0.9744\n",
      "Epoch 68/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0570 - val_acc: 0.9744\n",
      "Epoch 69/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0557 - val_acc: 0.9744\n",
      "Epoch 70/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0603 - val_acc: 0.9744\n",
      "Epoch 71/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0563 - val_acc: 0.9744\n",
      "Epoch 72/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0550 - val_acc: 0.9744\n",
      "Epoch 73/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0554 - val_acc: 0.9744\n",
      "Epoch 74/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0536 - val_acc: 0.9744\n",
      "Epoch 75/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0530 - val_acc: 0.9744\n",
      "Epoch 76/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0540 - val_acc: 0.9744\n",
      "Epoch 77/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0531 - val_acc: 0.9744\n",
      "Epoch 78/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0518 - val_acc: 0.9872\n",
      "Epoch 79/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0539 - val_acc: 0.9744\n",
      "Epoch 80/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0519 - val_acc: 0.9744\n",
      "Epoch 81/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0544 - val_acc: 0.9744\n",
      "Epoch 82/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0531 - val_acc: 0.9744\n",
      "Epoch 83/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0510 - val_acc: 0.9872\n",
      "Epoch 84/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0514 - val_acc: 0.9872\n",
      "Epoch 85/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0522 - val_acc: 0.9744\n",
      "Epoch 86/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0529 - val_acc: 0.9744\n",
      "Epoch 87/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0510 - val_acc: 0.9872\n",
      "Epoch 88/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0522 - val_acc: 0.9744\n",
      "Epoch 89/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0517 - val_acc: 0.9744\n",
      "Epoch 90/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0523 - val_acc: 0.9744\n",
      "Epoch 91/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0520 - val_acc: 0.9744\n",
      "Epoch 92/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0508 - val_acc: 0.9872\n",
      "Epoch 93/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0516 - val_acc: 0.9744\n",
      "Epoch 94/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0519 - val_acc: 0.9744\n",
      "Epoch 95/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0506 - val_acc: 0.9872\n",
      "Epoch 96/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0505 - val_acc: 0.9872\n",
      "Epoch 97/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0505 - val_acc: 0.9872\n",
      "Epoch 98/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0514 - val_acc: 0.9872\n",
      "Epoch 99/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0518 - val_acc: 0.9744\n",
      "Epoch 100/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0515 - val_acc: 0.9872\n",
      "78/78 [==============================] - 0s 330us/step\n",
      "Score for fold 5: loss of 1.046144934800955; acc of 98.71794871794873%\n",
      "Train on 702 samples, validate on 78 samples\n",
      "Epoch 1/100\n",
      "702/702 [==============================] - 16s 23ms/step - loss: 2.2465 - acc: 0.3319 - val_loss: 1.7796 - val_acc: 0.5641\n",
      "Epoch 2/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.6083 - acc: 0.6624 - val_loss: 1.4729 - val_acc: 0.6923\n",
      "Epoch 3/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.3245 - acc: 0.8319 - val_loss: 1.2504 - val_acc: 0.8846\n",
      "Epoch 4/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.2368 - acc: 0.8689 - val_loss: 1.2537 - val_acc: 0.8333\n",
      "Epoch 5/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1793 - acc: 0.8889 - val_loss: 1.2482 - val_acc: 0.7692\n",
      "Epoch 6/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1391 - acc: 0.9345 - val_loss: 1.4422 - val_acc: 0.7308\n",
      "Epoch 7/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1260 - acc: 0.9387 - val_loss: 1.1188 - val_acc: 0.9231\n",
      "Epoch 8/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1138 - acc: 0.9473 - val_loss: 1.0948 - val_acc: 0.9487\n",
      "Epoch 9/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0973 - acc: 0.9530 - val_loss: 1.1248 - val_acc: 0.8846\n",
      "Epoch 10/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0986 - acc: 0.9615 - val_loss: 1.0852 - val_acc: 0.9615\n",
      "Epoch 11/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0899 - acc: 0.9630 - val_loss: 1.1030 - val_acc: 0.9487\n",
      "Epoch 12/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0643 - acc: 0.9872 - val_loss: 1.0700 - val_acc: 0.9487\n",
      "Epoch 13/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1064 - acc: 0.9615 - val_loss: 1.0932 - val_acc: 0.9615\n",
      "Epoch 14/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0714 - acc: 0.9858 - val_loss: 1.0822 - val_acc: 0.9744\n",
      "Epoch 15/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0961 - acc: 0.9687 - val_loss: 1.1599 - val_acc: 0.8846\n",
      "Epoch 16/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0816 - acc: 0.9729 - val_loss: 1.0732 - val_acc: 0.9487\n",
      "Epoch 17/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0630 - acc: 0.9801 - val_loss: 1.0729 - val_acc: 0.9487\n",
      "Epoch 18/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0553 - acc: 0.9915 - val_loss: 1.0869 - val_acc: 0.9615\n",
      "Epoch 19/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0539 - acc: 0.9858 - val_loss: 1.0943 - val_acc: 0.9103\n",
      "Epoch 20/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0858 - acc: 0.9801 - val_loss: 1.0670 - val_acc: 0.9487\n",
      "Epoch 21/100\n",
      "702/702 [==============================] - 6s 8ms/step - loss: 1.0512 - acc: 0.9943 - val_loss: 1.0648 - val_acc: 0.9615\n",
      "Epoch 22/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0472 - acc: 0.9972 - val_loss: 1.0557 - val_acc: 1.0000\n",
      "Epoch 23/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0486 - acc: 0.9929 - val_loss: 1.0630 - val_acc: 0.9487\n",
      "Epoch 24/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0453 - acc: 0.9943 - val_loss: 1.0572 - val_acc: 0.9615\n",
      "Epoch 25/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0585 - acc: 0.9886 - val_loss: 1.0623 - val_acc: 0.9615\n",
      "Epoch 26/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0454 - acc: 0.9972 - val_loss: 1.0685 - val_acc: 0.9359\n",
      "Epoch 27/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0455 - acc: 0.9972 - val_loss: 1.0667 - val_acc: 0.9487\n",
      "Epoch 28/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0483 - acc: 0.9929 - val_loss: 1.0733 - val_acc: 0.9487\n",
      "Epoch 29/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0459 - acc: 0.9929 - val_loss: 1.0598 - val_acc: 0.9615\n",
      "Epoch 30/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0427 - acc: 1.0000 - val_loss: 1.0707 - val_acc: 0.9744\n",
      "Epoch 31/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0461 - acc: 0.9943 - val_loss: 1.0595 - val_acc: 0.9744\n",
      "Epoch 32/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0431 - acc: 0.9986 - val_loss: 1.0597 - val_acc: 0.9744\n",
      "Epoch 33/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0418 - acc: 1.0000 - val_loss: 1.0596 - val_acc: 0.9615\n",
      "Epoch 34/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0433 - acc: 0.9986 - val_loss: 1.0588 - val_acc: 0.9359\n",
      "Epoch 35/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0450 - acc: 0.9943 - val_loss: 1.0809 - val_acc: 0.9359\n",
      "Epoch 36/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0429 - acc: 1.0000 - val_loss: 1.0594 - val_acc: 0.9615\n",
      "Epoch 37/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0412 - acc: 1.0000 - val_loss: 1.0575 - val_acc: 0.9615\n",
      "Epoch 38/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0412 - acc: 1.0000 - val_loss: 1.0604 - val_acc: 0.9615\n",
      "Epoch 39/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0412 - acc: 1.0000 - val_loss: 1.0555 - val_acc: 0.9615\n",
      "Epoch 40/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0417 - acc: 1.0000 - val_loss: 1.0602 - val_acc: 0.9487\n",
      "Epoch 41/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0411 - acc: 1.0000 - val_loss: 1.0555 - val_acc: 0.9615\n",
      "Epoch 42/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0411 - acc: 1.0000 - val_loss: 1.0568 - val_acc: 0.9744\n",
      "Epoch 43/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0409 - acc: 1.0000 - val_loss: 1.0598 - val_acc: 0.9615\n",
      "Epoch 44/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0415 - acc: 0.9986 - val_loss: 1.0577 - val_acc: 0.9487\n",
      "Epoch 45/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0408 - acc: 1.0000 - val_loss: 1.0572 - val_acc: 0.9487\n",
      "Epoch 46/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0407 - acc: 1.0000 - val_loss: 1.0558 - val_acc: 0.9487\n",
      "Epoch 47/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0407 - acc: 1.0000 - val_loss: 1.0609 - val_acc: 0.9615\n",
      "Epoch 48/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0407 - acc: 1.0000 - val_loss: 1.0587 - val_acc: 0.9615\n",
      "Epoch 49/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0552 - val_acc: 0.9615\n",
      "Epoch 50/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0642 - val_acc: 0.9487\n",
      "Epoch 51/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0407 - acc: 1.0000 - val_loss: 1.0579 - val_acc: 0.9487\n",
      "Epoch 52/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0573 - val_acc: 0.9487\n",
      "Epoch 53/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0562 - val_acc: 0.9487\n",
      "Epoch 54/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0540 - val_acc: 0.9487\n",
      "Epoch 55/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0552 - val_acc: 0.9487\n",
      "Epoch 56/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0549 - val_acc: 0.9487\n",
      "Epoch 57/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0553 - val_acc: 0.9744\n",
      "Epoch 58/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0546 - val_acc: 0.9487\n",
      "Epoch 59/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0576 - val_acc: 0.9615\n",
      "Epoch 60/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0572 - val_acc: 0.9487\n",
      "Epoch 61/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0573 - val_acc: 0.9487\n",
      "Epoch 62/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0575 - val_acc: 0.9487\n",
      "Epoch 63/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0594 - val_acc: 0.9615\n",
      "Epoch 64/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0542 - val_acc: 0.9487\n",
      "Epoch 65/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0593 - val_acc: 0.9487\n",
      "Epoch 66/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0556 - val_acc: 0.9487\n",
      "Epoch 67/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0570 - val_acc: 0.9487\n",
      "Epoch 68/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0558 - val_acc: 0.9487\n",
      "Epoch 69/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0564 - val_acc: 0.9487\n",
      "Epoch 70/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0568 - val_acc: 0.9487\n",
      "Epoch 71/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0569 - val_acc: 0.9487\n",
      "Epoch 72/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0574 - val_acc: 0.9487\n",
      "Epoch 73/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0562 - val_acc: 0.9487\n",
      "Epoch 74/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0558 - val_acc: 0.9487\n",
      "Epoch 75/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0564 - val_acc: 0.9487\n",
      "Epoch 76/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0576 - val_acc: 0.9487\n",
      "Epoch 77/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0559 - val_acc: 0.9487\n",
      "Epoch 78/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0572 - val_acc: 0.9487\n",
      "Epoch 79/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0568 - val_acc: 0.9487\n",
      "Epoch 80/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0577 - val_acc: 0.9487\n",
      "Epoch 81/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0568 - val_acc: 0.9487\n",
      "Epoch 82/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0569 - val_acc: 0.9487\n",
      "Epoch 83/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0575 - val_acc: 0.9487\n",
      "Epoch 84/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0579 - val_acc: 0.9487\n",
      "Epoch 85/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0573 - val_acc: 0.9487\n",
      "Epoch 86/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0560 - val_acc: 0.9487\n",
      "Epoch 87/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0579 - val_acc: 0.9487\n",
      "Epoch 88/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0575 - val_acc: 0.9487\n",
      "Epoch 89/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0577 - val_acc: 0.9487\n",
      "Epoch 90/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0578 - val_acc: 0.9487\n",
      "Epoch 91/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0571 - val_acc: 0.9487\n",
      "Epoch 92/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0573 - val_acc: 0.9487\n",
      "Epoch 93/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0571 - val_acc: 0.9487\n",
      "Epoch 94/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0571 - val_acc: 0.9487\n",
      "Epoch 95/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0575 - val_acc: 0.9487\n",
      "Epoch 96/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0573 - val_acc: 0.9487\n",
      "Epoch 97/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0573 - val_acc: 0.9487\n",
      "Epoch 98/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0572 - val_acc: 0.9487\n",
      "Epoch 99/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0573 - val_acc: 0.9487\n",
      "Epoch 100/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0581 - val_acc: 0.9487\n",
      "78/78 [==============================] - 0s 334us/step\n",
      "Score for fold 6: loss of 1.0572088712301009; acc of 94.87179441329761%\n",
      "Train on 702 samples, validate on 78 samples\n",
      "Epoch 1/100\n",
      "702/702 [==============================] - 16s 23ms/step - loss: 2.2581 - acc: 0.3063 - val_loss: 1.9173 - val_acc: 0.4615\n",
      "Epoch 2/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.5840 - acc: 0.6980 - val_loss: 1.4532 - val_acc: 0.8077\n",
      "Epoch 3/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.3253 - acc: 0.8105 - val_loss: 1.2542 - val_acc: 0.8333\n",
      "Epoch 4/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.2119 - acc: 0.8946 - val_loss: 1.3772 - val_acc: 0.6795\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1804 - acc: 0.8689 - val_loss: 1.1271 - val_acc: 0.9231\n",
      "Epoch 6/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1345 - acc: 0.9160 - val_loss: 1.0936 - val_acc: 0.9615\n",
      "Epoch 7/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1166 - acc: 0.9145 - val_loss: 1.1119 - val_acc: 0.9231\n",
      "Epoch 8/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0854 - acc: 0.9687 - val_loss: 1.1189 - val_acc: 0.9487\n",
      "Epoch 9/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0860 - acc: 0.9758 - val_loss: 1.0824 - val_acc: 0.9487\n",
      "Epoch 10/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0880 - acc: 0.9644 - val_loss: 1.2600 - val_acc: 0.8846\n",
      "Epoch 11/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0691 - acc: 0.9715 - val_loss: 1.1223 - val_acc: 0.9231\n",
      "Epoch 12/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0782 - acc: 0.9601 - val_loss: 1.0708 - val_acc: 0.9872\n",
      "Epoch 13/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1102 - acc: 0.9701 - val_loss: 1.0726 - val_acc: 0.9615\n",
      "Epoch 14/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0568 - acc: 0.9929 - val_loss: 1.1171 - val_acc: 0.9231\n",
      "Epoch 15/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0716 - acc: 0.9744 - val_loss: 1.0644 - val_acc: 0.9744\n",
      "Epoch 16/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0538 - acc: 0.9915 - val_loss: 1.0735 - val_acc: 0.9615\n",
      "Epoch 17/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0662 - acc: 0.9858 - val_loss: 1.0701 - val_acc: 0.9487\n",
      "Epoch 18/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0534 - acc: 0.9915 - val_loss: 1.0746 - val_acc: 0.9615\n",
      "Epoch 19/100\n",
      "702/702 [==============================] - 6s 8ms/step - loss: 1.0522 - acc: 0.9872 - val_loss: 1.0880 - val_acc: 0.9359\n",
      "Epoch 20/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0553 - acc: 0.9843 - val_loss: 1.0777 - val_acc: 0.9487\n",
      "Epoch 21/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0468 - acc: 0.9972 - val_loss: 1.0681 - val_acc: 0.9615\n",
      "Epoch 22/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0487 - acc: 0.9957 - val_loss: 1.0535 - val_acc: 0.9615\n",
      "Epoch 23/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0469 - acc: 0.9957 - val_loss: 1.0693 - val_acc: 0.9615\n",
      "Epoch 24/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0450 - acc: 0.9986 - val_loss: 1.0705 - val_acc: 0.9359\n",
      "Epoch 25/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0463 - acc: 0.9943 - val_loss: 1.0534 - val_acc: 0.9744\n",
      "Epoch 26/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0438 - acc: 0.9972 - val_loss: 1.0729 - val_acc: 0.9615\n",
      "Epoch 27/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0427 - acc: 0.9986 - val_loss: 1.0516 - val_acc: 0.9744\n",
      "Epoch 28/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0432 - acc: 0.9986 - val_loss: 1.0565 - val_acc: 0.9615\n",
      "Epoch 29/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0424 - acc: 1.0000 - val_loss: 1.0915 - val_acc: 0.9359\n",
      "Epoch 30/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0425 - acc: 0.9986 - val_loss: 1.0628 - val_acc: 0.9615\n",
      "Epoch 31/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0415 - acc: 1.0000 - val_loss: 1.0605 - val_acc: 0.9615\n",
      "Epoch 32/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0415 - acc: 1.0000 - val_loss: 1.0491 - val_acc: 0.9744\n",
      "Epoch 33/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0413 - acc: 1.0000 - val_loss: 1.0566 - val_acc: 0.9872\n",
      "Epoch 34/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0419 - acc: 0.9986 - val_loss: 1.0671 - val_acc: 0.9744\n",
      "Epoch 35/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0411 - acc: 1.0000 - val_loss: 1.0603 - val_acc: 0.9744\n",
      "Epoch 36/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0410 - acc: 1.0000 - val_loss: 1.0646 - val_acc: 0.9615\n",
      "Epoch 37/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0414 - acc: 1.0000 - val_loss: 1.0704 - val_acc: 0.9615\n",
      "Epoch 38/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0408 - acc: 1.0000 - val_loss: 1.0642 - val_acc: 0.9615\n",
      "Epoch 39/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0407 - acc: 1.0000 - val_loss: 1.0630 - val_acc: 0.9615\n",
      "Epoch 40/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0408 - acc: 1.0000 - val_loss: 1.0668 - val_acc: 0.9615\n",
      "Epoch 41/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0407 - acc: 1.0000 - val_loss: 1.0623 - val_acc: 0.9615\n",
      "Epoch 42/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0408 - acc: 1.0000 - val_loss: 1.0670 - val_acc: 0.9744\n",
      "Epoch 43/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0759 - val_acc: 0.9615\n",
      "Epoch 44/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0635 - val_acc: 0.9872\n",
      "Epoch 45/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0682 - val_acc: 0.9744\n",
      "Epoch 46/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0664 - val_acc: 0.9744\n",
      "Epoch 47/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0744 - val_acc: 0.9744\n",
      "Epoch 48/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0680 - val_acc: 0.9744\n",
      "Epoch 49/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0736 - val_acc: 0.9615\n",
      "Epoch 50/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0714 - val_acc: 0.9744\n",
      "Epoch 51/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0669 - val_acc: 0.9744\n",
      "Epoch 52/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0652 - val_acc: 0.9744\n",
      "Epoch 53/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0673 - val_acc: 0.9744\n",
      "Epoch 54/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0728 - val_acc: 0.9615\n",
      "Epoch 55/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0662 - val_acc: 0.9744\n",
      "Epoch 56/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0636 - val_acc: 0.9744\n",
      "Epoch 57/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0668 - val_acc: 0.9615\n",
      "Epoch 58/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0669 - val_acc: 0.9615\n",
      "Epoch 59/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0712 - val_acc: 0.9615\n",
      "Epoch 60/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0663 - val_acc: 0.9744\n",
      "Epoch 61/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0638 - val_acc: 0.9615\n",
      "Epoch 62/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0644 - val_acc: 0.9615\n",
      "Epoch 63/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0675 - val_acc: 0.9615\n",
      "Epoch 64/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0653 - val_acc: 0.9744\n",
      "Epoch 65/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0643 - val_acc: 0.9615\n",
      "Epoch 66/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0640 - val_acc: 0.9744\n",
      "Epoch 67/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0738 - val_acc: 0.9615\n",
      "Epoch 68/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0653 - val_acc: 0.9744\n",
      "Epoch 69/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0683 - val_acc: 0.9615\n",
      "Epoch 70/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0650 - val_acc: 0.9744\n",
      "Epoch 71/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0656 - val_acc: 0.9615\n",
      "Epoch 72/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0647 - val_acc: 0.9744\n",
      "Epoch 73/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0659 - val_acc: 0.9744\n",
      "Epoch 74/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0641 - val_acc: 0.9744\n",
      "Epoch 75/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0649 - val_acc: 0.9744\n",
      "Epoch 76/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0636 - val_acc: 0.9744\n",
      "Epoch 77/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0657 - val_acc: 0.9744\n",
      "Epoch 78/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0633 - val_acc: 0.9744\n",
      "Epoch 79/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0639 - val_acc: 0.9744\n",
      "Epoch 80/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0638 - val_acc: 0.9744\n",
      "Epoch 81/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0638 - val_acc: 0.9744\n",
      "Epoch 82/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0640 - val_acc: 0.9744\n",
      "Epoch 83/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0654 - val_acc: 0.9744\n",
      "Epoch 84/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0651 - val_acc: 0.9744\n",
      "Epoch 85/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0642 - val_acc: 0.9744\n",
      "Epoch 86/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0634 - val_acc: 0.9744\n",
      "Epoch 87/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0637 - val_acc: 0.9744\n",
      "Epoch 88/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0644 - val_acc: 0.9744\n",
      "Epoch 89/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0632 - val_acc: 0.9744\n",
      "Epoch 90/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0640 - val_acc: 0.9744\n",
      "Epoch 91/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0633 - val_acc: 0.9744\n",
      "Epoch 92/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0636 - val_acc: 0.9744\n",
      "Epoch 93/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0634 - val_acc: 0.9744\n",
      "Epoch 94/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0636 - val_acc: 0.9744\n",
      "Epoch 95/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0640 - val_acc: 0.9744\n",
      "Epoch 96/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0639 - val_acc: 0.9744\n",
      "Epoch 97/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0633 - val_acc: 0.9744\n",
      "Epoch 98/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0635 - val_acc: 0.9744\n",
      "Epoch 99/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0631 - val_acc: 0.9744\n",
      "Epoch 100/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0632 - val_acc: 0.9744\n",
      "78/78 [==============================] - 0s 328us/step\n",
      "Score for fold 7: loss of 1.0541133727782812; acc of 97.43589697740018%\n",
      "Train on 702 samples, validate on 78 samples\n",
      "Epoch 1/100\n",
      "702/702 [==============================] - 16s 23ms/step - loss: 2.2622 - acc: 0.3262 - val_loss: 1.9530 - val_acc: 0.4487\n",
      "Epoch 2/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.6766 - acc: 0.6425 - val_loss: 1.5181 - val_acc: 0.7436\n",
      "Epoch 3/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.3608 - acc: 0.8248 - val_loss: 1.2653 - val_acc: 0.8718\n",
      "Epoch 4/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.2470 - acc: 0.8462 - val_loss: 1.2378 - val_acc: 0.8205\n",
      "Epoch 5/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1436 - acc: 0.9088 - val_loss: 1.2962 - val_acc: 0.7821\n",
      "Epoch 6/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1626 - acc: 0.8946 - val_loss: 1.1427 - val_acc: 0.9103\n",
      "Epoch 7/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1169 - acc: 0.9373 - val_loss: 1.3733 - val_acc: 0.7692\n",
      "Epoch 8/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0984 - acc: 0.9558 - val_loss: 1.1041 - val_acc: 0.8974\n",
      "Epoch 9/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0957 - acc: 0.9615 - val_loss: 1.1393 - val_acc: 0.8974\n",
      "Epoch 10/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0816 - acc: 0.9658 - val_loss: 1.0856 - val_acc: 0.9744\n",
      "Epoch 11/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0873 - acc: 0.9530 - val_loss: 1.2073 - val_acc: 0.8462\n",
      "Epoch 12/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0763 - acc: 0.9786 - val_loss: 1.1208 - val_acc: 0.8974\n",
      "Epoch 13/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0883 - acc: 0.9758 - val_loss: 1.1631 - val_acc: 0.8974\n",
      "Epoch 14/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0637 - acc: 0.9858 - val_loss: 1.0772 - val_acc: 0.9615\n",
      "Epoch 15/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0539 - acc: 0.9929 - val_loss: 1.0703 - val_acc: 0.9872\n",
      "Epoch 16/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0648 - acc: 0.9786 - val_loss: 1.0733 - val_acc: 0.9744\n",
      "Epoch 17/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0566 - acc: 0.9858 - val_loss: 1.0873 - val_acc: 0.9487\n",
      "Epoch 18/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0647 - acc: 0.9772 - val_loss: 1.1223 - val_acc: 0.8846\n",
      "Epoch 19/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0855 - acc: 0.9729 - val_loss: 1.0793 - val_acc: 0.9487\n",
      "Epoch 20/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0519 - acc: 0.9915 - val_loss: 1.0734 - val_acc: 0.9359\n",
      "Epoch 21/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0496 - acc: 0.9929 - val_loss: 1.0776 - val_acc: 0.9231\n",
      "Epoch 22/100\n",
      "702/702 [==============================] - 6s 8ms/step - loss: 1.0546 - acc: 0.9843 - val_loss: 1.0610 - val_acc: 0.9872\n",
      "Epoch 23/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0483 - acc: 0.9900 - val_loss: 1.0682 - val_acc: 0.9744\n",
      "Epoch 24/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0469 - acc: 0.9943 - val_loss: 1.0682 - val_acc: 0.9744\n",
      "Epoch 25/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0469 - acc: 0.9929 - val_loss: 1.0643 - val_acc: 0.9615\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0451 - acc: 0.9972 - val_loss: 1.0820 - val_acc: 0.9487\n",
      "Epoch 27/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0447 - acc: 0.9972 - val_loss: 1.0564 - val_acc: 0.9744\n",
      "Epoch 28/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0461 - acc: 0.9943 - val_loss: 1.1367 - val_acc: 0.9615\n",
      "Epoch 29/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0432 - acc: 1.0000 - val_loss: 1.0613 - val_acc: 0.9744\n",
      "Epoch 30/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0423 - acc: 0.9986 - val_loss: 1.0794 - val_acc: 0.9231\n",
      "Epoch 31/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0438 - acc: 0.9972 - val_loss: 1.0597 - val_acc: 0.9744\n",
      "Epoch 32/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0417 - acc: 1.0000 - val_loss: 1.0651 - val_acc: 0.9744\n",
      "Epoch 33/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0438 - acc: 0.9972 - val_loss: 1.0626 - val_acc: 0.9744\n",
      "Epoch 34/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0416 - acc: 0.9986 - val_loss: 1.0615 - val_acc: 0.9744\n",
      "Epoch 35/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0414 - acc: 1.0000 - val_loss: 1.0657 - val_acc: 0.9615\n",
      "Epoch 36/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0412 - acc: 1.0000 - val_loss: 1.0615 - val_acc: 0.9615\n",
      "Epoch 37/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0412 - acc: 1.0000 - val_loss: 1.0643 - val_acc: 0.9744\n",
      "Epoch 38/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0413 - acc: 0.9972 - val_loss: 1.0558 - val_acc: 0.9744\n",
      "Epoch 39/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0409 - acc: 1.0000 - val_loss: 1.0618 - val_acc: 0.9744\n",
      "Epoch 40/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0410 - acc: 1.0000 - val_loss: 1.0589 - val_acc: 0.9744\n",
      "Epoch 41/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0409 - acc: 1.0000 - val_loss: 1.0568 - val_acc: 0.9615\n",
      "Epoch 42/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0407 - acc: 1.0000 - val_loss: 1.0638 - val_acc: 0.9744\n",
      "Epoch 43/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0407 - acc: 1.0000 - val_loss: 1.0635 - val_acc: 0.9744\n",
      "Epoch 44/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0408 - acc: 1.0000 - val_loss: 1.0542 - val_acc: 0.9872\n",
      "Epoch 45/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0570 - val_acc: 0.9744\n",
      "Epoch 46/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0547 - val_acc: 0.9615\n",
      "Epoch 47/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0407 - acc: 1.0000 - val_loss: 1.0547 - val_acc: 0.9744\n",
      "Epoch 48/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0563 - val_acc: 0.9744\n",
      "Epoch 49/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0541 - val_acc: 0.9615\n",
      "Epoch 50/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0546 - val_acc: 0.9744\n",
      "Epoch 51/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0590 - val_acc: 0.9615\n",
      "Epoch 52/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0563 - val_acc: 0.9744\n",
      "Epoch 53/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0584 - val_acc: 0.9615\n",
      "Epoch 54/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0573 - val_acc: 0.9744\n",
      "Epoch 55/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0559 - val_acc: 0.9744\n",
      "Epoch 56/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0554 - val_acc: 0.9744\n",
      "Epoch 57/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0542 - val_acc: 0.9744\n",
      "Epoch 58/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0526 - val_acc: 0.9872\n",
      "Epoch 59/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0534 - val_acc: 0.9744\n",
      "Epoch 60/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0564 - val_acc: 0.9744\n",
      "Epoch 61/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0551 - val_acc: 0.9744\n",
      "Epoch 62/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0540 - val_acc: 0.9744\n",
      "Epoch 63/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0547 - val_acc: 0.9744\n",
      "Epoch 64/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0579 - val_acc: 0.9744\n",
      "Epoch 65/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0550 - val_acc: 0.9744\n",
      "Epoch 66/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0544 - val_acc: 0.9744\n",
      "Epoch 67/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0535 - val_acc: 0.9744\n",
      "Epoch 68/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0559 - val_acc: 0.9744\n",
      "Epoch 69/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0554 - val_acc: 0.9744\n",
      "Epoch 70/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0545 - val_acc: 0.9744\n",
      "Epoch 71/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0562 - val_acc: 0.9744\n",
      "Epoch 72/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0533 - val_acc: 0.9744\n",
      "Epoch 73/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0551 - val_acc: 0.9744\n",
      "Epoch 74/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0568 - val_acc: 0.9744\n",
      "Epoch 75/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0561 - val_acc: 0.9744\n",
      "Epoch 76/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0562 - val_acc: 0.9744\n",
      "Epoch 77/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0546 - val_acc: 0.9744\n",
      "Epoch 78/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0557 - val_acc: 0.9744\n",
      "Epoch 79/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0545 - val_acc: 0.9744\n",
      "Epoch 80/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0547 - val_acc: 0.9744\n",
      "Epoch 81/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0551 - val_acc: 0.9744\n",
      "Epoch 82/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0552 - val_acc: 0.9744\n",
      "Epoch 83/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0549 - val_acc: 0.9744\n",
      "Epoch 84/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0546 - val_acc: 0.9744\n",
      "Epoch 85/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0548 - val_acc: 0.9744\n",
      "Epoch 86/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0557 - val_acc: 0.9744\n",
      "Epoch 87/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0558 - val_acc: 0.9744\n",
      "Epoch 88/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0554 - val_acc: 0.9744\n",
      "Epoch 89/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0543 - val_acc: 0.9744\n",
      "Epoch 90/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0549 - val_acc: 0.9744\n",
      "Epoch 91/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0552 - val_acc: 0.9744\n",
      "Epoch 92/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0546 - val_acc: 0.9744\n",
      "Epoch 93/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0548 - val_acc: 0.9744\n",
      "Epoch 94/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0544 - val_acc: 0.9744\n",
      "Epoch 95/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0542 - val_acc: 0.9744\n",
      "Epoch 96/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0549 - val_acc: 0.9744\n",
      "Epoch 97/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0545 - val_acc: 0.9744\n",
      "Epoch 98/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0542 - val_acc: 0.9744\n",
      "Epoch 99/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0546 - val_acc: 0.9744\n",
      "Epoch 100/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0544 - val_acc: 0.9744\n",
      "78/78 [==============================] - 0s 324us/step\n",
      "Score for fold 8: loss of 1.0541160993086987; acc of 97.43589743589743%\n",
      "Train on 702 samples, validate on 78 samples\n",
      "Epoch 1/100\n",
      "702/702 [==============================] - 17s 24ms/step - loss: 2.2472 - acc: 0.3234 - val_loss: 1.9503 - val_acc: 0.5897\n",
      "Epoch 2/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.6624 - acc: 0.6624 - val_loss: 1.5652 - val_acc: 0.6667\n",
      "Epoch 3/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.3837 - acc: 0.7991 - val_loss: 1.6623 - val_acc: 0.6667\n",
      "Epoch 4/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.3141 - acc: 0.8291 - val_loss: 1.2519 - val_acc: 0.8205\n",
      "Epoch 5/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1668 - acc: 0.9046 - val_loss: 1.1356 - val_acc: 0.9103\n",
      "Epoch 6/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1665 - acc: 0.8989 - val_loss: 1.1736 - val_acc: 0.8590\n",
      "Epoch 7/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1378 - acc: 0.9245 - val_loss: 1.0988 - val_acc: 0.9359\n",
      "Epoch 8/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1201 - acc: 0.9402 - val_loss: 1.1054 - val_acc: 0.9103\n",
      "Epoch 9/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0979 - acc: 0.9630 - val_loss: 1.1402 - val_acc: 0.8718\n",
      "Epoch 10/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1104 - acc: 0.9530 - val_loss: 1.0970 - val_acc: 0.9359\n",
      "Epoch 11/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1029 - acc: 0.9530 - val_loss: 1.0892 - val_acc: 0.9615\n",
      "Epoch 12/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0645 - acc: 0.9801 - val_loss: 1.0984 - val_acc: 0.9231\n",
      "Epoch 13/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0995 - acc: 0.9544 - val_loss: 1.0911 - val_acc: 0.9359\n",
      "Epoch 14/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0593 - acc: 0.9872 - val_loss: 1.0802 - val_acc: 0.9487\n",
      "Epoch 15/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0873 - acc: 0.9801 - val_loss: 1.0873 - val_acc: 0.9359\n",
      "Epoch 16/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0850 - acc: 0.9644 - val_loss: 1.0869 - val_acc: 0.9487\n",
      "Epoch 17/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0523 - acc: 0.9929 - val_loss: 1.1017 - val_acc: 0.8974\n",
      "Epoch 18/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0528 - acc: 0.9900 - val_loss: 1.0790 - val_acc: 0.9872\n",
      "Epoch 19/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0659 - acc: 0.9829 - val_loss: 1.0771 - val_acc: 0.9615\n",
      "Epoch 20/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0555 - acc: 0.9872 - val_loss: 1.0717 - val_acc: 0.9744\n",
      "Epoch 21/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0531 - acc: 0.9858 - val_loss: 1.0847 - val_acc: 0.9615\n",
      "Epoch 22/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0520 - acc: 0.9872 - val_loss: 1.2268 - val_acc: 0.8462\n",
      "Epoch 23/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0622 - acc: 0.9829 - val_loss: 1.0710 - val_acc: 0.9872\n",
      "Epoch 24/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0460 - acc: 0.9972 - val_loss: 1.0909 - val_acc: 0.9487\n",
      "Epoch 25/100\n",
      "702/702 [==============================] - 6s 8ms/step - loss: 1.0617 - acc: 0.9843 - val_loss: 1.0713 - val_acc: 0.9744\n",
      "Epoch 26/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0438 - acc: 1.0000 - val_loss: 1.0693 - val_acc: 0.9744\n",
      "Epoch 27/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0434 - acc: 1.0000 - val_loss: 1.0731 - val_acc: 0.9487\n",
      "Epoch 28/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0448 - acc: 0.9986 - val_loss: 1.0685 - val_acc: 0.9744\n",
      "Epoch 29/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0439 - acc: 0.9972 - val_loss: 1.0665 - val_acc: 0.9615\n",
      "Epoch 30/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0439 - acc: 1.0000 - val_loss: 1.0669 - val_acc: 0.9872\n",
      "Epoch 31/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0447 - acc: 0.9943 - val_loss: 1.0644 - val_acc: 0.9744\n",
      "Epoch 32/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0425 - acc: 0.9986 - val_loss: 1.0640 - val_acc: 0.9744\n",
      "Epoch 33/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0416 - acc: 1.0000 - val_loss: 1.0664 - val_acc: 0.9744\n",
      "Epoch 34/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0422 - acc: 0.9986 - val_loss: 1.0688 - val_acc: 0.9744\n",
      "Epoch 35/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0415 - acc: 1.0000 - val_loss: 1.0662 - val_acc: 0.9744\n",
      "Epoch 36/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0417 - acc: 1.0000 - val_loss: 1.0629 - val_acc: 0.9744\n",
      "Epoch 37/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0421 - acc: 0.9986 - val_loss: 1.0659 - val_acc: 0.9744\n",
      "Epoch 38/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0412 - acc: 1.0000 - val_loss: 1.0654 - val_acc: 0.9744\n",
      "Epoch 39/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0411 - acc: 1.0000 - val_loss: 1.0652 - val_acc: 0.9744\n",
      "Epoch 40/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0409 - acc: 1.0000 - val_loss: 1.0646 - val_acc: 0.9744\n",
      "Epoch 41/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0411 - acc: 1.0000 - val_loss: 1.0633 - val_acc: 0.9744\n",
      "Epoch 42/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0410 - acc: 1.0000 - val_loss: 1.0658 - val_acc: 0.9615\n",
      "Epoch 43/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0410 - acc: 1.0000 - val_loss: 1.0648 - val_acc: 0.9744\n",
      "Epoch 44/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0407 - acc: 1.0000 - val_loss: 1.0642 - val_acc: 0.9744\n",
      "Epoch 45/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0407 - acc: 1.0000 - val_loss: 1.0652 - val_acc: 0.9744\n",
      "Epoch 46/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0407 - acc: 1.0000 - val_loss: 1.0642 - val_acc: 0.9744\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0407 - acc: 1.0000 - val_loss: 1.0663 - val_acc: 0.9744\n",
      "Epoch 48/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0409 - acc: 1.0000 - val_loss: 1.0633 - val_acc: 0.9744\n",
      "Epoch 49/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0412 - acc: 1.0000 - val_loss: 1.0642 - val_acc: 0.9744\n",
      "Epoch 50/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0646 - val_acc: 0.9744\n",
      "Epoch 51/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0643 - val_acc: 0.9744\n",
      "Epoch 52/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0637 - val_acc: 0.9744\n",
      "Epoch 53/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0649 - val_acc: 0.9744\n",
      "Epoch 54/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0642 - val_acc: 0.9744\n",
      "Epoch 55/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0647 - val_acc: 0.9744\n",
      "Epoch 56/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0643 - val_acc: 0.9744\n",
      "Epoch 57/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0646 - val_acc: 0.9744\n",
      "Epoch 58/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0640 - val_acc: 0.9744\n",
      "Epoch 59/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0647 - val_acc: 0.9744\n",
      "Epoch 60/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0633 - val_acc: 0.9744\n",
      "Epoch 61/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0645 - val_acc: 0.9744\n",
      "Epoch 62/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0640 - val_acc: 0.9744\n",
      "Epoch 63/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0640 - val_acc: 0.9744\n",
      "Epoch 64/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0640 - val_acc: 0.9744\n",
      "Epoch 65/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0644 - val_acc: 0.9744\n",
      "Epoch 66/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0642 - val_acc: 0.9744\n",
      "Epoch 67/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0637 - val_acc: 0.9744\n",
      "Epoch 68/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0639 - val_acc: 0.9744\n",
      "Epoch 69/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0643 - val_acc: 0.9744\n",
      "Epoch 70/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0642 - val_acc: 0.9744\n",
      "Epoch 71/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0638 - val_acc: 0.9744\n",
      "Epoch 72/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0639 - val_acc: 0.9744\n",
      "Epoch 73/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0642 - val_acc: 0.9744\n",
      "Epoch 74/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0639 - val_acc: 0.9744\n",
      "Epoch 75/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0640 - val_acc: 0.9744\n",
      "Epoch 76/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0642 - val_acc: 0.9744\n",
      "Epoch 77/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0642 - val_acc: 0.9744\n",
      "Epoch 78/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0638 - val_acc: 0.9744\n",
      "Epoch 79/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0640 - val_acc: 0.9744\n",
      "Epoch 80/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0640 - val_acc: 0.9744\n",
      "Epoch 81/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0642 - val_acc: 0.9744\n",
      "Epoch 82/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0642 - val_acc: 0.9744\n",
      "Epoch 83/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0641 - val_acc: 0.9744\n",
      "Epoch 84/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0642 - val_acc: 0.9744\n",
      "Epoch 85/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0642 - val_acc: 0.9744\n",
      "Epoch 86/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0641 - val_acc: 0.9744\n",
      "Epoch 87/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0639 - val_acc: 0.9744\n",
      "Epoch 88/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0639 - val_acc: 0.9744\n",
      "Epoch 89/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0641 - val_acc: 0.9744\n",
      "Epoch 90/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0639 - val_acc: 0.9744\n",
      "Epoch 91/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0640 - val_acc: 0.9744\n",
      "Epoch 92/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0639 - val_acc: 0.9744\n",
      "Epoch 93/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0637 - val_acc: 0.9744\n",
      "Epoch 94/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0639 - val_acc: 0.9744\n",
      "Epoch 95/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0639 - val_acc: 0.9744\n",
      "Epoch 96/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0639 - val_acc: 0.9744\n",
      "Epoch 97/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0640 - val_acc: 0.9744\n",
      "Epoch 98/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0641 - val_acc: 0.9744\n",
      "Epoch 99/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0639 - val_acc: 0.9744\n",
      "Epoch 100/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0639 - val_acc: 0.9744\n",
      "78/78 [==============================] - 0s 339us/step\n",
      "Score for fold 9: loss of 1.06408176666651; acc of 97.43589697740018%\n",
      "Train on 702 samples, validate on 78 samples\n",
      "Epoch 1/100\n",
      "702/702 [==============================] - 17s 24ms/step - loss: 2.2919 - acc: 0.3362 - val_loss: 2.0937 - val_acc: 0.3333\n",
      "Epoch 2/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.6586 - acc: 0.6567 - val_loss: 1.7610 - val_acc: 0.5897\n",
      "Epoch 3/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.4230 - acc: 0.7749 - val_loss: 1.3121 - val_acc: 0.8718\n",
      "Epoch 4/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.2727 - acc: 0.8590 - val_loss: 1.2869 - val_acc: 0.7949\n",
      "Epoch 5/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.2320 - acc: 0.8661 - val_loss: 1.1579 - val_acc: 0.9231\n",
      "Epoch 6/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1454 - acc: 0.9174 - val_loss: 1.1272 - val_acc: 0.9231\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1310 - acc: 0.9259 - val_loss: 1.1569 - val_acc: 0.8718\n",
      "Epoch 8/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1168 - acc: 0.9487 - val_loss: 1.1123 - val_acc: 0.9231\n",
      "Epoch 9/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1075 - acc: 0.9516 - val_loss: 1.1097 - val_acc: 0.9359\n",
      "Epoch 10/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0887 - acc: 0.9658 - val_loss: 1.1028 - val_acc: 0.9872\n",
      "Epoch 11/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0827 - acc: 0.9630 - val_loss: 1.1143 - val_acc: 0.8974\n",
      "Epoch 12/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0787 - acc: 0.9729 - val_loss: 1.0913 - val_acc: 0.9744\n",
      "Epoch 13/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0646 - acc: 0.9843 - val_loss: 1.0789 - val_acc: 1.0000\n",
      "Epoch 14/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0824 - acc: 0.9516 - val_loss: 1.0838 - val_acc: 0.9615\n",
      "Epoch 15/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0597 - acc: 0.9801 - val_loss: 1.0569 - val_acc: 1.0000\n",
      "Epoch 16/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0663 - acc: 0.9758 - val_loss: 1.0581 - val_acc: 1.0000\n",
      "Epoch 17/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0543 - acc: 0.9872 - val_loss: 1.0680 - val_acc: 0.9615\n",
      "Epoch 18/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0616 - acc: 0.9829 - val_loss: 1.0666 - val_acc: 0.9615\n",
      "Epoch 19/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0518 - acc: 0.9886 - val_loss: 1.0737 - val_acc: 0.9872\n",
      "Epoch 20/100\n",
      "702/702 [==============================] - 6s 8ms/step - loss: 1.0806 - acc: 0.9772 - val_loss: 1.0713 - val_acc: 0.9872\n",
      "Epoch 21/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0503 - acc: 0.9900 - val_loss: 1.0618 - val_acc: 1.0000\n",
      "Epoch 22/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0464 - acc: 0.9972 - val_loss: 1.1180 - val_acc: 0.9615\n",
      "Epoch 23/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0497 - acc: 0.9915 - val_loss: 1.0512 - val_acc: 1.0000\n",
      "Epoch 24/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0501 - acc: 0.9929 - val_loss: 1.0681 - val_acc: 0.9872\n",
      "Epoch 25/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0466 - acc: 0.9972 - val_loss: 1.0542 - val_acc: 1.0000\n",
      "Epoch 26/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0464 - acc: 0.9957 - val_loss: 1.0532 - val_acc: 0.9872\n",
      "Epoch 27/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0438 - acc: 0.9986 - val_loss: 1.0643 - val_acc: 0.9872\n",
      "Epoch 28/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0451 - acc: 0.9957 - val_loss: 1.0664 - val_acc: 0.9872\n",
      "Epoch 29/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0432 - acc: 0.9986 - val_loss: 1.0878 - val_acc: 0.9744\n",
      "Epoch 30/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0452 - acc: 0.9986 - val_loss: 1.0709 - val_acc: 0.9744\n",
      "Epoch 31/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0430 - acc: 1.0000 - val_loss: 1.0749 - val_acc: 0.9872\n",
      "Epoch 32/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0439 - acc: 0.9972 - val_loss: 1.0668 - val_acc: 0.9872\n",
      "Epoch 33/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0420 - acc: 1.0000 - val_loss: 1.0733 - val_acc: 0.9872\n",
      "Epoch 34/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0418 - acc: 1.0000 - val_loss: 1.0660 - val_acc: 0.9872\n",
      "Epoch 35/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0426 - acc: 0.9986 - val_loss: 1.0633 - val_acc: 0.9744\n",
      "Epoch 36/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0418 - acc: 1.0000 - val_loss: 1.0634 - val_acc: 0.9872\n",
      "Epoch 37/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0413 - acc: 1.0000 - val_loss: 1.0836 - val_acc: 0.9744\n",
      "Epoch 38/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0421 - acc: 1.0000 - val_loss: 1.0642 - val_acc: 0.9744\n",
      "Epoch 39/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0412 - acc: 1.0000 - val_loss: 1.0637 - val_acc: 0.9872\n",
      "Epoch 40/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0409 - acc: 1.0000 - val_loss: 1.0665 - val_acc: 0.9872\n",
      "Epoch 41/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0410 - acc: 1.0000 - val_loss: 1.0706 - val_acc: 0.9872\n",
      "Epoch 42/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0410 - acc: 1.0000 - val_loss: 1.0567 - val_acc: 0.9872\n",
      "Epoch 43/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0410 - acc: 1.0000 - val_loss: 1.0665 - val_acc: 0.9872\n",
      "Epoch 44/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0411 - acc: 1.0000 - val_loss: 1.0573 - val_acc: 0.9872\n",
      "Epoch 45/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0408 - acc: 1.0000 - val_loss: 1.0587 - val_acc: 0.9872\n",
      "Epoch 46/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0589 - val_acc: 0.9872\n",
      "Epoch 47/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0567 - val_acc: 0.9872\n",
      "Epoch 48/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0407 - acc: 1.0000 - val_loss: 1.0609 - val_acc: 0.9872\n",
      "Epoch 49/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0407 - acc: 1.0000 - val_loss: 1.0598 - val_acc: 0.9872\n",
      "Epoch 50/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0638 - val_acc: 0.9872\n",
      "Epoch 51/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0603 - val_acc: 0.9872\n",
      "Epoch 52/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0641 - val_acc: 0.9872\n",
      "Epoch 53/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0596 - val_acc: 0.9872\n",
      "Epoch 54/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0635 - val_acc: 0.9872\n",
      "Epoch 55/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0637 - val_acc: 0.9872\n",
      "Epoch 56/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0641 - val_acc: 0.9872\n",
      "Epoch 57/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0644 - val_acc: 0.9872\n",
      "Epoch 58/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0609 - val_acc: 0.9872\n",
      "Epoch 59/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0596 - val_acc: 0.9872\n",
      "Epoch 60/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0663 - val_acc: 0.9872\n",
      "Epoch 61/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0617 - val_acc: 0.9872\n",
      "Epoch 62/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0611 - val_acc: 0.9872\n",
      "Epoch 63/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0593 - val_acc: 0.9872\n",
      "Epoch 64/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0595 - val_acc: 0.9872\n",
      "Epoch 65/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0634 - val_acc: 0.9872\n",
      "Epoch 66/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0599 - val_acc: 0.9872\n",
      "Epoch 67/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0655 - val_acc: 0.9872\n",
      "Epoch 68/100\n",
      "702/702 [==============================] - ETA: 0s - loss: 1.0409 - acc: 1.000 - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0630 - val_acc: 0.9872\n",
      "Epoch 69/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0632 - val_acc: 0.9872\n",
      "Epoch 70/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0627 - val_acc: 0.9872\n",
      "Epoch 71/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0589 - val_acc: 0.9872\n",
      "Epoch 72/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0601 - val_acc: 0.9872\n",
      "Epoch 73/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0611 - val_acc: 0.9872\n",
      "Epoch 74/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0627 - val_acc: 0.9872\n",
      "Epoch 75/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0636 - val_acc: 0.9872\n",
      "Epoch 76/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0625 - val_acc: 0.9872\n",
      "Epoch 77/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0614 - val_acc: 0.9872\n",
      "Epoch 78/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0642 - val_acc: 0.9872\n",
      "Epoch 79/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0620 - val_acc: 0.9872\n",
      "Epoch 80/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0657 - val_acc: 0.9872\n",
      "Epoch 81/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0649 - val_acc: 0.9872\n",
      "Epoch 82/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0617 - val_acc: 0.9872\n",
      "Epoch 83/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0638 - val_acc: 0.9872\n",
      "Epoch 84/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0658 - val_acc: 0.9872\n",
      "Epoch 85/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0636 - val_acc: 0.9872\n",
      "Epoch 86/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0642 - val_acc: 0.9872\n",
      "Epoch 87/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0646 - val_acc: 0.9872\n",
      "Epoch 88/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0647 - val_acc: 0.9872\n",
      "Epoch 89/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0664 - val_acc: 0.9872\n",
      "Epoch 90/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0651 - val_acc: 0.9872\n",
      "Epoch 91/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0647 - val_acc: 0.9872\n",
      "Epoch 92/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0656 - val_acc: 0.9872\n",
      "Epoch 93/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0652 - val_acc: 0.9872\n",
      "Epoch 94/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0646 - val_acc: 0.9872\n",
      "Epoch 95/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0649 - val_acc: 0.9872\n",
      "Epoch 96/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0640 - val_acc: 0.9872\n",
      "Epoch 97/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0660 - val_acc: 0.9872\n",
      "Epoch 98/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0641 - val_acc: 0.9872\n",
      "Epoch 99/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0638 - val_acc: 0.9872\n",
      "Epoch 100/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0642 - val_acc: 0.9872\n",
      "78/78 [==============================] - 0s 344us/step\n",
      "Score for fold 10: loss of 1.0612779305531428; acc of 98.71794825945145%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1296x1296 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97.43589720664879\n",
      "Train on 702 samples, validate on 78 samples\n",
      "Epoch 1/100\n",
      "702/702 [==============================] - 17s 24ms/step - loss: 2.1982 - acc: 0.3519 - val_loss: 2.0579 - val_acc: 0.4359\n",
      "Epoch 2/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.6073 - acc: 0.7037 - val_loss: 1.4290 - val_acc: 0.7436\n",
      "Epoch 3/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.3410 - acc: 0.8177 - val_loss: 1.2870 - val_acc: 0.7821\n",
      "Epoch 4/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.2204 - acc: 0.8803 - val_loss: 1.2093 - val_acc: 0.8462\n",
      "Epoch 5/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1923 - acc: 0.8775 - val_loss: 1.1431 - val_acc: 0.8718\n",
      "Epoch 6/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1456 - acc: 0.9088 - val_loss: 1.5200 - val_acc: 0.7692\n",
      "Epoch 7/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1509 - acc: 0.9373 - val_loss: 1.3013 - val_acc: 0.7821\n",
      "Epoch 8/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0948 - acc: 0.9487 - val_loss: 1.1673 - val_acc: 0.8333\n",
      "Epoch 9/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0872 - acc: 0.9644 - val_loss: 1.1251 - val_acc: 0.8846\n",
      "Epoch 10/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0983 - acc: 0.9330 - val_loss: 1.1008 - val_acc: 0.9231\n",
      "Epoch 11/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0817 - acc: 0.9630 - val_loss: 1.1197 - val_acc: 0.9359\n",
      "Epoch 12/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0669 - acc: 0.9886 - val_loss: 1.1120 - val_acc: 0.9359\n",
      "Epoch 13/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0659 - acc: 0.9786 - val_loss: 1.1624 - val_acc: 0.7949\n",
      "Epoch 14/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0745 - acc: 0.9772 - val_loss: 1.1154 - val_acc: 0.9231\n",
      "Epoch 15/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0591 - acc: 0.9815 - val_loss: 1.0900 - val_acc: 0.9359\n",
      "Epoch 16/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0745 - acc: 0.9701 - val_loss: 1.1169 - val_acc: 0.8974\n",
      "Epoch 17/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0534 - acc: 0.9943 - val_loss: 1.0731 - val_acc: 0.9487\n",
      "Epoch 18/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0733 - acc: 0.9758 - val_loss: 1.0798 - val_acc: 0.9487\n",
      "Epoch 19/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0526 - acc: 0.9872 - val_loss: 1.0845 - val_acc: 0.9231\n",
      "Epoch 20/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0503 - acc: 0.9957 - val_loss: 1.1660 - val_acc: 0.8333\n",
      "Epoch 21/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0598 - acc: 0.9858 - val_loss: 1.0706 - val_acc: 0.9487\n",
      "Epoch 22/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0543 - acc: 0.9900 - val_loss: 1.0762 - val_acc: 0.9359\n",
      "Epoch 23/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0522 - acc: 0.9915 - val_loss: 1.0740 - val_acc: 0.9359\n",
      "Epoch 24/100\n",
      "702/702 [==============================] - 6s 8ms/step - loss: 1.0512 - acc: 0.9929 - val_loss: 1.1171 - val_acc: 0.8462\n",
      "Epoch 25/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0533 - acc: 0.9858 - val_loss: 1.0718 - val_acc: 0.9487\n",
      "Epoch 26/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0451 - acc: 0.9972 - val_loss: 1.0830 - val_acc: 0.9487\n",
      "Epoch 27/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0498 - acc: 0.9929 - val_loss: 1.0682 - val_acc: 0.9487\n",
      "Epoch 28/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0448 - acc: 0.9986 - val_loss: 1.6098 - val_acc: 0.8590\n",
      "Epoch 29/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0567 - acc: 0.9972 - val_loss: 1.0699 - val_acc: 0.9487\n",
      "Epoch 30/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0435 - acc: 0.9986 - val_loss: 1.0758 - val_acc: 0.9615\n",
      "Epoch 31/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0464 - acc: 0.9929 - val_loss: 1.0767 - val_acc: 0.9359\n",
      "Epoch 32/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0448 - acc: 0.9972 - val_loss: 1.0705 - val_acc: 0.9744\n",
      "Epoch 33/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0436 - acc: 0.9972 - val_loss: 1.1828 - val_acc: 0.9103\n",
      "Epoch 34/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0526 - acc: 0.9886 - val_loss: 1.0671 - val_acc: 0.9615\n",
      "Epoch 35/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0452 - acc: 0.9943 - val_loss: 1.0710 - val_acc: 0.9487\n",
      "Epoch 36/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0434 - acc: 1.0000 - val_loss: 1.0724 - val_acc: 0.9359\n",
      "Epoch 37/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0442 - acc: 1.0000 - val_loss: 1.0718 - val_acc: 0.9359\n",
      "Epoch 38/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0421 - acc: 1.0000 - val_loss: 1.0726 - val_acc: 0.9487\n",
      "Epoch 39/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0477 - acc: 0.9929 - val_loss: 1.0682 - val_acc: 0.9359\n",
      "Epoch 40/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0425 - acc: 1.0000 - val_loss: 1.0699 - val_acc: 0.9487\n",
      "Epoch 41/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0414 - acc: 1.0000 - val_loss: 1.0728 - val_acc: 0.9615\n",
      "Epoch 42/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0418 - acc: 1.0000 - val_loss: 1.0726 - val_acc: 0.9487\n",
      "Epoch 43/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0416 - acc: 1.0000 - val_loss: 1.0710 - val_acc: 0.9487\n",
      "Epoch 44/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0416 - acc: 0.9986 - val_loss: 1.0745 - val_acc: 0.9359\n",
      "Epoch 45/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0417 - acc: 1.0000 - val_loss: 1.0759 - val_acc: 0.9359\n",
      "Epoch 46/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0410 - acc: 1.0000 - val_loss: 1.0713 - val_acc: 0.9487\n",
      "Epoch 47/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0409 - acc: 1.0000 - val_loss: 1.0686 - val_acc: 0.9615\n",
      "Epoch 48/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0410 - acc: 1.0000 - val_loss: 1.0665 - val_acc: 0.9615\n",
      "Epoch 49/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0409 - acc: 1.0000 - val_loss: 1.0737 - val_acc: 0.9487\n",
      "Epoch 50/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0409 - acc: 1.0000 - val_loss: 1.0649 - val_acc: 0.9744\n",
      "Epoch 51/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0407 - acc: 1.0000 - val_loss: 1.0778 - val_acc: 0.9487\n",
      "Epoch 52/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0407 - acc: 1.0000 - val_loss: 1.0700 - val_acc: 0.9487\n",
      "Epoch 53/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0407 - acc: 1.0000 - val_loss: 1.0620 - val_acc: 0.9615\n",
      "Epoch 54/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0428 - acc: 0.9972 - val_loss: 1.0681 - val_acc: 0.9487\n",
      "Epoch 55/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0682 - val_acc: 0.9615\n",
      "Epoch 56/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0656 - val_acc: 0.9744\n",
      "Epoch 57/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0648 - val_acc: 0.9744\n",
      "Epoch 58/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0660 - val_acc: 0.9615\n",
      "Epoch 59/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0661 - val_acc: 0.9615\n",
      "Epoch 60/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0617 - val_acc: 0.9744\n",
      "Epoch 61/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0699 - val_acc: 0.9615\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 62/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0693 - val_acc: 0.9487\n",
      "Epoch 63/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0735 - val_acc: 0.9487\n",
      "Epoch 64/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0704 - val_acc: 0.9487\n",
      "Epoch 65/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0669 - val_acc: 0.9615\n",
      "Epoch 66/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0680 - val_acc: 0.9615\n",
      "Epoch 67/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0707 - val_acc: 0.9615\n",
      "Epoch 68/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0701 - val_acc: 0.9359\n",
      "Epoch 69/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0678 - val_acc: 0.9615\n",
      "Epoch 70/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0675 - val_acc: 0.9615\n",
      "Epoch 71/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0698 - val_acc: 0.9487\n",
      "Epoch 72/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0692 - val_acc: 0.9487\n",
      "Epoch 73/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0730 - val_acc: 0.9487\n",
      "Epoch 74/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0716 - val_acc: 0.9487\n",
      "Epoch 75/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0713 - val_acc: 0.9487\n",
      "Epoch 76/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0716 - val_acc: 0.9359\n",
      "Epoch 77/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0696 - val_acc: 0.9487\n",
      "Epoch 78/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0683 - val_acc: 0.9615\n",
      "Epoch 79/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0685 - val_acc: 0.9487\n",
      "Epoch 80/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0710 - val_acc: 0.9487\n",
      "Epoch 81/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0701 - val_acc: 0.9487\n",
      "Epoch 82/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0698 - val_acc: 0.9487\n",
      "Epoch 83/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0689 - val_acc: 0.9487\n",
      "Epoch 84/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0683 - val_acc: 0.9487\n",
      "Epoch 85/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0699 - val_acc: 0.9487\n",
      "Epoch 86/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0687 - val_acc: 0.9615\n",
      "Epoch 87/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0674 - val_acc: 0.9615\n",
      "Epoch 88/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0689 - val_acc: 0.9487\n",
      "Epoch 89/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0684 - val_acc: 0.9487\n",
      "Epoch 90/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0704 - val_acc: 0.9487\n",
      "Epoch 91/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0672 - val_acc: 0.9487\n",
      "Epoch 92/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0683 - val_acc: 0.9487\n",
      "Epoch 93/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0684 - val_acc: 0.9487\n",
      "Epoch 94/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0682 - val_acc: 0.9487\n",
      "Epoch 95/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0681 - val_acc: 0.9487\n",
      "Epoch 96/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0686 - val_acc: 0.9487\n",
      "Epoch 97/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0696 - val_acc: 0.9487\n",
      "Epoch 98/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0683 - val_acc: 0.9487\n",
      "Epoch 99/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0685 - val_acc: 0.9487\n",
      "Epoch 100/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0678 - val_acc: 0.9487\n",
      "78/78 [==============================] - 0s 352us/step\n",
      "Score for fold 1: loss of 1.0679269753969634; acc of 94.87179487179486%\n",
      "Train on 702 samples, validate on 78 samples\n",
      "Epoch 1/100\n",
      "702/702 [==============================] - 17s 25ms/step - loss: 2.2148 - acc: 0.3590 - val_loss: 1.9730 - val_acc: 0.4872\n",
      "Epoch 2/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.6633 - acc: 0.6567 - val_loss: 1.5393 - val_acc: 0.7308\n",
      "Epoch 3/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.3474 - acc: 0.8419 - val_loss: 1.3441 - val_acc: 0.7692\n",
      "Epoch 4/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.2646 - acc: 0.8590 - val_loss: 1.2838 - val_acc: 0.8077\n",
      "Epoch 5/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.2107 - acc: 0.8761 - val_loss: 1.4055 - val_acc: 0.7308\n",
      "Epoch 6/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1527 - acc: 0.9160 - val_loss: 1.2387 - val_acc: 0.8462\n",
      "Epoch 7/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1483 - acc: 0.9060 - val_loss: 1.1434 - val_acc: 0.8590\n",
      "Epoch 8/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1085 - acc: 0.9330 - val_loss: 1.1361 - val_acc: 0.8718\n",
      "Epoch 9/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0808 - acc: 0.9573 - val_loss: 1.1828 - val_acc: 0.9231\n",
      "Epoch 10/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1216 - acc: 0.9430 - val_loss: 1.1699 - val_acc: 0.8462\n",
      "Epoch 11/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0783 - acc: 0.9658 - val_loss: 1.1356 - val_acc: 0.8974\n",
      "Epoch 12/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0765 - acc: 0.9801 - val_loss: 1.0860 - val_acc: 0.9487\n",
      "Epoch 13/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0693 - acc: 0.9744 - val_loss: 1.1166 - val_acc: 0.8462\n",
      "Epoch 14/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0617 - acc: 0.9786 - val_loss: 1.1727 - val_acc: 0.8590\n",
      "Epoch 15/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0642 - acc: 0.9829 - val_loss: 1.0745 - val_acc: 0.9615\n",
      "Epoch 16/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0732 - acc: 0.9744 - val_loss: 1.1083 - val_acc: 0.9103\n",
      "Epoch 17/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0544 - acc: 0.9858 - val_loss: 1.0903 - val_acc: 0.9231\n",
      "Epoch 18/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0710 - acc: 0.9758 - val_loss: 1.1081 - val_acc: 0.9231\n",
      "Epoch 19/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0487 - acc: 0.9972 - val_loss: 1.1336 - val_acc: 0.9231\n",
      "Epoch 20/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0929 - acc: 0.9801 - val_loss: 1.0702 - val_acc: 0.9744\n",
      "Epoch 21/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0488 - acc: 0.9957 - val_loss: 1.0678 - val_acc: 0.9872\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0592 - acc: 0.9801 - val_loss: 1.0722 - val_acc: 0.9615\n",
      "Epoch 23/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0496 - acc: 0.9929 - val_loss: 1.0732 - val_acc: 0.9744\n",
      "Epoch 24/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0480 - acc: 0.9957 - val_loss: 1.0661 - val_acc: 0.9487\n",
      "Epoch 25/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0516 - acc: 0.9900 - val_loss: 1.2454 - val_acc: 0.8846\n",
      "Epoch 26/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0535 - acc: 0.9900 - val_loss: 1.0748 - val_acc: 0.9615\n",
      "Epoch 27/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0536 - acc: 0.9886 - val_loss: 1.0702 - val_acc: 0.9487\n",
      "Epoch 28/100\n",
      "702/702 [==============================] - 6s 8ms/step - loss: 1.0467 - acc: 0.9972 - val_loss: 1.0783 - val_acc: 0.9615\n",
      "Epoch 29/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0455 - acc: 0.9957 - val_loss: 1.0949 - val_acc: 0.9359\n",
      "Epoch 30/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0519 - acc: 0.9929 - val_loss: 1.0584 - val_acc: 0.9615\n",
      "Epoch 31/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0447 - acc: 0.9972 - val_loss: 1.0552 - val_acc: 0.9872\n",
      "Epoch 32/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0461 - acc: 0.9943 - val_loss: 1.0635 - val_acc: 0.9615\n",
      "Epoch 33/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0432 - acc: 0.9986 - val_loss: 1.0595 - val_acc: 0.9487\n",
      "Epoch 34/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0543 - acc: 0.9886 - val_loss: 1.0738 - val_acc: 0.9231\n",
      "Epoch 35/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0435 - acc: 0.9972 - val_loss: 1.0501 - val_acc: 0.9872\n",
      "Epoch 36/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0429 - acc: 0.9986 - val_loss: 1.0641 - val_acc: 0.9615\n",
      "Epoch 37/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0425 - acc: 0.9986 - val_loss: 1.0555 - val_acc: 0.9744\n",
      "Epoch 38/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0429 - acc: 0.9986 - val_loss: 1.0559 - val_acc: 0.9615\n",
      "Epoch 39/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0418 - acc: 1.0000 - val_loss: 1.0624 - val_acc: 0.9615\n",
      "Epoch 40/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0423 - acc: 0.9986 - val_loss: 1.0548 - val_acc: 0.9744\n",
      "Epoch 41/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0414 - acc: 1.0000 - val_loss: 1.0566 - val_acc: 0.9744\n",
      "Epoch 42/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0417 - acc: 1.0000 - val_loss: 1.0503 - val_acc: 0.9744\n",
      "Epoch 43/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0413 - acc: 0.9986 - val_loss: 1.0554 - val_acc: 0.9744\n",
      "Epoch 44/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0414 - acc: 1.0000 - val_loss: 1.0645 - val_acc: 0.9615\n",
      "Epoch 45/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0419 - acc: 0.9986 - val_loss: 1.0528 - val_acc: 0.9744\n",
      "Epoch 46/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0412 - acc: 1.0000 - val_loss: 1.0531 - val_acc: 0.9872\n",
      "Epoch 47/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0414 - acc: 1.0000 - val_loss: 1.0523 - val_acc: 0.9872\n",
      "Epoch 48/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0408 - acc: 1.0000 - val_loss: 1.0514 - val_acc: 0.9872\n",
      "Epoch 49/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0415 - acc: 0.9986 - val_loss: 1.0618 - val_acc: 0.9615\n",
      "Epoch 50/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0409 - acc: 1.0000 - val_loss: 1.0532 - val_acc: 0.9744\n",
      "Epoch 51/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0407 - acc: 1.0000 - val_loss: 1.0514 - val_acc: 0.9872\n",
      "Epoch 52/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0408 - acc: 1.0000 - val_loss: 1.0608 - val_acc: 0.9744\n",
      "Epoch 53/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0411 - acc: 0.9986 - val_loss: 1.0512 - val_acc: 0.9872\n",
      "Epoch 54/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0407 - acc: 1.0000 - val_loss: 1.0503 - val_acc: 0.9872\n",
      "Epoch 55/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0508 - val_acc: 0.9872\n",
      "Epoch 56/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0507 - val_acc: 0.9872\n",
      "Epoch 57/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0522 - val_acc: 0.9872\n",
      "Epoch 58/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0543 - val_acc: 0.9872\n",
      "Epoch 59/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0492 - val_acc: 0.9872\n",
      "Epoch 60/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0482 - val_acc: 0.9872\n",
      "Epoch 61/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0513 - val_acc: 0.9872\n",
      "Epoch 62/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0524 - val_acc: 0.9872\n",
      "Epoch 63/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0504 - val_acc: 0.9872\n",
      "Epoch 64/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0541 - val_acc: 0.9872\n",
      "Epoch 65/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0524 - val_acc: 0.9872\n",
      "Epoch 66/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0519 - val_acc: 0.9744\n",
      "Epoch 67/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0513 - val_acc: 0.9872\n",
      "Epoch 68/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0507 - val_acc: 0.9872\n",
      "Epoch 69/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0525 - val_acc: 0.9744\n",
      "Epoch 70/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0533 - val_acc: 0.9744\n",
      "Epoch 71/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0502 - val_acc: 0.9872\n",
      "Epoch 72/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0504 - val_acc: 0.9872\n",
      "Epoch 73/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0517 - val_acc: 0.9872\n",
      "Epoch 74/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0509 - val_acc: 0.9872\n",
      "Epoch 75/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0502 - val_acc: 0.9872\n",
      "Epoch 76/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0508 - val_acc: 0.9872\n",
      "Epoch 77/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0507 - val_acc: 0.9872\n",
      "Epoch 78/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0522 - val_acc: 0.9872\n",
      "Epoch 79/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0500 - val_acc: 0.9872\n",
      "Epoch 80/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0531 - val_acc: 0.9744\n",
      "Epoch 81/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0509 - val_acc: 0.9872\n",
      "Epoch 82/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0513 - val_acc: 0.9872\n",
      "Epoch 83/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0513 - val_acc: 0.9872\n",
      "Epoch 84/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0514 - val_acc: 0.9872\n",
      "Epoch 85/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0501 - val_acc: 0.9872\n",
      "Epoch 86/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0507 - val_acc: 0.9872\n",
      "Epoch 87/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0506 - val_acc: 0.9872\n",
      "Epoch 88/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0512 - val_acc: 0.9872\n",
      "Epoch 89/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0501 - val_acc: 0.9872\n",
      "Epoch 90/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0514 - val_acc: 0.9872\n",
      "Epoch 91/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0505 - val_acc: 0.9872\n",
      "Epoch 92/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0510 - val_acc: 0.9872\n",
      "Epoch 93/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0510 - val_acc: 0.9872\n",
      "Epoch 94/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0509 - val_acc: 0.9872\n",
      "Epoch 95/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0505 - val_acc: 0.9872\n",
      "Epoch 96/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0498 - val_acc: 0.9872\n",
      "Epoch 97/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0496 - val_acc: 0.9872\n",
      "Epoch 98/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0514 - val_acc: 0.9872\n",
      "Epoch 99/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0506 - val_acc: 0.9872\n",
      "Epoch 100/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0502 - val_acc: 0.9872\n",
      "78/78 [==============================] - 0s 346us/step\n",
      "Score for fold 2: loss of 1.049853028395237; acc of 98.71794871794873%\n",
      "Train on 702 samples, validate on 78 samples\n",
      "Epoch 1/100\n",
      "702/702 [==============================] - 17s 25ms/step - loss: 2.2306 - acc: 0.3732 - val_loss: 1.9153 - val_acc: 0.4359\n",
      "Epoch 2/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.6122 - acc: 0.6952 - val_loss: 1.4120 - val_acc: 0.7821\n",
      "Epoch 3/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.3195 - acc: 0.8490 - val_loss: 1.2823 - val_acc: 0.8974\n",
      "Epoch 4/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.2495 - acc: 0.8789 - val_loss: 1.2028 - val_acc: 0.8205\n",
      "Epoch 5/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1479 - acc: 0.9117 - val_loss: 1.1509 - val_acc: 0.9231\n",
      "Epoch 6/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1438 - acc: 0.9160 - val_loss: 1.1207 - val_acc: 0.9615\n",
      "Epoch 7/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1308 - acc: 0.9302 - val_loss: 1.0949 - val_acc: 0.9744\n",
      "Epoch 8/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0782 - acc: 0.9786 - val_loss: 1.4276 - val_acc: 0.7051\n",
      "Epoch 9/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1176 - acc: 0.9516 - val_loss: 1.1000 - val_acc: 0.9744\n",
      "Epoch 10/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1007 - acc: 0.9615 - val_loss: 1.0935 - val_acc: 0.9231\n",
      "Epoch 11/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0681 - acc: 0.9786 - val_loss: 1.1262 - val_acc: 0.8846\n",
      "Epoch 12/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0920 - acc: 0.9701 - val_loss: 1.0795 - val_acc: 0.9615\n",
      "Epoch 13/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0625 - acc: 0.9815 - val_loss: 1.3665 - val_acc: 0.8205\n",
      "Epoch 14/100\n",
      "702/702 [==============================] - 6s 9ms/step - loss: 1.0797 - acc: 0.9772 - val_loss: 1.0694 - val_acc: 0.9744\n",
      "Epoch 15/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0532 - acc: 0.9915 - val_loss: 1.0795 - val_acc: 0.9872\n",
      "Epoch 16/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0544 - acc: 0.9886 - val_loss: 1.0746 - val_acc: 0.9744\n",
      "Epoch 17/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0590 - acc: 0.9872 - val_loss: 1.0734 - val_acc: 0.9615\n",
      "Epoch 18/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0498 - acc: 0.9957 - val_loss: 1.1106 - val_acc: 0.8974\n",
      "Epoch 19/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0512 - acc: 0.9900 - val_loss: 1.1199 - val_acc: 0.9744\n",
      "Epoch 20/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0477 - acc: 0.9972 - val_loss: 1.0627 - val_acc: 0.9872\n",
      "Epoch 21/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0503 - acc: 0.9915 - val_loss: 1.0563 - val_acc: 0.9872\n",
      "Epoch 22/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0494 - acc: 0.9900 - val_loss: 1.0568 - val_acc: 0.9615\n",
      "Epoch 23/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0456 - acc: 0.9972 - val_loss: 1.0791 - val_acc: 0.9744\n",
      "Epoch 24/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0457 - acc: 0.9957 - val_loss: 1.0573 - val_acc: 0.9872\n",
      "Epoch 25/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0452 - acc: 0.9972 - val_loss: 1.0540 - val_acc: 0.9744\n",
      "Epoch 26/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0466 - acc: 0.9929 - val_loss: 1.0550 - val_acc: 0.9872\n",
      "Epoch 27/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0446 - acc: 0.9972 - val_loss: 1.0590 - val_acc: 0.9744\n",
      "Epoch 28/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0437 - acc: 0.9986 - val_loss: 1.0561 - val_acc: 0.9744\n",
      "Epoch 29/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0428 - acc: 0.9986 - val_loss: 1.0565 - val_acc: 0.9872\n",
      "Epoch 30/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0436 - acc: 0.9986 - val_loss: 1.0533 - val_acc: 0.9744\n",
      "Epoch 31/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0420 - acc: 1.0000 - val_loss: 1.0515 - val_acc: 0.9872\n",
      "Epoch 32/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0423 - acc: 1.0000 - val_loss: 1.0593 - val_acc: 0.9744\n",
      "Epoch 33/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0422 - acc: 1.0000 - val_loss: 1.0615 - val_acc: 0.9744\n",
      "Epoch 34/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0428 - acc: 0.9986 - val_loss: 1.0518 - val_acc: 0.9872\n",
      "Epoch 35/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0413 - acc: 1.0000 - val_loss: 1.0549 - val_acc: 0.9744\n",
      "Epoch 36/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0411 - acc: 1.0000 - val_loss: 1.0532 - val_acc: 0.9872\n",
      "Epoch 37/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0417 - acc: 0.9986 - val_loss: 1.0591 - val_acc: 0.9615\n",
      "Epoch 38/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0418 - acc: 1.0000 - val_loss: 1.0511 - val_acc: 0.9744\n",
      "Epoch 39/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0410 - acc: 1.0000 - val_loss: 1.0519 - val_acc: 0.9872\n",
      "Epoch 40/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0410 - acc: 1.0000 - val_loss: 1.0549 - val_acc: 0.9872\n",
      "Epoch 41/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0410 - acc: 1.0000 - val_loss: 1.0561 - val_acc: 0.9615\n",
      "Epoch 42/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0409 - acc: 1.0000 - val_loss: 1.0540 - val_acc: 0.9872\n",
      "Epoch 43/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0407 - acc: 1.0000 - val_loss: 1.0583 - val_acc: 0.9744\n",
      "Epoch 44/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0409 - acc: 1.0000 - val_loss: 1.0548 - val_acc: 0.9872\n",
      "Epoch 45/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0407 - acc: 1.0000 - val_loss: 1.0514 - val_acc: 0.9872\n",
      "Epoch 46/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0408 - acc: 1.0000 - val_loss: 1.0558 - val_acc: 0.9872\n",
      "Epoch 47/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0407 - acc: 1.0000 - val_loss: 1.0585 - val_acc: 0.9615\n",
      "Epoch 48/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0551 - val_acc: 0.9872\n",
      "Epoch 49/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0556 - val_acc: 0.9744\n",
      "Epoch 50/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0533 - val_acc: 0.9872\n",
      "Epoch 51/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0512 - val_acc: 0.9872\n",
      "Epoch 52/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0585 - val_acc: 0.9744\n",
      "Epoch 53/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0515 - val_acc: 0.9872\n",
      "Epoch 54/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0532 - val_acc: 0.9872\n",
      "Epoch 55/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0542 - val_acc: 0.9872\n",
      "Epoch 56/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0517 - val_acc: 0.9872\n",
      "Epoch 57/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0519 - val_acc: 0.9744\n",
      "Epoch 58/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0523 - val_acc: 0.9744\n",
      "Epoch 59/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0515 - val_acc: 0.9744\n",
      "Epoch 60/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0547 - val_acc: 0.9872\n",
      "Epoch 61/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0518 - val_acc: 0.9872\n",
      "Epoch 62/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0504 - val_acc: 0.9872\n",
      "Epoch 63/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0520 - val_acc: 0.9872\n",
      "Epoch 64/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0521 - val_acc: 0.9872\n",
      "Epoch 65/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0526 - val_acc: 0.9744\n",
      "Epoch 66/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0517 - val_acc: 0.9872\n",
      "Epoch 67/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0523 - val_acc: 0.9744\n",
      "Epoch 68/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0521 - val_acc: 0.9744\n",
      "Epoch 69/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0514 - val_acc: 0.9872\n",
      "Epoch 70/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0512 - val_acc: 0.9872\n",
      "Epoch 71/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0517 - val_acc: 0.9872\n",
      "Epoch 72/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0516 - val_acc: 0.9872\n",
      "Epoch 73/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0517 - val_acc: 0.9872\n",
      "Epoch 74/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0513 - val_acc: 0.9872\n",
      "Epoch 75/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0517 - val_acc: 0.9872\n",
      "Epoch 76/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0516 - val_acc: 0.9744\n",
      "Epoch 77/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0513 - val_acc: 0.9872\n",
      "Epoch 78/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0508 - val_acc: 0.9744\n",
      "Epoch 79/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0516 - val_acc: 0.9872\n",
      "Epoch 80/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0522 - val_acc: 0.9872\n",
      "Epoch 81/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0526 - val_acc: 0.9872\n",
      "Epoch 82/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0525 - val_acc: 0.9872\n",
      "Epoch 83/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0518 - val_acc: 0.9872\n",
      "Epoch 84/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0522 - val_acc: 0.9872\n",
      "Epoch 85/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0521 - val_acc: 0.9872\n",
      "Epoch 86/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0511 - val_acc: 0.9872\n",
      "Epoch 87/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0515 - val_acc: 0.9872\n",
      "Epoch 88/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0513 - val_acc: 0.9872\n",
      "Epoch 89/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0516 - val_acc: 0.9872\n",
      "Epoch 90/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0513 - val_acc: 0.9872\n",
      "Epoch 91/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0516 - val_acc: 0.9872\n",
      "Epoch 92/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0515 - val_acc: 0.9872\n",
      "Epoch 93/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0519 - val_acc: 0.9872\n",
      "Epoch 94/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0520 - val_acc: 0.9872\n",
      "Epoch 95/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0512 - val_acc: 0.9872\n",
      "Epoch 96/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0518 - val_acc: 0.9872\n",
      "Epoch 97/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0516 - val_acc: 0.9872\n",
      "Epoch 98/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0518 - val_acc: 0.9872\n",
      "Epoch 99/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0518 - val_acc: 0.9872\n",
      "Epoch 100/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0527 - val_acc: 0.9872\n",
      "78/78 [==============================] - 0s 354us/step\n",
      "Score for fold 3: loss of 1.0532414393547254; acc of 98.71794825945145%\n",
      "Train on 702 samples, validate on 78 samples\n",
      "Epoch 1/100\n",
      "702/702 [==============================] - 18s 25ms/step - loss: 2.2104 - acc: 0.3704 - val_loss: 1.7540 - val_acc: 0.6923\n",
      "Epoch 2/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.6095 - acc: 0.6866 - val_loss: 1.4820 - val_acc: 0.8462\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.3647 - acc: 0.8006 - val_loss: 1.3430 - val_acc: 0.6795\n",
      "Epoch 4/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.2409 - acc: 0.8661 - val_loss: 1.1814 - val_acc: 0.8590\n",
      "Epoch 5/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1772 - acc: 0.8875 - val_loss: 1.1579 - val_acc: 0.8718\n",
      "Epoch 6/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1347 - acc: 0.9231 - val_loss: 1.1104 - val_acc: 0.9615\n",
      "Epoch 7/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1196 - acc: 0.9359 - val_loss: 1.1585 - val_acc: 0.8974\n",
      "Epoch 8/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0877 - acc: 0.9672 - val_loss: 1.1560 - val_acc: 0.7692\n",
      "Epoch 9/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1077 - acc: 0.9459 - val_loss: 1.0771 - val_acc: 0.9487\n",
      "Epoch 10/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0683 - acc: 0.9829 - val_loss: 1.0840 - val_acc: 0.9487\n",
      "Epoch 11/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1215 - acc: 0.9330 - val_loss: 1.0699 - val_acc: 0.9872\n",
      "Epoch 12/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0691 - acc: 0.9843 - val_loss: 1.0833 - val_acc: 0.9615\n",
      "Epoch 13/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0615 - acc: 0.9829 - val_loss: 1.2368 - val_acc: 0.7821\n",
      "Epoch 14/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0913 - acc: 0.9729 - val_loss: 1.0620 - val_acc: 0.9744\n",
      "Epoch 15/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0592 - acc: 0.9872 - val_loss: 1.3211 - val_acc: 0.8333\n",
      "Epoch 16/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0763 - acc: 0.9701 - val_loss: 1.0647 - val_acc: 0.9872\n",
      "Epoch 17/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0539 - acc: 0.9886 - val_loss: 1.0697 - val_acc: 0.9487\n",
      "Epoch 18/100\n",
      "702/702 [==============================] - 6s 9ms/step - loss: 1.0550 - acc: 0.9900 - val_loss: 1.0557 - val_acc: 0.9744\n",
      "Epoch 19/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0511 - acc: 0.9943 - val_loss: 1.0555 - val_acc: 0.9744\n",
      "Epoch 20/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0485 - acc: 0.9943 - val_loss: 1.0569 - val_acc: 0.9744\n",
      "Epoch 21/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0479 - acc: 0.9943 - val_loss: 1.0525 - val_acc: 0.9872\n",
      "Epoch 22/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0496 - acc: 0.9915 - val_loss: 1.0465 - val_acc: 1.0000\n",
      "Epoch 23/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0659 - acc: 0.9829 - val_loss: 1.0484 - val_acc: 1.0000\n",
      "Epoch 24/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0445 - acc: 0.9972 - val_loss: 1.0543 - val_acc: 0.9872\n",
      "Epoch 25/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0490 - acc: 0.9929 - val_loss: 1.0560 - val_acc: 0.9615\n",
      "Epoch 26/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0522 - acc: 0.9943 - val_loss: 1.0915 - val_acc: 0.9615\n",
      "Epoch 27/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0436 - acc: 1.0000 - val_loss: 1.0473 - val_acc: 1.0000\n",
      "Epoch 28/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0440 - acc: 0.9986 - val_loss: 1.0458 - val_acc: 1.0000\n",
      "Epoch 29/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0483 - acc: 0.9886 - val_loss: 1.0539 - val_acc: 0.9872\n",
      "Epoch 30/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0430 - acc: 0.9986 - val_loss: 1.0461 - val_acc: 1.0000\n",
      "Epoch 31/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0425 - acc: 1.0000 - val_loss: 1.0596 - val_acc: 0.9744\n",
      "Epoch 32/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0435 - acc: 0.9972 - val_loss: 1.0473 - val_acc: 0.9872\n",
      "Epoch 33/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0419 - acc: 1.0000 - val_loss: 1.0490 - val_acc: 0.9872\n",
      "Epoch 34/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0421 - acc: 1.0000 - val_loss: 1.0469 - val_acc: 1.0000\n",
      "Epoch 35/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0439 - acc: 0.9986 - val_loss: 1.0441 - val_acc: 1.0000\n",
      "Epoch 36/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0420 - acc: 0.9986 - val_loss: 1.0468 - val_acc: 0.9872\n",
      "Epoch 37/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0412 - acc: 1.0000 - val_loss: 1.0450 - val_acc: 0.9872\n",
      "Epoch 38/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0416 - acc: 1.0000 - val_loss: 1.0452 - val_acc: 1.0000\n",
      "Epoch 39/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0411 - acc: 1.0000 - val_loss: 1.0495 - val_acc: 0.9872\n",
      "Epoch 40/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0413 - acc: 1.0000 - val_loss: 1.0456 - val_acc: 0.9872\n",
      "Epoch 41/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0412 - acc: 1.0000 - val_loss: 1.0464 - val_acc: 1.0000\n",
      "Epoch 42/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0411 - acc: 1.0000 - val_loss: 1.0438 - val_acc: 0.9872\n",
      "Epoch 43/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0408 - acc: 1.0000 - val_loss: 1.0430 - val_acc: 1.0000\n",
      "Epoch 44/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0407 - acc: 1.0000 - val_loss: 1.0442 - val_acc: 1.0000\n",
      "Epoch 45/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0414 - acc: 1.0000 - val_loss: 1.0445 - val_acc: 0.9872\n",
      "Epoch 46/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0407 - acc: 1.0000 - val_loss: 1.0434 - val_acc: 0.9872\n",
      "Epoch 47/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0432 - val_acc: 1.0000\n",
      "Epoch 48/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0422 - val_acc: 1.0000\n",
      "Epoch 49/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0426 - val_acc: 1.0000\n",
      "Epoch 50/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0431 - val_acc: 1.0000\n",
      "Epoch 51/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0438 - val_acc: 1.0000\n",
      "Epoch 52/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0444 - val_acc: 0.9872\n",
      "Epoch 53/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0468 - val_acc: 0.9744\n",
      "Epoch 54/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0437 - val_acc: 0.9872\n",
      "Epoch 55/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0432 - val_acc: 1.0000\n",
      "Epoch 56/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0440 - val_acc: 0.9872\n",
      "Epoch 57/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0460 - val_acc: 0.9872\n",
      "Epoch 58/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0443 - val_acc: 1.0000\n",
      "Epoch 59/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0460 - val_acc: 0.9872\n",
      "Epoch 60/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0451 - val_acc: 1.0000\n",
      "Epoch 61/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0459 - val_acc: 0.9872\n",
      "Epoch 62/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0435 - val_acc: 1.0000\n",
      "Epoch 63/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0434 - val_acc: 1.0000\n",
      "Epoch 64/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0419 - val_acc: 1.0000\n",
      "Epoch 65/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0428 - val_acc: 1.0000\n",
      "Epoch 66/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0421 - val_acc: 1.0000\n",
      "Epoch 67/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0426 - val_acc: 1.0000\n",
      "Epoch 68/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0430 - val_acc: 1.0000\n",
      "Epoch 69/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0422 - val_acc: 1.0000\n",
      "Epoch 70/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0422 - val_acc: 1.0000\n",
      "Epoch 71/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0441 - val_acc: 1.0000\n",
      "Epoch 72/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0441 - val_acc: 1.0000\n",
      "Epoch 73/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0442 - val_acc: 1.0000\n",
      "Epoch 74/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0439 - val_acc: 0.9872\n",
      "Epoch 75/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0431 - val_acc: 1.0000\n",
      "Epoch 76/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0428 - val_acc: 1.0000\n",
      "Epoch 77/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0427 - val_acc: 1.0000\n",
      "Epoch 78/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0424 - val_acc: 1.0000\n",
      "Epoch 79/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0427 - val_acc: 1.0000\n",
      "Epoch 80/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0432 - val_acc: 1.0000\n",
      "Epoch 81/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0432 - val_acc: 1.0000\n",
      "Epoch 82/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0427 - val_acc: 1.0000\n",
      "Epoch 83/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0427 - val_acc: 1.0000\n",
      "Epoch 84/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0430 - val_acc: 1.0000\n",
      "Epoch 85/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0435 - val_acc: 1.0000\n",
      "Epoch 86/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0425 - val_acc: 1.0000\n",
      "Epoch 87/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0430 - val_acc: 1.0000\n",
      "Epoch 88/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0430 - val_acc: 1.0000\n",
      "Epoch 89/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0429 - val_acc: 1.0000\n",
      "Epoch 90/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0431 - val_acc: 1.0000\n",
      "Epoch 91/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0433 - val_acc: 1.0000\n",
      "Epoch 92/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0438 - val_acc: 0.9872\n",
      "Epoch 93/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0431 - val_acc: 1.0000\n",
      "Epoch 94/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0433 - val_acc: 1.0000\n",
      "Epoch 95/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0431 - val_acc: 1.0000\n",
      "Epoch 96/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0433 - val_acc: 0.9872\n",
      "Epoch 97/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0434 - val_acc: 0.9872\n",
      "Epoch 98/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0433 - val_acc: 1.0000\n",
      "Epoch 99/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0430 - val_acc: 1.0000\n",
      "Epoch 100/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0428 - val_acc: 1.0000\n",
      "78/78 [==============================] - 0s 347us/step\n",
      "Score for fold 4: loss of 1.0437898360765898; acc of 98.71794825945145%\n",
      "Train on 702 samples, validate on 78 samples\n",
      "Epoch 1/100\n",
      "702/702 [==============================] - 18s 25ms/step - loss: 2.3141 - acc: 0.3048 - val_loss: 1.9841 - val_acc: 0.4487\n",
      "Epoch 2/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.6843 - acc: 0.6268 - val_loss: 1.7815 - val_acc: 0.5513\n",
      "Epoch 3/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.4144 - acc: 0.7635 - val_loss: 1.5070 - val_acc: 0.6538\n",
      "Epoch 4/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.2875 - acc: 0.8604 - val_loss: 1.4027 - val_acc: 0.7051\n",
      "Epoch 5/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.2099 - acc: 0.8704 - val_loss: 1.2133 - val_acc: 0.8462\n",
      "Epoch 6/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1375 - acc: 0.9145 - val_loss: 1.1889 - val_acc: 0.8974\n",
      "Epoch 7/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1580 - acc: 0.8946 - val_loss: 1.2272 - val_acc: 0.8590\n",
      "Epoch 8/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1008 - acc: 0.9373 - val_loss: 1.3435 - val_acc: 0.7564\n",
      "Epoch 9/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0950 - acc: 0.9444 - val_loss: 1.1423 - val_acc: 0.8590\n",
      "Epoch 10/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0826 - acc: 0.9644 - val_loss: 1.0967 - val_acc: 0.9487\n",
      "Epoch 11/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1320 - acc: 0.9231 - val_loss: 1.1206 - val_acc: 0.9487\n",
      "Epoch 12/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0748 - acc: 0.9744 - val_loss: 1.2012 - val_acc: 0.8846\n",
      "Epoch 13/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0714 - acc: 0.9744 - val_loss: 1.0946 - val_acc: 0.9359\n",
      "Epoch 14/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1119 - acc: 0.9501 - val_loss: 1.1067 - val_acc: 0.9615\n",
      "Epoch 15/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0669 - acc: 0.9801 - val_loss: 1.0986 - val_acc: 0.9487\n",
      "Epoch 16/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0584 - acc: 0.9886 - val_loss: 1.0665 - val_acc: 0.9872\n",
      "Epoch 17/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0588 - acc: 0.9815 - val_loss: 1.2407 - val_acc: 0.8077\n",
      "Epoch 18/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0665 - acc: 0.9672 - val_loss: 1.0928 - val_acc: 0.9359\n",
      "Epoch 19/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0555 - acc: 0.9872 - val_loss: 1.0864 - val_acc: 0.9487\n",
      "Epoch 20/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0652 - acc: 0.9900 - val_loss: 1.0852 - val_acc: 0.9615\n",
      "Epoch 21/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0592 - acc: 0.9843 - val_loss: 1.0640 - val_acc: 0.9872\n",
      "Epoch 22/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0741 - acc: 0.9772 - val_loss: 1.0826 - val_acc: 0.9744\n",
      "Epoch 23/100\n",
      "702/702 [==============================] - 6s 9ms/step - loss: 1.0494 - acc: 0.9915 - val_loss: 1.1081 - val_acc: 0.9615\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0571 - acc: 0.9915 - val_loss: 1.0620 - val_acc: 0.9744\n",
      "Epoch 25/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0492 - acc: 0.9957 - val_loss: 1.0516 - val_acc: 0.9744\n",
      "Epoch 26/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0473 - acc: 0.9957 - val_loss: 1.0820 - val_acc: 0.9744\n",
      "Epoch 27/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0505 - acc: 0.9943 - val_loss: 1.0574 - val_acc: 0.9872\n",
      "Epoch 28/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0476 - acc: 0.9957 - val_loss: 1.0660 - val_acc: 0.9744\n",
      "Epoch 29/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0449 - acc: 0.9986 - val_loss: 1.0700 - val_acc: 0.9487\n",
      "Epoch 30/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0436 - acc: 0.9986 - val_loss: 1.0809 - val_acc: 0.9615\n",
      "Epoch 31/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0443 - acc: 0.9986 - val_loss: 1.0636 - val_acc: 0.9615\n",
      "Epoch 32/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0433 - acc: 0.9986 - val_loss: 1.0681 - val_acc: 0.9744\n",
      "Epoch 33/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0435 - acc: 1.0000 - val_loss: 1.0663 - val_acc: 0.9487\n",
      "Epoch 34/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0434 - acc: 1.0000 - val_loss: 1.0669 - val_acc: 0.9744\n",
      "Epoch 35/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0430 - acc: 0.9986 - val_loss: 1.0671 - val_acc: 0.9872\n",
      "Epoch 36/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0421 - acc: 1.0000 - val_loss: 1.0576 - val_acc: 0.9872\n",
      "Epoch 37/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0418 - acc: 1.0000 - val_loss: 1.0567 - val_acc: 0.9872\n",
      "Epoch 38/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0428 - acc: 0.9986 - val_loss: 1.0554 - val_acc: 0.9872\n",
      "Epoch 39/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0414 - acc: 1.0000 - val_loss: 1.0524 - val_acc: 0.9872\n",
      "Epoch 40/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0412 - acc: 1.0000 - val_loss: 1.0551 - val_acc: 0.9872\n",
      "Epoch 41/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0413 - acc: 1.0000 - val_loss: 1.0617 - val_acc: 0.9872\n",
      "Epoch 42/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0411 - acc: 1.0000 - val_loss: 1.0529 - val_acc: 0.9872\n",
      "Epoch 43/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0411 - acc: 1.0000 - val_loss: 1.0537 - val_acc: 0.9872\n",
      "Epoch 44/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0411 - acc: 1.0000 - val_loss: 1.0510 - val_acc: 0.9872\n",
      "Epoch 45/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0409 - acc: 1.0000 - val_loss: 1.0556 - val_acc: 0.9872\n",
      "Epoch 46/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0411 - acc: 1.0000 - val_loss: 1.0514 - val_acc: 0.9872\n",
      "Epoch 47/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0409 - acc: 1.0000 - val_loss: 1.0528 - val_acc: 0.9872\n",
      "Epoch 48/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0407 - acc: 1.0000 - val_loss: 1.0538 - val_acc: 0.9872\n",
      "Epoch 49/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0408 - acc: 1.0000 - val_loss: 1.0500 - val_acc: 0.9872\n",
      "Epoch 50/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0407 - acc: 1.0000 - val_loss: 1.0512 - val_acc: 0.9872\n",
      "Epoch 51/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0407 - acc: 1.0000 - val_loss: 1.0514 - val_acc: 0.9872\n",
      "Epoch 52/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0500 - val_acc: 0.9872\n",
      "Epoch 53/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0498 - val_acc: 0.9872\n",
      "Epoch 54/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0485 - val_acc: 0.9872\n",
      "Epoch 55/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0515 - val_acc: 0.9872\n",
      "Epoch 56/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0515 - val_acc: 0.9872\n",
      "Epoch 57/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0489 - val_acc: 0.9872\n",
      "Epoch 58/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0499 - val_acc: 0.9872\n",
      "Epoch 59/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0522 - val_acc: 0.9872\n",
      "Epoch 60/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0493 - val_acc: 0.9872\n",
      "Epoch 61/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0515 - val_acc: 0.9872\n",
      "Epoch 62/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0489 - val_acc: 0.9872\n",
      "Epoch 63/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0497 - val_acc: 0.9872\n",
      "Epoch 64/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0506 - val_acc: 0.9872\n",
      "Epoch 65/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0537 - val_acc: 0.9872\n",
      "Epoch 66/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0523 - val_acc: 0.9872\n",
      "Epoch 67/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0526 - val_acc: 0.9872\n",
      "Epoch 68/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0527 - val_acc: 0.9872\n",
      "Epoch 69/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0516 - val_acc: 0.9872\n",
      "Epoch 70/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0521 - val_acc: 0.9872\n",
      "Epoch 71/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0523 - val_acc: 0.9872\n",
      "Epoch 72/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0521 - val_acc: 0.9872\n",
      "Epoch 73/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0517 - val_acc: 0.9872\n",
      "Epoch 74/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0509 - val_acc: 0.9872\n",
      "Epoch 75/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0520 - val_acc: 0.9872\n",
      "Epoch 76/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0507 - val_acc: 0.9872\n",
      "Epoch 77/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0508 - val_acc: 0.9872\n",
      "Epoch 78/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0511 - val_acc: 0.9872\n",
      "Epoch 79/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0503 - val_acc: 0.9872\n",
      "Epoch 80/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0515 - val_acc: 0.9872\n",
      "Epoch 81/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0504 - val_acc: 0.9872\n",
      "Epoch 82/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0506 - val_acc: 0.9872\n",
      "Epoch 83/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0500 - val_acc: 0.9872\n",
      "Epoch 84/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0503 - val_acc: 0.9872\n",
      "Epoch 85/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0504 - val_acc: 0.9872\n",
      "Epoch 86/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0507 - val_acc: 0.9872\n",
      "Epoch 87/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0512 - val_acc: 0.9872\n",
      "Epoch 88/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0510 - val_acc: 0.9872\n",
      "Epoch 89/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0518 - val_acc: 0.9872\n",
      "Epoch 90/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0508 - val_acc: 0.9872\n",
      "Epoch 91/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0498 - val_acc: 0.9872\n",
      "Epoch 92/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0502 - val_acc: 0.9872\n",
      "Epoch 93/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0505 - val_acc: 0.9872\n",
      "Epoch 94/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0512 - val_acc: 0.9872\n",
      "Epoch 95/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0507 - val_acc: 0.9872\n",
      "Epoch 96/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0506 - val_acc: 0.9872\n",
      "Epoch 97/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0508 - val_acc: 0.9872\n",
      "Epoch 98/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0508 - val_acc: 0.9872\n",
      "Epoch 99/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0507 - val_acc: 0.9872\n",
      "Epoch 100/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0505 - val_acc: 0.9872\n",
      "78/78 [==============================] - 0s 359us/step\n",
      "Score for fold 5: loss of 1.0502577683864496; acc of 98.71794871794873%\n",
      "Train on 702 samples, validate on 78 samples\n",
      "Epoch 1/100\n",
      "702/702 [==============================] - 18s 26ms/step - loss: 2.1905 - acc: 0.3889 - val_loss: 1.8616 - val_acc: 0.6026\n",
      "Epoch 2/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.6072 - acc: 0.7208 - val_loss: 1.5617 - val_acc: 0.7436\n",
      "Epoch 3/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.3340 - acc: 0.8348 - val_loss: 1.7168 - val_acc: 0.6026\n",
      "Epoch 4/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.2504 - acc: 0.8803 - val_loss: 1.2560 - val_acc: 0.8077\n",
      "Epoch 5/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1677 - acc: 0.8974 - val_loss: 1.2182 - val_acc: 0.7949\n",
      "Epoch 6/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1151 - acc: 0.9288 - val_loss: 1.4511 - val_acc: 0.7436\n",
      "Epoch 7/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1285 - acc: 0.9359 - val_loss: 1.1124 - val_acc: 0.9103\n",
      "Epoch 8/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1032 - acc: 0.9345 - val_loss: 1.1833 - val_acc: 0.7949\n",
      "Epoch 9/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1238 - acc: 0.9274 - val_loss: 1.1023 - val_acc: 0.9487\n",
      "Epoch 10/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0729 - acc: 0.9729 - val_loss: 1.1018 - val_acc: 0.9103\n",
      "Epoch 11/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1051 - acc: 0.9544 - val_loss: 1.0877 - val_acc: 0.9744\n",
      "Epoch 12/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0646 - acc: 0.9801 - val_loss: 1.0855 - val_acc: 0.9359\n",
      "Epoch 13/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0802 - acc: 0.9615 - val_loss: 1.0951 - val_acc: 0.9231\n",
      "Epoch 14/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0590 - acc: 0.9843 - val_loss: 1.1197 - val_acc: 0.9231\n",
      "Epoch 15/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0673 - acc: 0.9843 - val_loss: 1.1074 - val_acc: 0.9359\n",
      "Epoch 16/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0588 - acc: 0.9801 - val_loss: 1.2625 - val_acc: 0.8462\n",
      "Epoch 17/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0633 - acc: 0.9772 - val_loss: 1.0902 - val_acc: 0.9103\n",
      "Epoch 18/100\n",
      "702/702 [==============================] - 6s 9ms/step - loss: 1.0584 - acc: 0.9872 - val_loss: 1.0809 - val_acc: 0.9487\n",
      "Epoch 19/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0491 - acc: 0.9929 - val_loss: 1.0700 - val_acc: 0.9487\n",
      "Epoch 20/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0514 - acc: 0.9886 - val_loss: 1.0691 - val_acc: 0.9615\n",
      "Epoch 21/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0504 - acc: 0.9915 - val_loss: 1.0638 - val_acc: 0.9744\n",
      "Epoch 22/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0473 - acc: 0.9972 - val_loss: 1.0743 - val_acc: 0.9615\n",
      "Epoch 23/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0482 - acc: 0.9957 - val_loss: 1.0794 - val_acc: 0.9615\n",
      "Epoch 24/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0455 - acc: 0.9986 - val_loss: 1.0681 - val_acc: 0.9615\n",
      "Epoch 25/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0460 - acc: 0.9943 - val_loss: 1.0595 - val_acc: 0.9872\n",
      "Epoch 26/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0450 - acc: 0.9943 - val_loss: 1.0668 - val_acc: 0.9744\n",
      "Epoch 27/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0431 - acc: 0.9972 - val_loss: 1.0643 - val_acc: 0.9744\n",
      "Epoch 28/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0455 - acc: 0.9972 - val_loss: 1.0779 - val_acc: 0.9744\n",
      "Epoch 29/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0432 - acc: 0.9986 - val_loss: 1.0581 - val_acc: 0.9872\n",
      "Epoch 30/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0432 - acc: 0.9972 - val_loss: 1.0820 - val_acc: 0.9615\n",
      "Epoch 31/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0425 - acc: 0.9986 - val_loss: 1.1785 - val_acc: 0.9615\n",
      "Epoch 32/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0434 - acc: 0.9972 - val_loss: 1.0722 - val_acc: 0.9487\n",
      "Epoch 33/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0418 - acc: 1.0000 - val_loss: 1.0665 - val_acc: 0.9744\n",
      "Epoch 34/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0414 - acc: 1.0000 - val_loss: 1.0579 - val_acc: 0.9744\n",
      "Epoch 35/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0415 - acc: 1.0000 - val_loss: 1.0770 - val_acc: 0.9615\n",
      "Epoch 36/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0424 - acc: 0.9986 - val_loss: 1.0760 - val_acc: 0.9615\n",
      "Epoch 37/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0414 - acc: 1.0000 - val_loss: 1.0692 - val_acc: 0.9744\n",
      "Epoch 38/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0411 - acc: 1.0000 - val_loss: 1.0570 - val_acc: 0.9872\n",
      "Epoch 39/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0411 - acc: 1.0000 - val_loss: 1.0561 - val_acc: 0.9744\n",
      "Epoch 40/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0411 - acc: 1.0000 - val_loss: 1.0561 - val_acc: 0.9744\n",
      "Epoch 41/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0410 - acc: 1.0000 - val_loss: 1.0609 - val_acc: 0.9872\n",
      "Epoch 42/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0408 - acc: 1.0000 - val_loss: 1.0655 - val_acc: 0.9615\n",
      "Epoch 43/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0415 - acc: 1.0000 - val_loss: 1.0634 - val_acc: 0.9615\n",
      "Epoch 44/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0408 - acc: 1.0000 - val_loss: 1.0817 - val_acc: 0.9615\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0407 - acc: 1.0000 - val_loss: 1.0605 - val_acc: 0.9615\n",
      "Epoch 46/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0407 - acc: 1.0000 - val_loss: 1.0585 - val_acc: 0.9615\n",
      "Epoch 47/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0407 - acc: 1.0000 - val_loss: 1.0658 - val_acc: 0.9615\n",
      "Epoch 48/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0634 - val_acc: 0.9744\n",
      "Epoch 49/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0566 - val_acc: 0.9744\n",
      "Epoch 50/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0633 - val_acc: 0.9615\n",
      "Epoch 51/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0534 - val_acc: 0.9744\n",
      "Epoch 52/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0553 - val_acc: 0.9872\n",
      "Epoch 53/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0517 - val_acc: 0.9872\n",
      "Epoch 54/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0544 - val_acc: 0.9872\n",
      "Epoch 55/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0610 - val_acc: 0.9744\n",
      "Epoch 56/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0573 - val_acc: 0.9744\n",
      "Epoch 57/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0597 - val_acc: 0.9615\n",
      "Epoch 58/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0627 - val_acc: 0.9744\n",
      "Epoch 59/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0596 - val_acc: 0.9615\n",
      "Epoch 60/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0624 - val_acc: 0.9744\n",
      "Epoch 61/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0647 - val_acc: 0.9615\n",
      "Epoch 62/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0604 - val_acc: 0.9615\n",
      "Epoch 63/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0633 - val_acc: 0.9744\n",
      "Epoch 64/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0633 - val_acc: 0.9744\n",
      "Epoch 65/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0614 - val_acc: 0.9744\n",
      "Epoch 66/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0581 - val_acc: 0.9744\n",
      "Epoch 67/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0610 - val_acc: 0.9744\n",
      "Epoch 68/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0575 - val_acc: 0.9744\n",
      "Epoch 69/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0579 - val_acc: 0.9744\n",
      "Epoch 70/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0558 - val_acc: 0.9744\n",
      "Epoch 71/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0587 - val_acc: 0.9744\n",
      "Epoch 72/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0613 - val_acc: 0.9744\n",
      "Epoch 73/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0622 - val_acc: 0.9744\n",
      "Epoch 74/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0577 - val_acc: 0.9744\n",
      "Epoch 75/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0574 - val_acc: 0.9872\n",
      "Epoch 76/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0557 - val_acc: 0.9744\n",
      "Epoch 77/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0581 - val_acc: 0.9744\n",
      "Epoch 78/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0572 - val_acc: 0.9872\n",
      "Epoch 79/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0580 - val_acc: 0.9744\n",
      "Epoch 80/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0567 - val_acc: 0.9872\n",
      "Epoch 81/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0557 - val_acc: 0.9744\n",
      "Epoch 82/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0583 - val_acc: 0.9744\n",
      "Epoch 83/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0593 - val_acc: 0.9615\n",
      "Epoch 84/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0574 - val_acc: 0.9744\n",
      "Epoch 85/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0586 - val_acc: 0.9615\n",
      "Epoch 86/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0575 - val_acc: 0.9615\n",
      "Epoch 87/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0572 - val_acc: 0.9615\n",
      "Epoch 88/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0575 - val_acc: 0.9615\n",
      "Epoch 89/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0576 - val_acc: 0.9744\n",
      "Epoch 90/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0592 - val_acc: 0.9744\n",
      "Epoch 91/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0590 - val_acc: 0.9744\n",
      "Epoch 92/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0586 - val_acc: 0.9615\n",
      "Epoch 93/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0579 - val_acc: 0.9615\n",
      "Epoch 94/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0589 - val_acc: 0.9744\n",
      "Epoch 95/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0580 - val_acc: 0.9744\n",
      "Epoch 96/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0582 - val_acc: 0.9615\n",
      "Epoch 97/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0578 - val_acc: 0.9487\n",
      "Epoch 98/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0579 - val_acc: 0.9615\n",
      "Epoch 99/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0582 - val_acc: 0.9744\n",
      "Epoch 100/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0589 - val_acc: 0.9615\n",
      "78/78 [==============================] - 0s 337us/step\n",
      "Score for fold 6: loss of 1.0555539345129943; acc of 97.43589743589743%\n",
      "Train on 702 samples, validate on 78 samples\n",
      "Epoch 1/100\n",
      "702/702 [==============================] - 18s 26ms/step - loss: 2.1684 - acc: 0.3860 - val_loss: 1.9085 - val_acc: 0.5000\n",
      "Epoch 2/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.5714 - acc: 0.7422 - val_loss: 1.4594 - val_acc: 0.7692\n",
      "Epoch 3/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.3842 - acc: 0.8105 - val_loss: 1.3197 - val_acc: 0.8205\n",
      "Epoch 4/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.2072 - acc: 0.8974 - val_loss: 1.1517 - val_acc: 0.8974\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1754 - acc: 0.9003 - val_loss: 1.1278 - val_acc: 0.9103\n",
      "Epoch 6/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1286 - acc: 0.9416 - val_loss: 1.1374 - val_acc: 0.8974\n",
      "Epoch 7/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0958 - acc: 0.9430 - val_loss: 1.1421 - val_acc: 0.8590\n",
      "Epoch 8/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0815 - acc: 0.9587 - val_loss: 1.2197 - val_acc: 0.8333\n",
      "Epoch 9/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1031 - acc: 0.9516 - val_loss: 1.1697 - val_acc: 0.8205\n",
      "Epoch 10/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0861 - acc: 0.9658 - val_loss: 1.0897 - val_acc: 0.9744\n",
      "Epoch 11/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1080 - acc: 0.9530 - val_loss: 1.1239 - val_acc: 0.9359\n",
      "Epoch 12/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0625 - acc: 0.9900 - val_loss: 1.0927 - val_acc: 0.9231\n",
      "Epoch 13/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0652 - acc: 0.9729 - val_loss: 1.0997 - val_acc: 0.9744\n",
      "Epoch 14/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0620 - acc: 0.9772 - val_loss: 1.2838 - val_acc: 0.9231\n",
      "Epoch 15/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0661 - acc: 0.9758 - val_loss: 1.1026 - val_acc: 0.9487\n",
      "Epoch 16/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0529 - acc: 0.9957 - val_loss: 1.0713 - val_acc: 0.9615\n",
      "Epoch 17/100\n",
      "702/702 [==============================] - 6s 9ms/step - loss: 1.0737 - acc: 0.9729 - val_loss: 1.0679 - val_acc: 0.9744\n",
      "Epoch 18/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0502 - acc: 0.9943 - val_loss: 1.0734 - val_acc: 0.9872\n",
      "Epoch 19/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0464 - acc: 0.9986 - val_loss: 1.0853 - val_acc: 0.9744\n",
      "Epoch 20/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0507 - acc: 0.9900 - val_loss: 1.0954 - val_acc: 0.9744\n",
      "Epoch 21/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0509 - acc: 0.9886 - val_loss: 1.1065 - val_acc: 0.9744\n",
      "Epoch 22/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0471 - acc: 0.9957 - val_loss: 1.1010 - val_acc: 0.9744\n",
      "Epoch 23/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0466 - acc: 0.9957 - val_loss: 1.1055 - val_acc: 0.9744\n",
      "Epoch 24/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0447 - acc: 0.9972 - val_loss: 1.0924 - val_acc: 0.9744\n",
      "Epoch 25/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0455 - acc: 0.9957 - val_loss: 1.1192 - val_acc: 0.9615\n",
      "Epoch 26/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0464 - acc: 0.9957 - val_loss: 1.1039 - val_acc: 0.9615\n",
      "Epoch 27/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0431 - acc: 0.9986 - val_loss: 1.0956 - val_acc: 0.9744\n",
      "Epoch 28/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0428 - acc: 1.0000 - val_loss: 1.0868 - val_acc: 0.9744\n",
      "Epoch 29/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0425 - acc: 1.0000 - val_loss: 1.0837 - val_acc: 0.9872\n",
      "Epoch 30/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0425 - acc: 1.0000 - val_loss: 1.1009 - val_acc: 0.9744\n",
      "Epoch 31/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0426 - acc: 1.0000 - val_loss: 1.0857 - val_acc: 0.9872\n",
      "Epoch 32/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0418 - acc: 1.0000 - val_loss: 1.1114 - val_acc: 0.9744\n",
      "Epoch 33/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0424 - acc: 1.0000 - val_loss: 1.1067 - val_acc: 0.9744\n",
      "Epoch 34/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0413 - acc: 1.0000 - val_loss: 1.1051 - val_acc: 0.9744\n",
      "Epoch 35/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0412 - acc: 1.0000 - val_loss: 1.1046 - val_acc: 0.9744\n",
      "Epoch 36/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0424 - acc: 0.9972 - val_loss: 1.1042 - val_acc: 0.9744\n",
      "Epoch 37/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0411 - acc: 1.0000 - val_loss: 1.1043 - val_acc: 0.9744\n",
      "Epoch 38/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0411 - acc: 1.0000 - val_loss: 1.1189 - val_acc: 0.9744\n",
      "Epoch 39/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0411 - acc: 1.0000 - val_loss: 1.1008 - val_acc: 0.9744\n",
      "Epoch 40/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0407 - acc: 1.0000 - val_loss: 1.1133 - val_acc: 0.9615\n",
      "Epoch 41/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0407 - acc: 1.0000 - val_loss: 1.1035 - val_acc: 0.9744\n",
      "Epoch 42/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0409 - acc: 1.0000 - val_loss: 1.1044 - val_acc: 0.9744\n",
      "Epoch 43/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0407 - acc: 1.0000 - val_loss: 1.0995 - val_acc: 0.9744\n",
      "Epoch 44/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0413 - acc: 0.9986 - val_loss: 1.1089 - val_acc: 0.9744\n",
      "Epoch 45/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.1069 - val_acc: 0.9744\n",
      "Epoch 46/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.1041 - val_acc: 0.9744\n",
      "Epoch 47/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.1036 - val_acc: 0.9744\n",
      "Epoch 48/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.1093 - val_acc: 0.9744\n",
      "Epoch 49/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.1066 - val_acc: 0.9872\n",
      "Epoch 50/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.1066 - val_acc: 0.9744\n",
      "Epoch 51/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.1044 - val_acc: 0.9872\n",
      "Epoch 52/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.1077 - val_acc: 0.9744\n",
      "Epoch 53/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.1050 - val_acc: 0.9744\n",
      "Epoch 54/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.1025 - val_acc: 0.9744\n",
      "Epoch 55/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.1069 - val_acc: 0.9744\n",
      "Epoch 56/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.1061 - val_acc: 0.9744\n",
      "Epoch 57/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.1037 - val_acc: 0.9872\n",
      "Epoch 58/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.1090 - val_acc: 0.9744\n",
      "Epoch 59/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.1067 - val_acc: 0.9744\n",
      "Epoch 60/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.1059 - val_acc: 0.9744\n",
      "Epoch 61/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.1077 - val_acc: 0.9744\n",
      "Epoch 62/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.1088 - val_acc: 0.9744\n",
      "Epoch 63/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.1045 - val_acc: 0.9744\n",
      "Epoch 64/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.1074 - val_acc: 0.9744\n",
      "Epoch 65/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.1014 - val_acc: 0.9744\n",
      "Epoch 66/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0971 - val_acc: 0.9744\n",
      "Epoch 67/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.1080 - val_acc: 0.9744\n",
      "Epoch 68/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.1029 - val_acc: 0.9744\n",
      "Epoch 69/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.1024 - val_acc: 0.9744\n",
      "Epoch 70/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.1095 - val_acc: 0.9744\n",
      "Epoch 71/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.1055 - val_acc: 0.9744\n",
      "Epoch 72/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.1079 - val_acc: 0.9744\n",
      "Epoch 73/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.1056 - val_acc: 0.9744\n",
      "Epoch 74/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.1054 - val_acc: 0.9744\n",
      "Epoch 75/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.1049 - val_acc: 0.9744\n",
      "Epoch 76/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.1067 - val_acc: 0.9744\n",
      "Epoch 77/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.1030 - val_acc: 0.9744\n",
      "Epoch 78/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.1032 - val_acc: 0.9744\n",
      "Epoch 79/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.1031 - val_acc: 0.9744\n",
      "Epoch 80/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.1047 - val_acc: 0.9744\n",
      "Epoch 81/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.1040 - val_acc: 0.9744\n",
      "Epoch 82/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.1040 - val_acc: 0.9744\n",
      "Epoch 83/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.1010 - val_acc: 0.9744\n",
      "Epoch 84/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.1044 - val_acc: 0.9744\n",
      "Epoch 85/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.1046 - val_acc: 0.9744\n",
      "Epoch 86/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.1027 - val_acc: 0.9744\n",
      "Epoch 87/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.1004 - val_acc: 0.9744\n",
      "Epoch 88/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.1021 - val_acc: 0.9744\n",
      "Epoch 89/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.1024 - val_acc: 0.9744\n",
      "Epoch 90/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.1012 - val_acc: 0.9744\n",
      "Epoch 91/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.1019 - val_acc: 0.9744\n",
      "Epoch 92/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0998 - val_acc: 0.9744\n",
      "Epoch 93/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0992 - val_acc: 0.9744\n",
      "Epoch 94/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.1023 - val_acc: 0.9744\n",
      "Epoch 95/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.1014 - val_acc: 0.9744\n",
      "Epoch 96/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.1028 - val_acc: 0.9744\n",
      "Epoch 97/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.1012 - val_acc: 0.9744\n",
      "Epoch 98/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.1019 - val_acc: 0.9744\n",
      "Epoch 99/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.1035 - val_acc: 0.9744\n",
      "Epoch 100/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.1026 - val_acc: 0.9744\n",
      "78/78 [==============================] - 0s 347us/step\n",
      "Score for fold 7: loss of 1.0980009574156542; acc of 97.43589697740018%\n",
      "Train on 702 samples, validate on 78 samples\n",
      "Epoch 1/100\n",
      "702/702 [==============================] - 19s 27ms/step - loss: 2.2930 - acc: 0.3162 - val_loss: 1.9521 - val_acc: 0.3846\n",
      "Epoch 2/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.6711 - acc: 0.6439 - val_loss: 1.4484 - val_acc: 0.7949\n",
      "Epoch 3/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.4056 - acc: 0.7721 - val_loss: 1.2998 - val_acc: 0.8974\n",
      "Epoch 4/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.2975 - acc: 0.8234 - val_loss: 1.4313 - val_acc: 0.8077\n",
      "Epoch 5/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1730 - acc: 0.9160 - val_loss: 1.6832 - val_acc: 0.6154\n",
      "Epoch 6/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1981 - acc: 0.8875 - val_loss: 1.1442 - val_acc: 0.8462\n",
      "Epoch 7/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1403 - acc: 0.9274 - val_loss: 1.1582 - val_acc: 0.8590\n",
      "Epoch 8/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0970 - acc: 0.9587 - val_loss: 1.1897 - val_acc: 0.8077\n",
      "Epoch 9/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0902 - acc: 0.9516 - val_loss: 1.3491 - val_acc: 0.8205\n",
      "Epoch 10/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0960 - acc: 0.9359 - val_loss: 1.0924 - val_acc: 0.9231\n",
      "Epoch 11/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0959 - acc: 0.9459 - val_loss: 1.1044 - val_acc: 0.8846\n",
      "Epoch 12/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1142 - acc: 0.9530 - val_loss: 1.1472 - val_acc: 0.8462\n",
      "Epoch 13/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0637 - acc: 0.9815 - val_loss: 1.0865 - val_acc: 0.9487\n",
      "Epoch 14/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0967 - acc: 0.9701 - val_loss: 1.1974 - val_acc: 0.8718\n",
      "Epoch 15/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0678 - acc: 0.9786 - val_loss: 1.0833 - val_acc: 0.9615\n",
      "Epoch 16/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0670 - acc: 0.9758 - val_loss: 1.0811 - val_acc: 0.9231\n",
      "Epoch 17/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0550 - acc: 0.9900 - val_loss: 1.1025 - val_acc: 0.9359\n",
      "Epoch 18/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0740 - acc: 0.9729 - val_loss: 1.0785 - val_acc: 0.9744\n",
      "Epoch 19/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0545 - acc: 0.9915 - val_loss: 1.0809 - val_acc: 0.9487\n",
      "Epoch 20/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0558 - acc: 0.9886 - val_loss: 1.0765 - val_acc: 0.9615\n",
      "Epoch 21/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0526 - acc: 0.9915 - val_loss: 1.0965 - val_acc: 0.9487\n",
      "Epoch 22/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0538 - acc: 0.9886 - val_loss: 1.0750 - val_acc: 0.9615\n",
      "Epoch 23/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0515 - acc: 0.9915 - val_loss: 1.1033 - val_acc: 0.8974\n",
      "Epoch 24/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0621 - acc: 0.9801 - val_loss: 1.0883 - val_acc: 0.9615\n",
      "Epoch 25/100\n",
      "702/702 [==============================] - 6s 9ms/step - loss: 1.0475 - acc: 0.9957 - val_loss: 1.0679 - val_acc: 0.9744\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0551 - acc: 0.9972 - val_loss: 1.0699 - val_acc: 0.9487\n",
      "Epoch 27/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0455 - acc: 0.9972 - val_loss: 1.1049 - val_acc: 0.9487\n",
      "Epoch 28/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0451 - acc: 0.9972 - val_loss: 1.0636 - val_acc: 0.9487\n",
      "Epoch 29/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0464 - acc: 0.9943 - val_loss: 1.0764 - val_acc: 0.9744\n",
      "Epoch 30/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0450 - acc: 0.9957 - val_loss: 1.0652 - val_acc: 0.9615\n",
      "Epoch 31/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0439 - acc: 0.9972 - val_loss: 1.0589 - val_acc: 0.9744\n",
      "Epoch 32/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0427 - acc: 0.9986 - val_loss: 1.0585 - val_acc: 0.9744\n",
      "Epoch 33/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0428 - acc: 0.9986 - val_loss: 1.0821 - val_acc: 0.9615\n",
      "Epoch 34/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0428 - acc: 0.9986 - val_loss: 1.0649 - val_acc: 0.9744\n",
      "Epoch 35/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0424 - acc: 0.9972 - val_loss: 1.0691 - val_acc: 0.9615\n",
      "Epoch 36/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0426 - acc: 1.0000 - val_loss: 1.0583 - val_acc: 0.9872\n",
      "Epoch 37/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0420 - acc: 1.0000 - val_loss: 1.0727 - val_acc: 0.9615\n",
      "Epoch 38/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0420 - acc: 0.9986 - val_loss: 1.0596 - val_acc: 0.9744\n",
      "Epoch 39/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0416 - acc: 1.0000 - val_loss: 1.0599 - val_acc: 0.9744\n",
      "Epoch 40/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0432 - acc: 0.9972 - val_loss: 1.0705 - val_acc: 0.9487\n",
      "Epoch 41/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0415 - acc: 1.0000 - val_loss: 1.0602 - val_acc: 0.9744\n",
      "Epoch 42/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0412 - acc: 1.0000 - val_loss: 1.0600 - val_acc: 0.9744\n",
      "Epoch 43/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0420 - acc: 1.0000 - val_loss: 1.0666 - val_acc: 0.9744\n",
      "Epoch 44/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0420 - acc: 0.9986 - val_loss: 1.0550 - val_acc: 0.9872\n",
      "Epoch 45/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0408 - acc: 1.0000 - val_loss: 1.0590 - val_acc: 0.9744\n",
      "Epoch 46/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0409 - acc: 1.0000 - val_loss: 1.0565 - val_acc: 0.9872\n",
      "Epoch 47/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0409 - acc: 1.0000 - val_loss: 1.0582 - val_acc: 0.9744\n",
      "Epoch 48/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0418 - acc: 0.9972 - val_loss: 1.0560 - val_acc: 0.9744\n",
      "Epoch 49/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0408 - acc: 1.0000 - val_loss: 1.0557 - val_acc: 0.9744\n",
      "Epoch 50/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0408 - acc: 1.0000 - val_loss: 1.0518 - val_acc: 0.9872\n",
      "Epoch 51/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0533 - val_acc: 0.9872\n",
      "Epoch 52/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0536 - val_acc: 0.9744\n",
      "Epoch 53/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0577 - val_acc: 0.9744\n",
      "Epoch 54/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0647 - val_acc: 0.9744\n",
      "Epoch 55/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0561 - val_acc: 0.9744\n",
      "Epoch 56/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0549 - val_acc: 0.9744\n",
      "Epoch 57/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0553 - val_acc: 0.9744\n",
      "Epoch 58/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0573 - val_acc: 0.9744\n",
      "Epoch 59/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0565 - val_acc: 0.9744\n",
      "Epoch 60/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0559 - val_acc: 0.9744\n",
      "Epoch 61/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0566 - val_acc: 0.9744\n",
      "Epoch 62/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0569 - val_acc: 0.9744\n",
      "Epoch 63/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0559 - val_acc: 0.9744\n",
      "Epoch 64/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0542 - val_acc: 0.9744\n",
      "Epoch 65/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0582 - val_acc: 0.9744\n",
      "Epoch 66/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0562 - val_acc: 0.9744\n",
      "Epoch 67/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0561 - val_acc: 0.9744\n",
      "Epoch 68/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0556 - val_acc: 0.9744\n",
      "Epoch 69/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0552 - val_acc: 0.9744\n",
      "Epoch 70/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0572 - val_acc: 0.9744\n",
      "Epoch 71/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0542 - val_acc: 0.9744\n",
      "Epoch 72/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0550 - val_acc: 0.9744\n",
      "Epoch 73/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0558 - val_acc: 0.9744\n",
      "Epoch 74/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0549 - val_acc: 0.9744\n",
      "Epoch 75/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0557 - val_acc: 0.9744\n",
      "Epoch 76/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0566 - val_acc: 0.9744\n",
      "Epoch 77/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0558 - val_acc: 0.9744\n",
      "Epoch 78/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0556 - val_acc: 0.9744\n",
      "Epoch 79/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0550 - val_acc: 0.9744\n",
      "Epoch 80/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0549 - val_acc: 0.9744\n",
      "Epoch 81/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0563 - val_acc: 0.9744\n",
      "Epoch 82/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0556 - val_acc: 0.9744\n",
      "Epoch 83/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0553 - val_acc: 0.9744\n",
      "Epoch 84/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0555 - val_acc: 0.9744\n",
      "Epoch 85/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0558 - val_acc: 0.9744\n",
      "Epoch 86/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0563 - val_acc: 0.9744\n",
      "Epoch 87/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0558 - val_acc: 0.9744\n",
      "Epoch 88/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0551 - val_acc: 0.9744\n",
      "Epoch 89/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0549 - val_acc: 0.9744\n",
      "Epoch 90/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0555 - val_acc: 0.9744\n",
      "Epoch 91/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0552 - val_acc: 0.9744\n",
      "Epoch 92/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0550 - val_acc: 0.9744\n",
      "Epoch 93/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0559 - val_acc: 0.9744\n",
      "Epoch 94/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0551 - val_acc: 0.9744\n",
      "Epoch 95/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0554 - val_acc: 0.9744\n",
      "Epoch 96/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0558 - val_acc: 0.9744\n",
      "Epoch 97/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0558 - val_acc: 0.9744\n",
      "Epoch 98/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0562 - val_acc: 0.9744\n",
      "Epoch 99/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0561 - val_acc: 0.9744\n",
      "Epoch 100/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0561 - val_acc: 0.9744\n",
      "78/78 [==============================] - 0s 347us/step\n",
      "Score for fold 8: loss of 1.0560628787065163; acc of 97.43589743589743%\n",
      "Train on 702 samples, validate on 78 samples\n",
      "Epoch 1/100\n",
      "702/702 [==============================] - 19s 27ms/step - loss: 2.2230 - acc: 0.3789 - val_loss: 1.9457 - val_acc: 0.4615\n",
      "Epoch 2/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.6087 - acc: 0.7037 - val_loss: 1.5340 - val_acc: 0.6410\n",
      "Epoch 3/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.3884 - acc: 0.7821 - val_loss: 1.2979 - val_acc: 0.8462\n",
      "Epoch 4/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.2756 - acc: 0.8476 - val_loss: 1.1481 - val_acc: 0.9231\n",
      "Epoch 5/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1622 - acc: 0.9031 - val_loss: 1.1879 - val_acc: 0.9103\n",
      "Epoch 6/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1638 - acc: 0.9060 - val_loss: 1.1673 - val_acc: 0.8718\n",
      "Epoch 7/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1387 - acc: 0.9174 - val_loss: 1.1114 - val_acc: 0.9615\n",
      "Epoch 8/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1203 - acc: 0.9316 - val_loss: 1.1697 - val_acc: 0.9231\n",
      "Epoch 9/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0916 - acc: 0.9644 - val_loss: 1.1009 - val_acc: 0.9487\n",
      "Epoch 10/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0743 - acc: 0.9729 - val_loss: 1.1509 - val_acc: 0.8718\n",
      "Epoch 11/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1013 - acc: 0.9430 - val_loss: 1.0841 - val_acc: 0.9615\n",
      "Epoch 12/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0641 - acc: 0.9829 - val_loss: 1.0768 - val_acc: 0.9615\n",
      "Epoch 13/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0816 - acc: 0.9715 - val_loss: 1.0817 - val_acc: 0.9615\n",
      "Epoch 14/100\n",
      "702/702 [==============================] - 6s 9ms/step - loss: 1.0548 - acc: 0.9886 - val_loss: 1.0837 - val_acc: 0.9487\n",
      "Epoch 15/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0544 - acc: 0.9915 - val_loss: 1.0801 - val_acc: 0.9487\n",
      "Epoch 16/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0521 - acc: 0.9915 - val_loss: 1.0949 - val_acc: 0.9103\n",
      "Epoch 17/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0545 - acc: 0.9872 - val_loss: 1.0755 - val_acc: 0.9615\n",
      "Epoch 18/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0647 - acc: 0.9872 - val_loss: 1.0684 - val_acc: 0.9744\n",
      "Epoch 19/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0502 - acc: 0.9957 - val_loss: 1.0673 - val_acc: 0.9744\n",
      "Epoch 20/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0455 - acc: 0.9986 - val_loss: 1.0728 - val_acc: 0.9744\n",
      "Epoch 21/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0679 - acc: 0.9829 - val_loss: 1.0692 - val_acc: 0.9744\n",
      "Epoch 22/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0456 - acc: 0.9972 - val_loss: 1.0711 - val_acc: 0.9615\n",
      "Epoch 23/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0454 - acc: 0.9972 - val_loss: 1.0729 - val_acc: 0.9744\n",
      "Epoch 24/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0467 - acc: 0.9957 - val_loss: 1.0665 - val_acc: 0.9744\n",
      "Epoch 25/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0496 - acc: 0.9915 - val_loss: 1.0708 - val_acc: 0.9744\n",
      "Epoch 26/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0455 - acc: 0.9943 - val_loss: 1.0771 - val_acc: 0.9615\n",
      "Epoch 27/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0465 - acc: 0.9943 - val_loss: 1.0686 - val_acc: 0.9615\n",
      "Epoch 28/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0428 - acc: 1.0000 - val_loss: 1.0664 - val_acc: 0.9744\n",
      "Epoch 29/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0441 - acc: 0.9957 - val_loss: 1.0693 - val_acc: 0.9615\n",
      "Epoch 30/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0434 - acc: 0.9986 - val_loss: 1.0680 - val_acc: 0.9744\n",
      "Epoch 31/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0444 - acc: 0.9972 - val_loss: 1.0656 - val_acc: 0.9744\n",
      "Epoch 32/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0418 - acc: 1.0000 - val_loss: 1.0726 - val_acc: 0.9615\n",
      "Epoch 33/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0420 - acc: 1.0000 - val_loss: 1.0664 - val_acc: 0.9744\n",
      "Epoch 34/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0417 - acc: 1.0000 - val_loss: 1.0641 - val_acc: 0.9744\n",
      "Epoch 35/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0414 - acc: 1.0000 - val_loss: 1.0656 - val_acc: 0.9744\n",
      "Epoch 36/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0415 - acc: 1.0000 - val_loss: 1.0642 - val_acc: 0.9744\n",
      "Epoch 37/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0412 - acc: 1.0000 - val_loss: 1.0646 - val_acc: 0.9744\n",
      "Epoch 38/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0421 - acc: 0.9986 - val_loss: 1.0648 - val_acc: 0.9744\n",
      "Epoch 39/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0408 - acc: 1.0000 - val_loss: 1.0652 - val_acc: 0.9744\n",
      "Epoch 40/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0410 - acc: 1.0000 - val_loss: 1.0655 - val_acc: 0.9744\n",
      "Epoch 41/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0410 - acc: 1.0000 - val_loss: 1.0651 - val_acc: 0.9744\n",
      "Epoch 42/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0408 - acc: 1.0000 - val_loss: 1.0661 - val_acc: 0.9744\n",
      "Epoch 43/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0414 - acc: 0.9986 - val_loss: 1.0647 - val_acc: 0.9744\n",
      "Epoch 44/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0407 - acc: 1.0000 - val_loss: 1.0652 - val_acc: 0.9744\n",
      "Epoch 45/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0409 - acc: 0.9986 - val_loss: 1.0653 - val_acc: 0.9744\n",
      "Epoch 46/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0657 - val_acc: 0.9744\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0652 - val_acc: 0.9744\n",
      "Epoch 48/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0645 - val_acc: 0.9744\n",
      "Epoch 49/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0659 - val_acc: 0.9744\n",
      "Epoch 50/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0648 - val_acc: 0.9744\n",
      "Epoch 51/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0649 - val_acc: 0.9744\n",
      "Epoch 52/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0652 - val_acc: 0.9744\n",
      "Epoch 53/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0649 - val_acc: 0.9744\n",
      "Epoch 54/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0650 - val_acc: 0.9744\n",
      "Epoch 55/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0645 - val_acc: 0.9744\n",
      "Epoch 56/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0653 - val_acc: 0.9744\n",
      "Epoch 57/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0646 - val_acc: 0.9744\n",
      "Epoch 58/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0645 - val_acc: 0.9744\n",
      "Epoch 59/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0651 - val_acc: 0.9744\n",
      "Epoch 60/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0648 - val_acc: 0.9744\n",
      "Epoch 61/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0651 - val_acc: 0.9744\n",
      "Epoch 62/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0649 - val_acc: 0.9744\n",
      "Epoch 63/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0650 - val_acc: 0.9744\n",
      "Epoch 64/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0650 - val_acc: 0.9744\n",
      "Epoch 65/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0649 - val_acc: 0.9744\n",
      "Epoch 66/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0651 - val_acc: 0.9744\n",
      "Epoch 67/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0650 - val_acc: 0.9744\n",
      "Epoch 68/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0649 - val_acc: 0.9744\n",
      "Epoch 69/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0648 - val_acc: 0.9744\n",
      "Epoch 70/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0649 - val_acc: 0.9744\n",
      "Epoch 71/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0647 - val_acc: 0.9744\n",
      "Epoch 72/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0650 - val_acc: 0.9744\n",
      "Epoch 73/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0644 - val_acc: 0.9744\n",
      "Epoch 74/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0647 - val_acc: 0.9744\n",
      "Epoch 75/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0649 - val_acc: 0.9744\n",
      "Epoch 76/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0644 - val_acc: 0.9744\n",
      "Epoch 77/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0648 - val_acc: 0.9744\n",
      "Epoch 78/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0646 - val_acc: 0.9744\n",
      "Epoch 79/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0648 - val_acc: 0.9744\n",
      "Epoch 80/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0649 - val_acc: 0.9744\n",
      "Epoch 81/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0648 - val_acc: 0.9744\n",
      "Epoch 82/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0648 - val_acc: 0.9744\n",
      "Epoch 83/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0647 - val_acc: 0.9744\n",
      "Epoch 84/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0645 - val_acc: 0.9744\n",
      "Epoch 85/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0647 - val_acc: 0.9744\n",
      "Epoch 86/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0648 - val_acc: 0.9744\n",
      "Epoch 87/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0647 - val_acc: 0.9744\n",
      "Epoch 88/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0647 - val_acc: 0.9744\n",
      "Epoch 89/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0647 - val_acc: 0.9744\n",
      "Epoch 90/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0649 - val_acc: 0.9744\n",
      "Epoch 91/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0648 - val_acc: 0.9744\n",
      "Epoch 92/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0647 - val_acc: 0.9744\n",
      "Epoch 93/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0646 - val_acc: 0.9744\n",
      "Epoch 94/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0645 - val_acc: 0.9744\n",
      "Epoch 95/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0646 - val_acc: 0.9744\n",
      "Epoch 96/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0646 - val_acc: 0.9744\n",
      "Epoch 97/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0646 - val_acc: 0.9744\n",
      "Epoch 98/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0646 - val_acc: 0.9744\n",
      "Epoch 99/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0646 - val_acc: 0.9744\n",
      "Epoch 100/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0646 - val_acc: 0.9744\n",
      "78/78 [==============================] - 0s 354us/step\n",
      "Score for fold 9: loss of 1.0646987022497716; acc of 97.43589697740018%\n",
      "Train on 702 samples, validate on 78 samples\n",
      "Epoch 1/100\n",
      "702/702 [==============================] - 19s 27ms/step - loss: 2.2989 - acc: 0.2906 - val_loss: 1.7764 - val_acc: 0.6923\n",
      "Epoch 2/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.6749 - acc: 0.6382 - val_loss: 1.5701 - val_acc: 0.6154\n",
      "Epoch 3/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.4085 - acc: 0.8148 - val_loss: 1.5361 - val_acc: 0.7308\n",
      "Epoch 4/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.2692 - acc: 0.8519 - val_loss: 1.2717 - val_acc: 0.7564\n",
      "Epoch 5/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1924 - acc: 0.8789 - val_loss: 1.2015 - val_acc: 0.8462\n",
      "Epoch 6/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1613 - acc: 0.8989 - val_loss: 1.3883 - val_acc: 0.8205\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1240 - acc: 0.9330 - val_loss: 1.1505 - val_acc: 0.8974\n",
      "Epoch 8/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.1179 - acc: 0.9359 - val_loss: 1.0975 - val_acc: 0.9872\n",
      "Epoch 9/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0880 - acc: 0.9530 - val_loss: 1.1179 - val_acc: 0.8974\n",
      "Epoch 10/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0843 - acc: 0.9573 - val_loss: 1.1266 - val_acc: 0.9359\n",
      "Epoch 11/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0727 - acc: 0.9758 - val_loss: 1.1210 - val_acc: 0.9487\n",
      "Epoch 12/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0754 - acc: 0.9530 - val_loss: 1.2250 - val_acc: 0.8462\n",
      "Epoch 13/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0969 - acc: 0.9587 - val_loss: 1.1430 - val_acc: 0.8846\n",
      "Epoch 14/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0622 - acc: 0.9829 - val_loss: 1.0622 - val_acc: 1.0000\n",
      "Epoch 15/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0562 - acc: 0.9886 - val_loss: 1.2207 - val_acc: 0.8974\n",
      "Epoch 16/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0814 - acc: 0.9786 - val_loss: 1.0840 - val_acc: 0.9744\n",
      "Epoch 17/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0633 - acc: 0.9772 - val_loss: 1.0601 - val_acc: 0.9744\n",
      "Epoch 18/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0545 - acc: 0.9915 - val_loss: 1.0602 - val_acc: 0.9872\n",
      "Epoch 19/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0497 - acc: 0.9972 - val_loss: 1.0590 - val_acc: 0.9872\n",
      "Epoch 20/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0636 - acc: 0.9701 - val_loss: 1.1009 - val_acc: 0.8974\n",
      "Epoch 21/100\n",
      "702/702 [==============================] - 7s 9ms/step - loss: 1.0531 - acc: 0.9843 - val_loss: 1.0767 - val_acc: 0.9359\n",
      "Epoch 22/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0510 - acc: 0.9886 - val_loss: 1.0845 - val_acc: 0.9615\n",
      "Epoch 23/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0469 - acc: 0.9957 - val_loss: 1.1161 - val_acc: 0.9872\n",
      "Epoch 24/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0462 - acc: 0.9972 - val_loss: 1.0784 - val_acc: 0.9487\n",
      "Epoch 25/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0493 - acc: 0.9957 - val_loss: 1.0667 - val_acc: 0.9744\n",
      "Epoch 26/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0469 - acc: 0.9972 - val_loss: 1.0743 - val_acc: 0.9872\n",
      "Epoch 27/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0455 - acc: 0.9957 - val_loss: 1.0897 - val_acc: 0.9615\n",
      "Epoch 28/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0440 - acc: 0.9972 - val_loss: 1.1078 - val_acc: 0.9487\n",
      "Epoch 29/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0461 - acc: 0.9943 - val_loss: 1.1060 - val_acc: 0.9744\n",
      "Epoch 30/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0448 - acc: 0.9929 - val_loss: 1.0881 - val_acc: 0.9744\n",
      "Epoch 31/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0434 - acc: 0.9986 - val_loss: 1.0830 - val_acc: 0.9872\n",
      "Epoch 32/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0446 - acc: 0.9957 - val_loss: 1.0730 - val_acc: 0.9615\n",
      "Epoch 33/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0425 - acc: 0.9986 - val_loss: 1.0535 - val_acc: 0.9872\n",
      "Epoch 34/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0426 - acc: 0.9986 - val_loss: 1.0765 - val_acc: 0.9615\n",
      "Epoch 35/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0421 - acc: 0.9986 - val_loss: 1.0896 - val_acc: 0.9487\n",
      "Epoch 36/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0428 - acc: 0.9972 - val_loss: 1.1071 - val_acc: 0.9615\n",
      "Epoch 37/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0432 - acc: 0.9972 - val_loss: 1.0826 - val_acc: 0.9615\n",
      "Epoch 38/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0417 - acc: 0.9986 - val_loss: 1.0606 - val_acc: 0.9615\n",
      "Epoch 39/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0415 - acc: 1.0000 - val_loss: 1.0607 - val_acc: 0.9615\n",
      "Epoch 40/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0414 - acc: 1.0000 - val_loss: 1.0748 - val_acc: 0.9615\n",
      "Epoch 41/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0411 - acc: 1.0000 - val_loss: 1.0514 - val_acc: 0.9744\n",
      "Epoch 42/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0410 - acc: 1.0000 - val_loss: 1.0908 - val_acc: 0.9744\n",
      "Epoch 43/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0413 - acc: 1.0000 - val_loss: 1.0618 - val_acc: 0.9744\n",
      "Epoch 44/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0409 - acc: 1.0000 - val_loss: 1.0578 - val_acc: 0.9744\n",
      "Epoch 45/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0408 - acc: 1.0000 - val_loss: 1.0640 - val_acc: 0.9872\n",
      "Epoch 46/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0408 - acc: 1.0000 - val_loss: 1.0929 - val_acc: 0.9744\n",
      "Epoch 47/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0407 - acc: 1.0000 - val_loss: 1.0868 - val_acc: 0.9872\n",
      "Epoch 48/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0407 - acc: 1.0000 - val_loss: 1.0942 - val_acc: 0.9872\n",
      "Epoch 49/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0407 - acc: 1.0000 - val_loss: 1.0970 - val_acc: 0.9872\n",
      "Epoch 50/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0677 - val_acc: 0.9872\n",
      "Epoch 51/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0733 - val_acc: 0.9872\n",
      "Epoch 52/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0771 - val_acc: 0.9872\n",
      "Epoch 53/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.1062 - val_acc: 0.9872\n",
      "Epoch 54/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0877 - val_acc: 0.9872\n",
      "Epoch 55/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 1.0000 - val_loss: 1.0863 - val_acc: 0.9872\n",
      "Epoch 56/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0909 - val_acc: 0.9872\n",
      "Epoch 57/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0968 - val_acc: 0.9872\n",
      "Epoch 58/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.0782 - val_acc: 0.9872\n",
      "Epoch 59/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0787 - val_acc: 0.9872\n",
      "Epoch 60/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0823 - val_acc: 0.9744\n",
      "Epoch 61/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0887 - val_acc: 0.9872\n",
      "Epoch 62/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0865 - val_acc: 0.9872\n",
      "Epoch 63/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0890 - val_acc: 0.9872\n",
      "Epoch 64/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0851 - val_acc: 0.9872\n",
      "Epoch 65/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0787 - val_acc: 0.9872\n",
      "Epoch 66/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0404 - acc: 1.0000 - val_loss: 1.0774 - val_acc: 0.9872\n",
      "Epoch 67/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0685 - val_acc: 0.9872\n",
      "Epoch 68/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0785 - val_acc: 0.9872\n",
      "Epoch 69/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0705 - val_acc: 0.9872\n",
      "Epoch 70/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0846 - val_acc: 0.9872\n",
      "Epoch 71/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0822 - val_acc: 0.9872\n",
      "Epoch 72/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0839 - val_acc: 0.9872\n",
      "Epoch 73/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0874 - val_acc: 0.9872\n",
      "Epoch 74/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0788 - val_acc: 0.9872\n",
      "Epoch 75/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0888 - val_acc: 0.9872\n",
      "Epoch 76/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0879 - val_acc: 0.9872\n",
      "Epoch 77/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0857 - val_acc: 0.9872\n",
      "Epoch 78/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0896 - val_acc: 0.9872\n",
      "Epoch 79/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0861 - val_acc: 0.9872\n",
      "Epoch 80/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0881 - val_acc: 0.9872\n",
      "Epoch 81/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0884 - val_acc: 0.9872\n",
      "Epoch 82/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0857 - val_acc: 0.9872\n",
      "Epoch 83/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0820 - val_acc: 0.9872\n",
      "Epoch 84/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0832 - val_acc: 0.9872\n",
      "Epoch 85/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0855 - val_acc: 0.9872\n",
      "Epoch 86/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0837 - val_acc: 0.9872\n",
      "Epoch 87/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0829 - val_acc: 0.9872\n",
      "Epoch 88/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0852 - val_acc: 0.9872\n",
      "Epoch 89/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0840 - val_acc: 0.9872\n",
      "Epoch 90/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.0893 - val_acc: 0.9872\n",
      "Epoch 91/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0845 - val_acc: 0.9872\n",
      "Epoch 92/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0841 - val_acc: 0.9872\n",
      "Epoch 93/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0826 - val_acc: 0.9872\n",
      "Epoch 94/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0872 - val_acc: 0.9872\n",
      "Epoch 95/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0874 - val_acc: 0.9872\n",
      "Epoch 96/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0810 - val_acc: 0.9872\n",
      "Epoch 97/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0786 - val_acc: 0.9872\n",
      "Epoch 98/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0813 - val_acc: 0.9872\n",
      "Epoch 99/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0800 - val_acc: 0.9872\n",
      "Epoch 100/100\n",
      "702/702 [==============================] - 1s 1ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 1.0802 - val_acc: 0.9872\n",
      "78/78 [==============================] - 0s 335us/step\n",
      "Score for fold 10: loss of 1.0748802484610143; acc of 98.71794825945145%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1296x1296 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97.82051259126419\n"
     ]
    }
   ],
   "source": [
    "acc = create_model(esr[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "verbose = 1\n",
    "\n",
    "\n",
    "# Bounded region of parameter space\n",
    "pbounds = {'fa': (0.6, 0.999), 'pa': (5, 12)}\n",
    "\n",
    "optimizer = BayesianOptimization(\n",
    "    f=create_model,\n",
    "    pbounds=pbounds,\n",
    "    verbose=2,  # verbose = 1 prints only when a maximum is observed, verbose = 0 is silent\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "optimizer.maximize(init_points=4, n_iter=6,)\n",
    "\n",
    "\n",
    "for i, res in enumerate(optimizer.res):\n",
    "    print(\"Iteration {}: \\n\\t{}\".format(i, res))\n",
    "\n",
    "print(optimizer.max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
